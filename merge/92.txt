1 概述1
1.1 智能问答：让机器更好地服务于人 1
1.2 问答系统类型介绍 2
1.2.1 基于事实的问答系统 3
1.2.2 基于常见问题集的问答系统 3
1.2.3 开放域的问答系统 4
1.3 使用本书附带的源码程序 4
1.3.1 安装依赖软件 4
1.3.2 下载源码 5
1.3.3 执行示例程序 5
1.3.4 联系我们 6
1.4 全书结构 6
2 机器学习基础8
2.1 线性代数 8
2.1.1 标量、向量、矩阵和张量 8
2.1.2 矩阵运算 9
2.1.3 特殊类型的矩阵 10
2.1.4 线性相关 11
2.1.5 范数 12
2.2 概率论基础 12
2.2.1 随机变量 13
2.2.2 期望和方差 13
2.2.3 伯努利分布 14
2.2.4 二项分布 14
2.2.5 泊松分布 15
2.2.6 正态分布 15
2.2.7 条件概率、联合概率和全概率 17
2.2.8 先验概率与后验概率 18
2.2.9 边缘概率 18
2.2.10 贝叶斯公式 18
2.2.11 最大似然估计算法 19
2.2.12 线性回归模型 20
2.2.13 逻辑斯蒂回归模型 21
2.3 信息论基础 22
2.3.1 熵 23
2.3.2 联合熵和条件熵 23
2.3.3 相对熵与互信息 24
2.3.4 信道和信道容量 25
2.3.5 最大熵模型 26
2.3.6 信息论与机器学习 29
2.4 统计学习 29
2.4.1 输入空间、特征空间与输出空间 30
2.4.2 向量表示 30
2.4.3 数据集 31
2.4.4 从概率到函数 31
2.4.5 统计学习三要素 32
2.5 隐马尔可夫模型 33
2.5.1 随机过程和马尔可夫链 33
2.5.2 隐马尔可夫模型的定义 36
2.5.3 三个基本假设及适用场景 37
2.5.4 概率计算问题之直接计算 39
2.5.5 概率计算问题之前向算法 40
2.5.6 概率计算问题之后向算法 42
2.5.7 预测问题之维特比算法 45
2.5.8 学习问题之Baum-Welch 算法 48
2.6 条件随机场模型 52
2.6.1 超越HMM 52
2.6.2 项目实践 55
2.7 总结 59
3 自然语言处理基础60
3.1 中文自动分词 60
3.1.1 有向无环图 61
3.1.2 最大匹配算法 63
3.1.3 算法评测 69
3.1.4 由字构词的方法 72
3.2 词性标注 77
3.2.1 词性标注规范 77
3.2.2 隐马尔可夫模型词性标注 79
3.3 命名实体识别 81
3.4 上下文无关文法 82
3.4.1 原理介绍 83
3.4.2 算法浅析 83
3.5 依存关系分析 84
3.5.1 算法浅析 85
3.5.2 项目实践 92
3.5.3 小结 94
3.6 信息检索系统 95
3.6.1 什么是信息检索系统 95
3.6.2 衡量信息检索系统的关键指标 95
3.6.3 理解非结构化数据 97
3.6.4 倒排索引 98
3.6.5 处理查询 100
3.6.6 项目实践 102
3.6.7 Elasticsearch 103
3.6.8 小结 112
3.7 问答语料 113
3.7.1 WikiQA 113
3.7.2 中文版保险行业语料库InsuranceQA 113
3.8 总结 115
4 深度学习初步116
4.1 深度学习简史 116
4.1.1 感知机 116
4.1.2 寒冬和复苏 117
4.1.3 走出实验室 118
4.1.4 寒冬再临 119
4.1.5 走向大规模实际应用 119
4.2 基本架构 120
4.2.1 神经元 121
4.2.2 输入层、隐藏层和输出层 122
4.2.3 标准符号 123
4.3 神经网络是如何学习的 124
4.3.1 梯度下降 124
4.3.2 反向传播理论 127
4.3.3 神经网络全连接层的实现 130
4.3.4 使用简单神经网络实现问答任务 131
4.4 调整神经网络超参数 136
4.4.1 超参数 136
4.4.2 参考建议 137
4.5 卷积神经网络与池化 138
4.5.1 简介 138
4.5.2 卷积层的前向传播 139
4.5.3 池化层的前向传播 141
4.5.4 卷积层的实现 141
4.5.5 池化层的实现 145
4.5.6 使用卷积神经网络实现问答任务 148
4.6 循环神经网络及其变种 149
4.6.1 简介 149
4.6.2 循环神经网络 149
4.6.3 长短期记忆单元和门控循环单元 153
4.6.4 循环神经网络的实现 156
4.6.5 使用循环神经网络实现问答任务 159
4.7 简易神经网络工具包 160
5 词向量实现及应用161
5.1 语言模型 161
5.1.1 评测 162
5.1.2 ARPA 格式介绍 162
5.1.3 项目实践 163
5.2 One-hot 表示法 164
5.3 词袋模型 165
5.4 NNLM 和RNNLM 165
5.5 word2vec 168
5.5.1 C-BOW 的原理 169
5.5.2 Skip-gram 的原理 172
5.5.3 计算效率优化 174
5.5.4 项目实践 179
5.6 GloVe 189
5.6.1 GloVe 的原理 189
5.6.2 GloVe 与word2vec 的区别和联系 191
5.6.3 项目实践 193
5.7 fastText 198
5.7.1 fastText 的原理 198
5.7.2 fastText 与word2vec 的区别和联系 200
5.7.3 项目实践 201
5.8 中文近义词工具包 204
5.8.1 安装 205
5.8.2 接口 205
5.9 总结 205
6 社区问答中的QA 匹配206
6.1 社区问答任务简介 206
6.2 孪生网络模型 207
6.3 QACNN 模型 207
6.3.1 模型构建 207
6.3.2 实验结果 214
6.4 Decomposable Attention 模型 214
6.4.1 模型介绍 214
6.4.2 模型构建 216
6.5 多比较方式的比较C集成模型 216
6.5.1 模型介绍 216
6.5.2 模型构建 218
6.6 BiMPM 模型 219
6.6.1 模型介绍 219
6.6.2 模型构建 221
7 机器阅读理解222
7.1 完型填空型机器阅读理解任务 222
7.1.1 CNN/Daily Mail 数据集 222
7.1.2 Children’s Book Test（CBT）数据集 223
7.1.3 GA Reader 模型 226
7.1.4 SA Reader 模型 227
7.1.5 AoA Reader 模型 228
7.2 答案抽取型机器阅读理解任务 230
7.2.1 SQuAD 数据集 231
7.2.2 MS MARCO 数据集 232
7.2.3 TriviaQA 数据集 234
7.2.4 DuReader 数据集 235
7.2.5 BiDAF 模型 235
7.2.6 R-Net 模型 237
7.2.7 S-Net 模型 240
7.3 答案选择型机器阅读理解任务 243
7.4 展望 245
参考文献246
・ ・ ・ ・ ・ ・ (收起)