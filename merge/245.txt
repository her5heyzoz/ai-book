第 1 部分 自然语言处理基础
第 1 章 绪论 2
1.1 自然语言处理综述 3
1.1.1 自然语言处理的基本概念 3
1.1.2 自然语言处理的发展历程 4
1.1.3 自然语言处理的研究内容 5
1.1.4 自然语言处理的挑战与发展趋势 7
1.2 文本处理技能 9
1.2.1 字符串处理 9
1.2.2 中文分词及案例实现 11
1.3 文本数据处理 13
1.3.1 文本操作基础 13
1.3.2 案例实现――文本数据统计 15
1.3.3 案例实现――词云生成 17
本章总结 19
作业与练习 19
第 2 章 词向量技术 21
2.1 词向量概述 22
2.1.1 词向量基础 22
2.1.2 词向量表示的问题 22
2.2 词向量离散表示 23
2.2.1 独热编码 23
2.2.2 词袋模型 24
2.2.3 词频-逆文本频率 25
2.2.4 案例实现――文本离散表示 25
2.3 词向量分布表示 29
2.3.1 神经网络语言模型 29
2.3.2 Word2vec 模型 31
2.3.3 案例实现――中文词向量训练 33
本章总结 39
作业与练习 39
第 3 章 关键词提取 41
3.1 关键词提取概述 42
3.1.1 关键词提取基础 42
3.1.2 基于 TF-IDF 的关键词提取 42
3.1.3 基于 TextRank 的关键词提取 43
3.1.4 基于 Word2vec 词聚类的关键词提取 43
3.2 关键词提取的实现 44
3.2.1 案例介绍 44
3.2.2 案例实现――关键词提取综合案例 45
本章总结 57
作业与练习 57
第 2 部分 自然语言处理核心技术
第 4 章 朴素贝叶斯中文分类 60 4.1 朴素贝叶斯分类算法概述 60
4.1.1 概率基础 60
4.1.2 朴素贝叶斯分类器 62
4.2 机器学习库 sklearn 64
4.2.1 sklearn 获取数据 64
4.2.2 sklearn 数据预处理 64
4.2.3 sklearn 构建模型 65
4.3 案例实现――朴素贝叶斯中文分类 65
本章总结 71
作业与练习 72
第 5 章 N-gram 语言模型 73
5.1 N-gram 概述 73
5.1.1 N-gram 语言模型简介 73
5.1.2 N-gram 概率计算 74
5.1.3 案例――N-gram 的实现 75
5.2 案例实现――基于 N-gram 的新闻文本预测 77
本章总结 84
作业与练习 84
第 6 章 PyTorch 深度学习框架 85
6.1 PyTorch 基础 85
6.1.1 PyTorch 的介绍与安装 85
6.1.2 PyTorch 入门使用 87
6.1.3 梯度下降与反向传播 92
6.1.4 案例――使用 PyTorch 实现线性回归 95
6.2 PyTorch 数据加载 99
6.2.1 使用数据加载器的目的 99
6.2.2 DataSet 的使用方法 99
6.2.3 DataLoader 的使用方法 100
6.3 PyTorch 自带数据集加载 101
本章总结 102
作业与练习 102
第 7 章 FastText 模型文本分类 104
7.1 FastText 模型简介 104
7.1.1 FastText 模型原理 104
7.1.2 FastText 模型结构 105
7.1.3 FastText 模型优化 105
7.2 案例实现――FastText 模型文本分类 106
本章总结 118
作业与练习 118
第 8 章 基于深度学习的文本分类 119
8.1 基于 TextCNN 的文本分类 119
8.1.1 卷积神经网络 119
8.1.2 TextCNN 的原理 121
8.2 基于 TextRNN 的文本分类 122
8.2.1 LSTM 原理 122
8.2.2 LSTM 网络结构 123
8.3 基于 TextRCNN 的文本分类 124
8.3.1 TextRCNN 原理 124
8.3.2 TextRCNN 网络结构 125
8.4 案例实现――基于深度学习的文本分类 126
本章总结 146
作业与练习 146
第 3 部分 序列标注
第 9 章 HMM 的词性标注 148
9.1 词性标注简介 149
9.1.1 词性标注的基本概念 149
9.1.2 中文词性的分类及作用 149
9.1.3 词性标注体系 150
9.2 HMM 词性标注的原理和基本问题 151
9.2.1 HMM 词性标注的原理 151
9.2.2 HMM 的基本问题 151
9.3 案例实现――HMM 的中文词性标注 152
本章总结 158
作业与练习 158
第 10 章 HMM 的命名实体识别 159
10.1 命名实体识别 160
10.1.1 命名实体识别的概念 160
10.1.2 NER 的标注方法 160
10.2 NER 的 HMM 162
10.3 案例实现――HMM 的中文命名实体识别 162
本章总结 175
作业与练习 175
第 11 章 BiLSTM-CRF 的命名实体识别 176
11.1 CRF 简介 177
11.1.1 CRF 的基本概念 177
11.1.2 BiLSTM 的命名实体识别 177
11.1.3 CRF 的命名实体识别 178
11.2 BiLSTM-CRF 的原理 179
11.3 案例实现――BiLSTM-CRF 的中文命名实体识别 180
本章总结 189
作业与练习 189
第 4 部分 预训练模型
第 12 章 ALBERT 的命名实体识别 192
12.1 预训练模型简介 193
12.1.1 预训练模型的基本概念 193
12.1.2 经典的预训练模型 193
12.2 预训练模型 Hugging Face 195
12.2.1 Hugging Face 简介 195
12.2.2 案例实现――使用 Hugging Face完成情感分析 196
12.3 案例实现――ALBERT 的中文命名实体识别 198
本章总结 207
作业与练习 207
第 13 章 Transformer 的文本分类 209
13.1 Transformer 概述 210
13.1.1 Encoder-Decoder 模型 210
13.1.2 Transformer 简介 210
13.1.3 Transformer 总体结构 211
13.2 Self-Attention 机制 213
13.2.1 Self-Attention 机制的原理 213
13.2.2 Self-Attention 的计算过程 214
13.2.3 位置编码和 Layer Normalization 215
13.3 案例实现――Transformer 的文本分类 217
本章总结 234
作业与练习 234
第 14 章 BERT 的文本相似度计算 236
14.1 文本相似度简介 237
14.1.1 文本相似度的应用场景 237
14.1.2 文本相似度计算的方法 237
14.2 BERT 的文本相似度简介 238
14.3 案例实现――BERT 的文本相似度计算 239
本章总结 251
作业与练习 251
第 15 章 ERNIE 的情感分析 253
15.1 情感分析简介 254
15.1.1 情感分析的基本概念 254
15.1.2 情感分析的方法 254
15.2 ERNIE 简介 255
15.3 案例实现――ERNIE 的中文情感分析 257
本章总结 271
作业与练习 272
・ ・ ・ ・ ・ ・ (收起)