目　　录
第　1章 深度学习简介　1
1．1　深度学习与人工智能　1
1．2　深度学习的历史渊源　2
1．2．1　从感知机到人工神经网络　3
1．2．2　深度学习时代　4
1．2．3　巨头之间的角逐　5
1．3　深度学习的影响因素　6
1．3．1　大数据　6
1．3．2　深度网络架构　7
1．3．3　GPU　11
1．4　深度学习为什么如此成功　11
1．4．1　特征学习（representation learning）　11
1．4．2　迁移学习（transfer learning）　12
1．5　小结　13
参考文献　14
第　2章 PyTorch简介　15
2．1　PyTorch安装　15
2．2　初识PyTorch　15
2．2．1　与Python的完美融合　16
2．2．2　张量计算　16
2．2．3　动态计算图　20
2．3　PyTorch实例：预测房价　27
2．3．1　准备数据　27
2．3．2　模型设计　28
2．3．3　训练　29
2．3．4　预测　31
2．3．5　术语汇总　32
2．4　小结　33
第3章　单车预测器：你的第 一个
神经网络　35
3．1　共享单车的烦恼　35
3．2　单车预测器1．0　37
3．2．1　神经网络简介　37
3．2．2　人工神经元　38
3．2．3　两个隐含层神经元　40
3．2．4　训练与运行　42
3．2．5　失败的神经预测器　43
3．2．6　过拟合　48
3．3　单车预测器2．0　49
3．3．1　数据的预处理过程　49
3．3．2　构建神经网络　52
3．3．3　测试神经网络　55
3．4　剖析神经网络Neu　57
3．5　小结　61
3．6　Q&A　61
第4章　机器也懂感情――中文情绪
分类器　63
4．1　神经网络分类器　64
4．1．1　如何用神经网络做分类　64
4．1．2　分类问题的损失函数　66
4．2　词袋模型分类器　67
4．2．1　词袋模型简介　68
4．2．2　搭建简单文本分类器　69
4．3　程序实现　70
4．3．1　数据获取　70
4．3．2　数据处理　74
4．3．3　文本数据向量化　75
4．3．4　划分数据集　76
4．3．5　建立神经网络　78
4．4　运行结果　80
4．5　剖析神经网络　81
4．6　小结　85
4．7　Q&A　85
第5章　手写数字识别器――认识卷积
神经网络　87
5．1　什么是卷积神经网络　88
5．1．1　手写数字识别任务的CNN
网络及运算过程　88
5．1．2　卷积运算操作　90
5．1．3　池化操作　96
5．1．4　立体卷积核　97
5．1．5　超参数与参数　98
5．1．6　其他说明　99
5．2　手写数字识别器　100
5．2．1　数据准备　100
5．2．2　构建网络　103
5．2．3　运行模型　105
5．2．4　测试模型　106
5．3　剖析卷积神经网络　107
5．3．1　第 一层卷积核与特征图　107
5．3．2　第二层卷积核与特征图　109
5．3．3　卷积神经网络的健壮性试验　110
5．4　小结　112
5．5　Q&A　112
5．6　扩展阅读　112
第6章　手写数字加法机――迁移学习　113
6．1　什么是迁移学习　114
6．1．1　迁移学习的由来　114
6．1．2　迁移学习的分类　115
6．1．3　迁移学习的意义　115
6．1．4　如何用神经网络实现迁移
学习　116
6．2　应用案例：迁移学习如何抗击贫困　118
6．2．1　背景介绍　118
6．2．2　方法探寻　119
6．2．3　迁移学习方法　120
6．3　蚂蚁还是蜜蜂：迁移大型卷积神经
网络　121
6．3．1　任务描述与初步尝试　121
6．3．2　ResNet与模型迁移　122
6．3．3　代码实现　123
6．3．4　结果分析　127
6．3．5　更多的模型与数据　128
6．4　手写数字加法机　128
6．4．1　网络架构　128
6．4．2　代码实现　129
6．4．3　训练与测试　136
6．4．4　结果　138
6．4．5　大规模实验　138
6．5　小结　143
6．6　实践项目：迁移与效率　143
第7章　你自己的Prisma――图像
风格迁移　145
7．1　什么是风格迁移　145
7．1．1　什么是风格　145
7．1．2　风格迁移的涵义　146
7．2　风格迁移技术发展简史　147
7．2．1　神经网络之前的风格迁移　147
7．2．2　特定风格的实现　148
7．3　神经网络风格迁移　149
7．3．1　神经网络风格迁移的优势　150
7．3．2　神经网络风格迁移的基本
思想　150
7．3．3　卷积神经网络的选取　151
7．3．4　内容损失　152
7．3．5　风格损失　152
7．3．6　风格损失原理分析　153
7．3．7　损失函数与优化　156
7．4　神经网络风格迁移实战　157
7．4．1　准备工作　157
7．4．2　建立风格迁移网络　159
7．4．3　风格迁移训练　162
7．5　小结　165
7．6　扩展阅读　165
第8章　人工智能造假术――图像生成
与对抗学习　166
8．1　反卷积与图像生成　169
8．1．1　CNN回顾　169
8．1．2　反卷积操作　171
8．1．3　反池化过程　173
8．1．4　反卷积与分数步伐　174
8．1．5　输出图像尺寸公式　175
8．1．6　批正则化技术　176
8．2　图像生成实验1――最小均方误差
模型　177
8．2．1　模型思路　177
8．2．2　代码实现　178
8．2．3　运行结果　182
8．3　图像生成实验2――生成器-识别器
模型　184
8．3．1　生成器-识别器模型的实现　184
8．3．2　对抗样本　187
8．4　图像生成实验3――生成对抗网络
GAN　190
8．4．1　GAN的总体架构　191
8．4．2　程序实现　192
8．4．3　结果展示　195
8．5　小结　197
8．6　Q&A　197
8．7　扩展阅读　198
第9章　词汇的星空――神经语言模型
与Word2Vec　199
9．1　词向量技术介绍　199
9．1．1　初识词向量　199
9．1．2　传统编码方式　200
9．2　NPLM：神经概率语言模型　201
9．2．1　NPLM的基本思想　202
9．2．2　NPLM的运作过程详解　202
9．2．3　读取NPLM中的词向量　205
9．2．4　NPLM的编码实现　206
9．2．5　运行结果　209
9．2．6　NPLM的总结与局限　211
9．3　Word2Vec　211
9．3．1　CBOW模型和Skip-gram模型的结构　211
9．3．2　层级软最大　213
9．3．3　负采样　213
9．3．4　总结及分析　214
9．4　Word2Vec的应用　214
9．4．1　在自己的语料库上训练Word2Vec词向量　214
9．4．2　调用现成的词向量　216
9．4．3　女人-男人＝皇后-国王　218
9．4．4　使用向量的空间位置进行词对词翻译　220
9．4．5　Word2Vec小结　221
9．5　小结　221
9．5　Q&A　222
第　10章 LSTM作曲机――序列生成
模型　224
10．1　序列生成问题　224
10．2　RNN与LSTM　225
10．2．1　RNN　226
10．2．2　LSTM　231
10．3　简单01序列的学习问题　235
10．3．1　RNN的序列学习　236
10．3．2　LSTM的序列学习　245
10．4　LSTM作曲机　248
10．4．1　MIDI文件　248
10．4．2　数据准备　249
10．4．3　模型结构　249
10．4．4　代码实现　250
10．5　小结　259
10．6　Q&A　259
10．7　扩展阅读　259
・ ・ ・ ・ ・ ・ (收起)