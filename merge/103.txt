第1 部分深度学习基础篇1
1 概述2
1.1 人工智能 3
1.1.1 人工智能的分类 3
1.1.2 人工智能发展史 3
1.2 机器学习 7
1.2.1 机器学习的由来 7
1.2.2 机器学习发展史 9
1.2.3 机器学习方法分类 10
1.2.4 机器学习中的基本概念 11
1.3 神经网络 12
1.3.1 神经网络发展史 13
参考文献 16
2 神经网络17
2.1 在神经科学中对生物神经元的研究 17
2.1.1 神经元激活机制 17
2.1.2 神经元的特点 18
2.2 神经元模型 19
2.2.1 线性神经元 19
2.2.2 线性阈值神经元 19
2.2.3 Sigmoid 神经元 21
2.2.4 Tanh 神经元 22
2.2.5 ReLU 22
2.2.6 Maxout 24
2.2.7 Softmax 24
2.2.8 小结 25
2.3 感知机 27
2.3.1 感知机的提出 27
2.3.2 感知机的困境 28
2.4 DNN 29
2.4.1 输入层、输出层及隐层 30
2.4.2 目标函数的选取 30
2.4.3 前向传播 32
2.4.4 后向传播 33
2.4.5 参数更新 35
2.4.6 神经网络的训练步骤 36
参考文献 36
3 初始化模型38
3.1 受限玻尔兹曼机 38
3.1.1 能量模型 39
3.1.2 带隐藏单元的能量模型 40
3.1.3 受限玻尔兹曼机基本原理 41
3.1.4 二值RBM 43
3.1.5 对比散度 45
3.2 自动编码器 47
3.2.1 稀疏自动编码器 48
3.2.2 降噪自动编码器 48
3.2.3 栈式自动编码器 49
3.3 深度信念网络 50
参考文献 52
4 卷积神经网络53
4.1 卷积算子 53
4.2 卷积的特征 56
4.3 卷积网络典型结构 59
4.3.1 基本网络结构 59
4.3.2 构成卷积神经网络的层 59
4.3.3 网络结构模式 60
4.4 卷积网络的层 61
4.4.1 卷积层 61
4.4.2 池化层 66
参考文献 67
5 循环神经网络68
5.1 循环神经网络简介 68
5.2 RNN、LSTM 和GRU 69
5.3 双向RNN 76
5.4 RNN 语言模型的简单实现 77
参考文献 80
6 深度学习优化算法81
6.1 SGD 81
6.2 Momentum 82
6.3 NAG 83
6.4 Adagrad 85
6.5 RMSProp 86
6.6 Adadelta 87
6.7 Adam 88
6.8 AdaMax 90
6.9 Nadam 90
6.10 关于优化算法的使用 92
参考文献 92
7 深度学习训练技巧94
7.1 数据预处理 94
7.2 权重初始化 95
7.3 正则化 96
7.3.1 提前终止 96
7.3.2 数据增强 96
7.3.3 L2/L1 参数正则化 98
7.3.4 集成 100
7.3.5 Dropout 101
参考文献 102
8 深度学习框架103
8.1 Theano 103
8.1.1 Theano 103
8.1.2 安装 104
8.1.3 计算图 104
8.2 Torch 105
8.2.1 概述 105
8.2.2 安装 106
8.2.3 核心结构 107
8.2.4 小试牛刀 110
8.3 PyTorch 113
8.3.1 概述 113
8.3.2 安装 113
8.3.3 核心结构 114
8.3.4 小试牛刀 114
8.4 Caffe 117
8.4.1 概述 117
8.4.2 安装 118
8.4.3 核心组件 119
8.4.4 小试牛刀 125
8.5 TensorFlow 125
8.5.1 概述 125
8.5.2 安装 126
8.5.3 核心结构 126
8.5.4 小试牛刀 127
8.6 MXNet 131
8.6.1 概述 131
8.6.2 安装 131
8.6.3 核心结构 132
8.6.4 小试牛刀 133
8.7 Keras 135
8.7.1 概述 135
8.7.2 安装 136
8.7.3 模块介绍 136
8.7.4 小试牛刀 136
参考文献 139
第2 部分计算机视觉篇140
9 计算机视觉背景141
9.1 传统计算机视觉 141
9.2 基于深度学习的计算机视觉 145
9.3 参考文献 146
10 图像分类模型147
10.1 LeNet-5 147
10.2 AlexNet 149
10.3 VGGNet 154
10.3.1 网络结构 155
10.3.2 配置 157
10.3.3 讨论 157
10.3.4 几组实验 158
10.4 GoogLeNet 159
10.4.1 NIN 161
10.4.2 GoogLeNet 的动机 161
10.4.3 网络结构细节 162
10.4.4 训练方法 164
10.4.5 后续改进版本 165
10.5 ResNet 165
10.5.1 基本思想 165
10.5.2 网络结构 167
10.6 DenseNet 169
10.7 DPN 170
参考文献 170
11 目标检测173
11.1 相关研究 175
11.1.1 选择性搜索 175
11.1.2 OverFeat 177
11.2 基于区域提名的方法 179
11.2.1 R-CNN 179
11.2.2 SPP-net 181
11.2.3 Fast R-CNN 182
11.2.4 Faster R-CNN 184
11.2.5 R-FCN 185
11.3 端到端的方法 186
11.3.1 YOLO 186
11.3.2 SSD 187
11.4 小结 188
参考文献 190
12 语义分割192
12.1 全卷积网络 193
12.1.1 FCN 193
12.1.2 DeconvNet 195
12.1.3 SegNet 197
12.1.4 DilatedConvNet 198
12.2 CRF/MRF 的使用 199
12.2.1 DeepLab 199
12.2.2 CRFasRNN 201
12.2.3 DPN 203
12.3 实例分割 205
12.3.1 Mask R-CNN 205
参考文献 206
13 图像检索的深度哈希编码208
13.1 传统哈希编码方法 208
13.2 CNNH 209
13.3 DSH 210
13.4 小结 212
参考文献 212
第3 部分语音识别篇214
14 传统语音识别基础215
14.1 语音识别简介 215
14.2 HMM 简介 216
14.2.1 HMM 是特殊的混合模型 218
14.2.2 转移概率矩阵 219
14.2.3 发射概率 220
14.2.4 Baum-Welch 算法 220
14.2.5 后验概率 224
14.2.6 前向-后向算法 224
14.3 HMM 梯度求解 227
14.3.1 梯度算法1 228
14.3.2 梯度算法2 230
14.3.3 梯度求解的重要性 234
14.4 孤立词识别 234
14.4.1 特征提取 234
14.4.2 孤立词建模 235
14.4.3 GMM-HMM 237
14.5 连续语音识别 240
14.6 Viterbi 解码 243
14.7 三音素状态聚类 245
14.8 判别式训练 248
参考文献 254
15 基于WFST 的语音解码256
15.1 有限状态机 257
15.2 WFST 及半环定义 257
15.2.1 WFST 257
15.2.2 半环（Semiring） 258
15.3 自动机操作 260
15.3.1 自动机基本操作 261
15.3.2 转换器基本操作 262
15.3.3 优化操作 265
15.4 基于WFST 的语音识别系统 277
15.4.1 声学模型WFST 279
15.4.2 三音素WFST 281
15.4.3 发音字典WFST 281
15.4.4 语言模型WFST 282
15.4.5 WFST 组合和优化 284
15.4.6 组合和优化实验 285
15.4.7 WFST 解码 286
参考文献 287
16 深度语音识别288
16.1 CD-DNN-HMM 288
16.2 TDNN 292
16.3 CTC 295
16.4 EESEN 299
16.5 Deep Speech 301
16.6 Chain 310
参考文献 313
17 CTC 解码315
17.1 序列标注 315
17.2 序列标注任务的解决办法 316
17.2.1 序列分类 316
17.2.2 分割分类 317
17.2.3 时序分类 318
17.3 隐马模型 318
17.4 CTC 基本定义 319
17.5 CTC 前向算法 321
17.6 CTC 后向算法 324
17.7 CTC 目标函数 325
17.8 CTC 解码基本原理 327
17.8.1 最大概率路径解码 327
17.8.2 前缀搜索解码 328
17.8.3 约束解码 329
参考文献 333
第4 部分自然语言处理篇334
18 自然语言处理简介335
18.1 NLP 的难点 335
18.2 NLP 的研究范围 336
19 词性标注338
19.1 传统词性标注模型 338
19.2 基于神经网络的词性标注模型 340
19.3 基于Bi-LSTM 的神经网络词性标注模型 342
参考文献 344
20 依存句法分析345
20.1 背景 346
20.2 SyntaxNet 技术要点 348
20.2.1 Transition-based 系统 349
20.2.2 “模板化” 技术 353
20.2.3 Beam Search 355
参考文献 357
21 word2vec 358
21.1 背景 359
21.1.1 词向量 359
21.1.2 统计语言模型 359
21.1.3 神经网络语言模型 362
21.1.4 Log-linear 模型 364
21.1.5 Log-bilinear 模型 365
21.1.6 层次化Log-bilinear 模型 365
21.2 CBOW 模型 366
21.3 Skip-gram 模型 369
21.4 Hierarchical Softmax 与Negative Sampling 371
21.5 fastText 372
21.6 GloVe 373
21.7 小结 374
参考文献 374
22 神经网络机器翻译376
22.1 机器翻译简介 376
22.2 神经网络机器翻译基本模型 377
22.3 基于Attention 的神经网络机器翻译 379
22.4 谷歌机器翻译系统GNMT 381
22.5 基于卷积的机器翻译 382
22.6 小结 383
参考文献 384
第5 部分深度学习研究篇385
23 Batch Normalization 386
23.1 前向与后向传播 387
23.1.1 前向传播 387
23.1.2 后向传播 390
23.2 有效性分析 393
23.2.1 内部协移 393
23.2.2 梯度流 393
23.3 使用与优化方法 395
23.4 小结 396
参考文献 396
24 Attention 397
24.1 从简单RNN 到RNN + Attention 398
24.2 Soft Attention 与Hard Attention 398
24.3 Attention 的应用 399
24.4 小结 401
参考文献 402
25 多任务学习403
25.1 背景 403
25.2 什么是多任务学习 404
25.3 多任务分类与其他分类概念的关系 406
25.3.1 二分类 406
25.3.2 多分类 407
25.3.3 多标签分类 407
25.3.4 相关关系 408
25.4 多任务学习如何发挥作用 409
25.4.1 提高泛化能力的潜在原因 410
25.4.2 多任务学习机制 410
25.4.3 后向传播多任务学习如何发现任务是相关的 412
25.5 多任务学习被广泛应用 413
25.5.1 使用未来预测现在 413
25.5.2 多种表示和度量 413
25.5.3 时间序列预测 413
25.5.4 使用不可操作特征 414
25.5.5 使用额外任务来聚焦 414
25.5.6 有序迁移 414
25.5.7 多个任务自然地出现 414
25.5.8 将输入变成输出 414
25.6 多任务深度学习应用 416
25.6.1 脸部特征点检测 416
25.6.2 DeepID2 418
25.6.3 Fast R-CNN 419
25.6.4 旋转人脸网络 420
25.6.5 实例感知语义分割的MNC 422
25.7 小结 423
参考文献 425
26 模型压缩426
26.1 模型压缩的必要性 426
26.2 较浅的网络 428
26.3 剪枝 428
26.4 参数共享 434
26.5 紧凑网络 437
26.6 二值网络 438
26.7 小结 442
参考文献 442
27 增强学习445
27.1 什么是增强学习 445
27.2 增强学习的数学表达形式 448
27.2.1 MDP 449
27.2.2 策略函数 450
27.2.3 奖励与回报 450
27.2.4 价值函数 452
27.2.5 贝尔曼方程 453
27.2.6 最优策略性质 453
27.3 用动态规划法求解增强学习问题 454
27.3.1 Agent 的目标 454
27.3.2 策略评估 455
27.3.3 策略改进 456
27.3.4 策略迭代 457
27.3.5 策略迭代的例子 458
27.3.6 价值迭代 459
27.3.7 价值迭代的例子 461
27.3.8 策略函数和价值函数的关系 462
27.4 无模型算法 462
27.4.1 蒙特卡罗法 463
27.4.2 时序差分法 465
27.4.3 Q-Learning 466
27.5 Q-Learning 的例子 467
27.6 AlphaGo 原理剖析 469
27.6.1 围棋与机器博弈 469
27.6.2 Alpha-Beta 树 472
27.6.3 MCTS 473
27.6.4 UCT 476
27.6.5 AlphaGo 的训练策略 478
27.6.6 AlphaGo 的招式搜索算法 482
27.6.7 围棋的对称性 484
27.7 AlphaGo Zero 484
参考文献 484
28 GAN 486
28.1 生成模型 486
28.2 生成对抗模型的概念 488
28.3 GAN 实战 492
28.4 InfoGAN――探寻隐变量的内涵 493
28.5 Image-Image Translation 496
28.6 WGAN（Wasserstein GAN） 499
28.6.1 GAN 目标函数的弱点 500
28.6.2 Wasserstein 度量的优势 501
28.6.3 WGAN 的目标函数 504
参考文献 505
A 本书涉及的开源资源列表 506
・ ・ ・ ・ ・ ・ (收起)