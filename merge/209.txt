扉页
版权声明
内容提要
译者简介
译者序
序
前言
致谢
关于本书
关于作者
关于封面插画
资源与支持
目录
第一部分 处理文本的机器
第1章 NLP概述
1.1 自然语言与编程语言
1.2 神奇的魔法
1.2.1 会交谈的机器
1.2.2 NLP中的数学
1.3 实际应用
1.4 计算机“眼”中的语言
1.4.1 锁的语言（正则表达式）
1.4.2 正则表达式
1.4.3 一个简单的聊天机器人
1.4.4 另一种方法
1.5 超空间简述
1.6 词序和语法
1.7 聊天机器人的自然语言流水线
1.8 深度处理
1.9 自然语言智商
1.10 小结
第2章 构建自己的词汇表――分词
2.1 挑战（词干还原预览）
2.2 利用分词器构建词汇表
2.2.1 点积
2.2.2 度量词袋之间的重合度
2.2.3 标点符号的处理
2.2.4 将词汇表扩展到 n-gram
2.2.5 词汇表归一化
2.3 情感
2.3.1 VADER：一个基于规则的情感分析器
2.3.2 朴素贝叶斯
2.4 小结
第3章 词中的数学
3.1 词袋
3.2 向量化
向量空间
3.3 齐普夫定律
3.4 主题建模
3.4.1 回到齐普夫定律
3.4.2 相关度排序
3.4.3 工具
3.4.4 其他工具
3.4.5 Okapi BM25
3.4.6 未来展望
3.5 小结
第4章 词频背后的语义
4.1 从词频到主题得分
4.1.1 TF-IDF向量及词形归并
4.1.2 主题向量
4.1.3 思想实验
4.1.4 一个主题评分算法
4.1.5 一个 LDA分类器
4.2 潜在语义分析
思想实验的实际实现
4.3 奇异值分解
4.3.1 左奇异向量 U
4.3.2 奇异值向量 S
4.3.3 右奇异向量 V
4.3.4 SVD矩阵的方向
4.3.5 主题约简
4.4 主成分分析
4.4.1 三维向量上的 PCA
4.4.2 回归 NLP
4.4.3 基于 PCA的短消息语义分析
4.4.4 基于截断的 SVD的短消息语义分析
4.4.5 基于 LSA的垃圾短消息分类的效果
4.5 潜在狄利克雷分布（LDiA）
4.5.1 LDiA思想
4.5.2 基于 LDiA主题模型的短消息语义分析
4.5.3 LDiA+LDA=垃圾消息过滤器
4.5.4 更公平的对比：32 个 LDiA主题
4.6 距离和相似度
4.7 反馈及改进
线性判别分析
4.8 主题向量的威力
4.8.1 语义搜索
4.8.2 改进
4.9 小结
第二部分 深度学习（神经网络）
第5章 神经网络初步（感知机与反向传播）
5.1 神经网络的组成
5.2 小结
第6章 词向量推理（Word2vec）
6.1 语义查询与类比
类比问题
6.2 词向量
6.2.1 面向向量的推理
6.2.2 如何计算 Word2vec 表示
6.2.3 如何使用 gensim.word2vec 模块
6.2.4 生成定制化词向量表示
6.2.5 Word2vec 和 GloVe
6.2.6 fastText
6.2.7 Word2vec 和 LSA
6.2.8 词关系可视化
6.2.9 非自然词
6.2.10 利用 Doc2vec 计算文档相似度
6.3 小结
第7章 卷积神经网络（CNN）
7.1 语义理解
7.2 工具包
7.3 卷积神经网络
7.3.1 构建块
7.3.2 步长
7.3.3 卷积核的组成
7.3.4 填充
7.3.5 学习
7.4 狭窄的窗口
7.4.1 Keras 实现：准备数据
7.4.2 卷积神经网络架构
7.4.3 池化
7.4.4 dropout
7.4.5 输出层
7.4.6 开始学习（训练）
7.4.7 在流水线中使用模型
7.4.8 前景展望
7.5 小结
第8章 循环神经网络（RNN）
8.1 循环网络的记忆功能
8.2 整合各个部分
8.3 自我学习
8.4 超参数
8.5 预测
8.5.1 有状态性
8.5.2 双向 RNN
8.5.3 编码向量
8.6 小结
第9章 改进记忆力：长短期记忆网络（LSTM）
9.1 长短期记忆（LSTM）
9.1.1 随时间反向传播
9.1.2 模型的使用
9.1.3 脏数据
9.1.4 “未知”词条的处理
9.1.5 字符级建模
9.1.6 生成聊天文字
9.1.7 进一步生成文本
9.1.8 文本生成的问题：内容不受控
9.1.9 其他记忆机制
9.1.10 更深的网络
9.2 小结
第10章 序列到序列建模和注意力机制
10.1 编码-解码架构
10.1.1 解码思想
10.1.2 似曾相识？
10.1.3 序列到序列对话
10.1.4 回顾 LSTM
10.2 组装一个序列到序列的流水线
10.2.1 为序列到序列训练准备数据集
10.2.2 Keras 中的序列到序列模型
10.2.3 序列编码器
10.2.4 思想解码器
10.2.5 组装一个序列到序列网络
10.3 训练序列到序列网络
生成输出序列
10.4 使用序列到序列网络构建一个聊天机器人
10.4.1 为训练准备语料库
10.4.2 建立字符字典
10.4.3 生成独热编码训练集
10.4.4 训练序列到序列聊天机器人
10.4.5 组装序列生成模型
10.4.6 预测输出序列
10.4.7 生成回复
10.4.8 与聊天机器人交谈
10.5 增强
10.5.1 使用装桶法降低训练复杂度
10.5.2 注意力机制
10.6 实际应用
10.7 小结
第三部分 进入现实世界（现实中的 NLP 挑战）
第11章 信息提取（命名实体识别与问答系统）
11.1 命名实体与关系
11.1.1 知识库
11.1.2 信息提取
11.2 正则模式
11.2.1 正则表达式
11.2.2 把信息提取当作机器学习里的特征提取任务
11.3 值得提取的信息
11.3.1 提取 GPS位置
11.3.2 提取日期
11.4 提取人物关系（事物关系）
11.4.1 词性标注
11.4.2 实体名称标准化
11.4.3 实体关系标准化和提取
11.4.4 单词模式
11.4.5 文本分割
11.4.6 为什么 split('.!?')函数不管用
11.4.7 使用正则表达式进行断句
11.5 现实世界的信息提取
11.6 小结
第12章 开始聊天（对话引擎）
12.1 语言技能
12.1.1 现代方法
12.1.2 混合方法
12.2 模式匹配方法
12.2.1 基于 AIML 的模式匹配聊天机器人
12.2.2 模式匹配的网络视图
12.3 知识方法
12.4 检索（搜索）方法
12.4.1 上下文挑战
12.4.2 基于示例检索的聊天机器人
12.4.3 基于搜索的聊天机器人
12.5 生成式方法
12.5.1 聊聊 NLPIA
12.5.2 每种方法的利弊
12.6 四轮驱动
Will的成功
12.7 设计过程
12.8 技巧
12.8.1 用带有可预测答案的问题提问
12.8.2 要有趣
12.8.3 当其他所有方法都失败时，搜索
12.8.4 变得受欢迎
12.8.5 成为连接器
12.8.6 变得有情感
12.9 现实世界
12.10 小结
第13章 可扩展性（优化、并行化和批处理）
13.1 太多（数据）未必是好事
13.2 优化NLP算法
13.2.1 索引
13.2.2 高级索引
13.2.3 基于 Annoy 的高级索引
13.2.4 究竟为什么要使用近似索引
13.2.5 索引变通方法：离散化
13.3 常数级内存算法
13.3.1 gensim
13.3.2 图计算
13.4 并行化NLP计算
13.4.1 在 GPU上训练 NLP模型
13.4.2 租与买
13.4.3 GPU租赁选择
13.4.4 张量处理单元 TPU
13.5 减少模型训练期间的内存占用
13.6 使用TensorBoard 了解模型
如何可视化词嵌入
13.7 小结
附录A 本书配套的NLP工具
附录B 有趣的Python和正则表达式
附录C 向量和矩阵（线性代数基础）
附录D 机器学习常见工具与技术
附录E 设置亚马逊云服务（ AWS）上的GPU
附录F 局部敏感哈希
资源
词汇表
・ ・ ・ ・ ・ ・ (收起)