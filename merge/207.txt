推荐序III
推荐语IV
前言V
数学符号IX
第1 章绪论1
1.1 自然语言处理的概念 2
1.2 自然语言处理的难点2
1.2.1 抽象性 2
1.2.2 组合性 2
1.2.3 歧义性 3
1.2.4 进化性3
1.2.5 非规范性3
1.2.6 主观性3
1.2.7 知识性3
1.2.8 难移植性4
1.3 自然语言处理任务体系.4
1.3.1 任务层级4
1.3.2 任务类别5
1.3.3 研究对象与层次6
1.4 自然语言处理技术发展历史7
第2 章自然语言处理基础11
2.1 文本的表示.12
2.1.1 词的独热表示13
2.1.2 词的分布式表示13
2.1.3 词嵌入表示19
2.1.4 文本的词袋表示19
2.2 自然语言处理任务20
2.2.1 语言模型20
2.2.2 自然语言处理基础任务23
2.2.3 自然语言处理应用任务31
2.3 基本问题35
2.3.1 文本分类问题35
2.3.2 结构预测问题36
2.3.3 序列到序列问题38
2.4 评价指标40
2.5 小结43
第3 章基础工具集与常用数据集45
3.1 NLTK 工具集46
3.1.1 常用语料库和词典资源46
3.1.2 常用自然语言处理工具集.49
3.2 LTP 工具集51
3.2.1 中文分词51
3.2.2 其他中文自然语言处理功能.52
3.3 PyTorch 基础52
3.3.1 张量的基本概念53
3.3.2 张量的基本运算54
3.3.3 自动微分57
3.3.4 调整张量形状58
3.3.5 广播机制59
3.3.6 索引与切片60
3.3.7 降维与升维60
3.4 大规模预训练数据61
3.4.1 维基百科数据62
3.4.2 原始数据的获取62
3.4.3 语料处理方法62
3.4.4 Common Crawl 数据66
3.5 更多数据集.66
3.6 小结68
第4 章自然语言处理中的神经网络基础69
4.1 多层感知器模型70
4.1.1 感知器70
4.1.2 线性回归71
4.1.3 Logistic 回归71
4.1.4 Softmax 回归72
4.1.5 多层感知器74
4.1.6 模型实现76
4.2 卷积神经网络78
4.2.1 模型结构78
4.2.2 模型实现80
4.3 循环神经网络83
4.3.1 模型结构83
4.3.2 长短时记忆网络85
4.3.3 模型实现87
4.3.4 基于循环神经网络的序列到序列模型88
4.4 注意力模型.89
4.4.1 注意力机制89
4.4.2 自注意力模型90
4.4.3 Transformer 91
4.4.4 基于Transformer 的序列到序列模型93
4.4.5 Transformer 模型的优缺点94
4.4.6 模型实现94
4.5 神经网络模型的训练96
4.5.1 损失函数96
4.5.2 梯度下降98
4.6 情感分类实战101
4.6.1 词表映射101
4.6.2 词向量层102
4.6.3 融入词向量层的多层感知器103
4.6.4 数据处理106
4.6.5 多层感知器模型的训练与测试108
4.6.6 基于卷积神经网络的情感分类109
4.6.7 基于循环神经网络的情感分类110
4.6.8 基于Transformer 的情感分类111
4.7 词性标注实战113
4.7.1 基于前馈神经网络的词性标注114
4.7.2 基于循环神经网络的词性标注114
4.7.3 基于Transformer 的词性标注116
4.8 小结116
第5 章静态词向量预训练模型119
5.1 神经网络语言模型120
5.1.1 预训练任务120
5.1.2 模型实现124
5.2 Word2vec 词向量130
5.2.1 概述130
5.2.2 负采样133
5.2.3 模型实现134
5.3 GloVe 词向量140
5.3.1 概述140
5.3.2 预训练任务140
5.3.3 参数估计140
5.3.4 模型实现141
5.4 评价与应用.143
5.4.1 词义相关性144
5.4.2 类比性146
5.4.3 应用147
5.5 小结148
第6 章动态词向量预训练模型151
6.1 词向量――从静态到动态152
6.2 基于语言模型的动态词向量预训练153
6.2.1 双向语言模型153
6.2.2 ELMo 词向量155
6.2.3 模型实现156
6.2.4 应用与评价169
6.3 小结171
第7 章预训练语言模型173
7.1 概述174
7.1.1 大数据174
7.1.2 大模型175
7.1.3 大算力175
7.2 GPT 177
7.2.1 无监督预训练178
7.2.2 有监督下游任务精调179
7.2.3 适配不同的下游任务180
7.3 BERT 182
7.3.1 整体结构182
7.3.2 输入表示183
7.3.3 基本预训练任务184
7.3.4 更多预训练任务190
7.3.5 模型对比194
7.4 预训练语言模型的应用194
7.4.1 概述194
7.4.2 单句文本分类195
7.4.3 句对文本分类198
7.4.4 阅读理解201
7.4.5 序列标注206
7.5 深入理解BERT .211
7.5.1 概述211
7.5.2 自注意力可视化分析212
7.5.3 探针实验213
7.6 小结.215
第8 章预训练语言模型进阶217
8.1 模型优化.218
8.1.1 XLNet 218
8.1.2 RoBERTa .223
8.1.3 ALBERT .227
8.1.4 ELECTRA 229
8.1.5 MacBERT 232
8.1.6 模型对比234
8.2 长文本处理.234
8.2.1 概述234
8.2.2 Transformer-XL 235
8.2.3 Reformer .238
8.2.4 Longformer 242
8.2.5 BigBird .243
8.2.6 模型对比244
8.3 模型蒸馏与压缩244
8.3.1 概述244
8.3.2 DistilBERT 246
8.3.3 TinyBERT 248
8.3.4 MobileBERT 250
8.3.5 TextBrewer 252
8.4 生成模型257
8.4.1 BART 257
8.4.2 UniLM 260
8.4.3 T5 .263
8.4.4 GPT-3 264
8.4.5 可控文本生成265
8.5 小结.267
第9 章多模态融合的预训练模型269
9.1 多语言融合.270
9.1.1 多语言BERT .270
9.1.2 跨语言预训练语言模型272
9.1.3 多语言预训练语言模型的应用273
9.2 多媒体融合.274
9.2.1 VideoBERT 274
9.2.2 VL-BERT 275
9.2.3 DALL・E 275
9.2.4 ALIGN 276
9.3 异构知识融合276
9.3.1 融入知识的预训练277
9.3.2 多任务学习282
9.4 更多模态的预训练模型285
9.5 小结.285
参考文献287
术语表297
・ ・ ・ ・ ・ ・ (收起)