出版者的话
译者序
前言
缩写和符号
术语
第0章 导言1
0.1 什么是神经网络1
0.2 人类大脑4
0.3 神经元模型7
0.4 被看作有向图的神经网络10
0.5 反馈11
0.6 网络结构13
0.7 知识表示14
0.8 学习过程20
0.9 学习任务22
0.10 结束语27
注释和参考文献27
第1章 Rosenblatt感知器28
1.1 引言28
1.2 感知器28
1.3 感知器收敛定理29
1.4 高斯环境下感知器与贝叶斯分类器的关系33
1.5 计算机实验：模式分类36
1.6 批量感知器算法38
1.7 小结和讨论39
注释和参考文献39
习题40
第2章 通过回归建立模型28
2.1 引言41
2.2 线性回归模型：初步考虑41
2.3 参数向量的最大后验估计42
2.4 正则最小二乘估计和MAP估计之间的关系46
2.5 计算机实验：模式分类47
2.6 最小描述长度原则48
2.7 固定样本大小考虑50
2.8 工具变量方法53
2.9 小结和讨论54
注释和参考文献54
习题55
第3章 最小均方算法56
3.1 引言56
3.2 LMS算法的滤波结构56
3.3 无约束最优化：回顾58
3.4 维纳滤波器61
3.5 最小均方算法63
3.6 用马尔可夫模型来描画LMS算法和维纳滤波器的偏差64
3.7 朗之万方程：布朗运动的特点65
3.8 Kushner直接平均法66
3.9 小学习率参数下统计LMS学习理论67
3.10 计算机实验Ⅰ：线性预测68
3.11 计算机实验Ⅱ：模式分类69
3.12 LMS算法的优点和局限71
3.13 学习率退火方案72
3.14 小结和讨论73
注释和参考文献74
习题74
第4章 多层感知器77
4.1 引言77
4.2 一些预备知识78
4.3 批量学习和在线学习79
4.4 反向传播算法81
4.5 异或问题89
4.6 改善反向传播算法性能的试探法90
4.7 计算机实验：模式分类94
4.8 反向传播和微分95
4.9 Hessian矩阵及其在在线学习中的规则96
4.10 学习率的最优退火和自适应控制98
4.11 泛化102
4.12 函数逼近104
4.13 交叉验证107
4.14 复杂度正则化和网络修剪109
4.15 反向传播学习的优点和局限113
4.16 作为最优化问题看待的监督学习117
4.17 卷积网络126
4.18 非线性滤波127
4.19 小规模和大规模学习问题131
4.20 小结和讨论136
注释和参考文献137
习题138
第5章 核方法和径向基函数网络144
5.1 引言144
5.2 模式可分性的Cover定理144
5.3 插值问题148
5.4 径向基函数网络150
5.5 K-均值聚类152
5.6 权向量的递归最小二乘估计153
5.7 RBF网络的混合学习过程156
5.8 计算机实验：模式分类157
5.9 高斯隐藏单元的解释158
5.10 核回归及其与RBF网络的关系160
5.11 小结和讨论162
注释和参考文献164
习题165
第6章 支持向量机168
6.1 引言168
6.2 线性可分模式的最优超平面168
6.3 不可分模式的最优超平面173
6.4 使用核方法的支持向量机176
6.5 支持向量机的设计178
6.6 XOR问题179
6.7 计算机实验:模式分类181
6.8 回归：鲁棒性考虑184
6.9 线性回归问题的最优化解184
6.10 表示定理和相关问题187
6.11 小结和讨论191
注释和参考文献192
习题193
第7章 正则化理论197
7.1 引言197
7.2 良态问题的Hadamard条件198
7.3 Tikhonov正则化理论198
7.4 正则化网络205
7.5 广义径向基函数网络206
7.6 再论正则化最小二乘估计209
7.7 对正则化的附加要点211
7.8 正则化参数估计212
7.9 半监督学习215
7.10 流形正则化：初步的考虑216
7.11 可微流形217
7.12 广义正则化理论220
7.13 光谱图理论221
7.14 广义表示定理222
7.15 拉普拉斯正则化最小二乘算法223
7.16 用半监督学习对模式分类的实验225
7.17 小结和讨论227
注释和参考文献228
习题229
第8章 主分量分析232
8.1 引言232
8.2 自组织原则232
8.3 自组织的特征分析235
8.4 主分量分析：扰动理论235
8.5 基于Hebb的最大特征滤波器241
8.6 基于Hebb的主分量分析247
8.7 计算机实验：图像编码251
8.8 核主分量分析252
8.9 自然图像编码中的基本问题256
8.10 核Hebb算法257
8.11 小结和讨论260
注释和参考文献262
习题264
第9章 自组织映射268
9.1 引言268
9.2 两个基本的特征映射模型269
9.3 自组织映射270
9.4 特征映射的性质275
9.5 计算机实验Ⅰ：利用SOM解网格动力学问题280
9.6 上下文映射281
9.7 分层向量量化283
9.8 核自组织映射285
9.9 计算机实验Ⅱ：利用核SOM解点阵动力学问题290
9.10 核SOM和相对熵之间的关系291
9.11 小结和讨论293
注释和参考文献294
习题295
第10章 信息论学习模型299
10.1 引言299
10.2 熵300
10.3 最大熵原则302
10.4 互信息304
10.5 相对熵306
10.6 系词308
10.7 互信息作为最优化的目标函数310
10.8 最大互信息原则311
10.9 最大互信息和冗余减少314
10.10 空间相干特征316
10.11 空间非相干特征318
10.12 独立分量分析320
10.13 自然图像的稀疏编码以及与ICA编码的比较324
10.14 独立分量分析的自然梯度学习326
10.15 独立分量分析的最大似然估计332
10.16 盲源分离的最大熵学习334
10.17 独立分量分析的负熵最大化337
10.18 相关独立分量分析342
10.19 速率失真理论和信息瓶颈347
10.20 数据的最优流形表达350
10.21 计算机实验：模式分类354
10.22 小结和讨论354
注释和参考文献356
习题361
第11章 植根于统计力学的随机方法366
11.1 引言366
11.2 统计力学367
11.3 马尔可夫链368
11.4 Metropolis算法374
11.5 模拟退火375
11.6 Gibbs抽样377
11.7 Boltzmann机378
11.8 logistic信度网络382
11.9 深度信度网络383
11.10 确定性退火385
11.11 和EM算法的类比389
11.12 小结和讨论390
注释和参考文献390
习题392
第12章 动态规划396
12.1 引言396
12.2 马尔可夫决策过程397
12.3 Bellman最优准则399
12.4 策略迭代401
12.5 值迭代402
12.6 逼近动态规划：直接法406
12.7 时序差分学习406
12.8 Q学习410
12.9 逼近动态规划：非直接法412
12.10 最小二乘策略评估414
12.11 逼近策略迭代417
12.12 小结和讨论419
注释和参考文献421
习题422
第13章 神经动力学425
13.1 引言425
13.2 动态系统426
13.3 平衡状态的稳定性428
13.4 吸引子432
13.5 神经动态模型433
13.6 作为递归网络范例的吸引子操作435
13.7 Hopfield模型435
13.8 Cohen-Grossberg定理443
13.9 盒中脑状态模型445
13.10 奇异吸引子和混沌448
13.11 混沌过程的动态重构452
13.12 小结和讨论455
注释和参考文献457
习题458
第14章 动态系统状态估计的贝叶斯滤波461
14.1 引言461
14.2 状态空间模型462
14.3 卡尔曼滤波器464
14.4 发散现象及平方根滤波469
14.5 扩展的卡尔曼滤波器474
14.6 贝叶斯滤波器477
14.7 数值积分卡尔曼滤波器:基于卡尔曼滤波器480
14.8 粒子滤波器484
14.9 计算机实验：扩展的卡尔曼滤波器和粒子滤波器对比评价490
14.10 大脑功能建模中的
卡尔曼滤波493
14.11 小结和讨论494
注释和参考文献496
习题497
第15章 动态驱动递归网络501
15.1 引言501
15.2 递归网络体系结构502
15.3 通用逼近定理505
15.4 可控性和可观测性507
15.5 递归网络的计算能力510
15.6 学习算法511
15.7 通过时间的反向传播512
15.8 实时递归学习515
15.9 递归网络的消失梯度519
15.10 利用非线性逐次状态估计的递归网络监督学习框架521
15.11 计算机实验：Mackay-Glass吸引子的动态重构526
15.12 自适应考虑527
15.13 实例学习：应用于神经控制的模型参考529
15.14 小结和讨论530
注释和参考文献533
习题534
参考文献538
・ ・ ・ ・ ・ ・ (收起)