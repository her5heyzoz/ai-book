对本书的赞誉
前言
译者简介
学习环境配置
资源与支持
主要符号表
第 1章　引言　1
1.1　日常生活中的机器学习　2
1.2　机器学习中的关键组件　3
1.2.1　数据　3
1.2.2　模型　4
1.2.3　目标函数　4
1.2.4　优化算法　5
1.3　各种机器学习问题　5
1.3.1　监督学习　5
1.3.2　无监督学习　11
1.3.3　与环境互动　11
1.3.4　强化学习　12
1.4　起源　13
1.5　深度学习的发展　15
1.6　深度学习的成功案例　16
1.7　特点　17
第 2章　预备知识　20
2.1　数据操作　20
2.1.1　入门　21
2.1.2　运算符　22
2.1.3　广播机制　23
2.1.4　索引和切片　24
2.1.5　节省内存　24
2.1.6　转换为其他Python对象　25
2.2　数据预处理　26
2.2.1　读取数据集　26
2.2.2　处理缺失值　26
2.2.3　转换为张量格式　27
2.3　线性代数　27
2.3.1　标量　28
2.3.2　向量　28
2.3.3　矩阵　29
2.3.4　张量　30
2.3.5　张量算法的基本性质　31
2.3.6　降维　32
2.3.7　点积　33
2.3.8　矩阵-向量积　33
2.3.9　矩阵-矩阵乘法　34
2.3.10　范数　35
2.3.11　关于线性代数的更多信息　36
2.4　微积分　37
2.4.1　导数和微分　37
2.4.2　偏导数　40
2.4.3　梯度　41
2.4.4　链式法则　41
2.5　自动微分　42
2.5.1　一个简单的例子　42
2.5.2　非标量变量的反向传播　43
2.5.3　分离计算　43
2.5.4　Python控制流的梯度计算　44
2.6　概率　44
2.6.1　基本概率论　45
2.6.2　处理多个随机变量　48
2.6.3　期望和方差　50
2.7　查阅文档　51
2.7.1　查找模块中的所有函数和类　51
2.7.2　查找特定函数和类的用法　52
第3章　线性神经网络　54
3.1　线性回归　54
3.1.1　线性回归的基本元素　54
3.1.2　向量化加速　57
3.1.3　正态分布与平方损失　58
3.1.4　从线性回归到深度网络　60
3.2　线性回归的从零开始实现　61
3.2.1　生成数据集　62
3.2.2　读取数据集　63
3.2.3　初始化模型参数　63
3.2.4　定义模型　64
3.2.5　定义损失函数　64
3.2.6　定义优化算法　64
3.2.7　训练　64
3.3　线性回归的简洁实现　66
3.3.1　生成数据集　66
3.3.2　读取数据集　66
3.3.3　定义模型　67
3.3.4　初始化模型参数　67
3.3.5　定义损失函数　68
3.3.6　定义优化算法　68
3.3.7　训练　68
3.4　softmax回归　69
3.4.1　分类问题　69
3.4.2　网络架构　70
3.4.3　全连接层的参数开销　70
3.4.4　softmax运算　71
3.4.5　小批量样本的向量化　71
3.4.6　损失函数　72
3.4.7　信息论基础　73
3.4.8　模型预测和评估　74
3.5　图像分类数据集　74
3.5.1　读取数据集　75
3.5.2　读取小批量　76
3.5.3　整合所有组件　76
3.6　softmax回归的从零开始实现　77
3.6.1　初始化模型参数　77
3.6.2　定义softmax操作　78
3.6.3　定义模型　78
3.6.4　定义损失函数　79
3.6.5　分类精度　79
3.6.6　训练　80
3.6.7　预测　82
3.7　softmax回归的简洁实现　83
3.7.1　初始化模型参数　83
3.7.2　重新审视softmax的实现　84
3.7.3　优化算法　84
3.7.4　训练　84
第4章　多层感知机　86
4.1　多层感知机　86
4.1.1　隐藏层　86
4.1.2　激活函数　88
4.2　多层感知机的从零开始实现　92
4.2.1　初始化模型参数　92
4.2.2　激活函数　93
4.2.3　模型　93
4.2.4　损失函数　93
4.2.5　训练　93
4.3　多层感知机的简洁实现　94
模型　94
4.4　模型选择、欠拟合和过拟合　95
4.4.1　训练误差和泛化误差　96
4.4.2　模型选择　97
4.4.3　欠拟合还是过拟合　98
4.4.4　多项式回归　99
4.5　权重衰减　103
4.5.1　范数与权重衰减　103
4.5.2　高维线性回归　104
4.5.3　从零开始实现　104
4.5.4　简洁实现　106
4.6　暂退法　108
4.6.1　重新审视过拟合　108
4.6.2　扰动的稳健性　108
4.6.3　实践中的暂退法　109
4.6.4　从零开始实现　110
4.6.5　简洁实现　111
4.7　前向传播、反向传播和计算图　112
4.7.1　前向传播　113
4.7.2　前向传播计算图　113
4.7.3　反向传播　114
4.7.4　训练神经网络　115
4.8　数值稳定性和模型初始化　115
4.8.1　梯度消失和梯度爆炸　116
4.8.2　参数初始化　117
4.9　环境和分布偏移　119
4.9.1　分布偏移的类型　120
4.9.2　分布偏移示例　121
4.9.3　分布偏移纠正　122
4.9.4　学习问题的分类法　125
4.9.5　机器学习中的公平、责任和透明度　126
4.10　实战Kaggle比赛：预测房价　127
4.10.1　下载和缓存数据集　127
4.10.2　Kaggle　128
4.10.3　访问和读取数据集　129
4.10.4　数据预处理　130
4.10.5　训练　131
4.10.6　K折交叉验证　132
4.10.7　模型选择　133
4.10.8　提交Kaggle预测　133
第5章　深度学习计算　136
5.1　层和块　136
5.1.1　自定义块　138
5.1.2　顺序块　139
5.1.3　在前向传播函数中执行代码　139
5.1.4　效率　140
5.2　参数管理　141
5.2.1　参数访问　141
5.2.2　参数初始化　143
5.2.3　参数绑定　145
5.3　延后初始化　145
实例化网络　146
5.4　自定义层　146
5.4.1　不带参数的层　146
5.4.2　带参数的层　147
5.5　读写文件　148
5.5.1　加载和保存张量　148
5.5.2　加载和保存模型参数　149
5.6　GPU　150
5.6.1　计算设备　151
5.6.2　张量与GPU　152
5.6.3　神经网络与GPU　153
第6章　卷积神经网络　155
6.1　从全连接层到卷积　155
6.1.1　不变性　156
6.1.2　多层感知机的限制　157
6.1.3　卷积　158
6.1.4　“沃尔多在哪里”回顾　158
6.2　图像卷积　159
6.2.1　互相关运算　159
6.2.2　卷积层　161
6.2.3　图像中目标的边缘检测　161
6.2.4　学习卷积核　162
6.2.5　互相关和卷积　162
6.2.6　特征映射和感受野　163
6.3　填充和步幅　164
6.3.1　填充　164
6.3.2　步幅　165
6.4　多输入多输出通道　166
6.4.1　多输入通道　167
6.4.2　多输出通道　167
6.4.3　1×1卷积层　168
6.5　汇聚层　170
6.5.1　最大汇聚和平均汇聚　170
6.5.2　填充和步幅　171
6.5.3　多个通道　172
6.6　卷积神经网络（LeNet）　173
6.6.1　LeNet　173
6.6.2　模型训练　175
第7章　现代卷积神经网络　178
7.1　深度卷积神经网络（AlexNet）　178
7.1.1　学习表征　179
7.1.2　AlexNet　181
7.1.3　读取数据集　183
7.1.4　训练AlexNet　183
7.2　使用块的网络（VGG）　184
7.2.1　VGG块　184
7.2.2　VGG网络　185
7.2.3　训练模型　186
7.3　网络中的网络（NiN）　187
7.3.1　NiN块　187
7.3.2　NiN模型　188
7.3.3　训练模型　189
7.4　含并行连接的网络（GoogLeNet）　190
7.4.1　Inception块　190
7.4.2　GoogLeNet模型　191
7.4.3　训练模型　193
7.5　批量规范化　194
7.5.1　训练深层网络　194
7.5.2　批量规范化层　195
7.5.3　从零实现　196
7.5.4　使用批量规范化层的 LeNet　197
7.5.5　简明实现　198
7.5.6　争议　198
7.6　残差网络（ResNet）　200
7.6.1　函数类　200
7.6.2　残差块　201
7.6.3　ResNet模型　202
7.6.4　训练模型　204
7.7　稠密连接网络（DenseNet）　205
7.7.1　从ResNet到DenseNet　205
7.7.2　稠密块体　206
7.7.3　过渡层　206
7.7.4　DenseNet模型　207
7.7.5　训练模型　207
第8章　循环神经网络　209
8.1　序列模型　209
8.1.1　统计工具　210
8.1.2　训练　212
8.1.3　预测　213
8.2　文本预处理　216
8.2.1　读取数据集　216
8.2.2　词元化　217
8.2.3　词表　217
8.2.4　整合所有功能　219
8.3　语言模型和数据集　219
8.3.1　学习语言模型　220
8.3.2　马尔可夫模型与n元语法　221
8.3.3　自然语言统计　221
8.3.4　读取长序列数据　223
8.4　循环神经网络　226
8.4.1　无隐状态的神经网络　227
8.4.2　有隐状态的循环神经网络　227
8.4.3　基于循环神经网络的字符级语言模型　228
8.4.4　困惑度　229
8.5　循环神经网络的从零开始实现　230
8.5.1　独热编码　231
8.5.2　初始化模型参数　231
8.5.3　循环神经网络模型　232
8.5.4　预测　232
8.5.5　梯度截断　233
8.5.6　训练　234
8.6　循环神经网络的简洁实现　237
8.6.1　定义模型　237
8.6.2　训练与预测　238
8.7　通过时间反向传播　239
8.7.1　循环神经网络的梯度分析　239
8.7.2　通过时间反向传播的细节　241
第9章　现代循环神经网络　244
9.1　门控循环单元（GRU）　244
9.1.1　门控隐状态　245
9.1.2　从零开始实现　247
9.1.3　简洁实现　248
9.2　长短期记忆网络（LSTM）　249
9.2.1　门控记忆元　249
9.2.2　从零开始实现　252
9.2.3　简洁实现　253
9.3　深度循环神经网络　254
9.3.1　函数依赖关系　255
9.3.2　简洁实现　255
9.3.3　训练与预测　255
9.4　双向循环神经网络　256
9.4.1　隐马尔可夫模型中的动态规划　256
9.4.2　双向模型　258
9.4.3　双向循环神经网络的错误应用　259
9.5　机器翻译与数据集　260
9.5.1　下载和预处理数据集　261
9.5.2　词元化　262
9.5.3　词表　263
9.5.4　加载数据集　263
9.5.5　训练模型　264
9.6　编码器-解码器架构　265
9.6.1　编码器　265
9.6.2　解码器　266
9.6.3　合并编码器和解码器　266
9.7　序列到序列学习（seq2seq）　267
9.7.1　编码器　268
9.7.2　解码器　269
9.7.3　损失函数　270
9.7.4　训练　271
9.7.5　预测　272
9.7.6　预测序列的评估　273
9.8　束搜索　275
9.8.1　贪心搜索　275
9.8.2　穷举搜索　276
9.8.3　束搜索　276
第 10章　注意力机制　278
10.1　注意力提示　278
10.1.1　生物学中的注意力提示　279
10.1.2　查询、键和值　280
10.1.3　注意力的可视化　280
10.2　注意力汇聚：Nadaraya-Watson 核回归　281
10.2.1　生成数据集　282
10.2.2　平均汇聚　282
10.2.3　非参数注意力汇聚　283
10.2.4　带参数注意力汇聚　284
10.3　注意力评分函数　287
10.3.1　掩蔽softmax操作　288
10.3.2　加性注意力　289
10.3.3　缩放点积注意力　290
10.4　Bahdanau 注意力　291
10.4.1　模型　291
10.4.2　定义注意力解码器　292
10.4.3　训练　293
10.5　多头注意力　295
10.5.1　模型　295
10.5.2　实现　296
10.6　自注意力和位置编码　298
10.6.1　自注意力　298
10.6.2　比较卷积神经网络、循环神经网络和自注意力　298
10.6.3　位置编码　299
10.7　Transformer　302
10.7.1　模型　302
10.7.2　基于位置的前馈网络　303
10.7.3　残差连接和层规范化　304
10.7.4　编码器　304
10.7.5　解码器　305
10.7.6　训练　307
第 11章　优化算法　311
11.1　优化和深度学习　311
11.1.1　优化的目标　311
11.1.2　深度学习中的优化挑战　312
11.2　凸性　315
11.2.1　定义　315
11.2.2　性质　317
11.2.3　约束　319
11.3　梯度下降　322
11.3.1　一维梯度下降　322
11.3.2　多元梯度下降　324
11.3.3　自适应方法　326
11.4　随机梯度下降　329
11.4.1　随机梯度更新　329
11.4.2　动态学习率　331
11.4.3　凸目标的收敛性分析　332
11.4.4　随机梯度和有限样本　333
11.5　小批量随机梯度下降　334
11.5.1　向量化和缓存　335
11.5.2　小批量　336
11.5.3　读取数据集　337
11.5.4　从零开始实现　337
11.5.5　简洁实现　340
11.6　动量法　341
11.6.1　基础　341
11.6.2　实际实验　345
11.6.3　理论分析　346
11.7　AdaGrad算法　348
11.7.1　稀疏特征和学习率　348
11.7.2　预处理　349
11.7.3　算法　350
11.7.4　从零开始实现　351
11.7.5　简洁实现　352
11.8　RMSProp算法　353
11.8.1　算法　353
11.8.2　从零开始实现　354
11.8.3　简洁实现　355
11.9　Adadelta算法　356
11.9.1　算法　356
11.9.2　实现　356
11.10　Adam算法　358
11.10.1　算法　358
11.10.2　实现　359
11.10.3　Yogi　360
11.11　学习率调度器　361
11.11.1　一个简单的问题　361
11.11.2　学习率调度器　363
11.11.3　策略　364
第 12章　计算性能　369
12.1　编译器和解释器　369
12.1.1　符号式编程　370
12.1.2　混合式编程　371
12.1.3　Sequential的混合式编程　371
12.2　异步计算　372
通过后端异步处理　373
12.3　自动并行　375
12.3.1　基于GPU的并行计算　375
12.3.2　并行计算与通信　376
12.4　硬件　378
12.4.1　计算机　378
12.4.2　内存　379
12.4.3　存储器　380
12.4.4　CPU　381
12.4.5　GPU和其他加速卡　383
12.4.6　网络和总线　385
12.4.7　更多延迟　386
12.5　多GPU训练　388
12.5.1　问题拆分　388
12.5.2　数据并行性　390
12.5.3　简单网络　390
12.5.4　数据同步　391
12.5.5　数据分发　392
12.5.6　训练　392
12.6　多GPU的简洁实现　394
12.6.1　简单网络　394
12.6.2　网络初始化　395
12.6.3　训练　395
12.7　参数服务器　397
12.7.1　数据并行训练　397
12.7.2　环同步（ring
synchronization）　399
12.7.3　多机训练　400
12.7.4　键-值存储　402
第 13章　计算机视觉　404
13.1　图像增广　404
13.1.1　常用的图像增广方法　404
13.1.2　使用图像增广进行训练　408
13.2　微调　410
13.2.1　步骤　410
13.2.2　热狗识别　411
13.3　目标检测和边界框　415
边界框　415
13.4　锚框　417
13.4.1　生成多个锚框　417
13.4.2　交并比（IoU）　419
13.4.3　在训练数据中标注锚框　420
13.4.4　使用非极大值抑制预测
边界框　424
13.5　多尺度目标检测　427
13.5.1　多尺度锚框　427
13.5.2　多尺度检测　429
13.6　目标检测数据集　430
13.6.1　下载数据集　430
13.6.2　读取数据集　431
13.6.3　演示　432
13.7　单发多框检测（SSD）　433
13.7.1　模型　433
13.7.2　训练模型　437
13.7.3　预测目标　439
13.8　区域卷积神经网络（R-CNN）系列　441
13.8.1　R-CNN　441
13.8.2　Fast R-CNN　442
13.8.3　Faster R-CNN　443
13.8.4　Mask R-CNN　444
13.9　语义分割和数据集　445
13.9.1　图像分割和实例分割　445
13.9.2　Pascal VOC2012 语义分割数据集　446
13.10　转置卷积　450
13.10.1　基本操作　450
13.10.2　填充、步幅和多通道　451
13.10.3　与矩阵变换的联系　452
13.11　全卷积网络　453
13.11.1　构建模型　454
13.11.2　初始化转置卷积层　455
13.11.3　读取数据集　456
13.11.4　训练　456
13.11.5　预测　457
13.12　风格迁移　458
13.12.1　方法　459
13.12.2　阅读内容和风格图像　460
13.12.3　预处理和后处理　460
13.12.4　提取图像特征　461
13.12.5　定义损失函数　461
13.12.6　初始化合成图像　463
13.12.7　训练模型　463
13.13　实战 Kaggle竞赛：图像分类（CIFAR-10）　464
13.13.1　获取并组织数据集　465
13.13.2　图像增广 　467
13.13.3　读取数据集　468
13.13.4　定义模型　468
13.13.5　定义训练函数　468
13.13.6　训练和验证模型　469
13.13.7　在Kaggle上对测试集进行分类并提交结果　469
13.14　实战Kaggle竞赛：狗的品种识别（ImageNet Dogs）　470
13.14.1　获取和整理数据集　471
13.14.2　图像增广　472
13.14.3　读取数据集　472
13.14.4　微调预训练模型　473
13.14.5　定义训练函数　473
13.14.6　训练和验证模型　474
13.14.7　对测试集分类并在Kaggle提交结果　475
第 14章　自然语言处理：预训练　476
14.1　词嵌入（word2vec）　477
14.1.1　为何独热向量是一个糟糕的选择　477
14.1.2　自监督的word2vec　477
14.1.3　跳元模型　477
14.1.4　连续词袋模型　478
14.2　近似训练　480
14.2.1　负采样　480
14.2.2　层序softmax　481
14.3　用于预训练词嵌入的数据集　482
14.3.1　读取数据集　482
14.3.2　下采样　483
14.3.3　中心词和上下文词的提取　484
14.3.4　负采样　485
14.3.5　小批量加载训练实例　486
14.3.6　整合代码　487
14.4　预训练word2vec　488
14.4.1　跳元模型　488
14.4.2　训练　489
14.4.3　应用词嵌入　491
14.5　全局向量的词嵌入（GloVe）　491
14.5.1　带全局语料库统计的跳元模型　492
14.5.2　GloVe模型　492
14.5.3　从共现概率比值理解GloVe模型　493
14.6　子词嵌入　494
14.6.1　fastText模型　494
14.6.2　字节对编码　495
14.7　词的相似度和类比任务　497
14.7.1　加载预训练词向量　497
14.7.2　应用预训练词向量　499
14.8　来自Transformer的双向编码器表示（BERT）　500
14.8.1　从上下文无关到上下文敏感　500
14.8.2　从特定于任务到不可知任务　501
14.8.3　BERT：将ELMo与GPT结合起来　501
14.8.4　输入表示　502
14.8.5　预训练任务　504
14.8.6　整合代码　506
14.9　用于预训练BERT的数据集　507
14.9.1　为预训练任务定义辅助函数　508
14.9.2　将文本转换为预训练数据集　509
14.10　预训练BERT　512
14.10.1　预训练BERT　512
14.10.2　用BERT表示文本　514
第 15章　自然语言处理：应用　515
15.1　情感分析及数据集　516
15.1.1　读取数据集　516
15.1.2　预处理数据集　517
15.1.3　创建数据迭代器　517
15.1.4　整合代码　518
15.2　情感分析：使用循环神经网络　518
15.2.1　使用循环神经网络表示单个文本　519
15.2.2　加载预训练的词向量　520
15.2.3　训练和评估模型　520
15.3　情感分析：使用卷积神经网络　521
15.3.1　一维卷积　522
15.3.2　最大时间汇聚层　523
15.3.3　textCNN模型　523
15.4　自然语言推断与数据集　526
15.4.1　自然语言推断　526
15.4.2　斯坦福自然语言推断（SNLI）数据集　527
15.5　自然语言推断：使用注意力　530
15.5.1　模型　530
15.5.2　训练和评估模型　533
15.6　针对序列级和词元级应用微调BERT　535
15.6.1　单文本分类　535
15.6.2　文本对分类或回归　536
15.6.3　文本标注　537
15.6.4　问答　537
15.7　自然语言推断：微调BERT　538
15.7.1　加载预训练的BERT　539
15.7.2　微调BERT的数据集　540
15.7.3　微调BERT　541
附录A　深度学习工具　543
A.1　使用Jupyter记事本　543
A.1.1　在本地编辑和运行代码　543
A.1.2　高级选项　545
A.2　使用Amazon SageMaker　546
A.2.1　注册　547
A.2.2　创建SageMaker实例　547
A.2.3　运行和停止实例　548
A.2.4　更新Notebook　548
A.3　使用Amazon EC2实例　549
A.3.1　创建和运行EC2实例　549
A.3.2　安装CUDA　553
A.3.3　安装库以运行代码　553
A.3.4　远程运行Jupyter记事本　554
A.3.5　关闭未使用的实例　554
A.4　选择服务器和GPU　555
A.4.1　选择服务器　555
A.4.2　选择GPU　556
A.5　为本书做贡献　558
A.5.1　提交微小更改　558
A.5.2　大量文本或代码修改　559
A.5.3　提交主要更改　559
参考文献　562
・ ・ ・ ・ ・ ・ (收起)