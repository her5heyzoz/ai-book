第一部分 人工智能基础
第 1 章 绪论 2
1.1 什么是人工智能 2
1.1.1 类人行为：图灵测试方法 3
1.1.2 类人思考：认知建模方法 3
1.1.3 理性思考：“思维法则”方法 4
1.1.4 理性行为：理性智能体方法 4
1.1.5 益机 5
1.2 人工智能的基础 6
1.2.1 哲学 6
1.2.2 数学 8
1.2.3 经济学 9
1.2.4 神经科学 10
1.2.5 心理学 12
1.2.6 计算机工程 13
1.2.7 控制理论与控制论 14
1.2.8 语言学 15
1.3 人工智能的历史 16
1.3.1 人工智能的诞生（1943―1956） 16
1.3.2 早期热情高涨，期望无限（1952―1969） 17
1.3.3 一些现实（1966―1973） 19
1.3.4 专家系统（1969―1986） 20
1.3.5 神经网络的回归（1986―现在） 22
1.3.6 概率推理和机器学习（1987―现在） 22
1.3.7 大数据（2001―现在） 23
1.3.8 深度学习（2011―现在） 24
1.4 目前的先进技术 24
1.5 人工智能的风险和收益 27
小结 30
参考文献与历史注释 31
第 2 章 智能体 32
2.1 智能体和环境 32
2.2 良好行为：理性的概念 34
2.2.1 性能度量 34
2.2.2 理性 35
2.2.3 全知、学习和自主 36
2.3 环境的本质 37
2.3.1 指定任务环境 37
2.3.2 任务环境的属性 38
2.4 智能体的结构 41
2.4.1 智能体程序 41
2.4.2 简单反射型智能体 42
2.4.3 基于模型的反射型智能体 44
2.4.4 基于目标的智能体 45
2.4.5 基于效用的智能体 46
2.4.6 学习型智能体 47
2.4.7 智能体程序的组件如何工作 49
小结 50
参考文献与历史注释 51
第二部分 问题求解
第 3 章 通过搜索进行问题求解 54
3.1 问题求解智能体 54
3.1.1 搜索问题和解 55
3.1.2 问题形式化 56
3.2 问题示例 57
3.2.1 标准化问题 57
3.2.2 真实世界问题 59
3.3 搜索算法 61
3.3.1 最佳优先搜索 62
3.3.2 搜索数据结构 63
3.3.3 冗余路径 64
3.3.4 问题求解性能评估 65
3.4 无信息搜索策略 65
3.4.1 广度优先搜索 66
3.4.2 Dijkstra 算法或一致代价搜索 67
3.4.3 深度优先搜索与内存问题 68
3.4.4 深度受限和迭代加深搜索 69
3.4.5 双向搜索 712
3.4.6 无信息搜索算法对比 72
3.5 有信息（启发式）搜索策略 73
3.5.1 贪心最佳优先搜索 73
3.5.2 A* 搜索 75
3.5.3 搜索等值线 77
3.5.4 满意搜索：不可容许的启发式
函数与加权 A* 搜索 79
3.5.5 内存受限搜索 80
3.5.6 双向启发式搜索 83
3.6 启发式函数 85
3.6.1 启发式函数的准确性对性能的影响 85
3.6.2 从松弛问题出发生成启发式函数 86
3.6.3 从子问题出发生成启发式函数：模式数据库 87
3.6.4 使用地标生成启发式函数 88
3.6.5 学习以更好地搜索 90
3.6.6 从经验中学习启发式函数 90
小结 90
参考文献与历史注释 92
第 4 章 复杂环境中的搜索 95
4.1 局部搜索和最优化问题 95
4.1.1 爬山搜索 96
4.1.2 模拟退火 98
4.1.3 局部束搜索 99
4.1.4 进化算法 99
4.2 连续空间中的局部搜索 102
4.3 使用非确定性动作的搜索 104
4.3.1 不稳定的真空吸尘器世界 105
4.3.2 与或搜索树 106
4.3.3 反复尝试 107
4.4 部分可观测环境中的搜索 108
4.4.1 无观测信息的搜索 108
4.4.2 部分可观测环境中的搜索 111
4.4.3 求解部分可观测问题 112
4.4.4 部分可观测环境中的智能体 113
4.5 在线搜索智能体和未知环境 115
4.5.1 在线搜索问题 115
4.5.2 在线搜索智能体 117
4.5.3 在线局部搜索 118
4.5.4 在线搜索中的学习 119
小结 120
参考文献与历史注释 121
第 5 章 对抗搜索和博弈 124
5.1 博弈论 124
5.2 博弈中的优化决策 126
5.2.1 极小化极大搜索算法 127
5.2.2 多人博弈中的最优决策 128
5.2.3 α-β 剪枝 129
5.2.4 移动顺序 131
5.3 启发式 α-β 树搜索 132
5.3.1 评价函数 132
5.3.2 截断搜索 134
5.3.3 前向剪枝 135
5.3.4 搜索和查表 136
5.4 蒙特卡罗树搜索 136
5.5 随机博弈 139
5.6 部分可观测博弈 142
5.6.1 四国军棋：部分可观测的国际象棋 142
5.6.2 纸牌游戏 144
5.7 博弈搜索算法的局限性 146
小结 147
参考文献与历史注释 148
第 6 章 约束满足问题 152
6.1 定义约束满足问题 152
6.1.1 问题示例：地图着色 153
6.1.2 问题示例：车间作业调度 154
6.1.3 CSP 形式体系的变体 155
6.2 约束传播：CSP 中的推断 156
6.2.1 节点一致性 157
6.2.2 弧一致性 157
6.2.3 路径一致性 158
6.2.4 k 一致性 158
6.2.5 全局约束 159
6.2.6 数独 160
6.3 CSP 的回溯搜索 161
6.3.1 变量排序和值排序 163
6.3.2 交替进行搜索和推理 164
6.3.3 智能回溯：向后看 164
6.3.4 约束学习 166
6.4 CSP 的局部搜索 166
6.5 问题的结构 168
6.5.1 割集调整 169
6.5.2 树分解 170
6.5.3 值对称 171
小结 171
参考文献与历史注释 172
第三部分 知识、推理和规划
第 7 章 逻辑智能体 176
7.1 基于知识的智能体 176
7.2 wumpus 世界 178
7.3 逻辑 180
7.4 命题逻辑：一种非常简单的逻辑 183
7.4.1 语法 183
7.4.2 语义 184
7.4.3 一个简单的知识库 185
7.4.4 一个简单的推断过程 186
7.5 命题定理证明 187
7.5.1 推断与证明 188
7.5.2 通过归结证明 190
7.5.3 霍恩子句与确定子句 194
7.5.4 前向链接与反向链接 194
7.6 高效命题模型检验 196
7.6.1 完备的回溯算法 196
7.6.2 局部搜索算法 198
7.6.3 随机 SAT 问题概览 199
7.7 基于命题逻辑的智能体 200
7.7.1 世界的当前状态 200
7.7.2 混合智能体 203
7.7.3 逻辑状态估计 204
7.7.4 用命题推断进行规划 205
小结 207
参考文献与历史注释 208
第 8 章 一阶逻辑 211
8.1 回顾表示 211
8.1.1 思想的语言 212
8.1.2 结合形式语言和自然语言的优点 213
8.2 一阶逻辑的语法和语义 215
8.2.1 一阶逻辑模型 215
8.2.2 符号与解释 216
8.2.3 项 218
8.2.4 原子语句 218
8.2.5 复合语句 218
8.2.6 量词 219
8.2.7 等词 222
8.2.8 数据库语义 222
8.3 使用一阶逻辑 223
8.3.1 一阶逻辑的断言与查询 223
8.3.2 亲属关系论域 224
8.3.3 数、集合与列表 225
8.3.4 wumpus 世界 227
8.4 一阶逻辑中的知识工程 228
8.4.1 知识工程的过程 229
8.4.2 电子电路论域 230
小结 233
参考文献与历史注释 234
第 9 章 一阶逻辑中的推断 236
9.1 命题推断与一阶推断 236
9.2 合一与一阶推断 238
9.2.1 合一 239
9.2.2 存储与检索 240
9.3 前向链接 241
9.3.1 一阶确定子句 242
9.3.2 简单的前向链接算法 242
9.3.3 高效前向链接 244
9.4 反向链接 247
9.4.1 反向链接算法 247
9.4.2 逻辑编程 248
9.4.3 冗余推断和无限循环 249
9.4.4 Prolog 的数据库语义 251
9.4.5 约束逻辑编程 251
9.5 归结 252
9.5.1 一阶逻辑的合取范式 252
9.5.2 归结推断规则 253
9.5.3 证明范例 254
9.5.4 归结的完备性 256
9.5.5 等词 258
9.5.6 归结策略 260
小结 261
参考文献与历史注释 262
第 10 章 知识表示 265
10.1 本体论工程 265
10.2 类别与对象 267
10.2.1 物理组成 268
10.2.2 量度 269
10.2.3 对象：事物和物质 271
10.3 事件 272
10.3.1 时间 273
10.3.2 流和对象 275
10.4 精神对象和模态逻辑 275
10.5 类别的推理系统 278
10.5.1 语义网络 278
10.5.2 描述逻辑 280
10.6 用缺省信息推理 281
10.6.1 限定与缺省逻辑 281
10.6.2 真值维护系统 283
小结 284
参考文献与历史注释 285
第 11 章 自动规划 290
11.1 经典规划的定义 290
11.1.1 范例领域：航空货物运输 291
11.1.2 范例领域：备用轮胎问题 292
11.1.3 范例领域：积木世界 292
11.2 经典规划的算法 294
11.2.1 规划的前向状态空间搜索 294
11.2.2 规划的反向状态空间搜索 295
11.2.3 使用布尔可满足性规划 296
11.2.4 其他经典规划方法 296
11.3 规划的启发式方法 297
11.3.1 领域无关剪枝 299
11.3.2 规划中的状态抽象 300
11.4 分层规划 300
11.4.1 高层动作 301
11.4.2 搜索基元解 302
11.4.3 搜索抽象解 303
11.5 非确定性域的规划和行动 307
11.5.1 无传感器规划 309
11.5.2 应变规划 312
11.5.3 在线规划 313
11.6 时间、调度和资源 315
11.6.1 时间约束和资源约束的表示 315
11.6.2 解决调度问题 316
11.7 规划方法分析 318
小结 319
参考文献与历史注释 320
第四部分 不确定知识和不确定推理
第 12 章 不确定性的量化 326
12.1 不确定性下的动作 326
12.1.1 不确定性概述 327
12.1.2 不确定性与理性决策 328
12.2 基本概率记号 329
12.2.1 概率是关于什么的 329
12.2.2 概率断言中的命题语言 330
12.2.3 概率公理及其合理性 333
12.3 使用完全联合分布进行推断 334
12.4 独立性 336
12.5 贝叶斯法则及其应用 337
12.5.1 应用贝叶斯法则：简单实例 338
12.5.2 应用贝叶斯法则：合并证据 339
12.6 朴素贝叶斯模型 340
12.7 重游 wumpus 世界 342
小结 344
参考文献与历史注释 345
第 13 章 概率推理 348
13.1 不确定域的知识表示 348
13.2 贝叶斯网络的语义 350
13.2.1 贝叶斯网络中的条件独立性关系 353
13.2.2 条件分布的高效表示 354
13.2.3 连续变量的贝叶斯网络 356
13.2.4 案例研究：汽车保险 358
13.3 贝叶斯网络中的精确推断 360
13.3.1 通过枚举进行推断 361
13.3.2 变量消元算法 363
13.3.3 精确推断的复杂性 365
13.3.4 聚类算法 366
13.4 贝叶斯网络中的近似推理 367
13.4.1 直接采样方法 368
13.4.2 通过马尔可夫链模拟进行推断 372
13.4.3 编译近似推断 378
13.5 因果网络 379
13.5.1 表示动作：do 操作 380
13.5.2 后门准则 382
小结 382
参考文献与历史注释 383
第 14 章 时间上的概率推理 388
14.1 时间与不确定性 388
14.1.1 状态与观测 389
14.1.2 转移模型与传感器模型 389
14.2 时序模型中的推断 391
14.2.1 滤波与预测 392
14.2.2 平滑 394
14.2.3 寻找最可能序列 396
14.3 隐马尔可夫模型 398
14.3.1 简化矩阵算法 398
14.3.2 隐马尔可夫模型示例：定位 400
14.4 卡尔曼滤波器 403
14.4.1 更新高斯分布 403
14.4.2 简单的一维示例 404
14.4.3 一般情况 406
14.4.4 卡尔曼滤波的适用范围 407
14.5 动态贝叶斯网络 408
14.5.1 构建动态贝叶斯网络 409
14.5.2 动态贝叶斯网络中的精确推断 412
14.5.3 动态贝叶斯网络中的近似推断 413
小结 417
参考文献与历史注释 418
第 15 章 概率编程 421
15.1 关系概率模型 421
15.1.1 语法与语义 423
15.1.2 实例：评定玩家的技能等级 425
15.1.3 关系概率模型中的推断 426
15.2 开宇宙概率模型 427
15.2.1 语义与语法 428
15.2.2 开宇宙概率模型的推断 429
15.2.3 示例 430
15.3 追踪复杂世界 433
15.3.1 示例：多目标跟踪 433
15.3.2 示例：交通监控 436
15.4 作为概率模型的程序 436
15.4.1 示例：文本阅读 437
15.4.2 语法与语义 438
15.4.3 推断结果 438
15.4.4 结合马尔可夫模型改进生成程序 439
15.4.5 生成程序的推断 439
小结 440
参考文献与历史注释 440
第 16 章 做简单决策 444
16.1 在不确定性下结合信念与愿望 444
16.2 效用理论基础 445
16.2.1 理性偏好的约束 445
16.2.2 理性偏好导致效用 447
16.3 效用函数 448
16.3.1 效用评估和效用尺度 448
16.3.2 金钱的效用 449
16.3.3 期望效用与决策后失望 451
16.3.4 人类判断与非理性 452
16.4 多属性效用函数 454
16.4.1 占优 455
16.4.2 偏好结构与多属性效用 456
16.5 决策网络 458
16.5.1 使用决策网络表示决策问题 458
16.5.2 评估决策网络 460
16.6 信息价值 460
16.6.1 简单示例 460
16.6.2 完美信息的一般公式 461
16.6.3 价值信息的性质 462
16.6.4 信息收集智能体的实现 463
16.6.5 非短视信息收集 463
16.6.6 敏感性分析与健壮决策 464
16.7 未知偏好 465
16.7.1 个人偏好的不确定性 466
16.7.2 顺从人类 467
小结 468
参考文献与历史注释 469
第 17 章 做复杂决策 473
17.1 序贯决策问题 473
17.1.1 时间上的效用 475
17.1.2 最优策略与状态效用 477
17.1.3 奖励规模 479
17.1.4 表示 MDP 480
17.2 MDP 的算法 482
17.2.1 价值迭代 482
17.2.2 策略迭代 485
17.2.3 线性规划 487
17.2.4 MDP 的在线算法 487
17.3 老虎机问题 489
17.3.1 计算基廷斯指数 491
17.3.2 伯努利老虎机 492
17.3.3 近似最优老虎机策略 493
17.3.4 不可索引变体 493
17.4 部分可观测MDP 495
17.5 求解POMDP 的算法 497
17.5.1 POMDP的价值迭代 497
17.5.2 POMDP的在线算法 500
小结 501
参考文献与历史注释 502
第 18 章 多智能体决策 505
18.1 多智能体环境的特性 505
18.1.1 单个决策者 505
18.1.2 多决策者 506
18.1.3 多智能体规划 507
18.1.4 多智能体规划：合作与协调 509
18.2 非合作博弈论 510
18.2.1 单步博弈：正则形式博弈 510
18.2.2 社会福利 513
18.2.3 重复博弈 517
18.2.4 序贯博弈：扩展形式 520
18.2.5 不确定收益与辅助博弈 525
18.3 合作博弈论 527
18.3.1 联盟结构与结果 528
18.3.2 合作博弈中的策略 529
18.3.3 合作博弈中的计算 531
18.4 制定集体决策 533
18.4.1 在合同网中分配任务 533
18.4.2 通过拍卖分配稀缺资源 535
18.4.3 投票 539
18.4.4 议价 541
小结 544
参考文献与历史注释 545
第五部分 机器学习
第 19 章 样例学习 550
19.1 学习的形式 550
19.2 监督学习 552
19.3 决策树学习 555
19.3.1 决策树的表达能力 556
19.3.2 从样例中学习决策树 557
19.3.3 选择测试属性 559
19.3.4 泛化与过拟合 560
19.3.5 拓展决策树的适用范围 562
19.4 模型选择与模型优化 563
19.4.1 模型选择 564
19.4.2 从错误率到损失函数 566
19.4.3 正则化 567
19.4.4 超参数调整 568
19.5 学习理论 569
19.6 线性回归与分类 572
19.6.1 单变量线性回归 572
19.6.2 梯度下降 574
19.6.3 多变量线性回归 575
19.6.4 带有硬阈值的线性分类器 577
19.6.5 基于逻辑斯谛回归的线性分类器 579
19.7 非参数模型 581
19.7.1 最近邻模型 581
19.7.2 使用 k-d 树寻找最近邻 583
19.7.3 局部敏感哈希 584
19.7.4 非参数回归 585
19.7.5 支持向量机 586
19.7.6 核技巧 589
19.8 集成学习 589
19.8.1 自助聚合法 590
19.8.2 随机森林法 590
19.8.3 堆叠法 591
19.8.4 自适应提升法 592
19.8.5 梯度提升法 594
19.8.6 在线学习 595
19.9 开发机器学习系统 596
19.9.1 问题形式化 596
19.9.2 数据收集、评估和管理 597
19.9.3 模型选择与训练 601
19.9.4 信任、可解释性、可说明性 601
19.9.5 操作、监控和维护 603
小结 604
参考文献与历史注释 605
第 20 章 概率模型学习 610
20.1 统计学习 610
20.2 完全数据学习 613
20.2.1 最大似然参数学习：离散模型 613
20.2.2 朴素贝叶斯模型 615
20.2.3 生成模型和判别模型 616
20.2.4 最大似然参数学习：连续模型 616
20.2.5 贝叶斯参数学习 618
20.2.6 贝叶斯线性回归 620
20.2.7 贝叶斯网络结构学习 622
20.2.8 非参数模型密度估计 623
20.3 隐变量学习：EM 算法 624
20.3.1 无监督聚类：学习混合高斯 625
20.3.2 学习带隐变量的贝叶斯网络参数值 627
20.3.3 学习隐马尔可夫模型 630
20.3.4 EM 算法的一般形式 630
20.3.5 学习带隐变量的贝叶斯网络结构 631
小结 632
参考文献与历史注释 632
第 21 章 深度学习 635
21.1 简单前馈网络 636
21.1.1 网络作为复杂函数 636
21.1.2 梯度与学习 639
21.2 深度学习的计算图 640
21.2.1 输入编码 641
21.2.2 输出层与损失函数 641
21.2.3 隐藏层 642
21.3 卷积网络 643
21.3.1 池化与下采样 646
21.3.2 卷积神经网络的张量运算 646
21.3.3 残差网络 647
21.4 学习算法 648
21.4.1 计算图中的梯度计算 649
21.4.2 批量归一化 650
21.5 泛化 650
21.5.1 选择正确的网络架构 651
21.5.2 神经架构搜索 652
21.5.3 权重衰减 653
21.5.4 暂退法 653
21.6 循环神经网络 654
21.6.1 训练基本的循环神经网络 655
21.6.2 长短期记忆 RNN 656
21.7 无监督学习与迁移学习 657
21.7.1 无监督学习 657
21.7.2 迁移学习和多任务学习 661
21.8 应用 662
21.8.1 视觉 662
21.8.2 自然语言处理 663
21.8.3 强化学习 663
小结 664
参考文献与历史注释 664
第 22 章 强化学习 668
22.1 从奖励中学习 668
22.2 被动强化学习 670
22.2.1 直接效用估计 671
22.2.2 自适应动态规划 671
22.2.3 时序差分学习 672
22.3 主动强化学习 674
22.3.1 探索 675
22.3.2 安全探索 677
22.3.3 时序差分 Q 学习 678
22.4 强化学习中的泛化 680
22.4.1 近似直接效用估计 680
22.4.2 近似时序差分学习 681
22.4.3 深度强化学习 682
22.4.4 奖励函数设计 683
22.4.5 分层强化学习 683
22.5 策略搜索 686
22.6 学徒学习与逆强化学习 688
22.7 强化学习的应用 690
22.7.1 在电子游戏中的应用 690
22.7.2 在机器人控制中的应用 691
小结 692
参考文献与历史注释 693
第六部分 沟通、感知和行动
第 23 章 自然语言处理 698
23.1 语言模型 698
23.1.1 词袋模型 699
23.1.2 n 元单词模型 700
23.1.3 其他 n 元模型 701
23.1.4 n 元模型的平滑 701
23.1.5 单词表示 702
23.1.6 词性标注 703
23.1.7 语言模型的比较 706
23.2 文法 707
23.3 句法分析 709
23.3.1 依存分析 711
23.3.2 从样例中学习句法分析器 712
23.4 扩展文法 713
23.4.1 语义解释 715
23.4.2 学习语义文法 717
23.5 真实自然语言的复杂性 717
23.6 自然语言任务 720
小结 722
参考文献与历史注释 722
第 24 章 自然语言处理中的深度学习 727
24.1 词嵌入 727
24.2 自然语言处理中的循环神经网络 730
24.2.1 使用循环神经网络的语言模型 730
24.2.2 用循环神经网络进行分类 732
24.2.3 自然语言处理任务中的 LSTM模型 733
24.3 序列到序列模型 733
24.3.1 注意力 735
24.3.2 解码 736
24.4 Transformer 架构 737
24.4.1 自注意力 737
24.4.2 从自注意力到 Transformer 738
24.5 预训练和迁移学习 739
24.5.1 预训练词嵌入 740
24.5.2 预训练上下文表示 741
24.5.3 掩码语言模型 742
24.6 最高水平（SOTA） 742
小结 745
参考文献与历史注释 745
第 25 章 计算机视觉 748
25.1 引言 748
25.2 图像形成 749
25.2.1 无透镜成像：针孔照相机 749
25.2.2 透镜系统 751
25.2.3 缩放正交投影 752
25.2.4 光线与明暗 752
25.2.5 颜色 753
25.3 简单图像特征 754
25.3.1 边缘 755
25.3.2 纹理 757
25.3.3 光流 758
25.3.4 自然图像分割 759
25.4 图像分类 760
25.4.1 基于卷积神经网络的图像分类 761
25.4.2 卷积神经网络对图像分类问题
有效的原因 762
25.5 物体检测 763
25.6 三维世界 766
25.6.1 多个视图下的三维线索 766
25.6.2 双目立体视觉 766
25.6.3 移动摄像机给出的三维线索 768
25.6.4 单个视图的三维线索 769
25.7 计算机视觉的应用 769
25.7.1 理解人类行为 770
25.7.2 匹配图片与文字 772
25.7.3 多视图重建 773
25.7.4 单视图中的几何 774
25.7.5 生成图片 775
25.7.6 利用视觉控制运动 778
小结 780
参考文献与历史注释 781
第 26 章 机器人学 785
26.1 机器人 785
26.2 机器人硬件 786
26.2.1 机器人的硬件层面分类 786
26.2.2 感知世界 787
26.2.3 产生运动 789
26.3 机器人学解决哪些问题 789
26.4 机器人感知 790
26.4.1 定位与地图构建 791
26.4.2 其他感知类型 795
26.4.3 机器人感知中的监督学习与无监督学习 795
26.5 规划与控制 796
26.5.1 构形空间 796
26.5.2 运动规划 799
26.5.3 轨迹跟踪控制 806
26.5.4 最优控制 809
26.6 规划不确定的运动 810
26.7 机器人学中的强化学习 812
26.7.1 利用模型 812
26.7.2 利用其他信息 813
26.8 人类与机器人 814
26.8.1 协调 814
26.8.2 学习做人类期望的事情 817
26.9 其他机器人框架 820
26.9.1 反应式控制器 820
26.9.2 包容架构 821
26.10 应用领域 822
小结 825
参考文献与历史注释 826
第七部分 总结
第 27 章 人工智能的哲学、伦理和安全性 832
27.1 人工智能的极限 832
27.1.1 由非形式化得出的论据 832
27.1.2 由能力缺陷得出的论据 833
27.1.3 数学异议 833
27.1.4 衡量人工智能 834
27.2 机器能真正地思考吗 835
27.2.1 中文房间 835
27.2.2 意识与感质 836
27.3 人工智能的伦理 836
27.3.1 致命性自主武器 837
27.3.2 监控、安全与隐私 839
27.3.3 公平与偏见 841
27.3.4 信任与透明度 844
27.3.5 工作前景 845
27.3.6 机器人权利 847
27.3.7 人工智能安全性 848
小结 851
参考文献与历史注释 852
第 28 章 人工智能的未来 857
28.1 人工智能组件 857
28.2 人工智能架构 862
附录 A 数学背景知识 865
附录 B 关于语言与算法的说明 871
参考文献 873Ⅰ artificial intelligence
1 introduction
1.1what is al?
1.2the foundations of artificial intelligence
1.3the history of artificial intelligence
1.4the state of the art
1.5summary, bibliographical and historical notes, exercises
2 intelligent agents
2.1agents and environments
2.2good behavior: the concept of rationality
2.3the nature of environments
2.4the structure of agents
2.5summary, bibliographical and historical notes, exercises
Ⅱ problem-solving
3 solving problems by searching
3.1problem-solving agents
3.2example problems
3.3searching for solutions
3.4uninformed search strategies
3.5informed (heuristic) search strategies
3.6heuristic functions
3.7summary, bibliographical and historical notes, exercises
4 beyond classical search
4.1local search algorithms and optimization problems
4.2local search in continuous spaces
4.3searching with nondeterministic actions
4.4searching with partial observations
4.5online search agents and unknown environments
4.6summary, bibliographical and historical notes, exercises
5 adversarial search
5.1games
5.2optimal decisions in games
5.3alpha-beta pruning
5.4imperfect real-time decisions
5.5stochastic games
5.6partially observable games
5.7state-of-the-art game programs
5.8alternative approaches
5.9summary, bibliographical and historical notes, exercises
6 constraint satisfaction problems
6.1defining constraint satisfaction problems
6.2constraint propagation: inference in csps
6.3backtracking search for csps
6.4local search for csps
6.5the structure of problems
6.6summary, bibliographical and historical notes, exercises
Ⅲ knowledge, reasoning, and planning
7 logical agents
7.1knowledge-based agents
7.2the wumpus world
7.3logic
7.4propositional logic: a very simple logic
7.5propositional theorem proving
7.6effective propositional model checking
7.7agents based on propositional logic
7.8summary, bibliographical and historical notes, exercises
8 first-order logic
8.1representation revisited
8.2syntax and semantics of first-order logic
8.3using first-order logic
8.4knowledge engineering in first-order logic
8.5summary, bibliographical and historical notes, exercises
9 inference in first-order logic
9.1propositional vs. first-order inference
9.2unification and lifting
9.3forward chaining
9.4backward chaining
9.5resolution
9.6summary, bibliographical and historical notes, exercises
10 classical planning
10.1 definition of classical planning
10.2 algorithms for planning as state-space search
10.3 planning graphs
10.4 other classical planning approaches
10.5 analysis of planning approaches
10.6 summary, bibliographical and historical notes, exercises
11 planning and acting in the real world
11.1 time, schedules, and resources
11.2 hierarchical planning
11.3 planning and acting in nondeterministic domains
11.4 multiagent planning
11.5 summary, bibliographical and historical notes, exercises
12 knowledge representation
12.1 ontological engineering
12.2 categories and objects
12.3 events
12.4 mental events and mental objects
12.5 reasoning systems for categories
12.6 reasoning with default information
12.7 the intemet shopping world
12.8 summary, bibliographical and historical notes, exercises
Ⅳ uncertain knowledge and reasoning
13 quantifying uncertainty
13.1 acting under uncertainty
13.2 basic probability notation
13.3 inference using full joint distributions
13.4 independence
13.5 bayes' rule and its use
13.6 the wumpus world revisited
13.7 summary, bibliographical and historical notes, exercises
14 probabilistic reasoning
14.1 representing knowledge in an uncertain domain
14.2 the semantics of bayesian networks
14.3 efficient representation of conditional distributions
14.4 exact inference in bayesian networks
14.5 approximate inference in bayesian networks
14.6 relational and first-order probability models
14.7 other approaches to uncertain reasoning
14.8 summary, bibliographical and historical notes, exercises
15 probabilistic reasoning over time
15.1 time and uncertainty
15.2 inference in temporal models
15.3 hidden markov models
15.4 kalman filters
15.5 dynamic bayesian networks
15.6 keeping track of many objects
15.7 summary, bibliographical and historical notes, exercises
16 making simple decisions
16.1 combining beliefs and desires under uncertainty
16.2 the basis of utility theory
16.3 utility functions
16.4 multiattribute utility functions
16.5 decision networks
16.6 the value of information
16.7 decision-theoretic expert systems
16.8 summary, bibliographical and historical notes, exercises
17 making complex decisions
17.1 sequential decision problems
17.2 value iteration
17.3 policy iteration
17.4 partially observable mdps
17.5 decisions with multiple agents: game theory
17.6 mechanism design
17.7 summary, bibliographical and historical notes, exercises
V learning
18 learning from examples
18.1 forms of learning
18.2 supervised learning
18.3 leaming decision trees
18.4 evaluating and choosing the best hypothesis
18.5 the theory of learning
18.6 regression and classification with linear models
18.7 artificial neural networks
18.8 nonparametric models
18.9 support vector machines
18.10 ensemble learning
18.11 practical machine learning
18.12 summary, bibliographical and historical notes, exercises
19 knowledge in learning
19.1 a logical formulation of learning
19.2 knowledge in learning
19.3 explanation-based learning
19.4 learning using relevance information
19.5 inductive logic programming
19.6 summary, bibliographical and historical notes, exercis
20 learning probabilistic models
20.1 statistical learning
20.2 learning with complete data
20.3 learning with hidden variables: the em algorithm.
20.4 summary, bibliographical and historical notes, exercis
21 reinforcement learning
21. l introduction
21.2 passive reinforcement learning
21.3 active reinforcement learning
21.4 generalization in reinforcement learning
21.5 policy search
21.6 applications of reinforcement learning
21.7 summary, bibliographical and historical notes, exercis
VI communicating, perceiving, and acting
22 natural language processing
22.1 language models
22.2 text classification
22.3 information retrieval
22.4 information extraction
22.5 summary, bibliographical and historical notes, exercis
23 natural language for communication
23.1 phrase structure grammars
23.2 syntactic analysis (parsing)
23.3 augmented grammars and semantic interpretation
23.4 machine translation
23.5 speech recognition
23.6 summary, bibliographical and historical notes, exercis
24 perception
24.1 image formation
24.2 early image-processing operations
24.3 object recognition by appearance
24.4 reconstructing the 3d world
24.5 object recognition from structural information
24.6 using vision
24.7 summary, bibliographical and historical notes, exercises
25 robotics
25.1 introduction
25.2 robot hardware
25.3 robotic perception
25.4 planning to move
25.5 planning uncertain movements
25.6 moving
25.7 robotic software architectures
25.8 application domains
25.9 summary, bibliographical and historical notes, exercises
VII conclusions
26 philosophical foundations
26.1 weak ai: can machines act intelligently?
26.2 strong ai: can machines really think?
26.3 the ethics and risks of developing artificial intelligence
26.4 summary, bibliographical and historical notes, exercises
27 al: the present and future
27.1 agent components
27.2 agent architectures
27.3 are we going in the right direction?
27.4 what if ai does succeed?
a mathematical background
a. 1complexity analysis and o0 notation
a.2 vectors, matrices, and linear algebra
a.3 probability distributions
b notes on languages and algorithms
b.1defining languages with backus-naur form (bnf)
b.2describing algorithms with pseudocode
b.3online help
bibliography
index
・ ・ ・ ・ ・ ・ (收起)第1章 AIGC为何引发关注
1.1　《太空歌剧院》带来的冲击和影响　002
1.2　“生成”所引发的创意性工作革新　004
1.3　内容生成方式进入新阶段　005
1.4　AIGC在绘画领域率先破圈　006
1.5　典型的AIGC模型　008
海外模型　008
国内模型　010
第2章 模型即服务时代的到来
2.1　模型即服务的历史进程　017
早期人工智能在曲折中探索　017
深度学习引发关注　019
2.2　典型的深度学习网络　021
生成对抗网络　021
Transformer　024
2.3　大公司探索之路　026
DeepMind　026
OpenAI　027
2.4　基础模型普及的关键节点　028
基础模型的能力与服务　028
曾经热议的云，今后的基础模型　031
基础模型的通用性　033
2.5　人工智能的未来何在　033
人工智能逐步接近人类的思考模式　033
未来人工智能的发展特点　035
第3章 ChatGPT引发的潮流与思考
3.1　ChatGPT会成为人工智能的拐点吗　038
引发全球关注的ChatGPT　038
ChatGPT潜在的应用领域　039
3.2　ChatGPT能力大揭秘　040
3.3　ChatGPT是OpenAI对大模型的坚定实践　042
3.4　ChatGPT的局限性及其引发的思考　043
技术创新性与工程创新性　043
知识局限性　044
盈利与成本之间的平衡　044
应用落地所面临的困境　045
法律合规与应用抵制　045
网络安全风险　046
能耗挑战　047
3.5　ChatGPT引发的思考　048
如何看待人类创新与机器创新　048
ChatGPT在哪些方面值得我们学习　049
3.6?GPT-4未来已来，奇点时刻该如何面对　049
多模态　050
提示工程的价值　050
安全隐忧　050
第4章 大模型驱动的人工智能绘画“创作”
4.1　AI绘画的先驱――AARON　053
4.2　人工智能绘画的原理　054
神经网络是如何模仿人类思考的　054
如何让神经网络画一幅画　055
4.3　人工智能学习如何画一只猫　057
教会你的神经网络认识“猫咪”　057
人工智能真的画出了猫咪　058
4.4　DALL-E的初次尝试与突破　059
4.5　人工智能绘画的技术创新点　061
CLIP实现跨模态创新，打造图文匹配　061
用Diffusion加速AIGC落地普及　063
Diffusion模型为AIGC写下的注脚　064
Stable Diffusion岂止于开源　065
AIGC进一步降低模型的使用门槛　066
4.6　使AIGC绘画技术成熟的重要因素　068
提示词的重要性　068
算力资源的关键支撑　071
第5章 人类的创新能力会被AIGC替代吗
5.1　艺术创作会被AIGC取代吗　073
用户的猎奇与创作者的抵触　073
AIGC不会取代艺术创作工作　074
使用AIGC，需要具备什么能力　077
AIGC是直接消费品还是工具　078
5.2　创作者如何通过AIGC获得更大的收益　080
如何将AIGC应用于创作　080
创意工作者的收益探索　084
未来人工智能创作艺术的5个层次　085
5.3　AIGC――你的“达・芬奇”　089
内容输出的“平民化”　089
大众与艺术家“直连”　090
实时互动和精准化构建的“即时满足”　091
社区与共创的“想象力”　092
基于生成全新内容的平台　093
5.4　抓住AIGC的机遇　094
AIGC时代，做“短信”还是“微信”　094
AIGC的发展仍无法脱离技术周期　097
第6章 开源成就行业发展的未来
6.1　开源让我们站在巨人的肩膀上　099
6.2　开源成为引爆AIGC的导火索　099
6.3　大模型的开源之路　101
第7章 AIGC与商业化
7.1　AIGC商业化的3个阶段　106
感知冲击――尝鲜阶段　107
认知领悟――协助阶段　107
新生态链――原创阶段　108
7.2　AI领域的企业发展　108
平台型企业　109
应用型企业　111
现有产品的智能化　112
7.3　当下典型的AIGC变现手段　114
按照计算量收费　114
按照输出图像数量收费　114
软件按月付费　115
模型训练费　116
7.4　AIGC商业模式的困境　116
AIGC Inside的商业化并不容易　116
难以建立技术壁垒　117
探索自主的大模型及应用　118
第8章 AIGC的典型应用
8.1　文字创作　121
主要特点　121
典型应用　122
8.2　音频生成　126
主要特点　126
典型应用　127
8.3　视频生成　131
主要特点　131
典型应用　131
8.4　3D模型生成　135
主要特点　135
典型应用　135
8.5　编写代码　137
主要特点　137
典型应用　137
8.6　游戏创作开发　139
主要特点　139
典型应用　140
8.7　绘画产品　143
典型绘画产品的AIGC应用　144
AIGC绘画与NFT结合　148
8.8　建筑设计　149
将AIGC融入建筑设计　149
用AIGC实现装修设计　151
8.9　其他应用　152
DIY设计　152
儿童创意实现　155
内容营销　156
诊疗与心灵慰藉　156
第9章 AIGC的不足与挑战
9.1　技术与产业方面的不足与挑战　159
细节仍需打磨　159
成本问题　161
输出结果不一致　162
大模型到大应用的挑战　162
通用性较差　163
9.2　在确权方面面临的挑战　163
AIGC作品的著作权归属　163
著作权争议的潜在解决方案　165
法律监管出现争议　166
企业态度不统一　166
伦理与安全风险　167
第10章 业界和学界的专家洞察
10.1　AIGC可扩展潜力巨大，可能掀起新一波创新创业浪潮　170
从AIGC到AIGS，“服务规模化的个性化”时代到来　170
从科技圈体验到全民使用，AI首次成功破圈　171
OpenAI已经成功探索出AI领域科技创新落地的新模式　173
中国需要自主大模型，也有可能探索出自己的创新　175
10.2　AIGC火热的背后，需要深度思考治理难题　179
破解“克林格里奇困境”，要靠更敏捷的治理思路　179
加强对弱势群体的保护，平台应该做好“守门人”　180
AIGC内容知识产权还没有定论，但业界已有基本共识　182
探索人工智能领域“数据合作”新范式　183
10.3　AIGC火热背后的业界冷思考：中国AI行业的未来发展，需要有自己的思路　185
ChatGPT的流畅对话来源于预训练大模型　185
“AI幻觉”仍是阻碍产业发展的难题　186
大规模预训练技术仍处于早期探索阶段，人工智能公司还需耐心打磨　188
在AIGC技术浪潮中，一些行业将迎来全新挑战　189
中国AI行业的未来发展，需要有自己的思考和思路　191
・ ・ ・ ・ ・ ・ (收起)第 1章　从数学建模到人工智能 1
1.1　数学建模　1
1.1.1　数学建模与人工智能　1
1.1.2　数学建模中的常见问题　4
1.2　人工智能下的数学　12
1.2.1　统计量　12
1.2.2　矩阵概念及运算　13
1.2.3　概率论与数理统计　16
1.2.4　高等数学――导数、微分、不定积分、定积分　19
第　2章 Python快速入门　24
2.1　安装Python　24
2.1.1　Python安装步骤　24
2.1.2　IDE的选择　27
2.2　Python基本操作　28
2.2.1　第 一个小程序　28
2.2.2　注释与格式化输出　28
2.2.3　列表、元组、字典　34
2.2.4　条件语句与循环语句　37
2.2.5　break、continue、pass　40
2.3　Python高级操作　41
2.3.1　lambda　41
2.3.2　map　42
2.3.3　filter　43
第3章　Python科学计算库NumPy　45
3.1　NumPy简介与安装　45
3.1.1　NumPy简介　45
3.1.2　NumPy安装　45
3.2　基本操作　46
3.2.1　初识NumPy　46
3.2.2　NumPy数组类型　47
3.2.3　NumPy创建数组　49
3.2.4　索引与切片　56
3.2.5　矩阵合并与分割　60
3.2.6　矩阵运算与线性代数　62
3.2.7　NumPy的广播机制　69
3.2.8　NumPy统计函数　71
3.2.9　NumPy排序、搜索　75
3.2.10　NumPy数据的保存　79
第4章　常用科学计算模块快速入门　80
4.1　Pandas科学计算库　80
4.1.1　初识Pandas　80
4.1.2　Pandas基本操作　82
4.2　Matplotlib可视化图库　94
4.2.1　初识Matplotlib　94
4.2.2　Matplotlib基本操作　96
4.2.3　Matplotlib绘图案例　98
4.3　SciPy科学计算库　100
4.3.1　初识SciPy　100
4.3.2　SciPy基本操作　101
4.3.3　SciPy图像处理案例　103
第5章　Python网络爬虫　106
5.1　爬虫基础　106
5.1.1　初识爬虫　106
5.1.2　网络爬虫的算法　107
5.2　爬虫入门实战　107
5.2.1　调用API　107
5.2.2　爬虫实战　112
5.3　爬虫进阶―高效率爬虫　113
5.3.1　多进程　113
5.3.2　多线程　114
5.3.3　协程　115
5.3.4　小结　116
第6章　Python数据存储　117
6.1　关系型数据库MySQL　117
6.1.1　初识MySQL　117
6.1.2　Python操作MySQL　118
6.2　NoSQL之MongoDB　120
6.2.1　初识NoSQL　120
6.2.2　Python操作MongoDB　121
6.3　本章小结　123
6.3.1　数据库基本理论　123
6.3.2　数据库结合　124
6.3.3　结束语　125
第7章　Python数据分析　126
7.1　数据获取　126
7.1.1　从键盘获取数据　126
7.1.2　文件的读取与写入　127
7.1.3　Pandas读写操作　129
7.2　数据分析案例　130
7.2.1　普查数据统计分析案例　130
7.2.2　小结　139
第8章　自然语言处理　140
8.1　Jieba分词基础　140
8.1.1　Jieba中文分词　140
8.1.2　Jieba分词的3种模式　141
8.1.3　标注词性与添加定义词　142
8.2　关键词提取　144
8.2.1　TF-IDF关键词提取　145
8.2.2　TextRank关键词提取　147
8.3　word2vec介绍　150
8.3.1　word2vec基础原理简介　150
8.3.2　word2vec训练模型　153
8.3.3　基于gensim的word2vec实战　154
第9章　从回归分析到算法基础　160
9.1　回归分析简介　160
9.1.1　“回归”一词的来源　160
9.1.2　回归与相关　161
9.1.3　回归模型的划分与应用　161
9.2　线性回归分析实战　162
9.2.1　线性回归的建立与求解　162
9.2.2　Python求解回归模型案例　164
9.2.3　检验、预测与控制　166
第　10章 从K-Means聚类看算法调参　171
10.1　K-Means基本概述　171
10.1.1　K-Means简介　171
10.1.2　目标函数　171
10.1.3　算法流程　172
10.1.4　算法优缺点分析　174
10.2　K-Means实战　174
第　11章 从决策树看算法升级　180
11.1　决策树基本简介　180
11.2　经典算法介绍　181
11.2.1　信息熵　181
11.2.2　信息增益　182
11.2.3　信息增益率184
11.2.4　基尼系数　185
11.2.5　小结　185
11.3　决策树实战　186
11.3.1　决策树回归　186
11.3.2　决策树的分类　188
第　12章 从朴素贝叶斯看算法多变　193
12.1　朴素贝叶斯简介　193
12.1.1　认识朴素贝叶斯　193
12.1.2　朴素贝叶斯分类的工作过程　194
12.1.3　朴素贝叶斯算法的优缺点　195
12.2　3种朴素贝叶斯实战　195
第　13章 从推荐系统看算法场景　200
13.1　推荐系统简介　200
13.1.1　推荐系统的发展　200
13.1.2　协同过滤　201
13.2　基于文本的推荐　208
13.2.1　标签与知识图谱推荐案例　209
13.2.2　小结　217
第　14章 从TensorFlow开启深度学习之旅　218
14.1　初识TensorFlow　218
14.1.1　什么是TensorFlow　218
14.1.2　安装TensorFlow　219
14.1.3　TensorFlow基本概念与原理　219
14.2　TensorFlow数据结构　221
14.2.1　阶　221
14.2.2　形状　221
14.2.3　数据类型　221
14.3　生成数据十二法　222
14.3.1　生成Tensor　222
14.3.2　生成序列　224
14.3.3　生成随机数　225
14.4　TensorFlow实战　225
参考文献　230
・ ・ ・ ・ ・ ・ (收起)目 录

第1章 人工智能初印象 1
1.1 什么是人工智能？ 1
1.1.1 定义AI 2
1.1.2 理解数据是智能算法的核心 3
1.1.3 把算法看作“菜谱” 4
1.2 人工智能简史 6
1.3 问题类型与问题解决范式 7
1.4 人工智能概念的直观印象 9
1.5 人工智能算法的用途 13
1.5.1 农业：植物种植优化 13
1.5.2 银行业：欺诈检测 14
1.5.3 网络安全：攻击检测与处理 14
1.5.4 医疗：智能诊断 14
1.5.5 物流：路径规划与优化 15
1.5.6 通信：网络优化 16
1.5.7 游戏：主体创造 16
1.5.8 艺术：创造杰出作品 17
1.6 本章小结 17
第2章 搜索算法基础 21
2.1 什么是规划与搜索？ 21
2.2 计算成本：需要智能算法的原因 23
2.3 适合用搜索算法的问题 24
2.4 表示状态：创建一个表示问题空间与解的框架 26
2.4.1 图：表示搜索问题与解 28
2.4.2 用具体的数据结构表示图 28
2.4.3 树：表示搜索结果的具体结构 29
2.5 无知搜索：盲目地找寻解 31
2.6 广度优先搜索：先看广度，再看深度 33
2.7 深度优先搜索：先看深度，再看广度 39
2.8 盲目搜索算法的用例 45
2.9 可选：关于图的类别 46
2.10 可选：其他表示图的方法 47
2.10.1 关联矩阵 47
2.10.2 邻接表 48
2.11 本章小结 48
第3章 智能搜索 51
3.1 定义启发式方法：设计有根据的猜测 51
3.2 知情搜索：在指导下寻求解决方案 54
3.2.1 A*搜索 54
3.2.2 知情搜索算法的用例 61
3.3 对抗性搜索：在不断变化的环境中寻找解决方案 62
3.3.1 一个简单的对抗性问题 62
3.3.2 最小-最大搜索：模拟行动并选择最好的未来 63
3.3.3 启发式 64
3.3.4 阿尔法-贝塔剪枝：仅探索合理的路径 72
3.3.5 对抗搜索算法的典型案例 75
3.4 本章小结 75
第4章 进化算法 77
4.1 什么是进化？ 77
4.2 适合用进化算法的问题 80
4.3 遗传算法的生命周期 84
4.4 对解空间进行编码 86
4.5 创建解决方案种群 89
4.6 衡量种群中个体的适应度 91
4.7 根据适应度得分筛选亲本 93
4.8 由亲本繁殖个体 96
4.8.1 单点交叉：从每个亲本继承一部分 97
4.8.2 两点交叉：从每个亲本继承多个部分 98
4.8.3 均匀交叉：从每个亲本继承多个部分 98
4.8.4 二进制编码的位串突变 100
4.8.5 二进制编码的翻转位突变 101
4.9 繁衍下一代 101
4.9.1 探索与挖掘 102
4.9.2 停止条件 102
4.10 遗传算法的参数配置 104
4.11 进化算法的用例 105
4.12 本章小结 105
第5章 进化算法(高级篇) 107
5.1 进化算法的生命周期 107
5.2 其他筛选策略 109
5.2.1 排序筛选法：均分赛场 109
5.2.2 联赛筛选法：分组对抗 110
5.2.3 精英筛选法：只选最好的 111
5.3 实值编码：处理真实数值 111
5.3.1 实值编码的核心概念 112
5.3.2 算术交叉：数学化繁殖 113
5.3.3 边界突变 113
5.3.4 算术突变 114
5.4 顺序编码：处理序列 114
5.4.1 适应度函数的重要性 116
5.4.2 顺序编码的核心概念 116
5.4.3 顺序突变：适用于顺序编码 116
5.5 树编码：处理层次结构 117
5.5.1 树编码的核心概念 118
5.5.2 树交叉：继承树的分支 119
5.5.3 节点突变：更改节点的值 120
5.6 常见进化算法 120
5.6.1 遗传编程 120
5.6.2 进化编程 121
5.7 进化算法术语表 121
5.8 进化算法的其他用例 121
5.9 本章小结 122
第6章 群体智能：蚁群优化 125
6.1 什么是群体智能？ 125
6.2 适合用蚁群优化算法的问题 127
6.3 状态表达：如何表达蚂蚁和路径？ 130
6.4 蚁群优化算法的生命周期 134
6.4.1 初始化信息素印迹 135
6.4.2 建立蚂蚁种群 136
6.4.3 为蚂蚁选择下一个访问项目 138
6.4.4 更新信息素印迹 145
6.4.5 更新最佳解决方案 149
6.4.6 确定终止条件 150
6.5 蚁群优化算法的用例 152
6.6 本章小结 153
第7章 群体智能：粒子群优化 155
7.1 什么是粒子群优化？ 155
7.2 优化问题：略偏技术性的观点 157
7.3 适合用粒子群优化算法的问题 160
7.4 状态表达：粒子是什么样的？ 162
7.5 粒子群优化的生命周期 163
7.5.1 初始化粒子群 164
7.5.2 计算粒子的适应度 166
7.5.3 更新粒子的位置 169
7.5.4 确定终止条件 180
7.6 粒子群优化算法的用例 181
7.7 本章小结 183
第8章 机器学习 185
8.1 什么是机器学习？ 185
8.2 适合用机器学习的问题 187
8.2.1 监督学习 188
8.2.2 非监督学习 188
8.2.3 强化学习 188
8.3 机器学习的工作流程 188
8.3.1 收集和理解数据：掌握数据背景 189
8.3.2 准备数据：清洗和整理 191
8.3.3 训练模型：用线性回归预测 196
8.3.4 测试模型：验证模型精度 205
8.3.5 提高准确性 208
8.4 分类问题：决策树 210
8.4.1 分类问题：非此即彼 210
8.4.2 决策树的基础知识 211
8.4.3 训练决策树 213
8.4.4 用决策树对实例进行分类 223
8.5 其他常见的机器学习算法 226
8.6 机器学习算法的用例 227
8.7 本章小结 228
第9章 人工神经网络 231
9.1 什么是人工神经网络？ 231
9.2 感知器：表征神经元 234
9.3 定义人工神经网络 237
9.4 前向传播：使用训练好的人工神经网络 243
9.5 反向传播：训练人工神经网络 250
9.6 激活函数一览 259
9.7 设计人工神经网络 260
9.8 人工神经网络的类型和用例 263
9.8.1 卷积神经网络 263
9.8.2 递归神经网络 264
9.8.3 生成对抗网络 264
9.9 本章小结 266
第10章 基于Q-learning的强化学习 269
10.1 什么是强化学习？ 269
10.2 适合用强化学习的问题 272
10.3 强化学习的生命周期 273
10.3.1 模拟与数据：环境重现 274
10.3.2 使用Q-learning模拟训练 278
10.3.3 模拟并测试Q表 287
10.3.4 衡量训练的性能 287
10.3.5 无模型和基于模型的学习 288
10.4 强化学习的深度学习方法 289
10.5 强化学习的用例 289
10.5.1 机器人技术 290
10.5.2 推荐引擎 290
10.5.3 金融贸易 290
10.5.4 电子游戏 291
10.6 本章小结 291
・ ・ ・ ・ ・ ・ (收起)第一部分引言
第　1章人工智能概述　2
1.0　引言　2
1.0.1　人工智能的定义　3
1.0.2　思维是什么？智能是什么？　3
1.1　图灵测试　5
1.1.1　图灵测试的定义　6
1.1.2　图灵测试的争议和批评　8
1.2　强人工智能与弱人工智能　9
1.3　启发法　11
1.3.1　长方体的对角线：解决一个相对简单但相关的
问题　11
1.3.2　水壶问题：向后倒推　12
1.4　识别适用人工智能来求解的问题　13
1.5　应用和方法　15
1.5.1　搜索算法和拼图　16
1.5.2　二人博弈　18
1.5.3　自动推理　18
1.5.4　产生式规则和专家系统　19
1.5.5　细胞自动机　20
1.5.6　神经计算　21
1.5.7　遗传算法　23
1.5.8　知识表示　23
1.5.9　不确定性推理　24
1.6　人工智能的早期历史　25
1.7　人工智能的近期历史到现在　29
1.7.1　博弈　29
1.7.2　专家系统　30
1.7.3　神经计算　31
1.7.4　进化计算　31
1.7.5　自然语言处理　32
1.7.6　生物信息学　34
1.8　新千年人工智能的发展　34
1.9　本章小结　36
第二部分　基础知识
第　2章盲目搜索　46
2.0　简介：智能系统中的搜索　46
2.1　状态空间图　47
2.2　生成与测试范式　49
2.2.1　回溯　50
2.2.2　贪婪算法　54
2.2.3　旅行销售员问题　56
2.3　盲目搜索算法　58
2.3.1　深度优先搜索　58
2.3.2　广度优先搜索　60
2.4　盲目搜索算法的实现和比较　63
2.4.1　实现深度优先搜索　63
2.4.2　实现广度优先搜索　65
2.4.3　问题求解性能的测量指标　65
2.4.4　DFS和BFS的比较　66
2.5　本章小结　68
第3章　知情搜索　74
3.0　引言　74
3.1　启发法　76
3.2　知情搜索（第一部分）――找到任何解　81
3.2.1　爬山法　81
3.2.2　最陡爬坡法　82
3.3　最佳优先搜索　84
3.4　集束搜索　87
3.5　搜索算法的其他指标　89
3.6　知情搜索（第二部分）――找到最佳解　90
3.6.1　分支定界法　90
3.6.2　使用低估值的分支定界法　95
3.6.3　采用动态规划的分支定界法　98
3.6.4　A*搜索　99
3.7　知情搜索（第三部分）―高级搜索算法　100
3.7.1　约束满足搜索　100
3.7.2　与或树　101
3.7.3　双向搜索　102
3.8　本章小结　104
第4章　博弈中的搜索　109
4.0　引言　109
4.1　博弈树和极小化极大评估　110
4.1.1　启发式评估　112
4.1.2　博弈树的极小化极大评估　112
4.2　具有α-剪枝的极小化极大算法　115
4.3　极小化极大算法的变体和改进　120
4.3.1　负极大值算法　120
4.3.2　渐进深化法　122
4.3.3　启发式续篇和地平线效应　122
4.4　概率游戏和预期极小化极大值算法　123
4.5　博弈理论　125
迭代的囚徒困境　126
4.6　本章小结　127
第5章　人工智能中的逻辑　133
5.0　引言　133
5.1　逻辑和表示　134
5.2　命题逻辑　135
5.2.1　命题逻辑―基础　136
5.2.2　命题逻辑中的论证　140
5.2.3　证明命题逻辑论证有效的第二种方法　141
5.3　谓词逻辑――简要介绍　143
5.3.1　谓词逻辑中的合一　144
5.3.2　谓词逻辑中的反演　146
5.3.3　将谓词表达式转换为子句形式　148
5.4　其他一些逻辑　151
5.4.1　二阶逻辑　151
5.4.2　非单调逻辑　152
5.4.3　模糊逻辑　152
5.4.4　模态逻辑　153
5.5　本章小结　153
第6章　知识表示　160
6.0　引言　160
6.1　图形草图和人类视窗　163
6.2　图和哥尼斯堡桥问题　166
6.3　搜索树　167
6.4　表示方法的选择　169
6.5　产生式系统　172
6.6　面向对象　172
6.7　框架法　173
6.8　脚本和概念依赖系统　176
6.9　语义网络　179
6.10　关联　181
6.11　新近的方法　182
6.11.1　概念地图　182
6.11.2　概念图　184
6.11.3　Baecker的工作　184
6.12　智能体：智能或其他　185
6.12.1　智能体的一些历史　188
6.12.2　当代智能体　189
6.12.3　语义网　191
6.12.4　IBM眼中的未来世界　191
6.12.5　作者的观点　192
6.13　本章小结　192
第7章　产生式系统　199
7.0　引言　199
7.1　背景　199
7.2　基本示例　202
7.3　CARBUYER系统　204
7.4　产生式系统和推导方法　208
7.4.1　冲突消解　211
7.4.2　正向链接　213
7.4.3　反向链接　214
7.5　产生式系统和细胞自动机　219
7.6　随机过程与马尔可夫链　221
7.7　本章小结　222
第三部分　基于知识的系统
第8章　人工智能中的不确定性　228
8.0　引言　228
8.1　模糊集　229
8.2　模糊逻辑　231
8.3　模糊推理　232
8.4　概率理论和不确定性　235
8.5　本章小结　239
第9章　专家系统　242
9.0　引言　242
9.1　背景　242
9.2　专家系统的特点　249
9.3　知识工程　250
9.4　知识获取　252
9.5　经典的专家系统　254
9.5.1　DENDRAL　254
9.5.2　MYCIN　255
9.5.3　EMYCIN　258
9.5.4　PROSPECTOR　259
9.5.5　模糊知识和贝叶斯规则　261
9.6　提高效率的方法　262
9.6.1　守护规则　262
9.6.2　Rete算法　263
9.7　基于案例的推理　264
9.8　更多最新的专家系统　269
9.8.1　改善就业匹配系统　269
9.8.2　振动故障诊断的专家系统　270
9.8.3　自动牙科识别　270
9.8.4　更多采用案例推理的专家系统　271
9.9　本章小结　271
第　10章机器学习第一部分　277
10.0　引言　277
10.1　机器学习：简要概述　277
10.2　机器学习系统中反馈的作用　279
10.3　归纳学习　280
10.4　利用决策树进行学习　282
10.5　适用于决策树的问题　283
10.6　熵　284
10.7　使用ID3构建决策树　285
10.8　其余问题　287
10.9　本章小结　288
第　11章机器学习第二部分：神经网络　291
11.0　引言　291
11.1　人工神经网络的研究　292
11.2　麦卡洛克-皮茨网络　294
11.3　感知器学习规则　295
11.4　增量规则　303
11.5　反向传播　308
11.6　实现关注点　313
11.6.1　模式分析　316
11.6.2　训练方法　317
11.7　离散型霍普菲尔德网络　318
11.8　应用领域　323
11.9　本章小结　330
第　12章受到自然启发的搜索　337
12.0　引言　337
12.1　模拟退火　338
12.2　遗传算法　341
12.3　遗传规划　349
12.4　禁忌搜索　353
12.5　蚂蚁聚居地优化　356
12.6　本章小结　359
第四部分　高级专题
第　13章自然语言处理　368
13.0　引言　368
13.1　概述：语言的问题和可能性　368
13.2　自然语言处理的历史　371
13.2.1　基础期（20世纪40年代和50年代）　371
13.2.2　符号与随机方法（1957―1970）　372
13.2.3　４种范式（1970―1983）　372
13.2.4　经验主义和有限状态模型（1983―1993）　373
13.2.5　大融合（1994―1999）　373
13.2.6　机器学习的兴起（2000―2008）　374
13.3　句法和形式语法　374
13.3.1　语法类型　374
13.3.2　句法解析：CYK算法　379
13.4　语义分析和扩展语法　380
13.4.1　转换语法　381
13.4.2　系统语法　381
13.4.3　格语法　382
13.4.4　语义语法　383
13.4.5　Schank系统　383
13.5　NLP中的统计方法　387
13.5.1　统计解析　387
13.5.2　机器翻译（回顾）和IBM的Candide系统　388
13.5.3　词义消歧　389
13.6　统计NLP的概率模型　390
13.6.1　隐马尔可夫模型　390
13.6.2　维特比算法　391
13.7　统计NLP语言数据集　392
13.7.1　宾夕法尼亚州树库项目　392
13.7.2　WordNet　394
13.7.3　NLP中的隐喻模型　394
13.8　应用：信息提取和问答系统　396
13.8.1　问答系统　396
13.8.2　信息提取　401
13.9　现在和未来的研究（基于CHARNIAK的工作）　401
13.10　语音理解　402
13.11　语音理解技术的应用　405
13.12　本章小结　410
第　14章自动规划　417
14.0　引言　417
14.1　规划问题　418
14.1.1　规划术语　418
14.1.2　规划应用示例　419
14.2　一段简短的历史和一个著名的问题　424
14.3　规划方法　426
14.3.1　规划即搜索　426
14.3.2　部分有序规划　430
14.3.3　分级规划　432
14.3.4　基于案例的规划　433
14.3.5　规划方法集锦　434
14.4　早期规划系统　435
14.4.1　STRIPS　435
14.4.2　NOAH　436
14.4.3　NONLIN　436
14.5　更多现代规划系统　437
14.5.1　O-PLAN　438
14.5.2　Graphplan　439
14.5.3　规划系统集锦　441
14.5.4　学习系统的规划方法　441
14.5.5　SCIBox自动规划器　442
14.6　本章小结　444
第五部分　现在和未来
第　15章机器人技术　452
15.0　引言　452
15.1　历史：服务人类、仿效人类、增强人类和替代人类　455
15.1.1　早期机械机器人　455
15.1.2　电影与文学中的机器人　458
15.1.3　20世纪早期的机器人　458
15.2　技术问题　464
15.2.1　机器人的组件　464
15.2.2　运动　467
15.2.3　点机器人的路径规划　468
15.2.4　移动机器人运动学　469
15.3　应用：21世纪的机器人　471
15.4　本章小结　479
第　16章高级计算机博弈　482
16.0　引言　482
16.1　跳棋：从塞缪尔到舍弗尔　483
16.1.1　在跳棋博弈中用于机器学习的启发式方法　486
16.1.2　填鸭式学习与概括　488
16.1.3　签名表评估和棋谱学习　489
16.1.4　含有奇诺克程序的世界跳棋锦标赛　490
16.1.5　彻底解决跳棋游戏　491
16.2　国际象棋：人工智能的“果蝇”　494
16.2.1　计算机国际象棋的历史背景　495
16.2.2　编程方法　496
16.2.3　超越地平线效应　505
16.2.4　DeepThought和DeepBlue与特级大师的比赛（1988―1995年）　505
16.3　计算机国际象棋对人工智能的贡献　507
16.3.1　在机器中的搜索　507
16.3.2　在搜索方面，人与机器的对比　508
16.3.3　启发式、知识和问题求解　509
16.3.4　蛮力：知识vs.搜索；表现vs.能力　510
16.3.5　残局数据库和并行计算　511
16.3.6　本书第一作者的贡献　514
16.4　其他博弈　514
16.4.1　奥赛罗　515
16.4.2　西洋双陆棋　516
16.4.3　桥牌　518
16.4.4　扑克　519
16.5　围棋：人工智能的“新果蝇”？　520
16.6　本章小结　523
第　17章大事记　532
17.0　引言　532
17.1　提纲挈领――概述　532
17.2　普罗米修斯归来　534
17.3　提纲挈领――介绍人工智能的成果　535
17.4　IBM的沃森-危险边缘挑战赛　539
17.5　21世纪的人工智能　543
17.6　本章小结　545
附录A　CLIPS示例：专家系统外壳　548
附录B　用于隐马尔可夫链的维特比算法的实现（由HarunIftikhar提供）　552
附录C　对计算机国际象棋的贡献：令人惊叹的WalterShawnBrowne　555
附录D　应用程序和数据　559
附录E　部分练习的答案　560
・ ・ ・ ・ ・ ・ (收起)第0章 绪论
第1章 数学基础
1.1 导数
1.2 概率论基础
1.3 矩阵基础
习题
第2章 搜索
引言
2.1 搜索问题的定义
2.2 搜索算法基础
2.3 盲目搜索
2.4 启发式搜索
2.5 局部搜索
2.6 对抗搜索
本章总结
历史回顾
习题
第3章 机器学习
引言
3.1 监督学习的概念
3.2 数据集与损失函数
3.3 泛化
3.4 过拟合与欠拟合
3.5 创建数据集
3.6 无监督学习与半监督学习
本章总结
历史回顾
习题
参考文献
第4章 线性回归
引言
4.1 线性回归
4.2 优化方法
4.3 二分类问题
4.4 多分类问题
4.5 岭回归
4.6 套索回归
4.7 支持向量机算法
本章总结
习题
第5章 决策树模型
引言
5.1 决策树的例子
5.2 决策树的定义
5.3 决策树的训练算法
本章总结
历史回顾
习题
参考文献
第6章 集成学习
引言
6.1 集成学习
6.2 随机森林
6.3 梯度提升
本章总结
历史回顾
习题
参考文献
第7章 神经网络初步
引言
7.1 深度线性网络
7.2 非线性神经网络
7.3 反向传播计算导数
7.4 优化器
7.5 权值初始化
7.6 权值衰减
7.7 权值共享与卷积
7.8 循环神经网络
本章总结
历史回顾
习题
第8章 计算机视觉
引言
8.1 什么是计算机视觉
8.2 图像的形成
8.3 线性滤波器
8.4 边缘检测
8.5 立体视觉
8.6 卷积神经网络
8.7 物体检测
8.8 语义分割
本章总结
历史回顾
习题
参考文献
第9章 自然语言处理
引言
9.1 语言模型
9.2 向量语义
9.3 基于神经网络的语言模型处理
9.4 基于神经网络的机器翻译
9.5 语言模型预训练
本章总结
历史回顾
习题
第10章 马尔可夫决策过程与强化学习
引言
10.1 马尔可夫链
10.2 马尔可夫决策过程
10.3 马尔可夫决策过程的求解算法及分析
10.4 强化学习
本章总结
历史回顾
参考文献
习题
附录A 数学基础
A.1 导数
A.2 概率
A.3 矩阵
・ ・ ・ ・ ・ ・ (收起)第1 章 人工智能与数学基础..........１
1.1 什么是人工智能............................ 2
1.2 人工智能的发展 ............................ 2
1.3 人工智能的应用 ............................ 4
1.4 学习人工智能需要哪些知识 ............. 5
1.5 为什么要学习数学 ......................... 7
1.6 本书包括的数学知识 ...................... 8
第 1 篇
基础篇................................................................. 9
第 2 章 高等数学基础 ................. １0
2.1 函数.......................................... 11
2.2 极限..........................................13
2.3 无穷小与无穷大...........................17
2.4 连续性与导数..............................19
2.5 偏导数...................................... 24
2.6 方向导数................................... 27
2.7 梯度......................................... 29
2.8 综合实例―梯度下降法求函数的最小值.......................................31
2.9 高手点拨................................... 35
2.10 习题....................................... 38
第 3 章 微积分..............................39
3.1 微积分的基本思想 ....................... 40
3.2 微积分的解释..............................41
3.3 定积分...................................... 42
3.4 定积分的性质............................. 44
3.5 牛顿―莱布尼茨公式.................... 45
3.6 综合实例―Python 中常用的定积分求解方法................................... 49
3.7 高手点拨....................................51
3.8 习题 ........................................ 52
第 4 章 泰勒公式与拉格朗日乘子法..............................53
4.1 泰勒公式出发点.......................... 54
4.2 一点一世界................................ 54
4.3 阶数和阶乘的作用....................... 59
4.4 麦克劳林展开式的应用..................61
4.5 拉格朗日乘子法.......................... 63
4.6 求解拉格朗日乘子法.................... 64
4.7 综合实例―编程模拟实现 sinx 的n 阶泰勒多项式并验证结果.................. 67
4.8 高手点拨 ................................... 68
4.9 习题 ......................................... 68
第2 篇
核心篇............................................................... 69
第 5 章 将研究对象形式化―线性代数基础 ..........................70
5.1 向量..........................................71
5.2 矩阵......................................... 73
5.3 矩阵和向量的创建....................... 77
5.4 特殊的矩阵................................ 85
5.5 矩阵基本操作..............................91
5.6 转置矩阵和逆矩阵....................... 96
5.7 行列式..................................... 101
5.8 矩阵的秩..................................104
5.9 内积与正交...............................108
5.10 综合实例―线性代数在实际问题中的应用 ....................................... 114
5.11 高手点拨 ................................ 121
5.12 习题......................................126
第 6 章 从数据中提取重要信息―特征值与矩阵分解..........127
6.1 特征值与特征向量 .....................128
6.2 特征空间..................................133
6.3 特征值分解...............................133
6.4 SVD 解决的问题.......................135
6.5 奇异值分解（SVD）..................136
6.6 综合实例 1―利用 SVD 对图像进行压缩 .......................................140
6.7 综合实例 2―利用 SVD 推荐商品 .......................................143
6.8 高手点拨..................................150
6.9 习题 .......................................154
第 7 章 描述统计规律 1―概率论基础................................155
7.1 随机事件及其概率 ......................156
7.2 条件概率.................................. 161
7.3 独立性.....................................162
7.4 随机变量..................................165
7.5 二维随机变量............................173
7.6 边缘分布..................................177
7.7 综合实例―概率的应用.............180
7.8 高手点拨.................................. 181
7.9 习题........................................184
第 8 章 描述统计规律 2―随机变量与概率估计........................185
8.1 随机变量的数字特征 ..................186
8.2 大数定律和中心极限定理.............193
8.3 数理统计基本概念......................199
8.4 最大似然估计........................... 203
8.5 最大后验估计........................... 206
8.6 综合实例 1―贝叶斯用户满意度预测 ...................................... 209
8.7 综合实例 2―最大似然法求解模型参数 .......................................217
8.8 高手点拨 ................................ 222
8.9 习题 ....................................... 224
第 3 篇
提高篇............................................................. 225
第 9 章 随机变量的几种分布...... 226
9.1 正态分布 ................................ 227
9.2 二项分布................................. 240
9.3 泊松分布................................. 250
9.4 均匀分布..................................261
9.5 卡方分布................................. 266
9.6 Beta 分布 .............................. 273
9.7 综合实例―估算棒球运动员的击中率 ...................................... 283
9.8 高手点拨 ................................ 285
9.9 习题 ...................................... 286
第 10 章 数据的空间变换―核函数变换............................. 287
10.1 相关知识简介 ......................... 288
10.2 核函数的引入 ......................... 290
10.3 核函数实例............................ 290
10.4 常用核函数.............................291
10.5 核函数的选择......................... 294
10.6 SVM 原理 ............................ 295
10.7 非线性 SVM 与核函数的引入.... 305
10.8 综合实例―利用 SVM 构建分类
问题......................................310
10.9 高手点拨................................315
10.10 习题 ................................... 322
第 11 章 熵与激活函数 .............. 323
11.1 熵和信息熵............................ 324
11.2 激活函数 ............................... 328
11.3 综合案例―分类算法中信息熵的应用...................................... 339
11.4 高手点拨 ................................341
11.5 习题 ..................................... 342
第4 篇
应用篇............................................................. 333
第 12 章 假设检验 ..................... 344
12.1 假设检验的基本概念................. 345
12.2 Z 检验 ...................................351
12.3 t 检验 ................................... 353
12.4 卡方检验............................... 358
12.5 假设检验中的两类错误 ..............361
12.6 综合实例 1―体检数据中的假设检验问题..................................... 363
12.7 综合实例 2―种族对求职是否有影响..................................... 369
12.8 高手点拨............................... 372
12.9 习题..................................... 374
13 章 相关分析...................... 375
13.1 相关分析概述.......................... 376
13.2 皮尔森相关系数....................... 378
13.3 相关系数的计算与假设检验........ 379
13.4 斯皮尔曼等级相关.................... 385
13.5 肯德尔系数............................. 392
13.6 质量相关分析.......................... 396
13.7 品质相关分析.......................... 400
13.8 偏相关与复相关....................... 403
13.9 综合实例―相关系数计算........ 405
13.10 高手点拨.............................. 407
13.11 习题..................................... 408
第 14 章 回归分析......................409
14.1 回归分析概述...........................410
14.2 回归方程推导及应用..................412
14.3 回归直线拟合优度.....................416
14.4 线性回归的模型检验..................417
14.5 利用回归直线进行估计和预测......419
14.6 多元与曲线回归问题..................421
14.7 Python 工具包....................... 426
14.8 综合实例―个人医疗保费预测任务...................................... 432
14.9 高手点拨................................ 444
14.10 习题..................................... 446
第 15 章 方差分析......................449
15.1 方差分析概述.......................... 448
15.2 方差的比较............................. 450
15.3 方差分析.................................451
15.4 综合实例―连锁餐饮用户评级分析...................................... 460
15.5 高手点拨................................ 464
15.6 习题...................................... 466
第 16 章 聚类分析......................469
16.1 聚类分析概述.......................... 468
16.2 层次聚类................................ 470
16.3 K-Means 聚类...................... 484
16.4 DBSCAN 聚类....................... 494
16.5 综合实例―聚类分析.............. 499
16.6 高手点拨.................................512
16.7 习题.......................................512
第 17 章 贝叶斯分析....................513
17.1 贝叶斯分析概述........................514
17.2 MCMC 概述.......................... 520
17.3 MCMC 采样 ......................... 525
17.4 Gibbs 采样........................... 529
17.5 综合实例―利用 PyMC3 实现随机模拟样本分布......................... 532
17.6 高手点拨............................... 539
17.7 习题..................................... 540
・ ・ ・ ・ ・ ・ (收起)出版者的话
专家指导委员会
译者序
序
第2版序
致谢
第1章 基于知识的智能系统概述
1.1 智能机器概述
1.2 人工智能发展历史
1.3 小结
复习题
参考文献
第2章 基于规则的专家系统
2.1 知识概述
2.2 规则是一种知识表达技术
2.3 专家系统研发团队中的主要参与者
2.4 基于规则的专家系统的结构
2.5 专家系统的基本特征
2.6 前向链接和后向链接推理技术
2.7 实例
2.8 冲突的解决方案
2.9 基于规则的专家系统的优缺点
2.10 小结
复习题
参考文献
第3章 基于规则的专家系统的不确定管理
3.1 不确定性简介
3.2 基本概率论
3.3 贝叶斯推理
3.4 FORECAST：贝叶斯证据累积
3.5 贝叶斯方法的偏差
3.6 确定因子理论和证据推理
3.7 FORECAST：确定因子的应用
3.8 贝叶斯推理和确定因子的比较
3.9 小结
复习题
参考文献
第4章 模糊专家系统
4.1 概述
4.2 模糊集
4.3 语言变量和模糊限制语
4.4 模糊集的操作
4.5 模糊规则
4.6 模糊推理
4.7 建立模糊专家系统
4.8 小结
复习题
参考文献
参考书目
第5章 基于框架的专家系统
5.1 框架简介
5.2 作为知识表达技术的框架
5.3 基于框架系统中的继承
5.4 方法和守护程序
5.5 框架和规则的交互
5.6 基于框架的专家系统实例：Buy Smart
5.7 小结
复习题
参考文献
参考书目
第6章 人工神经网络
6.1 人脑工作机制简介
6.2 作为简单计算元素的神经元
6.3 感知器
6.4 多层神经网络
6.5 多层神经网络的加速学习
6.6 Hopfield神经网络
6.7 双向相关记忆
6.8 自组织神经网络
6.9 小结
复习题
参考文献
第7章 进化计算
7.1 进化是智能的吗
7.2 模拟自然进化
7.3 遗传算法
7.4 遗传算法如何工作
7.5 实例：用遗传算法来维护计划
7.6 进化策略
7.7 遗传编程
7.8 小结
复习题
参考文献
参考书目
第8章 混合智能系统
8.1 概述
8.2 神经专家系统
8.3 神经模糊系统
8.4 ANFIS：自适应性神经模糊推理系统
8.5 进化神经网络
8.6 模糊进化系统
8.7 小结
复习题
参考文献
第9章 知识工程和数据挖掘
9.1 知识工程简介
9.2 专家系统可以解决的问题
9.3 模糊专家系统可以解决的问题
9.4 神经网络可以解决的问题
9.5 遗传算法可以解决的问题
9.6 混合智能系统可以解决的问题
9.7 数据挖掘和知识发现
9.8 小结
复习题
参考文献
术语表
附录 人工智能工具和厂商
・ ・ ・ ・ ・ ・ (收起)推荐序 情感机器离我们有多远
李德毅
中国人工智能学会理事长
中国工程院院士
引言 人类思维与人工智能的未来
第一部分 情感，另一种人类思维方式
01 坠入爱河
我们的每一种主要的“情感状态”都是因为激活了一些资源,同时关闭了另外一些资源――大脑的运行方式由此改变了。如果每次这种改变都会激活更多其他资源,那么最终将导致资源的大规模“级联”。
“爱”的手提箱
精神奥秘之海
情绪与情感
本能机，让婴儿情感更好捉摸
云认知型思维
成人精神活动的6大层级
情感“瀑布”
思维维度的多样性
02 依恋与目标
人类的一些目标是天生的本能, 是由我们的基因决定的; 另一些目标则是通过“尝试和错误”学习，来实现已有目标的次级目标；而高层次目标, 则是由一种特殊的机器体系形成的。这种特殊的机器体系是指我们对身为依恋对象的父母、朋友或亲人的价值观的继承, 这些价值观积极地响应了我们的需要, 在我们体内产生了“自我意识”情感。
沙子游戏 ：从叉子到勺子
依恋与目标
印刻者
依恋性学习模式
学习、快乐和信用赋能
价值体系的塑造
幼儿和动物的依恋
谁是我们的印刻者
自律，构建目标一致的自我模型
公众印刻
03 从疼痛到煎熬
任何疼痛都会激活“摆脱疼痛”这一目标, 而这个目标的实现将有助于目标本身的消失。然而, 如果疼痛强烈而又持久, 就会激发其他大脑资源, 进而压制其他目标。如果这种情况级联式地爆发下去, 那么大脑
的大部分区域都会被痛苦占据 。可见，在处于某种精神状态中时, 我们也就失去了“选择的自由”。
疼痛之中
煎熬，大脑失去自由选择权
苦难机器
致命性的痛苦
心智“批评家”：纠正性警告、外显抑制和内隐束缚
弗洛伊德的思维“三明治”
控制我们的情绪和性情
情感利用
第二部分 洞悉思维本质，创建情感机器的6大维度
04 意识
“意识”是一个“手提箱”式词汇, 它被我们用来表示许多不同的精神活动。而这些精神活动并没有单一的原因或起源, 当然, 这也正是为何人们发现很难“理解意识是什么”的原因所在。心灵的每个阶段都
是一个同时存在多种可能性的剧场，而意识则将这些可能性相互比较, 通过注意力的强化和抑制作用, 选择一些可能性、抑制其他可能性。
什么是意识
打开意识的手提箱
A 脑、B 脑和C 脑
对意识的高估
如何开启意识
主观体验，心理学中的无解难题
自我模型与自我意识
笛卡儿剧场
不间断的意识流
05 精神活动层级
我们的大脑是如何产生如此多新事物和新想法的? 资源可以分为 6 种不同的层级――本能反应、后天反应、沉思、反思、自我反思、自我意识，以对想法和思维机制进行衡量。每一个层级模式都建立在下一个
层级模式的基础之上, 最上层的模式表现的是人们的最高理想和个人目标。
本能反应
后天反应
沉思
反思
自我反思
自我意识
想象
想象场景
预测机器
06 常识
我们所做的许多常识性事情和常识性推理，要比吸引更多关注、获得令人敬仰的专业技能复杂得多。你所“看到”的并不完全来自视觉, 还来自这些视觉引发的其他知识。常识性知识的主体, 即人类需要在文明
世界中相处下去会涉及的许多问题, 如我们所说的常识性问题, 目标是什么以及它们是如何实现的，我们平常是如何通过类比来推理, 以及我们如何猜测哪一项知识等，可能与我们的决策方式相关联。
什么是常识
常识性知识和推理
意图和目标
差异的“幻想”世界
在不确定性中，作出最优决策
相似推理
正面经验和负面经验的博弈
07 思维
我们几乎从未认识到常识性思考所创造的奇迹。人人都有不同的思维方式。在众多的兴趣爱好当中, 是什么选择了我们下一步将要思考的内容？每一种兴趣又会持续多久? 批评家又是如何选择所使用思维方式
的？事实上，工作被隐藏在“脑后”, 仍在继续运行。
是什么选择了我们思考的主题
批评家-选择器模型，思维跳跃之源
情感化思维
人类的19大思维方式
6 大批评家，选择最合适的思维方式
先有情感，还是先有行为
庞加莱无意识过程的4 大阶段
认知语境下的批评家选择
人类心理学的核心问题
08 智能
每个物种的个体智力都会从愚笨逐渐发展到优秀, 即使最高级的人类思维也本应从这个过程发展而来。我们可以通过多种视角来观察事物，我们拥有快速进行视角转换的方法、拥有高效学习的特殊方式、拥有获
得相关知识的有效方式并可以不断扩大思维方式的范围、拥有表征事物的多种方式。正是这种多样性造就了人类思维的多功能。
预估距离
平行类比
高效率学习的奥秘
信用赋能
创造力和天才
记忆与表征结构
表征等级
09 自我
是什么让人类变得独一无二? 任何其他动物都无法像人类这样拥有各种各样的人格。其中一些性格是与生俱来的, 而另一些性格则来自个人经验, 但在每一种情况中, 我们都具有各异
的特征。每当想尝试理解自己时, 我们都可能需要采取多种角度来看待自己。
多样的“自我”
人格特质
“自我”观念的魅力
为什么我们喜欢快乐
情感描述难题
发现感觉中独特的“质”
人类思维的组织方式
复杂的尊严
人类智能的3大时间跨度
致 谢
注 释
译者后记
・ ・ ・ ・ ・ ・ (收起)第 1 章 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 本书面向的读者 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7
1.2 深度学习的历史趋势 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.1 神经网络的众多名称和命运变迁 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.2 与日俱增的数据量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2.3 与日俱增的模型规模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13
1.2.4 与日俱增的精度、复杂度和对现实世界的冲击 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
第 1 部分 应用数学与机器学习基础
第 2 章 线性代数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.1 标量、向量、矩阵和张量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.2 矩阵和向量相乘. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21
2.3 单位矩阵和逆矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4 线性相关和生成子空间 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 范数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24
2.6 特殊类型的矩阵和向量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.7 特征分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.8 奇异值分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.9 Moore-Penrose 伪逆 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.10 迹运算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.11 行列式 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.12 实例：主成分分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30
第 3 章 概率与信息论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34
3.1 为什么要使用概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2 随机变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.3 概率分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.3.1 离散型变量和概率质量函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.3.2 连续型变量和概率密度函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4 边缘概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.5 条件概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.6 条件概率的链式法则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.7 独立性和条件独立性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.8 期望、方差和协方差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.9 常用概率分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.9.1 Bernoulli 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.2 Multinoulli 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.3 高斯分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.4 指数分布和 Laplace 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.9.5 Dirac 分布和经验分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.9.6 分布的混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.10 常用函数的有用性质. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
3.11 贝叶斯规则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.12 连续型变量的技术细节 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.13 信息论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.14 结构化概率模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
第 4 章 数值计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.1 上溢和下溢 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.2 病态条件 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3 基于梯度的优化方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3.1 梯度之上：Jacobian 和 Hessian 矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4 约束优化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.5 实例：线性最小二乘 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
第 5 章 机器学习基础. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63
5.1 学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.1 任务 T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.2 性能度量 P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1.3 经验 E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1.4 示例：线性回归 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
5.2 容量、过拟合和欠拟合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2.1 没有免费午餐定理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.2.2 正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3 超参数和验证集. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .76
5.3.1 交叉验证 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.4 估计、偏差和方差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .77
5.4.1 点估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.4.2 偏差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.4.3 方差和标准差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.4.4 权衡偏差和方差以最小化均方误差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.4.5 一致性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.5 最大似然估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.5.1 条件对数似然和均方误差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84
5.5.2 最大似然的性质 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.6 贝叶斯统计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.6.1 最大后验 (MAP) 估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.7 监督学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.1 概率监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.2 支持向量机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.3 其他简单的监督学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .90
5.8 无监督学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .91
5.8.1 主成分分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.8.2 k-均值聚类 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94
5.9 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.10 构建机器学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.11 促使深度学习发展的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.11.1 维数灾难 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.11.2 局部不变性和平滑正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.11.3 流形学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
第 2 部分 深度网络：现代实践
第 6 章 深度前馈网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.1 实例：学习 XOR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.2 基于梯度的学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.2.1 代价函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.2.2 输出单元 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.3 隐藏单元. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .119
6.3.1 整流线性单元及其扩展 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3.2 logistic sigmoid 与双曲正切函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.3.3 其他隐藏单元 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.4 架构设计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123
6.4.1 万能近似性质和深度. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123
6.4.2 其他架构上的考虑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126
6.5 反向传播和其他的微分算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126
6.5.1 计算图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.5.2 微积分中的链式法则. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .128
6.5.3 递归地使用链式法则来实现反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
6.5.4 全连接 MLP 中的反向传播计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
6.5.5 符号到符号的导数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .131
6.5.6 一般化的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .133
6.5.7 实例：用于 MLP 训练的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135
6.5.8 复杂化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5.9 深度学习界以外的微分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5.10 高阶微分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6.6 历史小记. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .139
第 7 章 深度学习中的正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.1 参数范数惩罚 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.1.1 L2 参数正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.1.2 L1 正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.2 作为约束的范数惩罚. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .146
7.3 正则化和欠约束问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .147
7.4 数据集增强 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.5 噪声鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
7.5.1 向输出目标注入噪声. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .150
7.6 半监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.7 多任务学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.8 提前终止. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .151
7.9 参数绑定和参数共享. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .156
7.9.1 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
7.10 稀疏表示. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .157
7.11 Bagging 和其他集成方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .158
7.12 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .159
7.13 对抗训练. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .165
7.14 切面距离、正切传播和流形正切分类器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
第 8 章 深度模型中的优化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .169
8.1 学习和纯优化有什么不同 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.1 经验风险最小化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.2 代理损失函数和提前终止 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.1.3 批量算法和小批量算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.2 神经网络优化中的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.1 病态 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.2 局部极小值 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.2.3 高原、鞍点和其他平坦区域 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .175
8.2.4 悬崖和梯度爆炸 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.2.5 长期依赖 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.2.6 非精确梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
8.2.7 局部和全局结构间的弱对应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
8.2.8 优化的理论限制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
8.3 基本算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .180
8.3.1 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
8.3.2 动量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
8.3.3 Nesterov 动量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183
8.4 参数初始化策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
8.5 自适应学习率算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
8.5.1 AdaGrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
8.5.2 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.5.3 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
8.5.4 选择正确的优化算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .190
8.6 二阶近似方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.6.1 牛顿法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.6.2 共轭梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.6.3 BFGS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
8.7 优化策略和元算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
8.7.1 批标准化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
8.7.2 坐标下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
8.7.3 Polyak 平均 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
8.7.4 监督预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
8.7.5 设计有助于优化的模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
8.7.6 延拓法和课程学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .199
第 9 章 卷积网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
9.1 卷积运算. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .201
9.2 动机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.3 池化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
9.4 卷积与池化作为一种无限强的先验 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
9.5 基本卷积函数的变体. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .211
9.6 结构化输出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
9.7 数据类型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .219
9.8 高效的卷积算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
9.9 随机或无监督的特征. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .220
9.10 卷积网络的神经科学基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
9.11 卷积网络与深度学习的历史 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
第 10 章 序列建模：循环和递归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
10.1 展开计算图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
10.2 循环神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .230
10.2.1 导师驱动过程和输出循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
10.2.2 计算循环神经网络的梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
10.2.3 作为有向图模型的循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
10.2.4 基于上下文的 RNN 序列建模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
10.3 双向 RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
10.4 基于编码 - 解码的序列到序列架构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
10.5 深度循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242
10.6 递归神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243
10.7 长期依赖的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
10.8 回声状态网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .245
10.9 渗漏单元和其他多时间尺度的策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
10.9.1 时间维度的跳跃连接. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .247
10.9.2 渗漏单元和一系列不同时间尺度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
10.9.3 删除连接 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10 长短期记忆和其他门控 RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10.1 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10.2 其他门控 RNN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .250
10.11 优化长期依赖. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .251
10.11.1 截断梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
10.11.2 引导信息流的正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
10.12 外显记忆 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
第 11 章 实践方法论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
11.1 性能度量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .256
11.2 默认的基准模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
11.3 决定是否收集更多数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
11.4 选择超参数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
11.4.1 手动调整超参数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .259
11.4.2 自动超参数优化算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .262
11.4.3 网格搜索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
11.4.4 随机搜索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
11.4.5 基于模型的超参数优化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
11.5 调试策略. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .264
11.6 示例：多位数字识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
第 12 章 应用. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .269
12.1 大规模深度学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.1 快速的 CPU 实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.2 GPU 实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.3 大规模的分布式实现. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .271
12.1.4 模型压缩 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
12.1.5 动态结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
12.1.6 深度网络的专用硬件实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
12.2 计算机视觉 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
12.2.1 预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
12.2.2 数据集增强 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
12.3 语音识别. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .278
12.4 自然语言处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .279
12.4.1 n-gram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .280
12.4.2 神经语言模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
12.4.3 高维输出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
12.4.4 结合 n-gram 和神经语言模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
12.4.5 神经机器翻译 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
12.4.6 历史展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
12.5 其他应用. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .290
12.5.1 推荐系统 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
12.5.2 知识表示、推理和回答 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
第 3 部分 深度学习研究
第 13 章 线性因子模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
13.1 概率 PCA 和因子分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
13.2 独立成分分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .298
13.3 慢特征分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
13.4 稀疏编码. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .301
13.5 PCA 的流形解释 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
第 14 章 自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.1 欠完备自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.2 正则自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .307
14.2.1 稀疏自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
14.2.2 去噪自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
14.2.3 惩罚导数作为正则. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .309
14.3 表示能力、层的大小和深度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
14.4 随机编码器和解码器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .310
14.5 去噪自编码器详解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
14.5.1 得分估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.5.2 历史展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.6 使用自编码器学习流形 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.7 收缩自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .317
14.8 预测稀疏分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .319
14.9 自编码器的应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
第 15 章 表示学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
15.1 贪心逐层无监督预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
15.1.1 何时以及为何无监督预训练有效有效 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
15.2 迁移学习和领域自适应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
15.3 半监督解释因果关系. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .329
15.4 分布式表示 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
15.5 得益于深度的指数增益 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
15.6 提供发现潜在原因的线索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
第 16 章 深度学习中的结构化概率模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
16.1 非结构化建模的挑战. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .339
16.2 使用图描述模型结构. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342
16.2.1 有向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
16.2.2 无向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
16.2.3 配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
16.2.4 基于能量的模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .346
16.2.5 分离和 d-分离 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .347
16.2.6 在有向模型和无向模型中转换 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
16.2.7 因子图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
16.3 从图模型中采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.4 结构化建模的优势 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.5 学习依赖关系 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .354
16.6 推断和近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
16.7 结构化概率模型的深度学习方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .355
16.7.1 实例：受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
第 17 章 蒙特卡罗方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
17.1 采样和蒙特卡罗方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.1.1 为什么需要采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.1.2 蒙特卡罗采样的基础. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.2 重要采样. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .360
17.3 马尔可夫链蒙特卡罗方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
17.4 Gibbs 采样. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .365
17.5 不同的峰值之间的混合挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
17.5.1 不同峰值之间通过回火来混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
17.5.2 深度也许会有助于混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
第 18 章 直面配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
18.1 对数似然梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .369
18.2 随机最大似然和对比散度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
18.3 伪似然 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
18.4 得分匹配和比率匹配. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .376
18.5 去噪得分匹配 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378
18.6 噪声对比估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378
18.7 估计配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380
18.7.1 退火重要采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
18.7.2 桥式采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
第 19 章 近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
19.1 把推断视作优化问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .385
19.2 期望最大化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
19.3 最大后验推断和稀疏编码 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
19.4 变分推断和变分学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .389
19.4.1 离散型潜变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
19.4.2 变分法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
19.4.3 连续型潜变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
19.4.4 学习和推断之间的相互作用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
19.5 学成近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397
19.5.1 醒眠算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
19.5.2 学成推断的其他形式. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .398
第 20 章 深度生成模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
20.1 玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
20.2 受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
20.2.1 条件分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
20.2.2 训练受限玻尔兹曼机. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .402
20.3 深度信念网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .402
20.4 深度玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
20.4.1 有趣的性质 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
20.4.2 DBM 均匀场推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
20.4.3 DBM 的参数学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
20.4.4 逐层预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
20.4.5 联合训练深度玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
20.5 实值数据上的玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
20.5.1 Gaussian-Bernoulli RBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
20.5.2 条件协方差的无向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
20.6 卷积玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
20.7 用于结构化或序列输出的玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
20.8 其他玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
20.9 通过随机操作的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
20.9.1 通过离散随机操作的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
20.10 有向生成网络. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422
20.10.1 sigmoid 信念网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
20.10.2 可微生成器网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423
20.10.3 变分自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .425
20.10.4 生成式对抗网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .427
20.10.5 生成矩匹配网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .429
20.10.6 卷积生成网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .430
20.10.7 自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
20.10.8 线性自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .430
20.10.9 神经自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .431
20.10.10 NADE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
20.11 从自编码器采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
20.11.1 与任意去噪自编码器相关的马尔可夫链 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
20.11.2 夹合与条件采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .434
20.11.3 回退训练过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .435
20.12 生成随机网络. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .435
20.12.1 判别性 GSN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
20.13 其他生成方案. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .436
20.14 评估生成模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .437
20.15 结论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
参考文献. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .439
索引 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
・ ・ ・ ・ ・ ・ (收起)译者序　　xiii
前言　　xv
第1章　Python入门　　1
1.1 Python是什么　　1
1.2 Python的安装　　2
1.2.1　Python版本　　2
1.2.2　使用的外部库　　2
1.2.3　Anaconda发行版　　3
1.3 Python解释器　　4
1.3.1　算术计算　　4
1.3.2　数据类型　　5
1.3.3　变量　　5
1.3.4　列表　　6
1.3.5　字典　　7
1.3.6　布尔型　　7
1.3.7　if 语句　　8
1.3.8　for 语句　　8
1.3.9　函数　　9
1.4 Python脚本文件　　9
1.4.1　保存为文件　　9
1.4.2　类　　10
1.5 NumPy　　11
1.5.1　导入NumPy　　11
1.5.2　生成NumPy数组　　12
1.5.3　NumPy 的算术运算　　12
1.5.4　NumPy的N维数组　　13
1.5.5　广播　　14
1.5.6　访问元素　　15
1.6 Matplotlib　　16
1.6.1　绘制简单图形　　16
1.6.2　pyplot 的功能　　17
1.6.3　显示图像　　18
1.7 小结　　19
第2章　感知机　　21
2.1 感知机是什么　　21
2.2 简单逻辑电路　　23
2.2.1　与门　　23
2.2.2　与非门和或门　　23
2.3 感知机的实现　　25
2.3.1　简单的实现　　25
2.3.2　导入权重和偏置　　26
2.3.3　使用权重和偏置的实现　　26
2.4 感知机的局限性　　28
2.4.1　异或门　　28
2.4.2　线性和非线性　　30
2.5 多层感知机　　31
2.5.1　已有门电路的组合　　31
2.5.2　异或门的实现　　33
2.6 从与非门到计算机　　35
2.7 小结　　36
第3章　神经网络　　37
3.1 从感知机到神经网络　　37
3.1.1　神经网络的例子　　37
3.1.2　复习感知机　　38
3.1.3　激活函数登场　　40
3.2 激活函数　　42
3.2.1　sigmoid 函数　　42
3.2.2　阶跃函数的实现　　43
3.2.3　阶跃函数的图形　　44
3.2.4　sigmoid 函数的实现　　45
3.2.5　sigmoid 函数和阶跃函数的比较　　46
3.2.6　非线性函数　　48
3.2.7　ReLU函数　　49
3.3 多维数组的运算　　50
3.3.1　多维数组　　50
3.3.2　矩阵乘法　　51
3.3.3　神经网络的内积　　55
3.4　　3 层神经网络的实现　　56
3.4.1　符号确认　　57
3.4.2　各层间信号传递的实现　　58
3.4.3　代码实现小结　　62
3.5 输出层的设计　　63
3.5.1　恒等函数和softmax 函数　　64
3.5.2　实现softmax 函数时的注意事项　　66
3.5.3　softmax 函数的特征　　67
3.5.4　输出层的神经元数量　　68
3.6 手写数字识别　　69
3.6.1　MNIST数据集　　70
3.6.2　神经网络的推理处理　　73
3.6.3　批处理　　75
3.7 小结　　79
第4章　神经网络的学习　　81
4.1 从数据中学习　　81
4.1.1　数据驱动　　82
4.1.2　训练数据和测试数据　　84
4.2 损失函数　　85
4.2.1　均方误差　　85
4.2.2　交叉熵误差　　87
4.2.3　mini-batch 学习　　88
4.2.4　mini-batch 版交叉熵误差的实现　　91
4.2.5　为何要设定损失函数　　92
4.3 数值微分　　94
4.3.1　导数　　94
4.3.2　数值微分的例子　　96
4.3.3　偏导数　　98
4.4 梯度　　100
4.4.1　梯度法　　102
4.4.2　神经网络的梯度　　106
4.5 学习算法的实现　　109
4.5.1　2 层神经网络的类　　110
4.5.2　mini-batch 的实现　　114
4.5.3　基于测试数据的评价　　116
4.6 小结　　118
第5章　误差反向传播法　　121
5.1 计算图　　121
5.1.1　用计算图求解　　122
5.1.2　局部计算　　124
5.1.3　为何用计算图解题　　125
5.2 链式法则　　126
5.2.1　计算图的反向传播　　127
5.2.2　什么是链式法则　　127
5.2.3　链式法则和计算图　　129
5.3 反向传播　　130
5.3.1　加法节点的反向传播　　130
5.3.2　乘法节点的反向传播　　132
5.3.3　苹果的例子　　133
5.4 简单层的实现　　135
5.4.1　乘法层的实现　　135
5.4.2　加法层的实现　　137
5.5 激活函数层的实现　　139
5.5.1　ReLU层　　139
5.5.2　Sigmoid 层　　141
5.6 AffineSoftmax层的实现　　144
5.6.1　Affine层　　144
5.6.2　批版本的Affine层　　148
5.6.3　Softmax-with-Loss 层　　150
5.7 误差反向传播法的实现　　154
5.7.1　神经网络学习的全貌图　　154
5.7.2　对应误差反向传播法的神经网络的实现　　155
5.7.3　误差反向传播法的梯度确认　　158
5.7.4　使用误差反向传播法的学习　　159
5.8 小结　　161
第6章　与学习相关的技巧　　163
6.1 参数的更新　　163
6.1.1　探险家的故事　　164
6.1.2　SGD　　164
6.1.3　SGD的缺点　　166
6.1.4　Momentum　　168
6.1.5　AdaGrad　　170
6.1.6　Adam　　172
6.1.7　使用哪种更新方法呢　　174
6.1.8　基于MNIST数据集的更新方法的比较　　175
6.2 权重的初始值　　176
6.2.1　可以将权重初始值设为0 吗　　176
6.2.2　隐藏层的激活值的分布　　177
6.2.3　ReLU的权重初始值　　181
6.2.4　基于MNIST数据集的权重初始值的比较　　183
6.3 Batch Normalization　　184
6.3.1　Batch Normalization 的算法　　184
6.3.2　Batch Normalization 的评估　　186
6.4 正则化　　188
6.4.1　过拟合　　189
6.4.2　权值衰减　　191
6.4.3　Dropout　　192
6.5 超参数的验证　　195
6.5.1　验证数据　　195
6.5.2　超参数的最优化　　196
6.5.3　超参数最优化的实现　　198
6.6 小结　　200
第7章　卷积神经网络　　201
7.1 整体结构　　201
7.2 卷积层　　202
7.2.1　全连接层存在的问题　　203
7.2.2　卷积运算　　203
7.2.3　填充　　206
7.2.4　步幅　　207
7.2.5　3 维数据的卷积运算　　209
7.2.6　结合方块思考　　211
7.2.7　批处理　　213
7.3 池化层　　214
7.4 卷积层和池化层的实现　　216
7.4.1　4 维数组　　216
7.4.2　基于im2col 的展开　　217
7.4.3　卷积层的实现　　219
7.4.4　池化层的实现　　222
7.5 CNN的实现　　224
7.6 CNN的可视化　　228
7.6.1　第1 层权重的可视化　　228
7.6.2　基于分层结构的信息提取　　230
7.7 具有代表性的CNN　　231
7.7.1　LeNet　　231
7.7.2　AlexNet　　232
7.8 小结　　233
第8章　深度学习　　235
8.1 加深网络　　235
8.1.1　向更深的网络出发　　235
8.1.2　进一步提高识别精度　　238
8.1.3　加深层的动机　　240
8.2 深度学习的小历史　　242
8.2.1　ImageNet　　243
8.2.2　VGG　　244
8.2.3　GoogLeNet　　245
8.2.4　ResNet　　246
8.3 深度学习的高速化　　248
8.3.1　需要努力解决的问题　　248
8.3.2　基于GPU的高速化　　249
8.3.3　分布式学习　　250
8.3.4　运算精度的位数缩减　　252
8.4 深度学习的应用案例　　253
8.4.1　物体检测　　253
8.4.2　图像分割　　255
8.4.3　图像标题的生成　　256
8.5 深度学习的未来　　258
8.5.1　图像风格变换　　258
8.5.2　图像的生成　　259
8.5.3　自动驾驶　　261
8.5.4　Deep Q-Network（强化学习）　　262
8.6 小结　　264
附录A　Softmax-with-Loss 层的计算图　　267
A.1 正向传播　　268
A.2 反向传播　　270
A.3 小结　　277
参考文献　　279
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
前言
译者简介
学习环境配置
资源与支持
主要符号表
第 1章　引言　1
1.1　日常生活中的机器学习　2
1.2　机器学习中的关键组件　3
1.2.1　数据　3
1.2.2　模型　4
1.2.3　目标函数　4
1.2.4　优化算法　5
1.3　各种机器学习问题　5
1.3.1　监督学习　5
1.3.2　无监督学习　11
1.3.3　与环境互动　11
1.3.4　强化学习　12
1.4　起源　13
1.5　深度学习的发展　15
1.6　深度学习的成功案例　16
1.7　特点　17
第 2章　预备知识　20
2.1　数据操作　20
2.1.1　入门　21
2.1.2　运算符　22
2.1.3　广播机制　23
2.1.4　索引和切片　24
2.1.5　节省内存　24
2.1.6　转换为其他Python对象　25
2.2　数据预处理　26
2.2.1　读取数据集　26
2.2.2　处理缺失值　26
2.2.3　转换为张量格式　27
2.3　线性代数　27
2.3.1　标量　28
2.3.2　向量　28
2.3.3　矩阵　29
2.3.4　张量　30
2.3.5　张量算法的基本性质　31
2.3.6　降维　32
2.3.7　点积　33
2.3.8　矩阵-向量积　33
2.3.9　矩阵-矩阵乘法　34
2.3.10　范数　35
2.3.11　关于线性代数的更多信息　36
2.4　微积分　37
2.4.1　导数和微分　37
2.4.2　偏导数　40
2.4.3　梯度　41
2.4.4　链式法则　41
2.5　自动微分　42
2.5.1　一个简单的例子　42
2.5.2　非标量变量的反向传播　43
2.5.3　分离计算　43
2.5.4　Python控制流的梯度计算　44
2.6　概率　44
2.6.1　基本概率论　45
2.6.2　处理多个随机变量　48
2.6.3　期望和方差　50
2.7　查阅文档　51
2.7.1　查找模块中的所有函数和类　51
2.7.2　查找特定函数和类的用法　52
第3章　线性神经网络　54
3.1　线性回归　54
3.1.1　线性回归的基本元素　54
3.1.2　向量化加速　57
3.1.3　正态分布与平方损失　58
3.1.4　从线性回归到深度网络　60
3.2　线性回归的从零开始实现　61
3.2.1　生成数据集　62
3.2.2　读取数据集　63
3.2.3　初始化模型参数　63
3.2.4　定义模型　64
3.2.5　定义损失函数　64
3.2.6　定义优化算法　64
3.2.7　训练　64
3.3　线性回归的简洁实现　66
3.3.1　生成数据集　66
3.3.2　读取数据集　66
3.3.3　定义模型　67
3.3.4　初始化模型参数　67
3.3.5　定义损失函数　68
3.3.6　定义优化算法　68
3.3.7　训练　68
3.4　softmax回归　69
3.4.1　分类问题　69
3.4.2　网络架构　70
3.4.3　全连接层的参数开销　70
3.4.4　softmax运算　71
3.4.5　小批量样本的向量化　71
3.4.6　损失函数　72
3.4.7　信息论基础　73
3.4.8　模型预测和评估　74
3.5　图像分类数据集　74
3.5.1　读取数据集　75
3.5.2　读取小批量　76
3.5.3　整合所有组件　76
3.6　softmax回归的从零开始实现　77
3.6.1　初始化模型参数　77
3.6.2　定义softmax操作　78
3.6.3　定义模型　78
3.6.4　定义损失函数　79
3.6.5　分类精度　79
3.6.6　训练　80
3.6.7　预测　82
3.7　softmax回归的简洁实现　83
3.7.1　初始化模型参数　83
3.7.2　重新审视softmax的实现　84
3.7.3　优化算法　84
3.7.4　训练　84
第4章　多层感知机　86
4.1　多层感知机　86
4.1.1　隐藏层　86
4.1.2　激活函数　88
4.2　多层感知机的从零开始实现　92
4.2.1　初始化模型参数　92
4.2.2　激活函数　93
4.2.3　模型　93
4.2.4　损失函数　93
4.2.5　训练　93
4.3　多层感知机的简洁实现　94
模型　94
4.4　模型选择、欠拟合和过拟合　95
4.4.1　训练误差和泛化误差　96
4.4.2　模型选择　97
4.4.3　欠拟合还是过拟合　98
4.4.4　多项式回归　99
4.5　权重衰减　103
4.5.1　范数与权重衰减　103
4.5.2　高维线性回归　104
4.5.3　从零开始实现　104
4.5.4　简洁实现　106
4.6　暂退法　108
4.6.1　重新审视过拟合　108
4.6.2　扰动的稳健性　108
4.6.3　实践中的暂退法　109
4.6.4　从零开始实现　110
4.6.5　简洁实现　111
4.7　前向传播、反向传播和计算图　112
4.7.1　前向传播　113
4.7.2　前向传播计算图　113
4.7.3　反向传播　114
4.7.4　训练神经网络　115
4.8　数值稳定性和模型初始化　115
4.8.1　梯度消失和梯度爆炸　116
4.8.2　参数初始化　117
4.9　环境和分布偏移　119
4.9.1　分布偏移的类型　120
4.9.2　分布偏移示例　121
4.9.3　分布偏移纠正　122
4.9.4　学习问题的分类法　125
4.9.5　机器学习中的公平、责任和透明度　126
4.10　实战Kaggle比赛：预测房价　127
4.10.1　下载和缓存数据集　127
4.10.2　Kaggle　128
4.10.3　访问和读取数据集　129
4.10.4　数据预处理　130
4.10.5　训练　131
4.10.6　K折交叉验证　132
4.10.7　模型选择　133
4.10.8　提交Kaggle预测　133
第5章　深度学习计算　136
5.1　层和块　136
5.1.1　自定义块　138
5.1.2　顺序块　139
5.1.3　在前向传播函数中执行代码　139
5.1.4　效率　140
5.2　参数管理　141
5.2.1　参数访问　141
5.2.2　参数初始化　143
5.2.3　参数绑定　145
5.3　延后初始化　145
实例化网络　146
5.4　自定义层　146
5.4.1　不带参数的层　146
5.4.2　带参数的层　147
5.5　读写文件　148
5.5.1　加载和保存张量　148
5.5.2　加载和保存模型参数　149
5.6　GPU　150
5.6.1　计算设备　151
5.6.2　张量与GPU　152
5.6.3　神经网络与GPU　153
第6章　卷积神经网络　155
6.1　从全连接层到卷积　155
6.1.1　不变性　156
6.1.2　多层感知机的限制　157
6.1.3　卷积　158
6.1.4　“沃尔多在哪里”回顾　158
6.2　图像卷积　159
6.2.1　互相关运算　159
6.2.2　卷积层　161
6.2.3　图像中目标的边缘检测　161
6.2.4　学习卷积核　162
6.2.5　互相关和卷积　162
6.2.6　特征映射和感受野　163
6.3　填充和步幅　164
6.3.1　填充　164
6.3.2　步幅　165
6.4　多输入多输出通道　166
6.4.1　多输入通道　167
6.4.2　多输出通道　167
6.4.3　1×1卷积层　168
6.5　汇聚层　170
6.5.1　最大汇聚和平均汇聚　170
6.5.2　填充和步幅　171
6.5.3　多个通道　172
6.6　卷积神经网络（LeNet）　173
6.6.1　LeNet　173
6.6.2　模型训练　175
第7章　现代卷积神经网络　178
7.1　深度卷积神经网络（AlexNet）　178
7.1.1　学习表征　179
7.1.2　AlexNet　181
7.1.3　读取数据集　183
7.1.4　训练AlexNet　183
7.2　使用块的网络（VGG）　184
7.2.1　VGG块　184
7.2.2　VGG网络　185
7.2.3　训练模型　186
7.3　网络中的网络（NiN）　187
7.3.1　NiN块　187
7.3.2　NiN模型　188
7.3.3　训练模型　189
7.4　含并行连接的网络（GoogLeNet）　190
7.4.1　Inception块　190
7.4.2　GoogLeNet模型　191
7.4.3　训练模型　193
7.5　批量规范化　194
7.5.1　训练深层网络　194
7.5.2　批量规范化层　195
7.5.3　从零实现　196
7.5.4　使用批量规范化层的 LeNet　197
7.5.5　简明实现　198
7.5.6　争议　198
7.6　残差网络（ResNet）　200
7.6.1　函数类　200
7.6.2　残差块　201
7.6.3　ResNet模型　202
7.6.4　训练模型　204
7.7　稠密连接网络（DenseNet）　205
7.7.1　从ResNet到DenseNet　205
7.7.2　稠密块体　206
7.7.3　过渡层　206
7.7.4　DenseNet模型　207
7.7.5　训练模型　207
第8章　循环神经网络　209
8.1　序列模型　209
8.1.1　统计工具　210
8.1.2　训练　212
8.1.3　预测　213
8.2　文本预处理　216
8.2.1　读取数据集　216
8.2.2　词元化　217
8.2.3　词表　217
8.2.4　整合所有功能　219
8.3　语言模型和数据集　219
8.3.1　学习语言模型　220
8.3.2　马尔可夫模型与n元语法　221
8.3.3　自然语言统计　221
8.3.4　读取长序列数据　223
8.4　循环神经网络　226
8.4.1　无隐状态的神经网络　227
8.4.2　有隐状态的循环神经网络　227
8.4.3　基于循环神经网络的字符级语言模型　228
8.4.4　困惑度　229
8.5　循环神经网络的从零开始实现　230
8.5.1　独热编码　231
8.5.2　初始化模型参数　231
8.5.3　循环神经网络模型　232
8.5.4　预测　232
8.5.5　梯度截断　233
8.5.6　训练　234
8.6　循环神经网络的简洁实现　237
8.6.1　定义模型　237
8.6.2　训练与预测　238
8.7　通过时间反向传播　239
8.7.1　循环神经网络的梯度分析　239
8.7.2　通过时间反向传播的细节　241
第9章　现代循环神经网络　244
9.1　门控循环单元（GRU）　244
9.1.1　门控隐状态　245
9.1.2　从零开始实现　247
9.1.3　简洁实现　248
9.2　长短期记忆网络（LSTM）　249
9.2.1　门控记忆元　249
9.2.2　从零开始实现　252
9.2.3　简洁实现　253
9.3　深度循环神经网络　254
9.3.1　函数依赖关系　255
9.3.2　简洁实现　255
9.3.3　训练与预测　255
9.4　双向循环神经网络　256
9.4.1　隐马尔可夫模型中的动态规划　256
9.4.2　双向模型　258
9.4.3　双向循环神经网络的错误应用　259
9.5　机器翻译与数据集　260
9.5.1　下载和预处理数据集　261
9.5.2　词元化　262
9.5.3　词表　263
9.5.4　加载数据集　263
9.5.5　训练模型　264
9.6　编码器-解码器架构　265
9.6.1　编码器　265
9.6.2　解码器　266
9.6.3　合并编码器和解码器　266
9.7　序列到序列学习（seq2seq）　267
9.7.1　编码器　268
9.7.2　解码器　269
9.7.3　损失函数　270
9.7.4　训练　271
9.7.5　预测　272
9.7.6　预测序列的评估　273
9.8　束搜索　275
9.8.1　贪心搜索　275
9.8.2　穷举搜索　276
9.8.3　束搜索　276
第 10章　注意力机制　278
10.1　注意力提示　278
10.1.1　生物学中的注意力提示　279
10.1.2　查询、键和值　280
10.1.3　注意力的可视化　280
10.2　注意力汇聚：Nadaraya-Watson 核回归　281
10.2.1　生成数据集　282
10.2.2　平均汇聚　282
10.2.3　非参数注意力汇聚　283
10.2.4　带参数注意力汇聚　284
10.3　注意力评分函数　287
10.3.1　掩蔽softmax操作　288
10.3.2　加性注意力　289
10.3.3　缩放点积注意力　290
10.4　Bahdanau 注意力　291
10.4.1　模型　291
10.4.2　定义注意力解码器　292
10.4.3　训练　293
10.5　多头注意力　295
10.5.1　模型　295
10.5.2　实现　296
10.6　自注意力和位置编码　298
10.6.1　自注意力　298
10.6.2　比较卷积神经网络、循环神经网络和自注意力　298
10.6.3　位置编码　299
10.7　Transformer　302
10.7.1　模型　302
10.7.2　基于位置的前馈网络　303
10.7.3　残差连接和层规范化　304
10.7.4　编码器　304
10.7.5　解码器　305
10.7.6　训练　307
第 11章　优化算法　311
11.1　优化和深度学习　311
11.1.1　优化的目标　311
11.1.2　深度学习中的优化挑战　312
11.2　凸性　315
11.2.1　定义　315
11.2.2　性质　317
11.2.3　约束　319
11.3　梯度下降　322
11.3.1　一维梯度下降　322
11.3.2　多元梯度下降　324
11.3.3　自适应方法　326
11.4　随机梯度下降　329
11.4.1　随机梯度更新　329
11.4.2　动态学习率　331
11.4.3　凸目标的收敛性分析　332
11.4.4　随机梯度和有限样本　333
11.5　小批量随机梯度下降　334
11.5.1　向量化和缓存　335
11.5.2　小批量　336
11.5.3　读取数据集　337
11.5.4　从零开始实现　337
11.5.5　简洁实现　340
11.6　动量法　341
11.6.1　基础　341
11.6.2　实际实验　345
11.6.3　理论分析　346
11.7　AdaGrad算法　348
11.7.1　稀疏特征和学习率　348
11.7.2　预处理　349
11.7.3　算法　350
11.7.4　从零开始实现　351
11.7.5　简洁实现　352
11.8　RMSProp算法　353
11.8.1　算法　353
11.8.2　从零开始实现　354
11.8.3　简洁实现　355
11.9　Adadelta算法　356
11.9.1　算法　356
11.9.2　实现　356
11.10　Adam算法　358
11.10.1　算法　358
11.10.2　实现　359
11.10.3　Yogi　360
11.11　学习率调度器　361
11.11.1　一个简单的问题　361
11.11.2　学习率调度器　363
11.11.3　策略　364
第 12章　计算性能　369
12.1　编译器和解释器　369
12.1.1　符号式编程　370
12.1.2　混合式编程　371
12.1.3　Sequential的混合式编程　371
12.2　异步计算　372
通过后端异步处理　373
12.3　自动并行　375
12.3.1　基于GPU的并行计算　375
12.3.2　并行计算与通信　376
12.4　硬件　378
12.4.1　计算机　378
12.4.2　内存　379
12.4.3　存储器　380
12.4.4　CPU　381
12.4.5　GPU和其他加速卡　383
12.4.6　网络和总线　385
12.4.7　更多延迟　386
12.5　多GPU训练　388
12.5.1　问题拆分　388
12.5.2　数据并行性　390
12.5.3　简单网络　390
12.5.4　数据同步　391
12.5.5　数据分发　392
12.5.6　训练　392
12.6　多GPU的简洁实现　394
12.6.1　简单网络　394
12.6.2　网络初始化　395
12.6.3　训练　395
12.7　参数服务器　397
12.7.1　数据并行训练　397
12.7.2　环同步（ring
synchronization）　399
12.7.3　多机训练　400
12.7.4　键-值存储　402
第 13章　计算机视觉　404
13.1　图像增广　404
13.1.1　常用的图像增广方法　404
13.1.2　使用图像增广进行训练　408
13.2　微调　410
13.2.1　步骤　410
13.2.2　热狗识别　411
13.3　目标检测和边界框　415
边界框　415
13.4　锚框　417
13.4.1　生成多个锚框　417
13.4.2　交并比（IoU）　419
13.4.3　在训练数据中标注锚框　420
13.4.4　使用非极大值抑制预测
边界框　424
13.5　多尺度目标检测　427
13.5.1　多尺度锚框　427
13.5.2　多尺度检测　429
13.6　目标检测数据集　430
13.6.1　下载数据集　430
13.6.2　读取数据集　431
13.6.3　演示　432
13.7　单发多框检测（SSD）　433
13.7.1　模型　433
13.7.2　训练模型　437
13.7.3　预测目标　439
13.8　区域卷积神经网络（R-CNN）系列　441
13.8.1　R-CNN　441
13.8.2　Fast R-CNN　442
13.8.3　Faster R-CNN　443
13.8.4　Mask R-CNN　444
13.9　语义分割和数据集　445
13.9.1　图像分割和实例分割　445
13.9.2　Pascal VOC2012 语义分割数据集　446
13.10　转置卷积　450
13.10.1　基本操作　450
13.10.2　填充、步幅和多通道　451
13.10.3　与矩阵变换的联系　452
13.11　全卷积网络　453
13.11.1　构建模型　454
13.11.2　初始化转置卷积层　455
13.11.3　读取数据集　456
13.11.4　训练　456
13.11.5　预测　457
13.12　风格迁移　458
13.12.1　方法　459
13.12.2　阅读内容和风格图像　460
13.12.3　预处理和后处理　460
13.12.4　提取图像特征　461
13.12.5　定义损失函数　461
13.12.6　初始化合成图像　463
13.12.7　训练模型　463
13.13　实战 Kaggle竞赛：图像分类（CIFAR-10）　464
13.13.1　获取并组织数据集　465
13.13.2　图像增广 　467
13.13.3　读取数据集　468
13.13.4　定义模型　468
13.13.5　定义训练函数　468
13.13.6　训练和验证模型　469
13.13.7　在Kaggle上对测试集进行分类并提交结果　469
13.14　实战Kaggle竞赛：狗的品种识别（ImageNet Dogs）　470
13.14.1　获取和整理数据集　471
13.14.2　图像增广　472
13.14.3　读取数据集　472
13.14.4　微调预训练模型　473
13.14.5　定义训练函数　473
13.14.6　训练和验证模型　474
13.14.7　对测试集分类并在Kaggle提交结果　475
第 14章　自然语言处理：预训练　476
14.1　词嵌入（word2vec）　477
14.1.1　为何独热向量是一个糟糕的选择　477
14.1.2　自监督的word2vec　477
14.1.3　跳元模型　477
14.1.4　连续词袋模型　478
14.2　近似训练　480
14.2.1　负采样　480
14.2.2　层序softmax　481
14.3　用于预训练词嵌入的数据集　482
14.3.1　读取数据集　482
14.3.2　下采样　483
14.3.3　中心词和上下文词的提取　484
14.3.4　负采样　485
14.3.5　小批量加载训练实例　486
14.3.6　整合代码　487
14.4　预训练word2vec　488
14.4.1　跳元模型　488
14.4.2　训练　489
14.4.3　应用词嵌入　491
14.5　全局向量的词嵌入（GloVe）　491
14.5.1　带全局语料库统计的跳元模型　492
14.5.2　GloVe模型　492
14.5.3　从共现概率比值理解GloVe模型　493
14.6　子词嵌入　494
14.6.1　fastText模型　494
14.6.2　字节对编码　495
14.7　词的相似度和类比任务　497
14.7.1　加载预训练词向量　497
14.7.2　应用预训练词向量　499
14.8　来自Transformer的双向编码器表示（BERT）　500
14.8.1　从上下文无关到上下文敏感　500
14.8.2　从特定于任务到不可知任务　501
14.8.3　BERT：将ELMo与GPT结合起来　501
14.8.4　输入表示　502
14.8.5　预训练任务　504
14.8.6　整合代码　506
14.9　用于预训练BERT的数据集　507
14.9.1　为预训练任务定义辅助函数　508
14.9.2　将文本转换为预训练数据集　509
14.10　预训练BERT　512
14.10.1　预训练BERT　512
14.10.2　用BERT表示文本　514
第 15章　自然语言处理：应用　515
15.1　情感分析及数据集　516
15.1.1　读取数据集　516
15.1.2　预处理数据集　517
15.1.3　创建数据迭代器　517
15.1.4　整合代码　518
15.2　情感分析：使用循环神经网络　518
15.2.1　使用循环神经网络表示单个文本　519
15.2.2　加载预训练的词向量　520
15.2.3　训练和评估模型　520
15.3　情感分析：使用卷积神经网络　521
15.3.1　一维卷积　522
15.3.2　最大时间汇聚层　523
15.3.3　textCNN模型　523
15.4　自然语言推断与数据集　526
15.4.1　自然语言推断　526
15.4.2　斯坦福自然语言推断（SNLI）数据集　527
15.5　自然语言推断：使用注意力　530
15.5.1　模型　530
15.5.2　训练和评估模型　533
15.6　针对序列级和词元级应用微调BERT　535
15.6.1　单文本分类　535
15.6.2　文本对分类或回归　536
15.6.3　文本标注　537
15.6.4　问答　537
15.7　自然语言推断：微调BERT　538
15.7.1　加载预训练的BERT　539
15.7.2　微调BERT的数据集　540
15.7.3　微调BERT　541
附录A　深度学习工具　543
A.1　使用Jupyter记事本　543
A.1.1　在本地编辑和运行代码　543
A.1.2　高级选项　545
A.2　使用Amazon SageMaker　546
A.2.1　注册　547
A.2.2　创建SageMaker实例　547
A.2.3　运行和停止实例　548
A.2.4　更新Notebook　548
A.3　使用Amazon EC2实例　549
A.3.1　创建和运行EC2实例　549
A.3.2　安装CUDA　553
A.3.3　安装库以运行代码　553
A.3.4　远程运行Jupyter记事本　554
A.3.5　关闭未使用的实例　554
A.4　选择服务器和GPU　555
A.4.1　选择服务器　555
A.4.2　选择GPU　556
A.5　为本书做贡献　558
A.5.1　提交微小更改　558
A.5.2　大量文本或代码修改　559
A.5.3　提交主要更改　559
参考文献　562
・ ・ ・ ・ ・ ・ (收起)第1章　什么是深度学习 1
1.1　人工智能、机器学习和深度学习 1
1.1.1　人工智能 2
1.1.2　机器学习 2
1.1.3　从数据中学习规则与表示 3
1.1.4　深度学习之“深度” 5
1.1.5　用三张图理解深度学习的工作原理 7
1.1.6　深度学习已取得的进展 8
1.1.7　不要相信短期炒作 9
1.1.8　人工智能的未来 10
1.2　深度学习之前：机器学习简史 10
1.2.1　概率建模 11
1.2.2　早期神经网络 11
1.2.3　核方法 11
1.2.4　决策树、随机森林和梯度提升机 12
1.2.5　回到神经网络 13
1.2.6　深度学习有何不同 14
1.2.7　机器学习现状 14
1.3　为什么要用深度学习，为什么是现在 16
1.3.1　硬件 17
1.3.2　数据 17
1.3.3　算法 18
1.3.4　新一轮投资热潮 18
1.3.5　深度学习的普及 19
1.3.6　这种趋势会持续下去吗 20
第2章　神经网络的数学基础 21
2.1　初识神经网络 21
2.2　神经网络的数据表示 25
2.2.1　标量（0阶张量） 25
2.2.2　向量（1阶张量） 25
2.2.3　矩阵（2阶张量） 26
2.2.4　3阶张量与更高阶的张量 26
2.2.5　关键属性 26
2.2.6　在NumPy中操作张量 28
2.2.7　数据批量的概念 28
2.2.8　现实世界中的数据张量实例 29
2.2.9　向量数据 29
2.2.10　时间序列数据或序列数据 29
2.2.11　图像数据 30
2.2.12　视频数据 31
2.3　神经网络的“齿轮”：张量运算 31
2.3.1　逐元素运算 32
2.3.2　广播 33
2.3.3　张量积 34
2.3.4　张量变形 36
2.3.5　张量运算的几何解释 37
2.3.6　深度学习的几何解释 40
2.4　神经网络的“引擎”：基于梯度的优化 40
2.4.1　什么是导数 41
2.4.2　张量运算的导数：梯度 42
2.4.3　随机梯度下降 44
2.4.4　链式求导：反向传播算法 46
2.5　回顾第一个例子 51
2.5.1　用TensorFlow 从头开始重新实现第一个例子 52
2.5.2　完成一次训练步骤 54
2.5.3　完整的训练循环 55
2.5.4　评估模型 55
2.6　本章总结 56
第3章　Keras 和TensorFlow 入门 57
3.1　TensorFlow 简介 57
3.2　Keras 简介 58
3.3　Keras 和TensorFlow 简史 59
3.4　建立深度学习工作区 60
3.4.1　Jupyter笔记本：运行深度学习实验的首选方法 60
3.4.2　使用Colaboratory 61
3.5　TensorFlow入门 63
3.5.1　常数张量和变量 64
3.5.2　张量运算：用TensorFlow进行数学运算 66
3.5.3　重温GradientTape API 66
3.5.4　一个端到端的例子：用TensorFlow编写线性分类器 67
3.6　神经网络剖析：了解核心Keras API 71
3.6.1　层：深度学习的基础模块 71
3.6.2　从层到模型 74
3.6.3　编译步骤：配置学习过程 75
3.6.4　选择损失函数 77
3.6.5　理解fit()方法 77
3.6.6　监控验证数据上的损失和指标 78
3.6.7　推断：在训练后使用模型 79
3.7　本章总结 80
第4章　神经网络入门：分类与回归 81
4.1　影评分类：二分类问题示例 82
4.1.1　IMDB 数据集 82
4.1.2　准备数据 83
4.1.3　构建模型 84
4.1.4　验证你的方法 87
4.1.5　利用训练好的模型对新数据进行预测 90
4.1.6　进一步实验 90
4.1.7　小结 90
4.2　新闻分类：多分类问题示例 91
4.2.1　路透社数据集 91
4.2.2　准备数据 92
4.2.3　构建模型 92
4.2.4　验证你的方法 93
4.2.5　对新数据进行预测 96
4.2.6　处理标签和损失的另一种方法 96
4.2.7　拥有足够大的中间层的重要性 96
4.2.8　进一步实验 97
4.2.9　小结 97
4.3　预测房价：标量回归问题示例 97
4.3.1　波士顿房价数据集 98
4.3.2　准备数据 98
4.3.3　构建模型 99
4.3.4　利用K折交叉验证来验证你的方法 99
4.3.5　对新数据进行预测 103
4.3.6　小结 103
4.4　本章总结 104
第5章　机器学习基础 105
5.1　泛化：机器学习的目标 105
5.1.1　欠拟合与过拟合 105
5.1.2　深度学习泛化的本质 110
5.2　评估机器学习模型 115
5.2.1　训练集、验证集和测试集 115
5.2.2　超越基于常识的基准 118
5.2.3　模型评估的注意事项 119
5.3　改进模型拟合 119
5.3.1　调节关键的梯度下降参数 119
5.3.2　利用更好的架构预设 121
5.3.3　提高模型容量 121
5.4　提高泛化能力 123
5.4.1　数据集管理 123
5.4.2　特征工程 124
5.4.3　提前终止 125
5.4.4　模型正则化 125
5.5　本章总结 132
第6章　机器学习的通用工作流程 133
6.1　定义任务 134
6.1.1　定义问题 134
6.1.2　收集数据集 135
6.1.3　理解数据 138
6.1.4　选择衡量成功的指标 139
6.2　开发模型 139
6.2.1　准备数据 139
6.2.2　选择评估方法 140
6.2.3　超越基准 141
6.2.4　扩大模型规模：开发一个过拟合的模型 142
6.2.5　模型正则化与调节超参数 142
6.3　部署模型 143
6.3.1　向利益相关者解释你的工作并设定预期 143
6.3.2　部署推断模型 143
6.3.3　监控模型在真实环境中的性能 146
6.3.4　维护模型 146
6.4　本章总结 147
第7章　深入Keras 148
7.1　Keras 工作流程 148
7.2　构建Keras 模型的不同方法 149
7.2.1　序贯模型 149
7.2.2　函数式API 152
7.2.3　模型子类化 157
7.2.4　混合使用不同的组件 159
7.2.5　用正确的工具完成工作 160
7.3　使用内置的训练循环和评估循环 160
7.3.1　编写自定义指标 161
7.3.2　使用回调函数 162
7.3.3　编写自定义回调函数 164
7.3.4　利用TensorBoard进行监控和可视化 165
7.4　编写自定义的训练循环和评估循环 167
7.4.1　训练与推断 168
7.4.2　指标的低阶用法 169
7.4.3　完整的训练循环和评估循环 169
7.4.4　利用tf.function加快运行速度 171
7.4.5　在fit()中使用自定义训练循环 172
7.5　本章总结 174
第8章　计算机视觉深度学习入门 175
8.1　卷积神经网络入门 176
8.1.1　卷积运算 178
8.1.2　最大汇聚运算 182
8.2　在小型数据集上从头开始训练一个卷积神经网络 184
8.2.1　深度学习对数据量很小的问题的适用性 184
8.2.2　下载数据 185
8.2.3　构建模型 . 187
8.2.4　数据预处理 189
8.2.5　使用数据增强 193
8.3　使用预训练模型 196
8.3.1　使用预训练模型做特征提取 197
8.3.2　微调预训练模型 204
8.4　本章总结 208
第9章　计算机视觉深度学习进阶 209
9.1　三项基本的计算机视觉任务 209
9.2　图像分割示例 210
9.3　现代卷积神经网络架构模式 218
9.3.1　模块化、层次结构和复用 218
9.3.2　残差连接 221
9.3.3　批量规范化 224
9.3.4　深度可分离卷积 226
9.3.5　综合示例：一个类似Xception的迷你模型 227
9.4　解释卷积神经网络学到的内容 229
9.4.1　中间激活值的可视化 230
9.4.2　卷积神经网络滤波器的可视化 235
9.4.3　类激活热力图的可视化 241
9.5　本章总结 246
第10章　深度学习处理时间序列 247
10.1　不同类型的时间序列任务 247
10.2　温度预测示例 248
10.2.1　准备数据 251
10.2.2　基于常识、不使用机器学习的基准 254
10.2.3　基本的机器学习模型 254
10.2.4　一维卷积模型 256
10.2.5　第一个RNN 基准 258
10.3　理解RNN 259
10.4　RNN 的高级用法 265
10.4.1　利用循环dropout 降低过拟合 265
10.4.2　循环层堆叠 268
10.4.3　使用双向RNN 269
10.4.4　进一步实验 271
10.5　本章总结 272
第11章　深度学习处理文本 273
11.1　自然语言处理概述 273
11.2　准备文本数据 274
11.2.1　文本标准化 275
11.2.2　文本拆分（词元化） 276
11.2.3　建立词表索引 277
11.2.4　使用TextVectorization层 278
11.3　表示单词组的两种方法：集合和序列 282
11.3.1　准备IMDB 影评数据 282
11.3.2　将单词作为集合处理：词袋方法 284
11.3.3　将单词作为序列处理：序列模型方法 289
11.4　Transformer架构 298
11.4.1　理解自注意力 298
11.4.2　多头注意力 302
11.4.3　Transformer编码器 303
11.4.4　何时使用序列模型而不是词袋模型 309
11.5　超越文本分类：序列到序列学习 310
11.5.1　机器翻译示例 312
11.5.2　RNN 的序列到序列学习 314
11.5.3　使用Transformer 进行序列到序列学习 318
11.6　本章总结 323
第12章　生成式深度学习 324
12.1　文本生成 325
12.1.1　生成式深度学习用于序列生成的简史 325
12.1.2　如何生成序列数据 326
12.1.3　采样策略的重要性 327
12.1.4　用Keras 实现文本生成 328
12.1.5　带有可变温度采样的文本生成回调函数 331
12.1.6　小结 334
12.2　DeepDream 334
12.2.1　用Keras 实现DeepDream 335
12.2.2　小结 341
12.3　　神经风格迁移 341
12.3.1　内容损失 342
12.3.2　风格损失 342
12.3.3　用Keras 实现神经风格迁移 343
12.3.4　小结 348
12.4　用变分自编码器生成图像 348
12.4.1　从图像潜在空间中采样 348
12.4.2　图像编辑的概念向量 350
12.4.3　变分自编码器 350
12.4.4　用Keras 实现变分自编码器 352
12.4.5　小结 357
12.5　生成式对抗网络入门 358
12.5.1　简要实现流程 359
12.5.2　诸多技巧 360
12.5.3　CelebA 数据集 360
12.5.4　判别器 361
12.5.5　生成器 362
12.5.6　对抗网络 364
12.5.7　小结 366
12.6　本章总结 367
第13章　适合现实世界的最佳实践 368
13.1　将模型性能发挥到极致 368
13.1.1　超参数优化 368
13.1.2　模型集成 375
13.2　加速模型训练 376
13.2.1　使用混合精度加快GPU上的训练速度 377
13.2.2　多GPU训练 380
13.2.3　TPU训练 382
13.3　本章总结 384
第14章　总结 385
14.1　重点概念回顾 385
14.1.1　人工智能的多种方法 385
14.1.2　深度学习在机器学习领域中的特殊之处 386
14.1.3　如何看待深度学习 386
14.1.4　关键的推动技术 387
14.1.5　机器学习的通用工作流程 388
14.1.6　关键网络架构 388
14.1.7　可能性空间 392
14.2　深度学习的局限性 394
14.2.1　将机器学习模型拟人化的风险 394
14.2.2　自动机与智能体 396
14.2.3　局部泛化与极端泛化 397
14.2.4　智能的目的 399
14.2.5　逐步提高泛化能力 400
14.3　如何实现更加通用的人工智能 401
14.3.1　设定正确目标的重要性：捷径法则 401
14.3.2　新目标 402
14.4　实现智能：缺失的内容 403
14.4.1　智能是对抽象类比的敏感性 404
14.4.2　两种抽象 405
14.4.3　深度学习所缺失的那一半 407
14.5　深度学习的未来 408
14.5.1　模型即程序 408
14.5.2　将深度学习与程序合成融合 409
14.5.3　终身学习和模块化子程序复用 411
14.5.4　长期愿景 412
14.6　了解快速发展的领域的最新进展 413
14.6.1　在Kaggle 上练习解决现实世界的问题 413
14.6.2　在arXiv上了解最新进展 414
14.6.3　探索Keras 生态系统 414
14.7　结束语 414
・ ・ ・ ・ ・ ・ (收起)第一部分　深度学习基础
第1章　什么是深度学习　　2
1.1　人工智能、机器学习与深度学习　　2
1.1.1　人工智能　　3
1.1.2　机器学习　　3
1.1.3　从数据中学习表示　　4
1.1.4　深度学习之“深度”　　6
1.1.5　用三张图理解深度学习的工作原理　　7
1.1.6　深度学习已经取得的进展　　9
1.1.7　不要相信短期炒作　　9
1.1.8　人工智能的未来　　10
1.2　深度学习之前：机器学习简史　　11
1.2.1　概率建模　　11
1.2.2　早期神经网络　　11
1.2.3　核方法　　12
1.2.4　决策树、随机森林与梯度提升机　　13
1.2.5　回到神经网络　　14
1.2.6　深度学习有何不同　　14
1.2.7　机器学习现状　　15
1.3　为什么是深度学习，为什么是现在　　15
1.3.1　硬件　　16
1.3.2　数据　　17
1.3.3　算法　　17
1.3.4　新的投资热潮　　17
1.3.5　深度学习的大众化　　18
1.3.6　这种趋势会持续吗　　18
第2章　神经网络的数学基础　　20
2.1　初识神经网络　　20
2.2　神经网络的数据表示　　23
2.2.1　标量（0D张量）　　23
2.2.2　向量（1D张量）　　24
2.2.3　矩阵（2D张量）　　24
2.2.4　3D张量与更高维张量　　24
2.2.5　关键属性　　25
2.2.6　在Numpy中操作张量　　26
2.2.7　数据批量的概念　　27
2.2.8　现实世界中的数据张量　　27
2.2.9　向量数据　　27
2.2.10　时间序列数据或序列数据　　28
2.2.11　图像数据　　28
2.2.12　视频数据　　29
2.3　神经网络的“齿轮”：张量运算　　29
2.3.1　逐元素运算　　30
2.3.2　广播　　31
2.3.3　张量点积　　32
2.3.4　张量变形　　34
2.3.5　张量运算的几何解释　　34
2.3.6　深度学习的几何解释　　35
2.4　神经网络的“引擎”：基于梯度的优化　　36
2.4.1　什么是导数　　37
2.4.2　张量运算的导数：梯度　　38
2.4.3　随机梯度下降　　38
2.4.4　链式求导：反向传播算法　　41
2.5　回顾第一个例子　　41
本章小结　　42
第3章　神经网络入门　　43
3.1　神经网络剖析　　43
3.1.1　层：深度学习的基础组件　　44
3.1.2　模型：层构成的网络　　45
3.1.3　损失函数与优化器：配置学习过程的关键　　45
3.2　Keras简介　　46
3.2.1　Keras、TensorFlow、Theano 和CNTK　　47
3.2.2　使用Keras 开发：概述　　48
3.3　建立深度学习工作站　　49
3.3.1　Jupyter笔记本：运行深度学习实验的首选方法　　49
3.3.2　运行Keras：两种选择　　50
3.3.3　在云端运行深度学习任务：优点和缺点　　50
3.3.4　深度学习的最佳GPU　　50
3.4　电影评论分类：二分类问题　　51
3.4.1　IMDB 数据集　　51
3.4.2　准备数据　　52
3.4.3　构建网络　　52
3.4.4　验证你的方法　　56
3.4.5　使用训练好的网络在新数据上生成预测结果　　59
3.4.6　进一步的实验　　59
3.4.7　小结　　59
3.5　新闻分类：多分类问题　　59
3.5.1　路透社数据集　　60
3.5.2　准备数据　　61
3.5.3　构建网络　　61
3.5.4　验证你的方法　　62
3.5.5　在新数据上生成预测结果　　65
3.5.6　处理标签和损失的另一种方法　　65
3.5.7　中间层维度足够大的重要性　　65
3.5.8　进一步的实验　　66
3.5.9　小结　　66
3.6　预测房价：回归问题　　66
3.6.1　波士顿房价数据集　　67
3.6.2　准备数据　　67
3.6.3　构建网络　　68
3.6.4　利用K折验证来验证你的方法　　68
3.6.5　小结　　72
本章小结　　73
第4章　机器学习基础　　74
4.1　机器学习的四个分支　　74
4.1.1　监督学习　　74
4.1.2　无监督学习　　75
4.1.3　自监督学习　　75
4.1.4　强化学习　　75
4.2　评估机器学习模型　　76
4.2.1　训练集、验证集和测试集　　77
4.2.2　评估模型的注意事项　　80
4.3　数据预处理、特征工程和特征学习　　80
4.3.1　神经网络的数据预处理　　80
4.3.2　特征工程　　81
4.4　过拟合与欠拟合　　83
4.4.1　减小网络大小　　83
4.4.2　添加权重正则化　　85
4.4.3　添加dropout正则化　　87
4.5　机器学习的通用工作流程　　89
4.5.1　定义问题，收集数据集　　89
4.5.2　选择衡量成功的指标　　89
4.5.3　确定评估方法　　90
4.5.4　准备数据　　90
4.5.5　开发比基准更好的模型　　90
4.5.6　扩大模型规模：开发过拟合的模型　　91
4.5.7　模型正则化与调节超参数　　92
本章小结　　92
第二部分　深度学习实践
第5章　深度学习用于计算机视觉　　94
5.1　卷积神经网络简介　　94
5.1.1　卷积运算　　96
5.1.2　最大池化运算　　101
5.2　在小型数据集上从头开始训练一个卷积神经网络　　102
5.2.1　深度学习与小数据问题的相关性　　103
5.2.2　下载数据　　103
5.2.3　构建网络　　106
5.2.4　数据预处理　　107
5.2.5　使用数据增强　　111
5.3　使用预训练的卷积神经网络　　115
5.3.1　特征提取　　116
5.3.2　微调模型　　124
5.3.3　小结　　130
5.4　卷积神经网络的可视化　　130
5.4.1　可视化中间激活　　131
5.4.2　可视化卷积神经网络的过滤器　　136
5.4.3　可视化类激活的热力图　　142
本章小结　　146
第6章　深度学习用于文本和序列　　147
6.1　处理文本数据　　147
6.1.1　单词和字符的one-hot编码　　149
6.1.2　使用词嵌入　　151
6.1.3　整合在一起：从原始文本到词嵌入　　155
6.1.4　小结　　162
6.2　理解循环神经网络　　162
6.2.1　Keras中的循环层　　164
6.2.2　理解LSTM层和GRU层　　168
6.2.3　Keras中一个LSTM的具体例子　　170
6.2.4　小结　　172
6.3　循环神经网络的高级用法　　172
6.3.1　温度预测问题　　172
6.3.2　准备数据　　175
6.3.3　一种基于常识的、非机器学习的基准方法　　177
6.3.4　一种基本的机器学习方法　　178
6.3.5　第一个循环网络基准　　180
6.3.6　使用循环dropout来降低过拟合　　181
6.3.7　循环层堆叠　　182
6.3.8　使用双向RNN　　184
6.3.9　更多尝试　　187
6.3.10　小结　　187
6.4　用卷积神经网络处理序列　　188
6.4.1　理解序列数据的一维卷积　　188
6.4.2　序列数据的一维池化　　189
6.4.3　实现一维卷积神经网络　　189
6.4.4　结合CNN和RNN来处理长序列　　191
6.4.5　小结　　195
本章总结　　195
第7章　高级的深度学习最佳实践　　196
7.1　不用Sequential模型的解决方案：Keras 函数式API　　196
7.1.1　函数式API简介　　199
7.1.2　多输入模型　　200
7.1.3　多输出模型　　202
7.1.4　层组成的有向无环图　　204
7.1.5　共享层权重　　208
7.1.6　将模型作为层　　208
7.1.7　小结　　209
7.2　使用Keras回调函数和TensorBoard来检查并监控深度学习模型　　210
7.2.1　训练过程中将回调函数作用于模型　　210
7.2.2　TensorBoard简介：TensorFlow的可视化框架　　212
7.2.3　小结　　219
7.3　让模型性能发挥到极致　　219
7.3.1　高级架构模式　　219
7.3.2　超参数优化　　222
7.3.3　模型集成　　223
7.3.4　小结　　224
本章总结　　225
第8章　生成式深度学习　　226
8.1　使用LSTM生成文本　　227
8.1.1　生成式循环网络简史　　227
8.1.2　如何生成序列数据　　228
8.1.3　采样策略的重要性　　229
8.1.4　实现字符级的LSTM文本生成　　230
8.1.5　小结　　234
8.2　DeepDream　　235
8.2.1　用Keras实现DeepDream　　236
8.2.2　小结　　241
8.3　神经风格迁移　　241
8.3.1　内容损失　　242
8.3.2　风格损失　　243
8.3.3　用Keras实现神经风格迁移　　243
8.3.4　小结　　249
8.4　用变分自编码器生成图像　　249
8.4.1　从图像的潜在空间中采样　　249
8.4.2　图像编辑的概念向量　　250
8.4.3　变分自编码器　　251
8.4.4　小结　　256
8.5　生成式对抗网络简介　　257
8.5.1　GAN 的简要实现流程　　258
8.5.2　大量技巧　　259
8.5.3　生成器　　260
8.5.4　判别器　　261
8.5.5　对抗网络　　261
8.5.6　如何训练DCGAN　　262
8.5.7　小结　　264
本章总结　　264
第9章　总结　　265
9.1　重点内容回顾　　265
9.1.1　人工智能的各种方法　　265
9.1.2　深度学习在机器学习领域中的特殊之处　　266
9.1.3　如何看待深度学习　　266
9.1.4　关键的推动技术　　267
9.1.5　机器学习的通用工作流程　　268
9.1.6　关键网络架构　　268
9.1.7　可能性空间　　272
9.2　深度学习的局限性　　273
9.2.1　将机器学习模型拟人化的风险　　273
9.2.2　局部泛化与极端泛化　　275
9.2.3　小结　　276
9.3　深度学习的未来　　277
9.3.1　模型即程序　　277
9.3.2　超越反向传播和可微层　　278
9.3.3　自动化机器学习　　279
9.3.4　终身学习与模块化子程序复用　　279
9.3.5　长期愿景　　281
9.4　了解一个快速发展领域的最新进展　　281
9.4.1　使用Kaggle练习解决现实世界的问题　　281
9.4.2　在arXiv阅读最新进展　　282
9.4.3　探索Keras生态系统　　282
9.5　结束语　　282
附录A　在Ubuntu上安装Keras及其依赖　　283
附录B　在EC2 GPU实例上运行Jupyter笔记本　　287
・ ・ ・ ・ ・ ・ (收起)序
前言
常用符号表
第一部分 机器学习基础
第1章 绪论3
1.1人工智能...............................4
1.1.1人工智能的发展历史....................5
1.1.2人工智能的流派.......................7
1.2机器学习...............................7
1.3表示学习...............................8
1.3.1局部表示和分布式表示...................9
1.3.2表示学习...........................11
1.4深度学习...............................11
1.4.1端到端学习..........................12
1.5神经网络...............................13
1.5.1人脑神经网络........................13
1.5.2人工神经网络........................14
1.5.3神经网络的发展历史....................15
1.6本书的知识体系...........................17
1.7常用的深度学习框架.........................18
1.8总结和深入阅读...........................20
第2章 机器学习概述23
2.1基本概念...............................24
2.2机器学习的三个基本要素......................26
2.2.1模型..............................26
2.2.2学习准则...........................27
2.2.3优化算法...........................30
2.3机器学习的简单示例――线性回归.................33
2.3.1参数学习...........................34
2.4偏差-方差分解............................38
2.5机器学习算法的类型.........................41
2.6数据的特征表示...........................43
2.6.1传统的特征学习.......................44
2.6.2深度学习方法........................46
2.7评价指标...............................46
2.8理论和定理..............................49
2.8.1PAC学习理论........................49
2.8.2没有免费午餐定理......................50
2.8.3奥卡姆剃刀原理.......................50
2.8.4丑小鸭定理..........................51
2.8.5归纳偏置...........................51
2.9总结和深入阅读...........................51
第3章 线性模型
3.1线性判别函数和决策边界......................56
3.1.1二分类............................56
3.1.2多分类............................58
3.2Logistic回归.............................59
3.2.1参数学习...........................60
3.3Softmax回归.............................61
3.3.1参数学习...........................62
3.4感知器.................................64
3.4.1参数学习...........................64
3.4.2感知器的收敛性.......................66
3.4.3参数平均感知器.......................67
3.4.4扩展到多分类........................69
3.5支持向量机..............................71
3.5.1参数学习...........................73
3.5.2核函数............................74
3.5.3软间隔............................74
3.6损失函数对比.............................75
3.7总结和深入阅读...........................76
第二部分 基础模型
第4章 前馈神经网络81
4.1神经元.................................82
4.1.1Sigmoid型函数.......................83
4.1.2ReLU函数..........................86
4.1.3Swish函数..........................88
4.1.4GELU函数..........................89
4.1.5Maxout单元.........................89
4.2网络结构...............................90
4.2.1前馈网络...........................90
4.2.2记忆网络...........................90
4.2.3图网络............................90
4.3前馈神经网络.............................91
4.3.1通用近似定理........................93
4.3.2应用到机器学习.......................94
4.3.3参数学习...........................95
4.4反向传播算法.............................95
4.5自动梯度计算.............................98
4.5.1数值微分...........................99
4.5.2符号微分...........................99
4.5.3自动微分...........................100
4.6优化问题...............................103
4.6.1非凸优化问题........................103
4.6.2梯度消失问题........................104
4.7总结和深入阅读...........................104
第5章 卷积神经网络109
5.1卷积..................................110
5.1.1卷积的定义..........................110
5.1.2互相关............................112
5.1.3卷积的变种..........................113
5.1.4卷积的数学性质.......................114
5.2卷积神经网络.............................115
5.2.1用卷积来代替全连接....................115
5.2.2卷积层............................116
5.2.3汇聚层............................118
5.2.4卷积网络的整体结构....................119
5.3参数学习...............................120
5.3.1卷积神经网络的反向传播算法...............120
5.4几种典型的卷积神经网络......................121
5.4.1LeNet-5............................122
5.4.2AlexNet...........................123
5.4.3Inception网络........................125
5.4.4残差网络...........................126
5.5其他卷积方式.............................127
5.5.1转置卷积...........................127
5.5.2空洞卷积...........................129
5.6总结和深入阅读...........................130
第6章 循环神经网络133
6.1给网络增加记忆能力.........................134
6.1.1延时神经网络........................134
6.1.2有外部输入的非线性自回归模型..............134
6.1.3循环神经网络........................135
6.2简单循环网络.............................135
6.2.1循环神经网络的计算能力..................136
6.3应用到机器学习...........................138
6.3.1序列到类别模式.......................138
6.3.2同步的序列到序列模式...................139
6.3.3异步的序列到序列模式...................139
6.4参数学习...............................140
6.4.1随时间反向传播算法....................141
6.4.2实时循环学习算法......................142
6.5长程依赖问题.............................143
6.5.1改进方案...........................144
6.6基于门控的循环神经网络......................145
6.6.1长短期记忆网络.......................145
6.6.2LSTM网络的各种变体...................147
6.6.3门控循环单元网络......................148
6.7深层循环神经网络..........................149
6.7.1堆叠循环神经网络......................150
6.7.2双向循环神经网络......................150
6.8扩展到图结构.............................151
6.8.1递归神经网络........................151
6.8.2图神经网络..........................152
6.9总结和深入阅读...........................153
第7章 网络优化与正则化157
7.1网络优化...............................157
7.1.1网络结构多样性.......................158
7.1.2高维变量的非凸优化....................158
7.1.3神经网络优化的改善方法..................160
7.2优化算法...............................160
7.2.1小批量梯度下降.......................160
7.2.2批量大小选择........................161
7.2.3学习率调整..........................162
7.2.4梯度估计修正........................167
7.2.5优化算法小结........................170
7.3参数初始化..............................171
7.3.1基于固定方差的参数初始化.................172
7.3.2基于方差缩放的参数初始化.................173
7.3.3正交初始化..........................175
7.4数据预处理..............................176
7.5逐层归一化..............................178
7.5.1批量归一化..........................179
7.5.2层归一化...........................181
7.5.3权重归一化..........................182
7.5.4局部响应归一化.......................182
7.6超参数优化..............................183
7.6.1网格搜索...........................183
7.6.2随机搜索...........................184
7.6.3贝叶斯优化..........................184
7.6.4动态资源分配........................185
7.6.5神经架构搜索........................186
7.7网络正则化..............................186
7.7.1?1和?2正则化........................187
7.7.2权重衰减...........................188
7.7.3提前停止...........................188
7.7.4丢弃法............................189
7.7.5数据增强...........................191
7.7.6标签平滑...........................191
7.8总结和深入阅读...........................192
第8章 注意力机制与外部记忆197
8.1认知神经学中的注意力.......................198
8.2注意力机制..............................199
8.2.1注意力机制的变体......................201
8.3自注意力模型.............................203
8.4人脑中的记忆.............................205
8.5记忆增强神经网络..........................207
8.5.1端到端记忆网络.......................208
8.5.2神经图灵机..........................210
8.6基于神经动力学的联想记忆.....................211
8.6.1Hopfiel网络........................212
8.6.2使用联想记忆增加网络容量.................215
8.7总结和深入阅读...........................215
第9章 无监督学习219
9.1无监督特征学习...........................220
9.1.1主成分分析..........................220
9.1.2稀疏编码...........................222
9.1.3自编码器...........................224
9.1.4稀疏自编码器........................225
9.1.5堆叠自编码器........................226
9.1.6降噪自编码器........................226
9.2概率密度估计.............................227
9.2.1参数密度估计........................227
9.2.2非参数密度估计.......................229
9.3总结和深入阅读...........................232
第10章 模型独立的学习方式235
10.1集成学习...............................235
10.1.1AdaBoost算法........................237
10.2自训练和协同训练..........................240
10.2.1自训练............................240
10.2.2协同训练...........................240
10.3多任务学习..............................242
10.4迁移学习...............................245
10.4.1归纳迁移学习........................246
10.4.2转导迁移学习........................247
10.5终身学习...............................249
10.6元学习.................................252
10.6.1基于优化器的元学习....................253
10.6.2模型无关的元学习......................254
10.7总结和深入阅读...........................255
第三部分 进阶模型
第11章 概率图模型261
11.1模型表示...............................262
11.1.1有向图模型..........................263
11.1.2常见的有向图模型......................264
11.1.3无向图模型..........................267
11.1.4无向图模型的概率分解...................267
11.1.5常见的无向图模型......................269
11.1.6有向图和无向图之间的转换.................270
11.2学习..................................271
11.2.1不含隐变量的参数估计...................271
11.2.2含隐变量的参数估计....................273
11.3推断..................................279
11.3.1精确推断...........................279
11.3.2近似推断...........................282
11.4变分推断...............................283
11.5基于采样法的近似推断.......................285
11.5.1采样法............................285
11.5.2拒绝采样...........................287
11.5.3重要性采样..........................288
11.5.4马尔可夫链蒙特卡罗方法..................289
11.6总结和深入阅读...........................292
第12章 深度信念网络297
12.1玻尔兹曼机..............................297
12.1.1生成模型...........................299
12.1.2能量最小化与模拟退火...................301
12.1.3参数学习...........................302
12.2受限玻尔兹曼机...........................304
12.2.1生成模型...........................305
12.2.2参数学习...........................307
12.2.3受限玻尔兹曼机的类型...................308
12.3深度信念网络.............................309
12.3.1生成模型...........................310
12.3.2参数学习...........................310
12.4总结和深入阅读...........................313
第13章 深度生成模型317
13.1概率生成模型.............................318
13.1.1密度估计...........................318
13.1.2生成样本...........................319
13.1.3应用于监督学习.......................319
13.2变分自编码器.............................319
13.2.1含隐变量的生成模型....................319
13.2.2推断网络...........................321
13.2.3生成网络...........................323
13.2.4模型汇总...........................323
13.2.5再参数化...........................325
13.2.6训练..............................325
13.3生成对抗网络.............................327
13.3.1显式密度模型和隐式密度模型...............327
13.3.2网络分解...........................327
13.3.3训练..............................329
13.3.4一个生成对抗网络的具体实现：DCGAN..........330
13.3.5模型分析...........................330
13.3.6改进模型...........................333
13.4总结和深入阅读...........................336
第14章 深度强化学习339
14.1强化学习问题.............................340
14.1.1典型例子...........................340
14.1.2强化学习定义........................340
14.1.3马尔可夫决策过程......................341
14.1.4强化学习的目标函数....................343
14.1.5值函数............................344
14.1.6深度强化学习........................345
14.2基于值函数的学习方法.......................346
14.2.1动态规划算法........................346
14.2.2蒙特卡罗方法........................349
14.2.3时序差分学习方法......................350
14.2.4深度Q网络..........................353
14.3基于策略函数的学习方法......................354
14.3.1REINFORCE算法......................356
14.3.2带基准线的REINFORCE算法...............356
14.4演员-评论员算法...........................358
14.5总结和深入阅读...........................360
第15章 序列生成模型365
15.1序列概率模型.............................366
15.1.1序列生成...........................367
15.2N元统计模型.............................368
15.3深度序列模型.............................370
15.3.1模型结构...........................370
15.3.2参数学习...........................373
15.4评价方法...............................373
15.4.1困惑度............................373
15.4.2BLEU算法..........................374
15.4.3ROUGE算法.........................375
15.5序列生成模型中的学习问题.....................375
15.5.1曝光偏差问题........................376
15.5.2训练目标不一致问题....................377
15.5.3计算效率问题........................377
15.6序列到序列模型...........................385
15.6.1基于循环神经网络的序列到序列模型...........386
15.6.2基于注意力的序列到序列模型...............387
15.6.3基于自注意力的序列到序列模型..............388
15.7总结和深入阅读...........................390
附录数学基础 393
附录A 线性代数 394
附录B 微积分 404
附录C 数学优化 413
附录D 概率论 420
附录E 信息论 433
索引 439
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
前言
如何使用本书
资源与支持
主要符号表
第1 章　深度学习简介… ………………… 1
1.1　起源…………………………………………… 2
1.2　发展…………………………………………… 4
1.3　成功案例……………………………………… 6
1.4　特点………………………………………… 7
小结…………………………………………… 8
练习…………………………………………… 8
第2 章　预备知识… ……………………… 9
2.1　获取和运行本书的代码……………………… 9
2.1.1　获取代码并安装运行环境 … ……… 9
2.1.2　更新代码和运行环境 … …………… 11
2.1.3　使用GPU版的MXNet … ………… 11
小结……………………………………………12
练习……………………………………………12
2.2　数据操作… ……………………………… 12
2.2.1　创建NDArray ………………………12
2.2.2　运算 …………………………………14
2.2.3　广播机制 ……………………………16
2.2.4　索引 …………………………………17
2.2.5　运算的内存开销 ……………………17
2.2.6　NDArray和NumPy相互变换………18
小结……………………………………………19
练习……………………………………………19
2.3　自动求梯度… …………………………… 19
2.3.1　简单例子 … …………………………19
2.3.2　训练模式和预测模式 …………… 20
2.3.3　对Python控制流求梯度 … …… 20
小结……………………………………………21
练习……………………………………………21
2.4　查阅文档… ……………………………… 21
2.4.1　查找模块里的所有函数和类 … ……21
2.4.2　查找特定函数和类的使用 ……… 22
2.4.3　在MXNet网站上查阅 …………… 23
小结………………………………………… 24
练习………………………………………… 24
第3 章　深度学习基础… ……………… 25
3.1　线性回归…………………………………… 25
3.1.1　线性回归的基本要素 … ………… 25
3.1.2　线性回归的表示方法 … ………… 28
小结………………………………………… 30
练习………………………………………… 30
3.2　线性回归的从零开始实现… …………… 30
3.2.1　生成数据集 … …………………… 30
3.2.2　读取数据集 ……………………… 32
3.2.3　初始化模型参数 ………………… 32
3.2.4　定义模型 ………………………… 33
3.2.5　定义损失函数 …………………… 33
3.2.6　定义优化算法 …………………… 33
3.2.7　训练模型 ………………………… 33
小结………………………………………… 34
练习………………………………………… 34
3.3　线性回归的简洁实现… ………………… 35
3.3.1　生成数据集 … …………………… 35
3.3.2　读取数据集 ……………………… 35
3.3.3　定义模型 ………………………… 36
3.3.4　初始化模型参数 ………………… 36
3.3.5　定义损失函数 …………………… 37
3.3.6　定义优化算法 …………………… 37
3.3.7　训练模型 ………………………… 37
小结………………………………………… 38
练习………………………………………… 38
3.4　softmax回归… ………………………… 38
3.4.1　分类问题 … ……………………… 38
3.4.2　softmax回归模型… …………… 39
3.4.3　单样本分类的矢量计算表达式…… 40
3.4.4　小批量样本分类的矢量计算表达式 …………………………… 40
3.4.5　交叉熵损失函数 ……………………41
3.4.6　模型预测及评价 ………………… 42
小结………………………………………… 42
练习………………………………………… 42
3.5　图像分类数据集（Fashion-MNIST）… ……………… 42
3.5.1　获取数据集 … …………………… 42
3.5.2　读取小批量 ……………………… 44
小结………………………………………… 45
练习………………………………………… 45
3.6　softmax回归的从零开始实现… ……… 45
3.6.1　读取数据集 … …………………… 45
3.6.2　初始化模型参数 ………………… 45
3.6.3　实现softmax运算 … …………… 46
3.6.4　定义模型 ………………………… 46
3.6.5　定义损失函数 …………………… 47
3.6.6　计算分类准确率 ………………… 47
3.6.7　训练模型 ………………………… 48
3.6.8　预测… …………………………… 48
小结………………………………………… 49
练习………………………………………… 49
3.7　softmax回归的简洁实现… …………… 49
3.7.1　读取数据集 … …………………… 49
3.7.2　定义和初始化模型 ……………… 50
3.7.3　softmax和交叉熵损失函数 … … 50
3.7.4　定义优化算法 …………………… 50
3.7.5　训练模型 ………………………… 50
小结………………………………………… 50
练习………………………………………… 50
3.8　多层感知机… …………………………… 51
3.8.1　隐藏层 … ……………………………51
3.8.2　激活函数 ………………………… 52
3.8.3　多层感知机 ……………………… 55
小结………………………………………… 55
练习………………………………………… 55
3.9　多层感知机的从零开始实现… ………… 56
3.9.1　读取数据集 … …………………… 56
3.9.2　定义模型参数 …………………… 56
3.9.3　定义激活函数 …………………… 56
3.9.4　定义模型 ………………………… 56
3.9.5　定义损失函数 …………………… 57
3.9.6　训练模型 ………………………… 57
小结………………………………………… 57
练习………………………………………… 57
3.10　多层感知机的简洁实现………………… 57
3.10.1　定义模型 ………………………… 58
3.10.2　训练模型 … …………………… 58
小结………………………………………… 58
练习………………………………………… 58
3.11　模型选择、欠拟合和过拟合… ………… 58
3.11.1　训练误差和泛化误差 …………… 59
3.11.2　模型选择 ………………………… 59
3.11.3　欠拟合和过拟合 ………………… 60
3.11.4　多项式函数拟合实验 ……………61
小结………………………………………… 65
练习………………………………………… 65
3.12　权重衰减………………………………… 65
3.12.1　方法 ……………………………… 65
3.12.2　高维线性回归实验 … ………… 66
3.12.3　从零开始实现 … ……………… 66
3.12.4　简洁实现 … …………………… 68
小结………………………………………… 70
练习………………………………………… 70
3.13　丢弃法…………………………………… 70
3.13.1　方法 ……………………………… 70
3.13.2　从零开始实现 … …………………71
3.13.3　简洁实现 … …………………… 73
小结………………………………………… 74
练习………………………………………… 74
3.14　正向传播、反向传播和计算图………… 74
3.14.1　正向传播 ……………………… 74
3.14.2　正向传播的计算图 … ………… 75
3.14.3　反向传播 … …………………… 75
3.14.4　训练深度学习模型 … ………… 76
小结………………………………………… 77
练习………………………………………… 77
3.15　数值稳定性和模型初始化……………… 77
3.15.1　衰减和爆炸 ……………………… 77
3.15.2　随机初始化模型参数 … ……… 78
小结………………………………………… 78
练习………………………………………… 79
3.16　实战Kaggle比赛：房价预测… ……… 79
3.16.1　Kaggle比赛 … ………………… 79
3.16.2　读取数据集 … ………………… 80
3.16.3　预处理数据集 … …………………81
3.16.4　训练模型 … …………………… 82
3.16.5　k 折交叉验证 …………………… 82
3.16.6　模型选择 … …………………… 83
3.16.7　预测并在Kaggle提交结果… … 84
小结………………………………………… 85
练习………………………………………… 85
第4 章　深度学习计算… ……………… 86
4.1　模型构造………………………………… 86
4.1.1　继承Block类来构造模型 … …… 86
4.1.2　Sequential类继承自Block类…………………………… 87
4.1.3　构造复杂的模型… ……………… 88
小结………………………………………… 89
练习………………………………………… 90
4.2　模型参数的访问、初始化和共享… …… 90
4.2.1　访问模型参数 … ………………… 90
4.2.2　初始化模型参数 ………………… 92
4.2.3　自定义初始化方法 ……………… 93
4.2.4　共享模型参数 …………………… 94
小结………………………………………… 94
练习………………………………………… 94
4.3　模型参数的延后初始化… ……………… 95
4.3.1　延后初始化 … …………………… 95
4.3.2　避免延后初始化 ………………… 96
小结………………………………………… 96
练习………………………………………… 97
4.4　自定义层… ……………………………… 97
4.4.1　不含模型参数的自定义层 … …… 97
4.4.2　含模型参数的自定义层 ………… 98
小结………………………………………… 99
练习………………………………………… 99
4.5　读取和存储… …………………………… 99
4.5.1　读写NDArray… ………………… 99
4.5.2　读写Gluon模型的参数… ……… 100
小结………………………………………… 101
练习………………………………………… 101
4.6　GPU计算………………………………… 101
4.6.1　计算设备 … ……………………… 102
4.6.2　NDArray的GPU计算…………… 102
4.6.3　Gluon的GPU计算 ……………… 104
小结………………………………………… 105
练习………………………………………… 105
第5 章　卷积神经网络… ……………… 106
5.1　二维卷积层………………………………… 106
5.1.1　二维互相关运算 … ……………… 106
5.1.2　二维卷积层 … …………………… 107
5.1.3　图像中物体边缘检测 … ………… 108
5.1.4　通过数据学习核数组 … ………… 109
5.1.5　互相关运算和卷积运算 … ……… 109
5.1.6　特征图和感受野… ……………… 110
小结………………………………………… 110
练习………………………………………… 110
5.2　填充和步幅… …………………………… 111
5.2.1　填充 … …………………………… 111
5.2.2　步幅 ……………………………… 112
小结………………………………………… 113
练习………………………………………… 113
5.3　多输入通道和多输出通道… …………… 114
5.3.1　多输入通道 … …………………… 114
5.3.2　多输出通道… …………………… 115
5.3.3　1×1卷积层 ……………………… 116
小结………………………………………… 117
练习………………………………………… 117
5.4　池化层… ………………………………… 117
5.4.1　二维最大池化层和平均池化层 … ………………………… 117
5.4.2　填充和步幅 ……………………… 119
5.4.3　多通道 …………………………… 120
小结………………………………………… 120
练习………………………………………… 121
5.5　卷积神经网络（LeNet）… …………… 121
5.5.1　LeNet模型 … …………………… 121
5.5.2　训练模型… ……………………… 122
小结………………………………………… 124
练习………………………………………… 124
5.6　深度卷积神经网络（AlexNet）… …… 124
5.6.1　学习特征表示 … ………………… 125
5.6.2　AlexNet… ……………………… 126
5.6.3　读取数据集 ……………………… 127
5.6.4　训练模型 ………………………… 128
小结………………………………………… 128
练习………………………………………… 129
5.7　使用重复元素的网络（VGG）………… 129
5.7.1　VGG块 …………………………… 129
5.7.2　VGG网络 … …………………… 129
5.7.3　训练模型… ……………………… 130
小结………………………………………… 131
练习………………………………………… 131
5.8　网络中的网络（NiN）… ……………… 131
5.8.1　NiN块 … ………………………… 131
5.8.2　NiN模型 … ……………………… 132
5.8.3　训练模型… ……………………… 133
小结………………………………………… 134
练习………………………………………… 134
5.9　含并行连结的网络（GoogLeNet）…… 134
5.9.1　Inception块 ……………………… 134
5.9.2　GoogLeNet模型 … …………… 135
5.9.3　训练模型 ………………………… 137
小结………………………………………… 137
练习………………………………………… 137
5.10　批量归一化……………………………… 138
5.10.1　批量归一化层 ………………… 138
5.10.2　从零开始实现 … ……………… 139
5.10.3　使用批量归一化层的LeNet … … 140
5.10.4　简洁实现 … …………………… 141
小结………………………………………… 142
练习………………………………………… 142
5.11　残差网络（ResNet） ……………… 143
5.11.1　残差块 …………………………… 143
5.11.2　ResNet模型… ………………… 145
5.11.3　训练模型………………………… 146
小结………………………………………… 146
练习………………………………………… 146
5.12　稠密连接网络（DenseNet）………… 147
5.12.1　稠密块 …………………………… 147
5.12.2　过渡层 … ……………………… 148
5.12.3　DenseNet模型 ………………… 148
5.12.4　训练模型 … …………………… 149
小结………………………………………… 149
练习………………………………………… 149
第6 章　循环神经网络… ……………… 150
6.1　语言模型………………………………… 150
6.1.1　语言模型的计算 … ……………… 151
6.1.2　n 元语法 … ……………………… 151
小结………………………………………… 152
练习………………………………………… 152
6.2　循环神经网络… ………………………… 152
6.2.1　不含隐藏状态的神经网络 … …… 152
6.2.2　含隐藏状态的循环神经网络… … 152
6.2.3　应用：基于字符级循环神经网络的语言模型 … ……………………… 154
小结………………………………………… 155
练习………………………………………… 155
6.3　语言模型数据集（歌词）…… 155
6.3.1　读取数据集 … …………………… 155
6.3.2　建立字符索引 …………………… 156
6.3.3　时序数据的采样 ………………… 156
小结………………………………………… 158
练习………………………………………… 159
6.4　循环神经网络的从零开始实现… ……… 159
6.4.1　one-hot向量 … ………………… 159
6.4.2　初始化模型参数 ………………… 160
6.4.3　定义模型 ………………………… 160
6.4.4　定义预测函数 …………………… 161
6.4.5　裁剪梯度 ………………………… 161
6.4.6　困惑度 …………………………… 162
6.4.7　定义模型训练函数 ……………… 162
6.4.8　训练模型并创作歌词 …………… 163
小结………………………………………… 164
练习………………………………………… 164
6.5　循环神经网络的简洁实现… …………… 165
6.5.1　定义模型 … ……………………… 165
6.5.2　训练模型 ………………………… 166
小结………………………………………… 168
练习………………………………………… 168
6.6　通过时间反向传播… …………………… 168
6.6.1　定义模型 … ……………………… 168
6.6.2　模型计算图 ……………………… 169
6.6.3　方法 ……………………………… 169
小结………………………………………… 170
练习………………………………………… 170
6.7　门控循环单元（GRU）………………… 170
6.7.1　门控循环单元 … ………………… 171
6.7.2　读取数据集 ……………………… 173
6.7.3　从零开始实现 …………………… 173
6.7.4　简洁实现 ………………………… 175
小结………………………………………… 176
练习………………………………………… 176
6.8　长短期记忆（LSTM）… ……………… 176
6.8.1　长短期记忆 … …………………… 176
6.8.2　读取数据集 ……………………… 179
6.8.3　从零开始实现 …………………… 179
6.8.4　简洁实现 ………………………… 181
小结………………………………………… 181
练习………………………………………… 182
6.9　深度循环神经网络… …………………… 182
小结………………………………………… 183
练习………………………………………… 183
6.10　双向循环神经网络……………………… 183
小结………………………………………… 184
练习………………………………………… 184
第7 章　优化算法… …………………… 185
7.1　优化与深度学习…………………………… 185
7.1.1　优化与深度学习的关系 … ……… 185
7.1.2　优化在深度学习中的挑战 … …… 186
小结………………………………………… 188
练习………………………………………… 189
7.2　梯度下降和随机梯度下降… …………… 189
7.2.1　一维梯度下降 … ………………… 189
7.2.2　学习率 …………………………… 190
7.2.3　多维梯度下降 …………………… 191
7.2.4　随机梯度下降 …………………… 193
小结………………………………………… 194
练习………………………………………… 194
7.3　小批量随机梯度下降… ………………… 194
7.3.1　读取数据集 … …………………… 195
7.3.2　从零开始实现 …………………… 196
7.3.3　简洁实现 ………………………… 198
小结………………………………………… 199
练习………………………………………… 199
7.4　动量法… …………………………………200
7.4.1　梯度下降的问题 … ……………… 200
7.4.2　动量法 …………………………… 201
・6・　目　　录
7.4.3　从零开始实现 …………………… 203
7.4.4　简洁实现 ………………………… 205
小结………………………………………… 205
练习………………………………………… 205
7.5　AdaGrad算法……………………………206
7.5.1　算法 … …………………………… 206
7.5.2　特点 ……………………………… 206
7.5.3　从零开始实现 …………………… 208
7.5.4　简洁实现 ………………………… 209
小结………………………………………… 209
练习………………………………………… 209
7.6　RMSProp算法… ………………………209
7.6.1　算法 … …………………………… 210
7.6.2　从零开始实现 …………………… 211
7.6.3　简洁实现 ………………………… 212
小结………………………………………… 212
练习………………………………………… 212
7.7　AdaDelta算法… ……………………… 212
7.7.1　算法… …………………………… 212
7.7.2　从零开始实现 …………………… 213
7.7.3　简洁实现 ………………………… 214
小结………………………………………… 214
练习………………………………………… 214
7.8　Adam算法… …………………………… 215
7.8.1　算法 … …………………………… 215
7.8.2　从零开始实现 …………………… 216
7.8.3　简洁实现 ………………………… 216
小结………………………………………… 217
练习………………………………………… 217
第8 章　计算性能… …………………… 218
8.1　命令式和符号式混合编程… …………… 218
8.1.1　混合式编程取两者之长 … ……… 220
8.1.2　使用HybridSequential类构造模型 … …………………………… 220
8.1.3　使用HybridBlock类构造模型… …………………………… 222
小结………………………………………… 224
练习………………………………………… 224
8.2　异步计算… ………………………………224
8.2.1　MXNet中的异步计算 …………… 224
8.2.2　用同步函数让前端等待计算结果 … …………………………… 226
8.2.3　使用异步计算提升计算性能 …… 226
8.2.4　异步计算对内存的影响 ………… 227
小结………………………………………… 229
练习………………………………………… 229
8.3　自动并行计算… …………………………229
8.3.1　CPU和GPU的并行计算 … …… 230
8.3.2　计算和通信的并行计算 ………… 231
小结………………………………………… 231
练习………………………………………… 231
8.4　多GPU计算……………………………… 232
8.4.1　数据并行 … ……………………… 232
8.4.2　定义模型 ………………………… 233
8.4.3　多GPU之间同步数据 … ……… 234
8.4.4　单个小批量上的多GPU训练 … …………………………… 236
8.4.5　定义训练函数 …………………… 236
8.4.6　多GPU训练实验 … …………… 237
小结………………………………………… 237
练习………………………………………… 237
8.5　多GPU计算的简洁实现………………… 237
8.5.1　多GPU上初始化模型参数……… 238
8.5.2　多GPU训练模型 … …………… 239
小结………………………………………… 241
练习………………………………………… 241
第9 章　计算机视觉… ………………… 242
9.1　图像增广…………………………………242
9.1.1　常用的图像增广方法 … ………… 243
9.1.2　使用图像增广训练模型 … ……… 246
小结………………………………………… 250
练习………………………………………… 250
9.2　微调… ……………………………………250
热狗识别 … ……………………………… 251
小结………………………………………… 255
练习………………………………………… 255
目　　录　・7・
9.3　目标检测和边界框… ……………………255
边界框 … ………………………………… 256
小结………………………………………… 257
练习………………………………………… 257
9.4　锚框… …………………………………… 257
9.4.1　生成多个锚框… ………………… 257
9.4.2　交并比 …………………………… 259
9.4.3　标注训练集的锚框 ……………… 260
9.4.4　输出预测边界框… ……………… 263
小结………………………………………… 265
练习………………………………………… 265
9.5　多尺度目标检测… ………………………265
小结………………………………………… 268
练习………………………………………… 268
9.6　目标检测数据集（皮卡丘）… …………268
9.6.1　获取数据集 … …………………… 269
9.6.2　读取数据集… …………………… 269
9.6.3　图示数据 ………………………… 270
小结………………………………………… 270
练习………………………………………… 271
9.7　单发多框检测（SSD）… ……………… 271
9.7.1　定义模型… ……………………… 271
9.7.2　训练模型 ………………………… 275
9.7.3　预测目标 ………………………… 277
小结………………………………………… 278
练习………………………………………… 278
9.8　区域卷积神经网络（R-CNN）系列……280
9.8.1　R-CNN … ……………………… 280
9.8.2　Fast R-CNN …………………… 281
9.8.3　Faster R-CNN ………………… 283
9.8.4　Mask R-CNN … ……………… 284
小结………………………………………… 285
练习………………………………………… 285
9.9　语义分割和数据集… ……………………285
9.9.1　图像分割和实例分割 … ………… 285
9.9.2　Pascal VOC2012语义分割数据集 … ………………………… 286
小结………………………………………… 290
练习………………………………………… 290
9.10　全卷积网络（FCN）… ………………290
9.10.1　转置卷积层 …………………… 291
9.10.2　构造模型 … …………………… 292
9.10.3　初始化转置卷积层……………… 294
9.10.4　读取数据集 … ………………… 295
9.10.5　训练模型………………………… 296
9.10.6　预测像素类别…………………… 296
小结………………………………………… 297
练习………………………………………… 297
9.11　样式迁移… ………………………………298
9.11.1　方法 ……………………………… 298
9.11.2　读取内容图像和样式图像……… 299
9.11.3　预处理和后处理图像 ………… 300
9.11.4　抽取特征 ……………………… 301
9.11.5　定义损失函数 ………………… 302
9.11.6　创建和初始化合成图像 ……… 303
9.11.7　训练模型………………………… 304
小结………………………………………… 306
练习………………………………………… 306
9.12　实战Kaggle比赛：图像
分类（CIFAR-10）……………………306
9.12.1　获取和整理数据集 ……………… 307
9.12.2　图像增广 … …………………… 310
9.12.3　读取数据集 … ………………… 310
9.12.4　定义模型………………………… 311
9.12.5　定义训练函数 … ……………… 312
9.12.6　训练模型 … …………………… 312
9.12.7　对测试集分类并在Kaggle
提交结果 … …………………… 313
小结………………………………………… 313
练习………………………………………… 313
9.13　实战Kaggle比赛：狗的品种
识别（ImageNet Dogs）…………… 314
9.13.1　获取和整理数据集 …………… 315
9.13.2　图像增广 … …………………… 316
9.13.3　读取数据集 … ………………… 317
9.13.4　定义模型 … …………………… 318
9.13.5　定义训练函数 … ……………… 318
9.13.6　训练模型 … …………………… 319
・8・　目　　录
9.13.7　对测试集分类并在Kaggle提交结果 … …………………… 319
小结………………………………………… 320
练习………………………………………… 320
第10 章　自然语言处理………………… 321
10.1　词嵌入（word2vec）………………… 321
10.1.1　为何不采用one-hot向量… …… 321
10.1.2　跳字模型 ………………………… 322
10.1.3　连续词袋模型 …………………… 323
小结………………………………………… 325
练习………………………………………… 325
10.2　近似训练…………………………………325
10.2.1　负采样 …………………………… 325
10.2.2　层序softmax …………………… 326
小结………………………………………… 327
练习………………………………………… 328
10.3　word2vec的实现………………………328
10.3.1　预处理数据集 …………………… 328
10.3.2　负采样 … ……………………… 331
10.3.3　读取数据集 … ………………… 331
10.3.4　跳字模型 … …………………… 332
10.3.5　训练模型 … …………………… 333
10.3.6　应用词嵌入模型 … …………… 335
小结………………………………………… 336
练习………………………………………… 336
10.4　子词嵌入（fastText）… ……………336
小结………………………………………… 337
练习………………………………………… 337
10.5　全局向量的词嵌入（GloVe）…………337
10.5.1　GloVe模型 …………………… 338
10.5.2　从条件概率比值理解GloVe模型……………………… 339
小结………………………………………… 340
练习………………………………………… 340
10.6　求近义词和类比词………………………340
10.6.1　使用预训练的词向量 ………… 340
10.6.2　应用预训练词向量 … ………… 341
小结………………………………………… 343
练习………………………………………… 343
10.7　文本情感分类：使用循环神经网络…… 343
10.7.1　文本情感分类数据集 ………… 343
10.7.2　使用循环神经网络的模型……… 345
小结………………………………………… 347
练习………………………………………… 347
10.8　文本情感分类：使用卷积神经网络（textCNN）… …………………347
10.8.1　一维卷积层 … ………………… 348
10.8.2　时序最大池化层 … …………… 349
10.8.3　读取和预处理IMDb数据集 … ……………………… 350
10.8.4　textCNN模型 … ……………… 350
小结………………………………………… 353
练习………………………………………… 353
10.9　编码器-解码器（seq2seq）…………353
10.9.1　编码器 ………………………… 354
10.9.2　解码器 … ……………………… 354
10.9.3　训练模型………………………… 355
小结………………………………………… 355
练习………………………………………… 355
10.10　 束搜索… ………………………………355
10.10.1　贪婪搜索 … …………………… 356
10.10.2　穷举搜索 ……………………… 357
10.10.3　束搜索 ………………………… 357
小结………………………………………… 358
练习………………………………………… 358
10.11　注意力机制… …………………………358
10.11.1　计算背景变量 … ……………… 359
10.11.2　更新隐藏状态 … ……………… 360
10.11.3　发展… ………………………… 361
小结………………………………………… 361
练习………………………………………… 361
10.12　机器翻译… …………………………… 361
10.12.1　读取和预处理数据集… ……… 361
10.12.2　含注意力机制的编码器-解码器 … …………… 363
10.12.3　训练模型 ……………………… 365
10.12.4　预测不定长的序列… ………… 367
10.12.5　评价翻译结果 ………………… 367
小结………………………………………… 369
练习………………………………………… 369
附录A　数学基础… …………………… 370
附录B　使用 Jupyter 记事本… ……… 376
附录C　使用 AWS 运行代码…………… 381
附录D　GPU 购买指南………………… 388
附录E　如何为本书做贡献… ………… 391
附录F　d2lzh 包索引…………………… 395
附录G　中英文术语对照表… ………… 397
参考文献………………………………… 402
索引……………………………………… 407
・ ・ ・ ・ ・ ・ (收起)前言
第1阶段 自动微分 1
步骤1 作为“箱子”的变量 3
1.1 什么是变量 3
1.2 实现Variable类 4
1.3 （补充）NumPy的多维数组 6
步骤2 创建变量的函数 8
2.1 什么是函数 8
2.2 Function类的实现 9
2.3 使用Function类 10
步骤3 函数的连续调用 13
3.1 Exp函数的实现 13
3.2 函数的连续调用 14
步骤4 数值微分 16
4.1 什么是导数 16
4.2 数值微分的实现 17
4.3 复合函数的导数 20
4.4 数值微分存在的问题 21
步骤5 反向传播的理论知识 22
5.1 链式法则 22
5.2 反向传播的推导 23
5.3 用计算图表示 25
步骤6 手动进行反向传播 27
6.1 Variable类的功能扩展 27
6.2 Function类的功能扩展 28
6.3 Square类和Exp类的功能扩展 28
6.4 反向传播的实现 29
步骤7 反向传播的自动化 32
7.1 为反向传播的自动化创造条件 33
7.2 尝试反向传播 36
7.3 增加backward方法 38
步骤8 从递归到循环 40
8.1 现在的Variable类 40
8.2 使用循环实现 41
8.3 代码验证 42
步骤9 让函数更易用 43
9.1 作为Python函数使用 43
9.2 简化backward方法 45
9.3 只支持ndarray 46
步骤10 测试 50
10.1 Python的单元测试 50
10.2 square函数反向传播的测试 52
10.3 通过梯度检验来自动测试 53
10.4 测试小结 54
第2阶段 用自然的代码表达 59
步骤11 可变长参数（正向传播篇) 61
11.1 修改Function类 62
11.2 Add类的实现 64
步骤12 可变长参数（改进篇) 65
12.1 第1项改进：使函数更容易使用 65
12.2 第2项改进：使函数更容易实现 67
12.3 add函数的实现 69
步骤13 可变长参数（反向传播篇) 70
13.1 支持可变长参数的Add类的反向传播 70
13.2 修改Variable类 71
13.3 Square类的实现 73
步骤14 重复使用同一个变量 75
14.1 问题的原因 76
14.2 解决方案 77
14.3 重置导数 79
步骤15 复杂的计算图（理论篇）81
15.1 反向传播的正确顺序 82
15.2 当前的DeZero 84
15.3 函数的优先级 87
步骤16 复杂的计算图（实现篇）88
16.1 增加“辈分”变量 88
16.2 按照“辈分”顺序取出元素 90
16.3 Variable类的backward 92
16.4 代码验证 93
步骤17 内存管理和循环引用 97
17.1 内存管理 97
17.2 引用计数方式的内存管理 98
17.3 循环引用 100
17.4 weakref模块 102
17.5 代码验证 104
步骤18 减少内存使用量的模式 106
18.1 不保留不必要的导数 106
18.2 回顾Function类 109
18.3 使用Confifig类进行切换 110
18.4 模式的切换 111
18.5 使用with语句切换 112
步骤19 让变量更易用 116
19.1 命名变量 116
19.2 实例变量ndarray 117
19.3 len函数和print函数 119
步骤20 运算符重载（1）122
20.1 Mul类的实现 122
20.2 运算符重载 125
步骤21 运算符重载（2）128
21.1 与ndarray一起使用 128
21.2 与flfloat和int一起使用 130
21.3 问题1：左项为flfloat或int的情况 131
21.4 问题2：左项为ndarray实例的情况 133
步骤22 运算符重载（3）134
22.1 负数 135
22.2 减法 136
22.3 除法 138
22.4 幂运算 139
步骤23 打包 141
23.1 文件结构 142
23.2 将代码移到核心类 142
23.3 运算符重载 144
23.4 实际的_ _init_ _.py文件 146
23.5 导入dezero 147
步骤24 复杂函数的求导 149
24.1 Sphere函数 150
24.2 matyas函数 151
24.3 GoldsteinPrice函数 152
第3阶段 实现高阶导数 161
步骤25 计算图的可视化（1） 163
25.1 安装Graphviz 163
25.2 使用DOT语言描述图形 165
25.3 指定节点属性 165
25.4 连接节点 167
步骤26 计算图的可视化（2）169
26.1 可视化代码的使用示例 169
26.2 从计算图转换为DOT语言 171
26.3 从DOT语言转换为图像 174
26.4 代码验证 176
步骤27 泰勒展开的导数 178
27.1 sin函数的实现 178
27.2 泰勒展开的理论知识 179
27.3 泰勒展开的实现 180
27.4 计算图的可视化 182
步骤28 函数优化 184
28.1 Rosenbrock函数 184
28.2 求导 185
28.3 梯度下降法的实现 186
步骤29 使用牛顿法进行优化（手动计算）190
29.1 使用牛顿法进行优化的理论知识 191
29.2 使用牛顿法实现优化 195
步骤30 高阶导数（准备篇） 197
30.1 确认工作①：Variable实例变量 197
30.2 确认工作②：Function类 199
30.3 确认工作③：Variable类的反向传播 201
步骤31 高阶导数（理论篇） 204
31.1 在反向传播时进行的计算 204
31.2 创建反向传播的计算图的方法 206
步骤32 高阶导数（实现篇） 209
32.1 新的DeZero 209
32.2 函数类的反向传播 210
32.3 实现更有效的反向传播（增加模式控制代码）211
32.4 修改_ _init_ _.py 213
步骤33 使用牛顿法进行优化（自动计算） 215
33.1 求二阶导数 215
33.2 使用牛顿法进行优化 217
步骤34 sin函数的高阶导数 219
34.1 sin函数的实现 219
34.2 cos函数的实现 220
34.3 sin函数的高阶导数 221
步骤35 高阶导数的计算图 225
35.1 tanh函数的导数 226
35.2 tanh函数的实现 226
35.3 高阶导数的计算图可视化 227
步骤36 DeZero的其他用途 234
36.1 double backprop的用途 234
36.2 深度学习研究中的应用示例 236
第4阶段 创建神经网络 243
步骤37 处理张量 245
37.1 对各元素进行计算 245
37.2 使用张量时的反向传播 247
37.3 使用张量时的反向传播（补充内容）249
步骤38 改变形状的函数 254
38.1 reshape函数的实现 254
38.2 从Variable对象调用reshape 258
38.3 矩阵的转置 259
38.4 实际的transpose函数（补充内容）262
步骤39 求和的函数 264
39.1 sum函数的反向传播 264
39.2 sum函数的实现 266
39.3 axis和keepdims 268
步骤40 进行广播的函数 272
40.1 broadcast_to函数和sum_to函数 272
40.2 DeZero的broadcast_to函数和sum_to函数 275
40.3 支持广播 277
步骤41 矩阵的乘积 280
41.1 向量的内积和矩阵的乘积 280
41.2 检查矩阵的形状 282
41.3 矩阵乘积的反向传播 282
步骤42 线性回归 288
42.1 玩具数据集 288
42.2 线性回归的理论知识 289
42.3 线性回归的实现 291
42.4 DeZero的mean_squared_error函数（补充内容） 295
步骤43 神经网络 298
43.1 DeZero中的linear函数 298
43.2 非线性数据集 301
43.3 激活函数和神经网络 302
43.4 神经网络的实现 303
步骤44 汇总参数的层 307
44.1 Parameter类的实现 307
44.2 Layer类的实现 309
44.3 Linear类的实现 312
44.4 使用Layer实现神经网络 314
步骤45 汇总层的层 316
45.1 扩展Layer类 316
45.2 Model类 319
45.3 使用Model来解决问题 321
45.4 MLP类 323
步骤46 通过Optimizer更新参数 325
46.1 Optimizer类 325
46.2 SGD类的实现 326
46.3 使用SGD类来解决问题 327
46.4 SGD以外的优化方法 328
步骤47 softmax函数和交叉熵误差 331
47.1 用于切片操作的函数 331
47.2 softmax函数 334
47.3 交叉熵误差 337
步骤48 多分类 340
48.1 螺旋数据集 340
48.2 用于训练的代码 341
步骤49 Dataset类和预处理 346
49.1 Dataset类的实现 346
49.2 大型数据集的情况 348
49.3 数据的连接 349
49.4 用于训练的代码 350
49.5 数据集的预处理 351
步骤50 用于取出小批量数据的DataLoader 354
50.1 什么是迭代器 354
50.2 使用DataLoader 358
50.3 accuracy函数的实现 359
50.4 螺旋数据集的训练代码 360
步骤51 MINST的训练 363
51.1 MNIST数据集 364
51.2 训练MNIST 366
51.3 改进模型 368
第5阶段 DeZero高级挑战 377
步骤52 支持GPU 379
52.1 CuPy的安装和使用方法 379
52.2 cuda模块 382
52.3 向Variable / Layer / DataLoader类添加代码 383
52.4 函数的相应修改 386
52.5 在GPU上训练MNIST 388
步骤53 模型的保存和加载 391
53.1 NumPy的save函数和load函数 391
53.2 Layer类参数的扁平化 394
53.3 Layer类的save函数和load函数 395
步骤54 Dropout和测试模式 398
54.1 什么是Dropout 398
54.2 Inverted Dropout 401
54.3 增加测试模式 401
54.4 Dropout的实现 402
步骤55 CNN的机制（1） 404
55.1 CNN的网络结构 404
55.2 卷积运算 405
55.3 填充 407
55.4 步幅 408
55.5 输出大小的计算方法 409
步骤56 CNN的机制（2）411
56.1 三阶张量 411
56.2 结合方块进行思考 412
56.3 小批量处理 414
56.4 池化层 415
步骤57 conv2d函数和pooling函数 418
57.1 使用im2col展开 418
57.2 conv2d函数的实现 420
57.3 Conv2d层的实现 425
57.4 pooling函数的实现 426
步骤58 具有代表性的CNN（VGG16）429
58.1 VGG16的实现 429
58.2 已训练的权重数据 431
58.3 使用已训练的VGG16 435
步骤59 使用RNN处理时间序列数据 438
59.1 RNN层的实现 438
59.2 RNN模型的实现 442
59.3 切断连接的方法 445
59.4 正弦波的预测 446
步骤60 LSTM与数据加载器 451
60.1 用于时间序列数据的数据加载器 451
60.2 LSTM层的实现 453
附录A inplace运算（步骤14的补充内容）463
A.1 问题确认 463
A.2 关于复制和覆盖 464
A.3 DeZero的反向传播 465
附录B 实现get_item函数（步骤47的补充内容）466
附录C 在Google Colaboratory上运行 469
后 记 473
参考文献 477
・ ・ ・ ・ ・ ・ (收起)推 荐 序 面对科技拐点，我们的判断与选择
中文版序 人工智能会放大认知能力
前 言 深度学习与智能的本质
第一部分 智能的新构想
01 机器学习的崛起
汽车新生态：无人驾驶将全面走入人们生活
自然语言翻译：从语言到句子的飞跃
语音识别：实时跨文化交流不再遥远
AI医疗：医学诊断将更加准确
金融科技：利用数据和算法获取最佳回报
深度法律：效率的提高与费用的降低
德州扑克：当机器智能学会了虚张声势
AlphaGo奇迹：神经科学与人工智能的协同
弗林效应：深度学习让人类更加智能
新教育体系：每个人都需要终身学习
正面影响：新兴技术不是生存威胁
回到未来：当人类智能遇到人工智能
02 人工智能的重生
看似简单的视觉识别
计算机视觉的进步
早期人工智能发展缓慢
从神经网络到人工智能
03 神经网络的黎明
深度学习的起点
从样本中学习
利用感知器区分性别
被低估的神经网络
04 大脑式的计算
网络模型能够模仿智能行为
神经网络先驱者
乔治・布尔与机器学习
利用神经科学理解大脑
大脑如何处理问题
计算神经科学的兴起
05 洞察视觉系统
人眼是如何看到东西的
大脑皮层中的视觉
突触的可塑性
通过阴影脑补立体全貌
视觉区域的层级结构
认知神经科学的诞生
第二部分 深度学习的演进
06 语音识别的突破
在嘈杂中找到你的声音
将独立分量分析应用于大脑
什么在操控我们的言行
07 霍普菲尔德网络和玻尔兹曼机
约翰・霍普菲尔德的伟大之处
内容可寻址存储器
局部最小值与全局最小值
玻尔兹曼机
赫布理论
学习识别镜像对称
学习识别手写数字
无监督学习和皮层发育
08 反向传播算法
算法的优化
语音合成的突破
神经网络的重生
理解真正的深度学习
神经网络的局限性
09 卷积学习
机器学习的稳步发展
卷积网络的渐进式改进
当深度学习遇到视觉层级结构
有工作记忆的神经网络
生成式对抗网络
应对现实社会的复杂性
10 奖励学习
机器如何学会下棋
大脑的奖励机制
用“感知-行动”框架提高绩效
学习如何翱翔
学习如何歌唱
人工智能的可塑性
更多需要被解决的问题
11 火爆的NIPS
为什么NIPS如此受欢迎
谁拥有最多数据，谁就是赢家
为未来做准备
第三部分 人类，智能与未来
12 智能时代
21世纪的生活
未来的身份认证
社交机器人的崛起
机器已经会识别人类面部表情
新技术改变教育方式
成为更好的学习者
训练你的大脑
智能商业
13 算法驱动
用算法把复杂问题简单化
理解、分析复杂系统
大脑的逻辑深度
尝试所有可能的策略
14 芯片崛起
神经形态芯片
视网膜芯片
神经形态工程
摩尔定律的终结
15 信息科学
用字节丈量世界
用数学思维解决通信难题
预测是如何产生的
深度理解大脑
大脑的操作系统
生物学与计算科学
人工智能能拥有媲美人类大脑的操作系统
16 生命与意识
视觉意识
视觉感知的过程
视觉感知的时机
视觉感知的部位
视觉搜索的机理
创造意识比理解意识更容易
17 进化的力量
大自然比我们聪明
认知科学的兴起
不能把语言问题只留给语言学家
难预测的行为规律
神经网络的寒冬
从深度学习到通用人工智能
18 深度智能
遗传密码
每个物种都有智能
进化的起源
人类终将解决智能难题
・ ・ ・ ・ ・ ・ (收起)第 1部分　PyTorch核心
第　1章 深度学习和PyTorch库简介　3
1．1　深度学习革命　4
1．2　PyTorch深度学习　5
1．3　为什么用PyTorch　6
1．4　PyTorch如何支持深度学习概述　8
1．5　硬件和软件要求　10
1．6　练习题　12
1．7　本章小结　13
第　2章 预训练网络　14
2．1　一个识别图像主体的预训练网络　15
2．1．1　获取一个预先训练好的网络用于图像识别　16
2．1．2　AlexNet　17
2．1．3　ResNet　19
2．1．4　准备运行　19
2．1．5　运行模型　21
2．2　一个足以以假乱真的预训练模型　23
2．2．1　GAN游戏　24
2．2．2　CycleGAN　25
2．2．3　一个把马变成斑马的网络　26
2．3　一个描述场景的预训练网络　29
2．4　Torch Hub　31
2．5　总结　32
2．6　练习题　32
2．7　本章小结　33
第3章　从张量开始　34
3．1　实际数据转为浮点数　34
3．2　张量：多维数组　36
3．2．1　从Python列表到PyTorch张量　36
3．2．2　构造第 1个张量　37
3．2．3　张量的本质　37
3．3　索引张量　40
3．4　命名张量　40
3．5　张量的元素类型　43
3．5．1　使用dtype指定数字类型　43
3．5．2　适合任何场合的dtype　44
3．5．3　管理张量的dtype属性　44
3．6　张量的API　45
3．7　张量的存储视图　46
3．7．1　索引存储区　47
3．7．2　修改存储值：就地操作　48
3．8　张量元数据：大小、偏移量和步长　48
3．8．1　另一个张量的存储视图　49
3．8．2　无复制转置　51
3．8．3　高维转置　52
3．8．4　连续张量　53
3．9　将张量存储到GPU　55
3．10　NumPy互操作性　57
3．11　广义张量也是张量　57
3．12　序列化张量　58
3．13　总结　60
3．14　练习题　60
3．15　本章小结　60
第4章　使用张量表征真实数据　61
4．1　处理图像　62
4．1．1　添加颜色通道　62
4．1．2　加载图像文件　63
4．1．3　改变布局　63
4．1．4　正规化数据　64
4．2　三维图像：体数据　65
4．3　表示表格数据　66
4．3．1　使用真实的数据集　67
4．3．2　加载葡萄酒数据张量　68
4．3．3　表示分数　70
4．3．4　独热编码　70
4．3．5　何时分类　72
4．3．6　寻找阈值　73
4．4　处理时间序列　75
4．4．1　增加时间维度　76
4．4．2　按时间段调整数据　77
4．4．3　准备训练　79
4．5　表示文本　81
4．5．1　将文本转化为数字　81
4．5．2　独热编码字符　82
4．5．3　独热编码整个词　83
4．5．4　文本嵌入　85
4．5．5　作为蓝图的文本嵌入　87
4．6　总结　88
4．7　练习题　88
4．8　本章小结　88
第5章　学习的机制　90
5．1　永恒的建模经验　90
5．2　学习就是参数估计　92
5．2．1　一个热点问题　93
5．2．2　收集一些数据　93
5．2．3　可视化数据　94
5．2．4　选择线性模型首试　94
5．3　减少损失是我们想要的　95
5．4　沿着梯度下降　98
5．4．1　减小损失　99
5．4．2　进行分析　99
5．4．3　迭代以适应模型　101
5．4．4　归一化输入　104
5．4．5　再次可视化数据　106
5．5　PyTorch自动求导：反向传播的一切　107
5．5．1　自动计算梯度　107
5．5．2　优化器　111
5．5．3　训练、验证和过拟合　115
5．5．4　自动求导更新及关闭　120
5．6　总结　121
5．7　练习题　122
5．8　本章小结　122
第6章　使用神经网络拟合数据　123
6．1　人工神经网络　124
6．1．1　组成一个多层网络　125
6．1．2　理解误差函数　125
6．1．3　我们需要的只是激活函数　126
6．1．4　更多激活函数　128
6．1．5　选择最佳激活函数　128
6．1．6　学习对于神经网络意味着什么　129
6．2　PyTorch nn模块　131
6．2．1　使用__call__()而不是forward()　132
6．2．2　回到线性模型　133
6．3　最终完成一个神经网络　137
6．3．1　替换线性模型　137
6．3．2　检查参数　138
6．3．3　与线性模型对比　141
6．4　总结　142
6．5　练习题　142
6．6　本章小结　142
第7章　区分鸟和飞机：从图像学习　143
7．1　微小图像数据集　143
7．1．1　下载CIFAR-10　144
7．1．2　Dataset类　145
7．1．3　Dataset变换　146
7．1．4　数据归一化　149
7．2　区分鸟和飞机　150
7．2．1　构建数据集　151
7．2．2　一个全连接模型　152
7．2．3　分类器的输出　153
7．2．4　用概率表示输出　154
7．2．5　分类的损失　157
7．2．6　训练分类器　159
7．2．7　全连接网络的局限　165
7．3　总结　167
7．4　练习题　167
7．5　本章小结　168
第8章　使用卷积进行泛化　169
8．1　卷积介绍　169
8．2　卷积实战　172
8．2．1　填充边界　173
8．2．2　用卷积检测特征　175
8．2．3　使用深度和池化技术进一步研究　177
8．2．4　为我们的网络整合一切　179
8．3　子类化nn．Module　181
8．3．1　将我们的网络作为一个nn．Module　182
8．3．2　PyTorch如何跟踪参数和子模块　183
8．3．3　函数式API　184
8．4　训练我们的convnet　185
8．4．1　测量精度　187
8．4．2　保存并加载我们的模型　188
8．4．3　在GPU上训练　188
8．5　模型设计　190
8．5．1　增加内存容量：宽度　191
8．5．2　帮助我们的模型收敛和泛化：正则化　192
8．5．3　深入学习更复杂的结构：深度　195
8．5．4　本节设计的比较　200
8．5．5　已经过时了　201
8．6　总结　201
8．7　练习题　201
8．8　本章小结　202
第　2部分 从现实世界的图像中学习：肺癌的早期检测
第9章　使用PyTorch来检测癌症　205
9．1　用例简介　205
9．2　为一个大型项目做准备　206
9．3　到底什么是CT扫描　207
9．4　项目：肺癌的端到端检测仪　210
9．4．1　为什么我们不把数据扔给神经网络直到它起作用呢　213
9．4．2　什么是结节　216
9．4．3　我们的数据来源：LUNA大挑战赛　217
9．4．4　下载LUNA数据集　218
9．5　总结　219
9．6　本章小结　219
第　10章 将数据源组合成统一的数据集　220
10．1　原始CT数据文件　222
10．2　解析LUNA的标注 数据　222
10．2．1　训练集和验证集　224
10．2．2　统一标注和候选 数据　225
10．3　加载单个CT扫描　227
10．4　使用病人坐标系定位结节　230
10．4．1　病人坐标系　230
10．4．2　CT扫描形状和体素大小　232
10．4．3　毫米和体素地址之间的转换　233
10．4．4　从CT扫描中取出一个结节　234
10．5　一个简单的数据集实现　235
10．5．1　使用getCtRawCandidate()函数缓存候选数组　238
10．5．2　在LunaDataset．__init__()中构造我们的数据集　238
10．5．3　分隔训练集和验证集　239
10．5．4　呈现数据　240
10．6　总结　241
10．7　练习题　241
10．8　本章小结　242
第　11章 训练分类模型以检测可疑肿瘤　243
11．1　一个基本的模型和训练循环　243
11．2　应用程序的主入口点　246
11．3　预训练和初始化　247
11．3．1　初始化模型和优化器　247
11．3．2　数据加载器的维护和供给　249
11．4　我们的首次神经网络设计　251
11．4．1　核心卷积　251
11．4．2　完整模型　254
11．5　训练和验证模型　257
11．5．1　computeBatchLoss()函数　258
11．5．2　类似的验证循环　260
11．6　输出性能指标　261
11．7　运行训练脚本　265
11．7．1　训练所需的数据　266
11．7．2　插曲：enumerateWithEstimate()函数　266
11．8　评估模型：得到99．7%的正确率是否意味着我们完成了任务　268
11．9　用TensorBoard绘制训练指标　269
11．9．1　运行TensorBoard　269
11．9．2　增加TensorBoard对指标记录函数的支持　272
11．10　为什么模型不学习检测结节　274
11．11　总结　275
11．12　练习题　275
11．13　本章小结　275
第　12章 通过指标和数据增强来提升训练　277
12．1　高级改进计划　278
12．2　好狗与坏狗：假阳性与假阴性　279
12．3　用图表表示阳性与阴性　280
12．3．1　召回率是Roxie的强项　282
12．3．2　精度是Preston的强项　283
12．3．3　在logMetrics()中实现精度和召回率　284
12．3．4　我们的终极性能指标：F1分数　285
12．3．5　我们的模型在新指标下表现如何　289
12．4　理想的数据集是什么样的　290
12．4．1　使数据看起来更理想化　292
12．4．2　使用平衡的LunaDataset与之前的数据集运行情况对比　296
12．4．3　认识过拟合　298
12．5　重新审视过拟合的问题　300
12．6　通过数据增强防止过拟合　300
12．6．1　具体的数据增强技术　301
12．6．2　看看数据增强带来的改进　306
12．7　总结　308
12．8　练习题　308
12．9　本章小结　309
第　13章 利用分割法寻找可疑结节　310
13．1　向我们的项目添加第 2个模型　310
13．2　各种类型的分割　312
13．3　语义分割：逐像素分类　313
13．4　更新分割模型　317
13．5　更新数据集以进行分割　319
13．5．1　U-Net有非常具体的对输入大小的要求　320
13．5．2　U-Net对三维和二维数据的权衡　320
13．5．3　构建真实、有效的数据集　321
13．5．4　实现Luna2dSegmentationDataset　327
13．5．5　构建训练和验证数据　331
13．5．6　实现TrainingLuna2dSegmentationDataset　332
13．5．7　在GPU上增强数据　333
13．6　更新用于分割的训练脚本　335
13．6．1　初始化分割和增强模型　336
13．6．2　使用Adam优化器　336
13．6．3　骰子损失　337
13．6．4　将图像导入TensorBoard　340
13．6．5　更新指标日志　343
13．6．6　保存模型　344
13．7　结果　345
13．8　总结　348
13．9　练习题　348
13．10　本章小结　349
第　14章 端到端的结节分析及下一步的方向　350
14．1　接近终点线　350
14．2　验证集的独立性　352
14．3　连接CT分割和候选结节分类　353
14．3．1　分割　354
14．3．2　将体素分组为候选结节　355
14．3．3　我们发现结节了吗？分类以减少假阳性　357
14．4　定量验证　360
14．5　预测恶性肿瘤　361
14．5．1　获取恶性肿瘤信息　361
14．5．2　曲线基线下的区域：按直径分类　362
14．5．3　重用预先存在的权重：微调　365
14．5．4　TensorBoard中的输出　370
14．6　在诊断时所见的内容　374
14．7　接下来呢？其他灵感和数据的来源　376
14．7．1　防止过拟合：更好的正则化　377
14．7．2　精细化训练数据　379
14．7．3　竞赛结果及研究论文　380
14．8　总结　381
14．9　练习题　382
14．10　本章小结　383
第3部分　部署
第　15章 部署到生产环境　387
15．1　PyTorch模型的服务　388
15．1．1　支持Flask服务的模型　388
15．1．2　我们想从部署中得到的东西　390
15．1．3　批处理请求　391
15．2　导出模型　395
15．2．1　PyTorch与ONNX的互操作性　396
15．2．2　PyTorch自己的导出：跟踪　397
15．2．3　具有跟踪模型的服务器　398
15．3　与PyTorch JIT编译器交互　398
15．3．1　超越经典Python/PyTorch的期望是什么　399
15．3．2　PyTorch作为接口和后端的双重特性　400
15．3．3　TorchScript　400
15．3．4　为可追溯的差异编写脚本　404
15．4　LibTorch：C++中的PyTorch　405
15．4．1　从C++中运行JITed模型　405
15．4．2　从C++ API开始　408
15．5　部署到移动设备　411
15．6　新兴技术：PyTorch
模型的企业服务　416
15．7　总结　416
15．8　练习题　416
15．9　本章小结　416
・ ・ ・ ・ ・ ・ (收起)第1章 互联网的增长引擎――推荐系统
1.1 为什么推荐系统是互联网的增长引擎
1.1.1 推荐系统的作用和意义
1.1.2 推荐系统与YouTube的观看时长增长
1.1.3 推荐系统与电商网站的收入增长
1.2 推荐系统的架构
1.2.1 推荐系统的逻辑框架
1.2.2 推荐系统的技术架构
1.2.3 推荐系统的数据部分
1.2.4 推荐系统的模型部分
1.2.5 深度学习对推荐系统的革命性贡献
1.2.6 把握整体，补充细节
1.3 本书的整体结构
第2章 前深度学习时代――推荐系统的进化之路
2.1 传统推荐模型的演化关系图
2.2 协同过滤――经典的推荐算法
2.2.1 什么是协同过滤
2.2.2 用户相似度计算
2.2.3 终结果的排序
2.2.4 ItemCF
2.2.5 UserCF与ItemCF的应用场景
2.2.6 协同过滤的下一步发展
2.3 矩阵分解算法――协同过滤的进化
2.3.1 矩阵分解算法的原理
2.3.2 矩阵分解的求解过程
2.3.3 消除用户和物品打分的偏差
2.3.4 矩阵分解的优点和局限性
2.4 逻辑回归――融合多种特征的推荐模型
2.4.1 基于逻辑回归模型的推荐流程
2.4.2 逻辑回归模型的数学形式
2.4.3 逻辑回归模型的训练方法
2.4.4 逻辑回归模型的优势
2.4.5 逻辑回归模型的局限性
2.5 从FM到FFM――自动特征交叉的解决方案
2.5.1 POLY2模型――特征交叉的开始
2.5.2 FM模型――隐向量特征交叉
2.5.3 FFM模型――引入特征域的概念
2.5.4 从POLY2到FFM的模型演化过程
2.6 GBDT+LR――特征工程模型化的开端
2.6.1 GBDT+LR组合模型的结构
2.6.2 GBDT进行特征转换的过程
2.6.3 GBDT+LR 组合模型开启的特征工程新趋势
2.7 LS-PLM――阿里巴巴曾经的主流推荐模型
2.7.1 LS-PLM 模型的主要结构
2.7.2 LS-PLM模型的优点
2.7.3 从深度学习的角度重新审视LS-PLM模型
2.8 总结――深度学习推荐系统的前夜
第3章 浪潮之巅――深度学习在推荐系统中的应用
3.1 深度学习推荐模型的演化关系图
3.2 AutoRec――单隐层神经网络推荐模型
3.2.1 AutoRec模型的基本原理
3.2.2 AutoRec模型的结构
3.2.3 基于AutoRec模型的推荐过程
3.2.4 AutoRec模型的特点和局限性
3.3 Deep Crossing模型――经典的深度学习架构
3.3.1 Deep Crossing模型的应用场景
3.3.2 Deep Crossing模型的网络结构
3.3.3 Deep Crossing模型对特征交叉方法的革命
3.4 NeuralCF模型――CF与深度学习的结合
3.4.1 从深度学习的视角重新审视矩阵分解模型
3.4.2 NeuralCF模型的结构
3.4.3 NeuralCF模型的优势和局限性
3.5 PNN模型――加强特征交叉能力
3.5.1 PNN模型的网络架构
3.5.2 Product层的多种特征交叉方式
3.5.3 PNN模型的优势和局限性
3.6 Wide&Deep 模型――记忆能力和泛化能力的综合
3.6.1 模型的记忆能力与泛化能力
3.6.2 Wide&Deep模型的结构
3.6.3 Wide&Deep模型的进化――Deep&Cross模型
3.6.4 Wide&Deep模型的影响力
3.7 FM与深度学习模型的结合
3.7.1 FNN――用FM的隐向量完成Embedding层初始化
3.7.2 DeepFM――用FM代替Wide部分
3.7.3 NFM――FM的神经网络化尝试
3.7.4 基于FM的深度学习模型的优点和局限性
3.8 注意力机制在推荐模型中的应用
3.8.1 AFM――引入注意力机制的FM
3.8.2 DIN――引入注意力机制的深度学习网络
3.8.3 注意力机制对推荐系统的启发
3.9 DIEN――序列模型与推荐系统的结合
3.9.1 DIEN的“进化”动机
3.9.2 DIEN模型的架构
3.9.3 兴趣抽取层的结构
3.9.4 兴趣进化层的结构
3.9.5 序列模型对推荐系统的启发
3.10 强化学习与推荐系统的结合
3.10.1 深度强化学习推荐系统框架
3.10.2 深度强化学习推荐模型
3.10.3 DRN的学习过程
3.10.4 DRN的在线学习方法――竞争梯度下降算法
3.10.5 强化学习对推荐系统的启发
3.11 总结――推荐系统的深度学习时代
第4章 Embedding技术在推荐系统中的应用
4.1 什么是Embedding
4.1.1 词向量的例子
4.1.2 Embedding 技术在其他领域的扩展
4.1.3 Embedding 技术对于深度学习推荐系统的重要性
4.2 Word2vec――经典的Embedding方法
4.2.1 什么是Word2vec
4.2.2 Word2vec模型的训练过程
4.2.3 Word2vec的“负采样”训练方法
4.2.4 Word2vec对Embedding技术的奠基性意义
4.3 Item2vec――Word2vec 在推荐系统领域的推广
4.3.1 Item2vec的基本原理
4.3.2 “广义”的Item2vec
4.3.3 Item2vec方法的特点和局限性
4.4 Graph Embedding――引入更多结构信息的图嵌入技术
4.4.1 DeepWalk――基础的Graph Embedding方法
4.4.2 Node2vec――同质性和结构性的权衡
4.4.3 EGES――阿里巴巴的综合性Graph Embedding方法
4.5 Embedding与深度学习推荐系统的结合
4.5.1 深度学习网络中的Embedding层
4.5.2 Embedding的预训练方法
4.5.3 Embedding作为推荐系统召回层的方法
4.6 局部敏感哈希――让Embedding插上翅膀的快速搜索方法
4.6.1 “快速”Embedding近邻搜索
4.6.2 局部敏感哈希的基本原理
4.6.3 局部敏感哈希多桶策略
4.7 总结――深度学习推荐系统的核心操作
第5章 多角度审视推荐系统
5.1 推荐系统的特征工程
5.1.1 构建推荐系统特征工程的原则
5.1.2 推荐系统中的常用特征
5.1.3 常用的特征处理方法
5.1.4 特征工程与业务理解
5.2 推荐系统召回层的主要策略
5.2.1 召回层和排序层的功能特点
5.2.2 多路召回策略
5.2.3 基于Embedding的召回方法
5.3 推荐系统的实时性
5.3.1 为什么说推荐系统的实时性是重要的
5.3.2 推荐系统“特征”的实时性
5.3.3 推荐系统“模型”的实时性
5.3.4 用“木桶理论”看待推荐系统的迭代升级
5.4 如何合理设定推荐系统中的优化目标
5.4.1 YouTube以观看时长为优化目标的合理性
5.4.2 模型优化和应用场景的统一性
5.4.3 优化目标是和其他团队的接口性工作
5.5 推荐系统中比模型结构更重要的是什么
5.5.1 有解决推荐问题的“银弹”吗
5.5.2 Netflix对用户行为的观察
5.5.3 观察用户行为，在模型中加入有价值的用户信息
5.5.4 DIN模型的改进动机
5.5.5 算法工程师不能只是一个“炼金术士”
5.6 冷启动的解决办法
5.6.1 基于规则的冷启动过程
5.6.2 丰富冷启动过程中可获得的用户和物品特征
5.6.3 利用主动学习、迁移学习和“探索与利用”机制
5.6.4 “巧妇难为无米之炊”的困境
5.7 探索与利用
5.7.1 传统的探索与利用方法
5.7.2 个性化的探索与利用方法
5.7.3 基于模型的探索与利用方法
5.7.4 “探索与利用”机制在推荐系统中的应用
第6章 深度学习推荐系统的工程实现
6.1 推荐系统的数据流
6.1.1 批处理大数据架构
6.1.2 流计算大数据架构
6.1.3 Lambda架构
6.1.4 Kappa架构
6.1.5 大数据平台与推荐系统的整合
6.2 推荐模型离线训练之Spark MLlib
6.2.1 Spark的分布式计算原理
6.2.2 Spark MLlib的模型并行训练原理
6.2.3 Spark MLlib并行训练的局限性
6.3 推荐模型离线训练之Parameter Server
6.3.1 Parameter Server的分布式训练原理
6.3.2 一致性与并行效率之间的取舍
6.3.3 多server节点的协同和效率问题
6.3.4 Parameter Server技术要点总结
6.4 推荐模型离线训练之TensorFlow
6.4.1 TensorFlow的基本原理
6.4.2 TensorFlow基于任务关系图的并行训练过程
6.4.3 TensorFlow的单机训练与分布式训练模式
6.4.4 TensorFlow技术要点总结
6.5 深度学习推荐模型的上线部署
6.5.1 预存推荐结果或Embedding结果
6.5.2 自研模型线上服务平台
6.5.3 预训练Embedding+轻量级线上模型
6.5.4 利用PMML转换并部署模型
6.5.5 TensorFlow Serving
6.5.6 灵活选择模型服务方法
6.6 工程与理论之间的权衡
6.6.1 工程师职责的本质
6.6.2 Redis容量和模型上线方式之间的权衡
6.6.3 研发周期限制和技术选型的权衡
6.6.4 硬件平台环境和模型结构间的权衡
6.6.5 处理好整体和局部的关系
第7章 推荐系统的评估
7.1 离线评估方法与基本评价指标
7.1.1 离线评估的主要方法
7.1.2 离线评估的指标
7.2 直接评估推荐序列的离线指标
7.2.1 P-R曲线
7.2.2 ROC曲线
7.2.3 平均精度均值
7.2.4 合理选择评估指标
7.3 更接近线上环境的离线评估方法――Replay
7.3.1 模型评估的逻辑闭环
7.3.2 动态离线评估方法
7.3.3 Netflix的Replay评估方法实践
7.4 A/B测试与线上评估指标
7.4.1 什么是A/B测试
7.4.2 A/B测试的“分桶”原则
7.4.3 线上A/B测试的评估指标
7.5 快速线上评估方法――Interleaving
7.5.1 传统A/B测试存在的统计学问题
7.5.2 Interleaving方法的实现
7.5.3 Interleaving方法与传统A/B测试的灵敏度比较
7.5.4 Interleaving方法指标与A/B测试指标的相关性
7.5.5 Interleaving方法的优点与缺点
7.6 推荐系统的评估体系
第8章 深度学习推荐系统的前沿实践
8.1 Facebook的深度学习推荐系统
8.1.1 推荐系统应用场景
8.1.2 以GBDT+LR组合模型为基础的CTR预估模型
8.1.3 实时数据流架构
8.1.4 降采样和模型校正
8.1.5 Facebook GBDT+LR组合模型的工程实践
8.1.6 Facebook的深度学习模型DLRM
8.1.7 DLRM模型并行训练方法
8.1.8 DLRM模型的效果
8.1.9 Facebook深度学习推荐系统总结
8.2 Airbnb基于Embedding的实时搜索推荐系统
8.2.1 推荐系统应用场景
8.2.2 基于短期兴趣的房源Embedding方法
8.2.3 基于长期兴趣的用户Embedding和房源Embedding
8.2.4 Airbnb搜索词的Embedding
8.2.5 Airbnb的实时搜索排序模型及其特征工程
8.2.6 Airbnb实时搜索推荐系统总结
8.3 YouTube深度学习视频推荐系统
8.3.1 推荐系统应用场景
8.3.2 YouTube推荐系统架构
8.3.3 候选集生成模型
8.3.4 候选集生成模型独特的线上服务方法
8.3.5 排序模型
8.3.6 训练和测试样本的处理
8.3.7 如何处理用户对新视频的偏好
8.3.8 YouTube深度学习视频推荐系统总结
8.4 阿里巴巴深度学习推荐系统的进化
8.4.1 推荐系统应用场景
8.4.2 阿里巴巴的推荐模型体系
8.4.3 阿里巴巴深度学习推荐模型的进化过程
8.4.4 模型服务模块的技术架构
8.4.5 阿里巴巴推荐技术架构总结
第9章 构建属于你的推荐系统知识框架
9.1 推荐系统的整体知识架构图
9.2 推荐模型发展的时间线
9.3 如何成为一名优秀的推荐工程师
9.3.1 推荐工程师的4项能力
9.3.2 能力的深度和广度
9.3.3 推荐工程师的能力总结
后记
・ ・ ・ ・ ・ ・ (收起)第1 章绪论1
1.1 简介2
1.2 图深度学习的动机2
1.3 本书内容4
1.4 本书读者定位6
1.5 图特征学习的简要发展史7
1.5.1 图特征选择8
1.5.2 图表示学习9
1.6 小结10
1.7 扩展阅读11
第1 篇基础理论
第2 章图论基础15
2.1 简介16
2.2 图的表示16
2.3 图的性质17
2.3.1 度17
2.3.2 连通度19
2.3.3 中心性21
2.4 谱图论24
2.4.1 拉普拉斯矩阵24
2.4.2 拉普拉斯矩阵的特征值和特征向量26
2.5 图信号处理27
2.6 复杂图30
2.6.1 异质图30
2.6.2 二分图30
2.6.3 多维图31
2.6.4 符号图32
2.6.5 超图33
2.6.6 动态图33
2.7 图的计算任务34
2.7.1 侧重于节点的任务35
2.7.2 侧重于图的任务36
2.8 小结37
2.9 扩展阅读37
第3 章深度学习基础39
3.1 简介40
3.2 深度前馈神经网络41
3.2.1 网络结构42
3.2.2 激活函数43
3.2.3 输出层和损失函数45
3.3 卷积神经网络47
3.3.1 卷积操作和卷积层48
3.3.2 实际操作中的卷积层51
3.3.3 非线性激活层52
3.3.4 池化层53
3.3.5 卷积神经网络总体框架53
3.4 循环神经网络54
3.4.1 传统循环神经网络的网络结构55
3.4.2 长短期记忆网络56
3.4.3 门控循环单元58
3.5 自编码器59
3.5.1 欠完备自编码器59
3.5.2 正则化自编码器60
3.6 深度神经网络的训练61
3.6.1 梯度下降61
3.6.2 反向传播62
3.6.3 预防过拟合64
3.7 小结65
3.8 扩展阅读65
第2 篇模型方法
第4 章图嵌入69
4.1 简介70
4.2 简单图的图嵌入71
4.2.1 保留节点共现71
4.2.2 保留结构角色80
4.2.3 保留节点状态83
4.2.4 保留社区结构84
4.3 复杂图的图嵌入86
4.3.1 异质图嵌入87
4.3.2 二分图嵌入89
4.3.3 多维图嵌入90
4.3.4 符号图嵌入91
4.3.5 超图嵌入93
4.3.6 动态图嵌入95
4.4 小结96
4.5 扩展阅读97
第5 章图神经网络99
5.1 简介100
5.2 图神经网络基本框架102
5.2.1 侧重于节点的任务的图神经网络框架102
5.2.2 侧重于图的任务的图神经网络框架103
5.3 图滤波器104
5.3.1 基于谱的图滤波器104
5.3.2 基于空间的图滤波器114
5.4 图池化120
5.4.1 平面图池化120
5.4.2 层次图池化121
5.5 图卷积神经网络的参数学习125
5.5.1 节点分类中的参数学习126
5.5.2 图分类中的参数学习126
5.6 小结127
5.7 扩展阅读128
第6 章图神经网络的健壮性129
6.1 简介130
6.2 图对抗攻击130
6.2.1 图对抗攻击的分类131
6.2.2 白盒攻击132
6.2.3 灰盒攻击135
6.2.4 黑盒攻击139
6.3 图对抗防御142
6.3.1 图对抗训练142
6.3.2 图净化144
6.3.3 图注意力机制144
6.3.4 图结构学习148
6.4 小结149
6.5 扩展阅读149
第7 章可扩展图神经网络151
7.1 简介152
7.2 逐点采样法155
7.3 逐层采样法158
7.4 子图采样法162
7.5 小结164
7.6 扩展阅读164
第8 章复杂图神经网络165
8.1 简介166
8.2 异质图神经网络166
8.3 二分图神经网络168
8.4 多维图神经网络168
8.5 符号图神经网络170
8.6 超图神经网络173
8.7 动态图神经网络174
8.8 小结175
8.9 扩展阅读175
第9 章图上的其他深度模型177
9.1 简介178
9.2 图上的自编码器178
9.3 图上的循环神经网络180
9.4 图上的变分自编码器182
9.4.1 用于节点表示学习的变分自编码器184
9.4.2 用于图生成的变分自编码器184
9.4.3 编码器：推论模型185
9.4.4 解码器: 生成模型186
9.4.5 重建的损失函数186
9.5 图上的生成对抗网络187
9.5.1 用于节点表示学习的生成对抗网络188
9.5.2 用于图生成的生成对抗网络189
9.6 小结191
9.7 扩展阅读191
第3 篇实际应用
第10 章自然语言处理中的图神经网络195
10.1 简介196
10.2 语义角色标注196
10.3 神经机器翻译199
10.4 关系抽取199
10.5 问答系统200
10.5.1 多跳问答任务201
10.5.2 Entity-GCN 202
10.6 图到序列学习203
10.7 知识图谱中的图神经网络205
10.7.1 知识图谱中的图滤波205
10.7.2 知识图谱到简单图的转换206
10.7.3 知识图谱补全207
10.8 小结208
10.9 扩展阅读208
第11 章计算机视觉中的图神经网络209
11.1 简介210
11.2 视觉问答210
11.2.1 图像表示为图211
11.2.2 图像和问题表示为图212
11.3 基于骨架的动作识别214
11.4 图像分类215
11.4.1 零样本图像分类216
11.4.2 少样本图像分类217
11.4.3 多标签图像分类218
11.5 点云学习219
11.6 小结220
11.7 扩展阅读220
第12 章数据挖掘中的图神经网络221
12.1 简介222
12.2 万维网数据挖掘222
12.2.1 社交网络分析222
12.2.2 推荐系统225
12.3 城市数据挖掘229
12.3.1 交通预测229
12.3.2 空气质量预测231
12.4 网络安全数据挖掘231
12.4.1 恶意账户检测231
12.4.2 虚假新闻检测233
12.5 小结234
12.6 扩展阅读234
第13 章生物化学和医疗健康中的
图神经网络235
13.1 简介236
13.2 药物开发与发现236
13.2.1 分子表示学习236
13.2.2 蛋白质相互作用界面预测237
13.2.3 药物C靶标结合亲和力预测239
13.3 药物相似性整合240
13.4 复方药物副作用预测242
13.5 疾病预测244
13.6 小结245
13.7 扩展阅读245
第4 篇前沿进展
第14 章图神经网络的高级方法249
14.1 简介250
14.2 深层图神经网络250
14.2.1 Jumping Knowledge 252
14.2.2 DropEdge 253
14.2.3 PairNorm 253
14.3 通过自监督学习探索未标记数据253
14.3.1 侧重于节点的任务254
14.3.2 侧重于图的任务256
14.4 图神经网络的表达能力257
14.4.1 WL 测试258
14.4.2 表达能力259
14.5 小结260
14.6 扩展阅读260
第15 章图神经网络的高级应用261
15.1 简介262
15.2 图的组合优化262
15.3 学习程序表示264
15.4 物理学中相互作用的动力系统推断265
15.5 小结266
15.6 扩展阅读266
参考文献267
索引295
・ ・ ・ ・ ・ ・ (收起)Contents 目　　录
前言
第一部分　PyTorch基础
第1章　Numpy基础2
1.1　生成Numpy数组3
1.1.1　从已有数据中创建数组3
1.1.2　利用random模块生成数组4
1.1.3　创建特定形状的多维数组5
1.1.4　利用arange、linspace函数生成数组6
1.2　获取元素7
1.3　Numpy的算术运算9
1.3.1　对应元素相乘9
1.3.2　点积运算10
1.4　数组变形11
1.4.1　更改数组的形状11
1.4.2　合并数组14
1.5　批量处理16
1.6　通用函数17
1.7　广播机制19
1.8　小结20
第2章　PyTorch基础21
2.1　为何选择PyTorch？21
2.2　安装配置22
2.2.1　安装CPU版PyTorch22
2.2.2　安装GPU版PyTorch24
2.3　Jupyter Notebook环境配置26
2.4　Numpy与Tensor28
2.4.1　Tensor概述28
2.4.2　创建Tensor28
2.4.3　修改Tensor形状30
2.4.4　索引操作31
2.4.5　广播机制32
2.4.6　逐元素操作32
2.4.7　归并操作33
2.4.8　比较操作34
2.4.9　矩阵操作35
2.4.10　PyTorch与Numpy比较35
2.5　Tensor与Autograd36
2.5.1　自动求导要点36
2.5.2　计算图37
2.5.3　标量反向传播38
2.5.4　非标量反向传播39
2.6　使用Numpy实现机器学习41
2.7　使用Tensor及Antograd实现机器学习44
2.8　使用TensorFlow架构46
2.9　小结48
第3章　PyTorch神经网络工具箱49
3.1　神经网络核心组件49
3.2　实现神经网络实例50
3.2.1　背景说明51
3.2.2　准备数据52
3.2.3　可视化源数据53
3.2.4　构建模型53
3.2.5　训练模型54
3.3　如何构建神经网络？56
3.3.1　构建网络层56
3.3.2　前向传播57
3.3.3　反向传播57
3.3.4　训练模型58
3.4　神经网络工具箱nn58
3.4.1　nn.Module58
3.4.2　nn.functional58
3.5　优化器59
3.6　动态修改学习率参数60
3.7　优化器比较60
3.8　小结62
第4章　PyTorch数据处理工具箱63
4.1　数据处理工具箱概述63
4.2　utils.data简介64
4.3　torchvision简介66
4.3.1　transforms67
4.3.2　ImageFolder67
4.4　可视化工具69
4.4.1　tensorboardX简介69
4.4.2　用tensorboardX可视化神经网络71
4.4.3　用tensorboardX可视化损失值72
4.4.4　用tensorboardX可视化特征图73
4.5　本章小结74
第二部分　深度学习基础
第5章　机器学习基础76
5.1　机器学习的基本任务76
5.1.1　监督学习77
5.1.2　无监督学习77
5.1.3　半监督学习78
5.1.4　强化学习78
5.2　机器学习一般流程78
5.2.1　明确目标79
5.2.2　收集数据79
5.2.3　数据探索与预处理79
5.2.4　选择模型及损失函数80
5.2.5　评估及优化模型81
5.3　过拟合与欠拟合81
5.3.1　权重正则化82
5.3.2　Dropout正则化83
5.3.3　批量正则化86
5.3.4　权重初始化88
5.4　选择合适激活函数89
5.5　选择合适的损失函数90
5.6　选择合适优化器92
5.6.1　传统梯度优化的不足93
5.6.2　动量算法94
5.6.3　AdaGrad算法96
5.6.4　RMSProp算法97
5.6.5　Adam算法98
5.7　GPU加速99
5.7.1　单GPU加速100
5.7.2　多GPU加速101
5.7.3　使用GPU注意事项104
5.8　本章小结104
第6章　视觉处理基础105
6.1　卷积神经网络简介105
6.2　卷积层107
6.2.1　卷积核108
6.2.2　步幅109
6.2.3　填充111
6.2.4　多通道上的卷积111
6.2.5　激活函数113
6.2.6　卷积函数113
6.2.7　转置卷积114
6.3　池化层115
6.3.1　局部池化116
6.3.2　全局池化117
6.4　现代经典网络119
6.4.1　LeNet-5模型119
6.4.2　AlexNet模型120
6.4.3　VGG模型121
6.4.4　GoogleNet模型122
6.4.5　ResNet模型123
6.4.6　胶囊网络简介124
6.5　PyTorch实现CIFAR-10多分类125
6.5.1　数据集说明125
6.5.2　加载数据125
6.5.3　构建网络127
6.5.4　训练模型128
6.5.5　测试模型129
6.5.6　采用全局平均池化130
6.5.7　像Keras一样显示各层参数131
6.6　模型集成提升性能133
6.6.1　使用模型134
6.6.2　集成方法134
6.6.3　集成效果135
6.7　使用现代经典模型提升性能136
6.8　本章小结137
第7章　自然语言处理基础138
7.1　循环神经网络基本结构138
7.2　前向传播与随时间反向传播140
7.3　循环神经网络变种143
7.3.1　LSTM144
7.3.2　GRU145
7.3.3　Bi-RNN146
7.4　循环神经网络的PyTorch实现146
7.4.1　RNN实现147
7.4.2　LSTM实现149
7.4.3　GRU实现151
7.5　文本数据处理152
7.6　词嵌入153
7.6.1　Word2Vec原理154
7.6.2　CBOW模型155
7.6.3　Skip-Gram模型155
7.7　PyTorch实现词性判别156
7.7.1　词性判别主要步骤156
7.7.2　数据预处理157
7.7.3　构建网络157
7.7.4　训练网络158
7.7.5　测试模型160
7.8　用LSTM预测股票行情160
7.8.1　 导入数据160
7.8.2　数据概览161
7.8.3　预处理数据162
7.8.4　定义模型163
7.8.5　训练模型163
7.8.6　测试模型164
7.9　循环神经网络应用场景165
7.10　小结166
第8章　生成式深度学习167
8.1　用变分自编码器生成图像167
8.1.1　自编码器168
8.1.2　变分自编码器168
8.1.3　用变分自编码器生成图像169
8.2　GAN简介173
8.2.1　GAN架构173
8.2.2　GAN的损失函数174
8.3　用GAN生成图像175
8.3.1　判别器175
8.3.2　生成器175
8.3.3　训练模型175
8.3.4　可视化结果177
8.4　VAE与GAN的优缺点178
8.5　ConditionGAN179
8.5.1　CGAN的架构179
8.5.2　CGAN生成器180
8.5.3　CGAN判别器180
8.5.4　CGAN损失函数181
8.5.5　CGAN可视化181
8.5.6　查看指定标签的数据182
8.5.7　可视化损失值182
8.6　DCGAN183
8.7　提升GAN训练效果的一些技巧184
8.8　小结185
第三部分　深度学习实践
第9章　人脸检测与识别188
9.1　人脸识别一般流程188
9.2　人脸检测189
9.2.1　目标检测189
9.2.2　人脸定位191
9.2.3　人脸对齐191
9.2.4　MTCNN算法192
9.3　特征提取193
9.4　人脸识别198
9.4.1　人脸识别主要原理198
9.4.2　人脸识别发展198
9.5　PyTorch实现人脸检测与识别199
9.5.1　验证检测代码199
9.5.2　检测图像200
9.5.3　检测后进行预处理200
9.5.4　查看经检测后的图像201
9.5.5　人脸识别202
9.6　小结202
第10章　迁移学习实例203
10.1　迁移学习简介203
10.2　特征提取204
10.2.1　PyTorch提供的预处理模块205
10.2.2　特征提取实例206
10.3　数据增强209
10.3.1　按比例缩放209
10.3.2　裁剪210
10.3.3　翻转210
10.3.4　改变颜色211
10.3.5　组合多种增强方法211
10.4　微调实例212
10.4.1　数据预处理212
10.4.2　加载预训练模型213
10.4.3　修改分类器213
10.4.4　选择损失函数及优化器213
10.4.5　训练及验证模型214
10.5　清除图像中的雾霾214
10.6　小结217
第11章　神经网络机器翻译实例218
11.1　Encoder-Decoder模型原理218
11.2　注意力框架220
11.3　PyTorch实现注意力Decoder224
11.3.1　构建Encoder224
11.3.2　构建简单Decoder225
11.3.3　构建注意力Decoder226
11.4　用注意力机制实现中英文互译227
11.4.1　导入需要的模块228
11.4.2　数据预处理228
11.4.3　构建模型231
11.4.4　训练模型234
11.4.5　随机采样，对模型进行测试235
11.4.6　可视化注意力236
11.5　小结237
第12章　实战生成式模型238
12.1　DeepDream模型238
12.1.1　Deep Dream原理238
12.1.2　DeepDream算法流程239
12.1.3　用PyTorch实现Deep Dream240
12.2　风格迁移243
12.2.1　内容损失244
12.2.2　风格损失245
12.2.3　用PyTorch实现神经网络风格迁移247
12.3　PyTorch实现图像修复252
12.3.1　网络结构252
12.3.2　损失函数252
12.3.3　图像修复实例253
12.4　PyTorch实现DiscoGAN255
12.4.1　DiscoGAN架构256
12.4.2　损失函数258
12.4.3　DiscoGAN实现258
12.4.4　用PyTorch实现从边框生成鞋子260
12.5　小结262
第13章　Caffe2模型迁移实例263
13.1　Caffe2简介263
13.2　Caffe如何升级到Caffe2264
13.3　PyTorch如何迁移到Caffe2265
13.4　小结268
第14章　AI新方向：对抗攻击269
14.1　对抗攻击简介269
14.1.1　白盒攻击与黑盒攻击270
14.1.2　无目标攻击与有目标攻击270
14.2　常见对抗样本生成方式271
14.2.1　快速梯度符号法271
14.2.2　快速梯度算法271
14.3　PyTorch实现对抗攻击272
14.3.1　实现无目标攻击272
14.3.2　实现有目标攻击274
14.4　对抗攻击和防御措施276
14.4.1　对抗攻击276
14.4.2　常见防御方法分类276
14.5　总结277
第15章　强化学习278
15.1　强化学习简介278
15.2　Q-Learning原理281
15.2.1　Q-Learning主要流程281
15.2.2　Q函数282
15.2.3　贪婪策略283
15.3　用PyTorch实现Q-Learning283
15.3.1　定义Q-Learing主函数283
15.3.2　执行Q-Learing284
15.4　SARSA算法285
15.4.1　SARSA算法主要步骤285
15.4.2　用PyTorch实现SARSA算法286
15.5　小结287
第16章　深度强化学习288
16.1　DQN算法原理288
16.1.1　Q-Learning方法的局限性289
16.1.2　用DL处理RL需要解决的问题289
16.1.3　用DQN解决方法289
16.1.4　定义损失函数290
16.1.5　DQN的经验回放机制290
16.1.6　目标网络290
16.1.7　网络模型291
16.1.8　DQN算法291
16.2　用PyTorch实现DQN算法292
16.3　小结295
附录A　PyTorch0.4版本变更296
附录B　AI在各行业的最新应用301
・ ・ ・ ・ ・ ・ (收起)目录

第1章 深度学习简介：为什么应该学习深度学习 1
1.1　欢迎阅读《深度学习图解》 1
1.2　为什么要学习深度学习 2
1.3　这很难学吗? 3
1.4　为什么要阅读本书 3
1.5　准备工作 4
1.6　你可能需要掌握一部分Python知识 5
1.7　本章小结 6
第2章 基本概念：机器该如何学习？ 7
2.1　什么是深度学习? 7
2.2　什么是机器学习？ 8
2.3　监督机器学习 9
2.4　无监督机器学习 10
2.5　参数学习和非参数学习 10
2.6　监督参数学习 11
2.7　无监督参数学习 13
2.8　非参数学习 14
2.9　本章小结 15
第3章 神经网络预测导论：前向传播 17
3.1　什么是预测 17
3.2　能够进行预测的简单神经网络 19
3.3　什么是神经网络? 20
3.4　这个神经网络做了什么? 21
3.5　使用多个输入进行预测 23
3.6　多个输入：这个神经网络做了什么? 24
3.7　多个输入：完整的可运行代码 29
3.8　预测多个输出 30
3.9　使用多个输入和输出进行预测 32
3.10　多输入多输出神经网络的工作原理 33
3.11　用预测结果进一步预测 35
3.12　NumPy快速入门 37
3.13　本章小结 40
第4章 神经网络学习导论：梯度下降 41
4.1　预测、比较和学习 41
4.2　什么是比较 42
4.3　学习 42
4.4　比较：你的神经网络是否做出了好的预测？ 43
4.5　为什么需要测量误差？ 44
4.6　最简单的神经学习形式是什么？ 45
4.7　冷热学习 46
4.8　冷热学习的特点 47
4.9　基于误差调节权重 48
4.10　梯度下降的一次迭代 50
4.11　学习就是减少误差 52
4.12　回顾学习的步骤 54
4.13　权重增量到底是什么? 55
4.14　狭隘的观点 57
4.15　插着小棍的盒子 58
4.16　导数：两种方式 59
4.17　你真正需要知道的 60
4.18　你不需要知道的 60
4.19　如何使用导数来学习 61
4.20　看起来熟悉吗? 62
4.21　破坏梯度下降 63
4.22　过度修正的可视化 64
4.23　发散 65
4.24　引入α 66
4.25　在代码中实现α 66
4.26　记忆背诵 67
第5章 通用梯度下降：一次学习多个权重 69
5.1　多输入梯度下降学习 69
5.2　多输入梯度下降详解 71
5.3　回顾学习的步骤 75
5.4　单项权重冻结：它有什么作用? 77
5.5　具有多个输出的梯度下降学习 79
5.6　具有多个输入和输出的梯度下降 81
5.7　这些权重学到了什么? 83
5.8　权重可视化 85
5.9　点积(加权和)可视化 86
5.10　本章小结 87
第6章 建立你的第一个深度神经网络：反向传播 89
6.1　交通信号灯问题 89
6.2　准备数据 91
6.3　矩阵和矩阵关系 92
6.4　使用Python创建矩阵 95
6.5　建立神经网络 96
6.6　学习整个数据集 97
6.7　完全、批量和随机梯度下降 97
6.8　神经网络对相关性的学习 98
6.9　向上与向下的压力 99
6.10　边界情况：过拟合 101
6.11　边界情况：压力冲突 101
6.12　学习间接相关性 103
6.13　创建关联 104
6.14　堆叠神经网络：回顾 105
6.15　反向传播：远程错误归因 106
6.16　反向传播：为什么有效? 107
6.17　线性与非线性 107
6.18　为什么神经网络仍然不起作用 109
6.19　选择性相关的秘密 110
6.20　快速冲刺 111
6.21　你的第一个深度神经网络 111
6.22　反向传播的代码 112
6.23　反向传播的一次迭代 114
6.24　整合代码 116
6.25　为什么深度网络这么重要? 117
第7章 如何描绘神经网络：在脑海里，在白纸上 119
7.1　到了简化的时候了 119
7.2　关联抽象 120
7.3　旧的可视化方法过于复杂 121
7.4　简化版可视化 122
7.5　进一步简化 123
7.6　观察神经网络是如何进行预测的 124
7.7　用字母而不是图片来进行可视化 125
7.8　连接变量 126
7.9　信息整合 127
7.10　可视化工具的重要性 127
第8章 学习信号，忽略噪声：正则化和批处理介绍 129
8.1　用在MNIST上的三层网络 129
8.2　好吧，这很简单 131
8.3　记忆与泛化 132
8.4　神经网络中的过拟合 133
8.5　过拟合从何而来 134
8.6　最简单的正则化：提前停止 135
8.7　行业标准正则化：dropout 136
8.8　为什么dropout有效：整合是有效的 137
8.9　dropout的代码 137
8.10　在MNIST数据集上对dropout进行测试 139
8.11　批量梯度下降 140
8.12　本章小结 143
第9章 概率和非线性建模：激活函数 145
9.1　什么是激活函数? 145
9.2　标准隐藏层激活函数 148
9.3　标准输出层激活函数 149
9.4　核心问题：输入具有
相似性 151
9.5　计算softmax 152
9.6　激活函数使用说明 153
9.7　将增量与斜率相乘 156
9.8　将输出转换为斜率(导数) 157
9.9　升级MNIST网络 157
第10章 卷积神经网络概论：关于边与角的神经学习 161
10.1　在多个位置复用权重 161
10.2　卷积层 162
10.3　基于NumPy的简单实现 164
10.4　本章小结 167
第11章 能够理解自然语言的神经网络：国王-男人+女人=？ 169
11.1　理解语言究竟是指什么? 170
11.2　自然语言处理(NLP) 170
11.3　监督NLP学习 171
11.4　IMDB电影评论数据集 172
11.5　在输入数据中提取单词相关性 173
11.6　对影评进行预测 174
11.7　引入嵌入层 175
11.8　解释输出 177
11.9　神经网络结构 178
11.10　单词嵌入表达的对比 180
11.11　神经元是什么意思? 181
11.12　完形填空 182
11.13　损失函数的意义 183
11.14　国王-男人+女人~=女王 186
11.15　单词类比 187
11.16　本章小结 188
第12章 像莎士比亚一样写作的神经网络：变长数据的递归层 189
12.1　任意长度的挑战 189
12.2　做比较真的重要吗？ 190
12.3　平均词向量的神奇力量 191
12.4　信息是如何存储在这些向量嵌入中的？ 192
12.5　神经网络是如何使用嵌入的？ 193
12.6　词袋向量的局限 194
12.7　用单位向量求词嵌入之和 195
12.8　不改变任何东西的矩阵 196
12.9　学习转移矩阵 197
12.10　学习创建有用的句子向量 198
12.11　Python下的前向传播 199
12.12　如何反向传播？ 200
12.13　让我们训练它！ 201
12.14　进行设置 201
12.15　任意长度的前向传播 202
12.16　任意长度的反向传播 203
12.17　任意长度的权重更新 204
12.18　运行代码，并分析输出 205
12.19　本章小结 207
第13章 介绍自动优化：搭建深度学习框架 209
13.1　深度学习框架是什么？ 209
13.2　张量介绍 210
13.3　自动梯度计算(autograd)介绍 211
13.4　快速检查 213
13.5　多次使用的张量 214
13.6　升级autograd以支持多次使用的张量 215
13.7　加法的反向传播如何工作？ 217
13.8　增加取负值操作的支持 218
13.9　添加更多函数的支持 219
13.10　使用autograd训练神经网络 222
13.11　增加自动优化 224
13.12　添加神经元层类型的支持 225
13.13　包含神经元层的神经元层 226
13.14　损失函数层 227
13.15　如何学习一个框架 228
13.16　非线性层 228
13.17　嵌入层 230
13.18　将下标操作添加到
autograd 231
13.19　再看嵌入层 232
13.20　交叉熵层 233
13.21　递归神经网络层 235
13.22　本章小结 238
第14章 像莎士比亚一样写作：长短期记忆网络 239
14.1　字符语言建模 239
14.2　截断式反向传播的必要性 240
14.3　截断式反向传播 241
14.4　输出样例 244
14.5　梯度消失与梯度激增 245
14.6　RNN反向传播的小例子 246
14.7　长短期记忆(LSTM)元胞 247
14.8　关于LSTM门限的直观理解 248
14.9　长短期记忆层 249
14.10　升级字符语言模型 250
14.11　训练LSTM字符语言模型 251
14.12　调优LSTM字符语言模型 252
14.13　本章小结 253
第15章 在看不见的数据上做深度学习：联邦学习导论 255
15.1　深度学习的隐私问题 255
15.2　联邦学习 256
15.3　学习检测垃圾邮件 257
15.4　让我们把它联邦化 259
15.5　深入联邦学习 260
15.6　安全聚合 261
15.7　同态加密 262
15.8　同态加密联邦学习 263
15.9　本章小结 264
第16章 往哪里去：简要指引 265
・ ・ ・ ・ ・ ・ (收起)第　一部分 基础知识
第　1章 走近深度学习：机器学习入门　3
1．1　什么是机器学习　4
1．1．1　机器学习与AI的关系　5
1．1．2　机器学习能做什么，不能做什么　6
1．2　机器学习示例　7
1．2．1　在软件应用中使用机器学习　9
1．2．2　监督学习　11
1．2．3　无监督学习　12
1．2．4　强化学习　12
1．3　深度学习　13
1．4　阅读本书能学到什么　14
1．5　小结　15
第　2章 围棋与机器学习　16
2．1　为什么选择游戏　16
2．2　围棋快速入门　17
2．2．1　了解棋盘　17
2．2．2　落子与吃子　18
2．2．3　终盘与胜负计算　19
2．2．4　理解劫争　20
2．2．5　让子　20
2．3　更多学习资源　20
2．4　我们可以教会计算机什么　21
2．4．1　如何开局　21
2．4．2　搜索游戏状态　21
2．4．3　减少需要考虑的动作数量　22
2．4．4　评估游戏状态　22
2．5　如何评估围棋AI的能力　23
2．5．1　传统围棋评级　23
2．5．2　对围棋AI进行基准测试　24
2．6　小结　24
第3章　实现第 一个围棋机器人　25
3．1　在Python中表达围棋游戏　25
3．1．1　实现围棋棋盘　28
3．1．2　在围棋中跟踪相连的棋组：棋链　28
3．1．3　在棋盘上落子和提子　30
3．2　跟踪游戏状态并检查非法动作　32
3．2．1　自吃　33
3．2．2　劫争　34
3．3　终盘　36
3．4　创建自己的第 一个机器人：理论上最弱的围棋AI　37
3．5　使用Zobrist哈希加速棋局　41
3．6　人机对弈　46
3．7　小结　47
第二部分　机器学习和游戏AI
第4章　使用树搜索下棋　51
4．1　游戏分类　52
4．2　利用极小化极大搜索预测对手　53
4．3　井字棋推演：一个极小化极大算法的示例　56
4．4　通过剪枝算法缩减搜索空间　58
4．4．1　通过棋局评估减少搜索深度　60
4．4．2　利用α-β剪枝缩减搜索宽度　63
4．5　使用蒙特卡洛树搜索评估游戏状态　66
4．5．1　在Python中实现蒙特卡洛树搜索　69
4．5．2　如何选择继续探索的分支　72
4．5．3　将蒙特卡洛树搜索应用于围棋　74
4．6　小结　76
第5章　神经网络入门　77
5．1　一个简单的用例：手写数字分类　78
5．1．1　MNIST手写数字数据集　78
5．1．2　MNIST数据的预处理　79
5．2　神经网络基础　85
5．2．1　将对率回归描述为简单的神经网络　85
5．2．2　具有多个输出维度的神经网络　85
5．3　前馈网络　86
5．4　我们的预测有多好？损失函数及优化　89
5．4．1　什么是损失函数　89
5．4．2　均方误差　89
5．4．3　在损失函数中找极小值　90
5．4．4　使用梯度下降法找极小值　91
5．4．5　损失函数的随机梯度下降算法　92
5．4．6　通过网络反向传播梯度　93
5．5　在Python中逐步训练神经网络　95
5．5．1　Python中的神经网络层　96
5．5．2　神经网络中的激活层　97
5．5．3　在Python中实现稠密层　98
5．5．4　Python顺序神经网络　100
5．5．5　将网络集成到手写数字分类应用中　102
5．6　小结　103
第6章　为围棋数据设计神经网络　105
6．1　为神经网络编码围棋棋局　107
6．2　生成树搜索游戏用作网络训练数据　109
6．3　使用Keras深度学习库　112
6．3．1　了解Keras的设计原理　112
6．3．2　安装Keras深度学习库　113
6．3．3　热身运动：在Keras中运行一个熟悉的示例　113
6．3．4　使用Keras中的前馈神经网络进行动作预测　115
6．4　使用卷积网络分析空间　119
6．4．1　卷积的直观解释　119
6．4．2　用Keras构建卷积神经网络　122
6．4．3　用池化层缩减空间　123
6．5　预测围棋动作概率　124
6．5．1　在最后一层使用softmax激活函数　125
6．5．2　分类问题的交叉熵损失函数　126
6．6　使用丢弃和线性整流单元构建更深的网络　127
6．6．1　通过丢弃神经元对网络进行正则化　128
6．6．2　线性整流单元激活函数　129
6．7　构建更强大的围棋动作预测网络　130
6．8　小结　133
第7章　从数据中学习：构建深度学习机器人　134
7．1　导入围棋棋谱　135
7．1．1　SGF文件格式　136
7．1．2　从KGS下载围棋棋谱并复盘　136
7．2　为深度学习准备围棋数据　137
7．2．1　从SGF棋谱中复盘围棋棋局　138
7．2．2　构建围棋数据处理器　139
7．2．3　构建可以高效地加载数据的围棋数据生成器　146
7．2．4　并行围棋数据处理和生成器　147
7．3　基于真实棋局数据训练深度学习模型　148
7．4　构建更逼真的围棋数据编码器　152
7．5　使用自适应梯度进行高效的训练　155
7．5．1　在SGD中采用衰减和动量　155
7．5．2　使用Adagrad优化神经网络　156
7．5．3　使用Adadelta优化自适应梯度　157
7．6　运行自己的实验并评估性能　157
7．6．1　测试架构与超参数的指南　158
7．6．2　评估训练与测试数据的性能指标　159
7．7　小结　160
第8章　实地部署围棋机器人　162
8．1　用深度神经网络创建动作预测代理　163
8．2　为围棋机器人提供Web前端　165
8．3　在云端训练与部署围棋机器人　169
8．4　与其他机器人对话：围棋文本协议　170
8．5　在本地与其他机器人对弈　172
8．5．1　机器人应该何时跳过回合或认输　172
8．5．2　让机器人与其他围棋程序进行对弈　173
8．6　将围棋机器人部署到在线围棋服务器　178
8．7　小结　182
第9章　通过实践学习：强化学习　183
9．1　强化学习周期　184
9．2　经验包括哪些内容　185
9．3　建立一个有学习能力的代理　188
9．3．1　从某个概率分布中进行抽样　189
9．3．2　剪裁概率分布　190
9．3．3　初始化一个代理实例　191
9．3．4　在磁盘上加载并保存代理　191
9．3．5　实现动作选择　193
9．4　自我对弈：计算机程序进行实践训练的方式　194
9．4．1　经验数据的表示　194
9．4．2　模拟棋局　197
9．5　小结　199
第　10章 基于策略梯度的强化学习　200
10．1　如何在随机棋局中识别更佳的决策　201
10．2　使用梯度下降法修改神经网络的策略　204
10．3　使用自我对弈进行训练的几个小技巧　208
10．3．1　评估学习的进展　208
10．3．2　衡量强度的细微差别　209
10．3．3　SGD优化器的微调　210
10．4　小结　213
第　11章 基于价值评估方法的强化学习　214
11．1　使用Q学习进行游戏　214
11．2　在Keras中实现Q学习　218
11．2．1　在Keras中构建双输入网络　218
11．2．2　用Keras实现ε贪婪策略　222
11．2．3　训练一个行动-价值函数　225
11．3　小结　226
第　12章 基于演员-评价方法的强化学习　227
12．1　优势能够告诉我们哪些决策更加重要　227
12．1．1　什么是优势　228
12．1．2　在自我对弈过程中计算优势值　230
12．2　为演员-评价学习设计神经网络　232
12．3　用演员-评价代理下棋　234
12．4　用经验数据训练一个演员-评价代理　235
12．5　小结　240
第三部分　一加一大于二
第　13章 AlphaGo：全部集结　243
13．1　为AlphaGo训练深度神经网络　245
13．1．1　AlphaGo的网络架构　246
13．1．2　AlphaGo棋盘编码器　248
13．1．3　训练AlphaGo风格的策略网络　250
13．2　用策略网络启动自我对弈　252
13．3　从自我对弈数据衍生出一个价值网络　254
13．4　用策略网络和价值网络做出更好的搜索　254
13．4．1　用神经网络改进蒙特卡洛推演　255
13．4．2　用合并价值函数进行树搜索　256
13．4．3　实现AlphaGo的搜索算法　258
13．5　训练自己的AlphaGo可能遇到的实践问题　263
13．6　小结　265
第　14章 AlphaGo Zero：将强化学习集成到树搜索中　266
14．1　为树搜索构建一个神经网络　267
14．2　使用神经网络来指导树搜索　268
14．2．1　沿搜索树下行　271
14．2．2　扩展搜索树　274
14．2．3　选择一个动作　276
14．3　训练　277
14．4　用狄利克雷噪声改进探索　281
14．5　处理超深度神经网络的相关最新技术　282
14．5．1　批量归一化　282
14．5．2　残差网络　283
14．6　探索额外资源　284
14．7　结语　285
14．8　小结　285
附录A　数学基础　286
附录B　反向传播算法　293
附录C　围棋程序与围棋服务器　297
附录D　用AWS来训练和部署围棋程序与围棋服务器　300
附录E　将机器人发布到OGS　307
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习简介 1
1.1 人工智能、机器学习与深度学习 2
1.2 深度学习的发展历程 7
1.3 深度学习的应用 10
1.3.1 计算机视觉 10
1.3.2 语音识别 14
1.3.3 自然语言处理 15
1.3.4 人机博弈 18
1.4 深度学习工具介绍和对比 19
小结 23
第2章 TensorFlow环境搭建 25
2.1 TensorFlow的主要依赖包 25
2.1.1 Protocol Buffer 25
2.1.2 Bazel 27
2.2 TensorFlow安装 29
2.2.1 使用Docker安装 30
2.2.2 使用pip安装 32
2.2.3 从源代码编译安装 33
2.3 TensorFlow测试样例 37
小结 38
第3章 TensorFlow入门 40
3.1 TensorFlow计算模型――计算图 40
3.1.1 计算图的概念 40
3.1.2 计算图的使用 41
3.2 TensorFlow数据模型――张量 43
3.2.1 张量的概念 43
3.2.2 张量的使用 45
3.3 TensorFlow运行模型――会话 46
3.4 TensorFlow实现神经网络 48
3.4.1 TensorFlow游乐场及神经网络简介 48
3.4.2 前向传播算法简介 51
3.4.3 神经网络参数与TensorFlow变量 54
3.4.4 通过TensorFlow训练神经网络模型 58
3.4.5 完整神经网络样例程序 62
小结 65
第4章 深层神经网络 66
4.1 深度学习与深层神经网络 66
4.1.1 线性模型的局限性 67
4.1.2 激活函数实现去线性化 70
4.1.3 多层网络解决异或运算 73
4.2 损失函数定义 74
4.2.1 经典损失函数 75
4.2.2 自定义损失函数 79
4.3 神经网络优化算法 81
4.4 神经网络进一步优化 84
4.4.1 学习率的设置 85
4.4.2 过拟合问题 87
4.4.3 滑动平均模型 90
小结 92
第5章 MNIST数字识别问题 94
5.1 MNIST数据处理 94
5.2 神经网络模型训练及不同模型结果对比 97
5.2.1 TensorFlow训练神经网络 97
5.2.2 使用验证数据集判断模型效果 102
5.2.3 不同模型效果比较 103
5.3 变量管理 107
5.4 TensorFlow模型持久化 112
5.4.1 持久化代码实现 112
5.4.2 持久化原理及数据格式 117
5.5 TensorFlow最佳实践样例程序 126
小结 132
第6章 图像识别与卷积神经网络 134
6.1 图像识别问题简介及经典数据集 135
6.2 卷积神经网络简介 139
6.3 卷积神经网络常用结构 142
6.3.1 卷积层 142
6.3.2 池化层 147
6.4 经典卷积网络模型 149
6.4.1 LeNet-5模型 150
6.4.2 Inception-v3模型 156
6.5 卷积神经网络迁移学习 160
6.5.1 迁移学习介绍 160
6.5.2 TensorFlow实现迁移学习 161
小结 169
第7章 图像数据处理 170
7.1 TFRecord输入数据格式 170
7.1.1 TFRecord格式介绍 171
7.1.2 TFRecord样例程序 171
7.2 图像数据处理 173
7.2.1 TensorFlow图像处理函数 174
7.2.2 图像预处理完整样例 183
7.3 多线程输入数据处理框架 185
7.3.1 队列与多线程 186
7.3.2 输入文件队列 190
7.3.3 组合训练数据（batching） 193
7.3.4 输入数据处理框架 196
小结 198
第8章 循环神经网络 200
8.1 循环神经网络简介 200
8.2 长短时记忆网络（LTSM）结构 206
8.3 循环神经网络的变种 212
8.3.1 双向循环神经网络和深层循环神经网络 212
8.3.2 循环神经网络的dropout 214
8.4 循环神经网络样例应用 215
8.4.1 自然语言建模 216
8.4.2 时间序列预测 225
小结 230
第9章 TensorBoard可视化 232
9.1 TensorBoard简介 232
9.2 TensorFlow计算图可视化 234
9.2.1 命名空间与TensorBoard图上节点 234
9.2.2 节点信息 241
9.3 监控指标可视化 246
小结 252
第10章 TensorFlow计算加速 253
10.1 TensorFlow使用GPU 253
10.2 深度学习训练并行模式 258
10.3 多GPU并行 261
10.4 分布式TensorFlow 268
10.4.1 分布式TensorFlow原理 269
10.4.2 分布式TensorFlow模型训练 272
10.4.3 使用Caicloud运行分布式TensorFlow 282
小结 287
・ ・ ・ ・ ・ ・ (收起)前言 .1
第一部分 生成式深度学习概述
第1 章 生成建模 11
1.1 什么是生成建模？ 11
1.1.1 生成建模与判别建模 13
1.1.2 机器学习的发展 . 14
1.1.3 生成建模的兴起 . 15
1.1.4 生成建模的框架 . 18
1.2 概率生成模型 21
1.2.1 你好，Wrodl ！ 24
1.2.2 你的第一个概率生成模型 . 25
1.2.3 朴素贝叶斯 28
1.2.4 你好，Wrodl ！续篇 . 31
1.3 生成建模的难题 33
表示学习 34
1.4 设置环境 37
1.5 小结 40
第2 章 深度学习 41
2.1 结构化与非结构化数据 41
2.2 深度神经网络 43
Keras 和TensorFlow 44
2.3 第一个深度神经网络 . 45
2.3.1 加载数据. 46
2.3.2 建立模型. 48
2.3.3 编译模型. 52
2.3.4 训练模型. 54
2.3.5 评估模型. 55
2.4 改进模型 58
2.4.1 卷积层 . 58
2.4.2 批标准化. 64
2.4.3 Dropout 层 . 66
2.4.4 结合所有层 68
2.5 小结 71
第3 章 变分自动编码器 73
3.1 画展 73
3.2 自动编码器 . 76
3.2.1 第一个自动编码器 . 77
3.2.2 编码器 . 78
3.2.3 解码器 . 80
3.2.4 连接编码器与解码器 82
3.2.5 分析自动编码器 . 84
3.3 变化后的画展 87
3.4 构建变分自动编码器 . 89
3.4.1 编码器 . 89
3.4.2 损失函数. 94
3.4.3 分析变分自动编码器 97
3.5 使用VAE 生成面部图像 98
3.5.1 训练VAE 99
3.5.2 分析VAE . 102
3.5.3 生成新面孔 . 103
3.5.4 隐空间的算术 104
3.5.5 面部变形 106
3.6 小结 . 107
第4 章 生成对抗网络 108
4.1 神秘兽 108
4.2 生成对抗网络简介 111
4.3 第一个生成对抗网络 112
4.3.1 判别器 113
4.3.2 生成器 115
4.3.3 训练GAN 119
4.4 GAN 面临的难题 125
4.4.1 损失震荡 125
4.4.2 模式收缩 126
4.4.3 不提供信息的损失函数 126
4.4.4 超参数 127
4.4.5 解决GAN 面临的难题 . 127
4.5 WGAN 127
4.5.1 Wasserstein 损失 128
4.5.2 利普希茨约束 130
4.5.3 权重裁剪 131
4.5.4 训练WGAN 132
4.5.5 分析WGAN 133
4.6 WGAN-GP 134
4.6.1 梯度惩罚损失 135
4.6.2 分析WGAN-GP 139
4.7 小结 . 140
第二部分 教机器绘画、写作、作曲和玩游戏
第5 章 绘画 145
5.1 苹果和橙子 146
5.2 CycleGAN 149
5.3 第一个CycleGAN 模型 . 151
5.3.1 简介 151
5.3.2 生成器（U-Net） 153
5.3.3 判别器 157
5.3.4 编译CycleGAN 158
5.3.5 训练CycleGAN 161
5.3.6 分析CycleGAN 162
5.4 创建一个模仿莫奈作品的CycleGAN . 164
5.4.1 生成器（ResNet） 165
5.4.2 分析CycleGAN 166
5.5 神经风格迁移 . 168
5.5.1 内容损失 169
5.5.2 风格损失 172
5.5.3 总方差损失 . 175
5.5.4 运行神经风格迁移 176
5.5.5 分析神经风格迁移模型 177
5.6 小结 . 178
第6 章 写作 179
6.1 坏家伙们的文学社 180
6.2 长短期记忆网络 181
6.3 第一个LSTM 网络 182
6.3.1 分词 183
6.3.2 建立数据集 . 185
6.3.3 LSTM 架构 . 187
6.3.4 嵌入层 187
6.3.5 LSTM 层 188
6.3.6 LSTM 元胞 . 190
6.4 生成新文本 192
6.5 RNN 扩展 . 196
6.5.1 堆叠式循环网络 196
6.5.2 门控制循环单元 198
6.5.3 双向元胞 200
6.6 编码器- 解码器模型 200
6.7 问答生成器 203
6.7.1 问答数据集 . 204
6.7.2 模型架构 205
6.7.3 推断 210
6.7.4 模型的结果 . 212
6.8 小结 . 214
第7 章 作曲 215
7.1 前提知识 216
音符 216
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习简介
1.1 人工智能、机器学习与深度学习
1.2 深度学习的发展历程
1.3 深度学习的应用
1.3.1 计算机视觉
1.3.2 语音识别
1.3.3 自然语言处理
1.3.4 人机博弈
1.4 深度学习工具介绍和对比
小结
第2章 TensorFlow环境搭建
2.1 TensorFlow的主要依赖包
2.1.1 Protocol Buffer
2.1.2 Bazel
2.2 TensorFlow安装
2.2.1 使用Docker安装
2.2.2 使用pip安装
2.2.3 从源代码编译安装
2.3 TensorFlow测试样例
小结
第3章 TensorFlow入门
3.1 TensorFlow计算模型――计算图
3.1.1 计算图的概念
3.1.2 计算图的使用
3.2 TensorFlow数据模型――张量
3.2.1 张量的概念
3.2.2 张量的使用
3.3 TensorFlow运行模型――会话
3.4 TensorFlow实现神经网络
3.4.1 TensorFlow游乐场及神经网络简介
3.4.2 前向传播算法简介
3.4.3 神经网络参数与TensorFlow变量
3.4.4 通过TensorFlow训练神经网络模型
3.4.5 完整神经网络样例程序
小结
第4章 深层神经网络
4.1 深度学习与深层神经网络
4.1.1 线性模型的局限性
4.1.2 激活函数实现去线性化
4.1.3 多层网络解决异或运算
4.2 损失函数定义
4.2.1 经典损失函数
4.2.2 自定义损失函数
4.3 神经网络优化算法
4.4 神经网络进一步优化
4.4.1 学习率的设置
4.4.2 过拟合问题
4.4.3 滑动平均模型
小结
第5章 MNIST数字识别问题
5.1 MNIST数据处理
5.2 神经网络模型训练及不同模型结果对比
5.2.1 TensorFlow训练神经网络
5.2.2 使用验证数据集判断模型效果
5.2.3 不同模型效果比较
5.3 变量管理
5.4 TensorFlow模型持久化
5.4.1 持久化代码实现
5.4.2 持久化原理及数据格式
5.5 TensorFlow最佳实践样例程序
小结
第6章 图像识别与卷积神经网络
6.1 图像识别问题简介及经典数据集
6.2 卷积神经网络简介
6.3 卷积神经网络常用结构
6.3.1 卷积层
6.3.2 池化层
6.4 经典卷积网络模型
6.4.1 LeNet-5模型
6.4.2 Inception-v3模型
6.5 卷积神经网络迁移学习
6.5.1 迁移学习介绍
6.5.2 TensorFlow实现迁移学习
小结
第7章 图像数据处理
7.1 TFRecord输入数据格式
7.1.1 TFRecord格式介绍
7.1.2 TFRecord样例程序
7.2 图像数据处理
7.2.1 TensorFlow图像处理函数
7.2.2 图像预处理完整样例
7.3 多线程输入数据处理框架
7.3.1 队列与多线程
7.3.2 输入文件队列
7.3.3 组合训练数据（batching）
7.3.4 输入数据处理框架
7.4 数据集（Dataset）
7.4.1 数据集的基本使用方法
7.4.2 数据集的高层操作
小结
第8章 循环神经网络
8.1 循环神经网络简介
8.2 长短时记忆网络（LSTM）结构
8.3 循环神经网络的变种
8.3.1 双向循环神经网络和深层循环神经网络
8.3.2 循环神经网络的dropout
8.4 循环神经网络样例应用
小结
第9章 自然语言处理
9.1 语言模型的背景知识
9.1.1 语言模型简介
9.1.2 语言模型的评价方法
9.2 神经语言模型
9.2.1 PTB数据集的预处理
9.2.2 PTB数据的batching方法
9.2.3 基于循环神经网络的神经语言模型
9.3 神经网络机器翻译
9.3.1 机器翻译背景与Seq2Seq模型介绍
9.3.2 机器翻译文本数据的预处理
9.3.3 Seq2Seq模型的代码实现
9.3.4 注意力机制
小结
第10章 TensorFlow高层封装
10.1 TensorFlow高层封装总览
10.2 Keras介绍
10.2.1 Keras基本用法
10.2.2 Keras高级用法
10.3 Estimator介绍
10.3.1 Estimator基本用法
10.3.2 Estimator自定义模型
10.3.3 使用数据集（Dataset）作为Estimator输入
小结
第11章 TensorBoard可视化
11.1 TensorBoard简介
11.2 TensorFlow计算图可视化
11.2.1 命名空间与TensorBoard图上节点
11.2.2 节点信息
11.3 监控指标可视化
11.4 高维向量可视化
小结
第12章 TensorFlow计算加速
12.1 TensorFlow使用GPU
12.2 深度学习训练并行模式
12.3 多GPU并行
12.4 分布式TensorFlow
12.4.1 分布式TensorFlow原理
12.4.2 分布式TensorFlow模型训练
小结
・ ・ ・ ・ ・ ・ (收起)第一部分　原理篇
第1章　机器学习与模型 2
1.1　模型 2
1.2　参数与训练 4
1.3　损失函数 9
1.4　计算图的训练 10
1.5　小结 12
第2章　计算图 13
2.1　什么是计算图 13
2.2　前向传播 14
2.3　函数优化与梯度下降法 18
2.4　链式法则与反向传播 29
2.5　在计算图上执行梯度下降法 36
2.6　节点类及其子类 36
2.7　用计算图搭建ADALINE并训练 44
2.8　小结 48
第3章　优化器 49
3.1　优化流程的抽象实现 49
3.2　BGD、SGD和MBGD 53
3.3　梯度下降优化器 58
3.4　朴素梯度下降法的局限 60
3.5　冲量优化器 61
3.6　AdaGrad优化器 62
3.7　RMSProp优化器 64
3.8　Adam优化器 65
3.9　小结 68
第二部分　模型篇
第4章　逻辑回归 70
4.1　对数损失函数 70
4.2　Logistic函数 73
4.3　二分类逻辑回归 75
4.4　多分类逻辑回归 78
4.5　交叉熵 81
4.6　实例：鸢尾花 85
4.7　小结 88
第5章　神经网络 90
5.1　神经元与激活函数 90
5.2　神经网络 95
5.3　多层全连接神经网络 99
5.4　多个全连接层的意义 101
5.5　实例：鸢尾花 108
5.6　实例：手写数字识别 110
5.7　小结 116
第6章　非全连接神经网络 117
6.1　带二次项的逻辑回归 117
6.2　因子分解机 124
6.3　Wide & Deep 132
6.4　DeepFM 137
6.5　实例：泰坦尼克号幸存者 141
6.6　小结 150
第7章　循环神经网络 151
7.1　RNN的结构 151
7.2　RNN的输出 152
7.3　实例：正弦波与方波 155
7.4　变长序列 159
7.5　实例：3D电磁发音仪单词识别 164
7.6　小结 167
第8章　卷积神经网络 168
8.1　蒙德里安与莫奈 168
8.2　滤波器 170
8.3　可训练的滤波器 176
8.4　卷积层 183
8.5　池化层 186
8.6　CNN的结构 189
8.7　实例：手写数字识别 190
8.8　小结 194
第三部分　工程篇
第9章　训练与评估 196
9.1　训练和Trainer训练器 196
9.2　评估和Metrics节点 202
9.3　混淆矩阵 204
9.4　正确率 204
9.5　查准率 206
9.6　查全率 206
9.7　ROC曲线和AUC 208
9.8　小结 211
第10章　模型保存、预测和服务 212
10.1　模型保存 213
10.2　模型加载和预测 216
10.3　模型服务 216
10.4　客户端 222
10.5　小结 223
第11章　分布式训练 224
11.1　分布式训练的原理 224
11.2　基于参数服务器的架构 230
11.3　Ring AllReduce原理 241
11.4　Ring AllReduce架构实现 248
11.5　分布式训练性能评测 257
11.6　小结 259
第12章　工业级深度学习框架 261
12.1　张量 262
12.2　计算加速 263
12.3　GPU 265
12.4　数据接口 266
12.5　模型并行 266
12.6　静态图和动态图 267
12.7　混合精度训练 268
12.8　图优化和编译优化 270
12.9　移动端和嵌入式端 270
12.10　小结 271
・ ・ ・ ・ ・ ・ (收起)第 1章　PyTorch与深度学习 1
1．1　人工智能　1
1．2　机器学习　3
1．3　深度学习　4
1．3．1　深度学习的应用　4
1．3．2　深度学习的浮夸宣传　6
1．3．3　深度学习发展史　6
1．3．4　为何是现在　7
1．3．5　硬件可用性　7
1．3．6　数据和算法　8
1．3．7　深度学习框架　9
1．4　小结　10
第　2章 神经网络的构成　11
2．1　安装PyTorch　11
2．2　实现第 一个神经网络　12
2．2．1　准备数据　13
2．2．2　为神经网络创建数据　20
2．2．3　加载数据　24
2．3　小结　25
第3章　深入了解神经网络　26
3．1　详解神经网络的组成部分　26
3．1．1　层―神经网络的基本组成　27
3．1．2　非线性激活函数　29
3．1．3　PyTorch中的非线性激活函数　32
3．1．4　使用深度学习进行图像分类　36
3．2　小结　46
第4章　机器学习基础　47
4．1　三类机器学习问题　47
4．1．1　有监督学习　48
4．1．2　无监督学习　48
4．1．3　强化学习　48
4．2　机器学习术语　49
4．3　评估机器学习模型　50
4．4　数据预处理与特征工程　54
4．4．1　向量化　54
4．4．2　值归一化　54
4．4．3　处理缺失值　55
4．4．4　特征工程　55
4．5　过拟合与欠拟合　56
4．5．1　获取更多数据　56
4．5．2　缩小网络规模　57
4．5．3　应用权重正则化　58
4．5．4　应用dropout　58
4．5．5　欠拟合　60
4．6　机器学习项目的工作流　60
4．6．1　问题定义与数据集创建　60
4．6．2　成功的衡量标准　61
4．6．3　评估协议　61
4．6．4　准备数据　62
4．6．5　模型基线　62
4．6．6　大到过拟合的模型　63
4．6．7　应用正则化　63
4．6．8　学习率选择策略　64
4．7　小结　65
第5章　深度学习之计算机视觉　66
5．1　神经网络简介　66
5．2　从零开始构建CNN模型　69
5．2．1　Conv2d　71
5．2．2　池化　74
5．2．3　非线性激活―ReLU　75
5．2．4　视图　76
5．2．5　训练模型　77
5．2．6　狗猫分类问题―从零开始构建CNN　80
5．2．7　利用迁移学习对狗猫分类　82
5．3　创建和探索VGG16模型　84
5．3．1　冻结层　85
5．3．2　微调VGG16模型　85
5．3．3　训练VGG16模型　86
5．4　计算预卷积特征　88
5．5　理解CNN模型如何学习　91
5．6　CNN层的可视化权重　94
5．7　小结　95
第6章　序列数据和文本的深度学习　96
6．1　使用文本数据　96
6．1．1　分词　98
6．1．2　向量化　100
6．2　通过构建情感分类器训练词向量　104
6．2．1　下载IMDB数据并对文本分词　104
6．2．2　构建词表　106
6．2．3　生成向量的批数据　107
6．2．4　使用词向量创建网络模型　108
6．2．5　训练模型　109
6．3　使用预训练的词向量　110
6．3．1　下载词向量　111
6．3．2　在模型中加载词向量　112
6．3．3　冻结embedding层权重　113
6．4　递归神经网络（RNN）　113
6．5　LSTM　117
6．5．1　长期依赖　117
6．5．2　LSTM网络　117
6．6　基于序列数据的卷积网络　123
6．7　小结　125
第7章　生成网络　126
7．1　神经风格迁移　126
7．1．1　加载数据　129
7．1．2　创建VGG模型　130
7．1．3　内容损失　131
7．1．4　风格损失　131
7．1．5　提取损失　133
7．1．6　为网络层创建损失函数　136
7．1．7　创建优化器　136
7．1．8　训练　137
7．2　生成对抗网络（GAN）　138
7．3　深度卷机生成对抗网络　139
7．3．1　定义生成网络　140
7．3．2　定义判别网络　144
7．3．3　定义损失函数和优化器　145
7．3．4　训练判别网络　145
7．3．5　训练生成网络　146
7．3．6　训练整个网络　147
7．3．7　检验生成的图片　148
7．4　语言建模　150
7．4．1　准备数据　151
7．4．2　生成批数据　152
7．4．3　定义基于LSTM的模型　153
7．4．4　定义训练和评估函数　155
7．4．5　训练模型　157
7．5　小结　159
第8章　现代网络架构　160
8．1　现代网络架构　160
8．1．1　ResNet　160
8．1．2　Inception　168
8．2　稠密连接卷积网络（DenseNet）　175
8．2．1　DenseBlock　175
8．2．2　DenseLayer　176
8．3　模型集成　180
8．3．1　创建模型　181
8．3．2　提取图片特征　182
8．3．3　创建自定义数据集和数据加载器　183
8．3．4　创建集成模型　184
8．3．5　训练和验证模型　185
8．4　encoder-decoder架构　186
8．4．1　编码器　188
8．4．2　解码器　188
8．5　小结　188
第9章　未来走向　189
9．1　未来走向　189
9．2　回顾　189
9．3　有趣的创意应用　190
9．3．1　对象检测　190
9．3．2　图像分割　191
9．3．3　PyTorch中的OpenNMT　192
9．3．4　Allen NLP　192
9．3．5　fast．ai―神经网络不再神秘　192
9．3．6　Open Neural Network Exchange　192
9．4　如何跟上前沿　193
9．5　小结　193
・ ・ ・ ・ ・ ・ (收起)第1章　深度学习入门　　1
1.1　机器学习简介　　1
1.1.1　监督学习　　2
1.1.2　无监督学习　　2
1.1.3　强化学习　　3
1.2　深度学习定义　　3
1.2.1　人脑的工作机制　　3
1.2.2　深度学习历史　　4
1.2.3　应用领域　　5
1.3　神经网络　　5
1.3.1　生物神经元　　5
1.3.2　人工神经元　　6
1.4　人工神经网络的学习方式　　8
1.4.1　反向传播算法　　8
1.4.2　权重优化　　8
1.4.3　随机梯度下降法　　9
1.5　神经网络架构　　10
1.5.1　多层感知器　　10
1.5.2　DNN架构　　11
1.5.3　卷积神经网络　　12
1.5.4　受限玻尔兹曼机　　12
1.6　自编码器　　13
1.7　循环神经网络　　14
1.8　几种深度学习框架对比　　14
1.9　小结　　16
第2章　TensorFlow初探　　17
2.1　总览　　17
2.1.1　TensorFlow 1.x版本特性　　18
2.1.2　使用上的改进　　18
2.1.3　TensorFlow安装与入门　　19
2.2　在Linux上安装TensorFlow　　19
2.3　为TensorFlow启用NVIDIA GPU　　20
2.3.1　第1步：安装NVIDIA CUDA　　20
2.3.2　　第2步：安装NVIDIA cuDNN v5.1+　　21
2.3.3　　第3步：确定GPU卡的CUDA计算能力为3.0+　　22
2.3.4　第4步：安装libcupti-dev库　　22
2.3.5　　第5步：安装Python
（或Python 3）　　22
2.3.6　第6步：安装并升级PIP
（或PIP3）　　22
2.3.7　第7步：安装TensorFlow　　23
2.4　如何安装TensorFlow　　23
2.4.1　直接使用pip安装　　23
2.4.2　使用virtualenv安装　　24
2.4.3　从源代码安装　　26
2.5　在Windows上安装TensorFlow　　27
2.5.1　在虚拟机上安装TensorFlow　　27
2.5.2　直接安装到Windows　　27
2.6　测试安装是否成功　　28
2.7　计算图　　28
2.8　为何采用计算图　　29
2.9　编程模型　　30
2.10　数据模型　　33
2.10.1　阶　　33
2.10.2　形状　　33
2.10.3　数据类型　　34
2.10.4　变量　　36
2.10.5　取回　　37
2.10.6　注入　　38
2.11　TensorBoard　　38
2.12　实现一个单输入神经元　　39
2.13　单输入神经元源代码　　43
2.14　迁移到TensorFlow 1.x版本　　43
2.14.1　如何用脚本升级　　44
2.14.2　局限　　47
2.14.3　手动升级代码　　47
2.14.4　变量　　47
2.14.5　汇总函数　　47
2.14.6　简化的数学操作　　48
2.14.7　其他事项　　49
2.15　小结　　49
第3章　用TensorFlow构建前馈
神经网络　　51
3.1　前馈神经网络介绍　　51
3.1.1　前馈和反向传播　　52
3.1.2　权重和偏差　　53
3.1.3　传递函数　　53
3.2　手写数字分类　　54
3.3　探究MNIST数据集　　55
3.4　softmax分类器　　57
3.5　TensorFlow模型的保存和还原　　63
3.5.1　保存模型　　63
3.5.2　还原模型　　63
3.5.3　softmax源代码　　65
3.5.4　softmax启动器源代码　　66
3.6　实现一个五层神经网络　　67
3.6.1　可视化　　69
3.6.2　五层神经网络源代码　　70
3.7　ReLU分类器　　72
3.8　可视化　　73
3.9　dropout优化　　76
3.10　可视化　　78
3.11　小结　　80
第4章　TensorFlow与卷积神经网络　　82
4.1　CNN简介　　82
4.2　CNN架构　　84
4.3　构建你的第一个CNN　　86
4.4　CNN表情识别　　95
4.4.1　表情分类器源代码　　104
4.4.2　使用自己的图像测试模型　　107
4.4.3　源代码　　109
4.5　小结　　111
第5章　优化TensorFlow自编码器　　112
5.1　自编码器简介　　112
5.2　实现一个自编码器　　113
5.3　增强自编码器的鲁棒性　　119
5.4　构建去噪自编码器　　120
5.5　卷积自编码器　　127
5.5.1　编码器　　127
5.5.2　解码器　　128
5.5.3　卷积自编码器源代码　　134
5.6　小结　　138
第6章　循环神经网络　　139
6.1　RNN的基本概念　　139
6.2　RNN的工作机制　　140
6.3　RNN的展开　　140
6.4　梯度消失问题　　141
6.5　LSTM网络　　142
6.6　RNN图像分类器　　143
6.7　双向RNN　　149
6.8　文本预测　　155
6.8.1　数据集　　156
6.8.2　困惑度　　156
6.8.3　PTB模型　　156
6.8.4　运行例程　　157
6.9　小结　　158
第7章　GPU计算　　160
7.1　GPGPU计算　　160
7.2　GPGPU的历史　　161
7.3　CUDA架构　　161
7.4　GPU编程模型　　162
7.5　TensorFlow中GPU的设置　　163
7.6　TensorFlow的GPU管理　　165
7.7　GPU内存管理　　168
7.8　在多GPU系统上分配单个GPU　　168
7.9　使用多个GPU　　170
7.10　小结　　171
第8章　TensorFlow高级编程　　172
8.1　Keras简介　　172
8.2　构建深度学习模型　　174
8.3　影评的情感分类　　175
8.4　添加一个卷积层　　179
8.5　Pretty Tensor　　181
8.6　数字分类器　　182
8.7　TFLearn　　187
8.8　泰坦尼克号幸存者预测器　　188
8.9　小结　　191
第9章　TensorFlow高级多媒体编程　　193
9.1　多媒体分析简介　　193
9.2　基于深度学习的大型对象检测　　193
9.2.1　瓶颈层　　195
9.2.2　使用重训练的模型　　195
9.3　加速线性代数　　197
9.3.1　TensorFlow的核心优势　　197
9.3.2　加速线性代数的准时编译　　197
9.4　TensorFlow和Keras　　202
9.4.1　Keras简介　　202
9.4.2　拥有Keras的好处　　203
9.4.3　视频问答系统　　203
9.5　Android上的深度学习　　209
9.5.1　TensorFlow演示程序　　209
9.5.2　Android入门　　211
9.6　小结　　214
第10章　强化学习　　215
10.1　强化学习基本概念　　216
10.2　Q-learning算法　　217
10.3　OpenAI Gym框架简介　　218
10.4　FrozenLake-v0实现问题　　220
10.5　使用TensorFlow实现Q-learning　　223
10.6　小结　　227
・ ・ ・ ・ ・ ・ (收起)上篇 初见
第1天 什么是深度学习 2
1.1 星星之火，可以燎原 3
1.2 师夷长技 4
1.2.1 谷歌与微软 4
1.2.2 Facebook、亚马逊与NVIDIA 5
1.3 中国崛起 6
1.3.1 BAT在路上 6
1.3.2 星光闪耀 7
1.3.3 企业热是风向标 8
1.4 练习题 9
第2天 深度学习的过往 10
2.1 传统机器学习的局限性 10
2.2 从表示学习到深度学习 11
2.3 监督学习 12
2.4 反向传播算法 13
2.5 卷积神经网络 15
2.6 深度学习反思 17
2.7 练习题 18
2.8 参考资料 18
第3天 深度学习工具汇总 19
3.1 Caffe 19
3.2 Torch & OverFeat 20
3.3 MxNet 22
3.4 TensorFlow 22
3.5 Theano 24
3.6 CNTK 24
3.7 练习题 25
3.8 参考资料 26
第4天 准备Caffe环境 27
4.1 Mac OS环境准备 27
4.2 Ubuntu环境准备 28
4.3 RHEL/Fedora/CentOS环境准备 29
4.4 Windows环境准备 29
4.5 常见问题 32
4.6 练习题 32
4.7 参考资料 33
第5天 Caffe依赖包解析 34
5.1 ProtoBuffer 34
5.2 Boost 38
5.3 GFLAGS 38
5.4 GLOG 39
5.5 BLAS 40
5.6 HDF5 41
5.7 OpenCV 42
5.8 LMDB和LEVELDB 42
5.9 Snappy 43
5.10 小结 43
5.11 练习题 49
5.12 参考资料 49
第6天 运行手写体数字识别例程 50
6.1 MNIST数据集 50
6.1.1 下载MNIST数据集 50
6.1.2 MNIST数据格式描述 51
6.1.3 转换格式 53
6.2 LeNet-5模型 60
6.2.1 LeNet-5模型描述 60
6.2.2 训练超参数 65
6.2.3 训练日志 66
6.2.4 用训练好的模型对数据进行预测 76
6.2.5 Windows下训练模型 76
6.3 回顾 78
6.4 练习题 79
6.5 参考资料 79
篇尾语 80
中篇 热恋
第7天 Caffe代码梳理 82
7.1 Caffe目录结构 82
7.2 如何有效阅读Caffe源码 84
7.3 Caffe支持哪些深度学习特性 86
7.3.1 卷积层 86
7.3.2 全连接层 89
7.3.3 激活函数 91
7.4 小结 99
7.5 练习题 99
7.6 参考资料 100
第8天 Caffe数据结构 101
8.1 Blob 101
8.1.1 Blob基本用法 102
8.1.2 数据结构描述 108
8.1.3 Blob是怎样炼成的 109
8.2 Layer 125
8.2.1 数据结构描述 126
8.2.2 Layer是怎样建成的 127
8.3 Net 136
8.3.1 Net基本用法 136
8.3.2 数据结构描述 139
8.3.3 Net是怎样绘成的 139
8.4 机制和策略 146
8.5 练习题 147
8.6 参考资料 148
第9天 Caffe I/O模块 149
9.1 数据读取层 149
9.1.1 数据结构描述 149
9.1.2 数据读取层实现 150
9.2 数据变换器 155
9.2.1 数据结构描述 155
9.2.2 数据变换器的实现 156
9.3 练习题 171
第10天 Caffe模型 172
10.1 prototxt表示 173
10.2 内存中的表示 176
10.3 磁盘上的表示 176
10.4 Caffe Model Zoo 178
10.5 练习题 180
10.6 参考资料 180
第11天 Caffe前向传播计算 181
11.1 前向传播的特点 181
11.2 前向传播的实现 182
11.2.1 DAG构造过程 182
11.2.2 Net Forward实现 190
11.3 练习题 192
第12天 Caffe反向传播计算 193
12.1 反向传播的特点 193
12.2 损失函数 193
12.2.1 算法描述 194
12.2.2 参数描述 195
12.2.3 源码分析 195
12.3 反向传播的实现 203
12.4 练习题 205
第13天 Caffe最优化求解过程 207
13.1 求解器是什么 207
13.2 求解器是如何实现的 208
13.2.1 算法描述 208
13.2.2 数据结构描述 210
13.2.3 CNN训练过程 218
13.2.4 CNN预测过程 225
13.2.5 Solver的快照和恢复功能 227
13.3 练习题 230
第14天 Caffe实用工具 231
14.1 训练和预测 231
14.2 特征提取 241
14.3 转换图像格式 247
14.4 计算图像均值 254
14.5 自己编写工具 257
14.6 练习题 257
篇尾语 258
下篇 升华
第15天 Caffe计算加速 260
15.1 Caffe计时功能 260
15.2 Caffe GPU加速模式 262
15.2.1 GPU是什么 262
15.2.2 CUDA是什么 263
15.2.3 GPU、CUDA和深度学习 263
15.2.4 Caffe GPU环境准备 264
15.2.5 切换到Caffe GPU加速模式 268
15.3 Caffe cuDNN加速模式 269
15.3.1 获取cuDNN 270
15.3.2 切换到Caffe cuDNN加速模式 270
15.3.3 Caffe不同硬件配置性能 272
15.4 练习题 273
15.5 参考资料 273
第16天 Caffe可视化方法 275
16.1 数据可视化 275
16.1.1 MNIST数据可视化 275
16.1.2 CIFAR10数据可视化 277
16.1.3 ImageNet数据可视化 278
16.2 模型可视化 279
16.2.1 网络结构可视化 279
16.2.2 网络权值可视化 281
16.3 特征图可视化 288
16.4 学习曲线 295
16.5 小结 298
16.6 练习题 298
16.7 参考资料 299
第17天 Caffe迁移和部署 300
17.1 从开发测试到生产部署 300
17.2 使用Docker 302
17.2.1 Docker基本概念 302
17.2.2 Docker安装 303
17.2.3 Docker入门 305
17.2.4 Docker使用进阶 312
17.3 练习题 317
17.4 参考资料 317
第18天 关于ILSVRC不得不说的一些事儿 318
18.1 ImageNet数据集 318
18.2 ILSVRC比赛项目 319
18.2.1 图像分类（CLS） 320
18.2.2 目标定位（LOC） 320
18.2.3 目标检测（DET） 321
18.2.4 视频目标检测（VID） 322
18.2.5 场景分类 322
18.3 Caffe ILSVRC实践 323
18.4 练习题 326
18.5 参考资料 326
第19天 放之四海而皆准 327
19.1 图像分类 327
19.1.1 问题描述 327
19.1.2 应用案例--商品分类 330
19.2 图像中的字符识别 332
19.2.1 问题描述 332
19.2.2 应用案例--身份证实名认证 333
19.3 目标检测 337
19.3.1 问题描述 337
19.3.2 最佳实践--运行R-CNN例程 337
19.4 人脸识别 340
19.4.1 问题描述 340
19.4.2 最佳实践--使用Face++ SDK实现人脸检测 342
19.5 自然语言处理 343
19.5.1 问题描述 343
19.5.2 最佳实践--NLP-Caffe 344
19.6 艺术风格 350
19.6.1 问题描述 350
19.6.2 最佳实践--style-transfer 352
19.7 小结 354
19.8 练习题 354
19.9 参考资料 355
第20天 继往开来的领路人 356
20.1 Caffe Traps and Pitfalls 356
20.1.1 不支持任意数据类型 356
20.1.2 不够灵活的高级接口 357
20.1.3 繁杂的依赖包 357
20.1.4 堪忧的卷积层实现 357
20.1.5 架构之殇 358
20.1.6 应用场景局限性 358
20.2 最佳实践--Caffe2 359
20.3 练习题 361
20.4 参考资料 362
第21天 新生 363
21.1 三人行，必有我师 363
21.2 路漫漫其修远兮，吾将上下而求索 364
篇尾语 366
结束语 367
附录A 其他深度学习工具
・ ・ ・ ・ ・ ・ (收起)第 1 章 深度学习介绍 1
1.1 人工智能 1
1.2 数据挖掘、机器学习与深度学习2
1.2.1 数据挖掘 3
1.2.2 机器学习 3
1.2.3 深度学习 4
1.3 学习资源与建议 8
第 2 章 深度学习框架 11
2.1 深度学习框架介绍 . 11
2.2 PyTorch 介绍. 13
2.2.1 什么是 PyTorch. 13
2.2.2 为何要使用 PyTorch 14
2.3 配置 PyTorch 深度学习环境 15
2.3.1 操作系统的选择. 15
2.3.2 Python 开发环境的安装 16
2.3.3 PyTorch 的安装. 18
第 3 章 多层全连接神经网络 24
3.1 热身：PyTorch 基础 24
3.1.1 Tensor（张量）. 24
3.1.2 Variable（变量）26
3.1.3 Dataset（数据集）28
3.1.4 nn.Module（模组） 29
3.1.5 torch.optim（优化） 30
3.1.6 模型的保存和加载 31
3.2 线性模型 32
3.2.1 问题介绍 32
3.2.2 一维线性回归33
3.2.3 多维线性回归34
3.2.4 一维线性回归的代码实现. 35
3.2.5 多项式回归 38
3.3 分类问题 42
3.3.1 问题介绍 42
3.3.2 Logistic 起源 42
3.3.3 Logistic 分布 42
3.3.4 二分类的 Logistic 回归 43
3.3.5 模型的参数估计. 44
3.3.6 Logistic 回归的代码实现45
3.4 简单的多层全连接前向网络 . 49
3.4.1 模拟神经元 49
3.4.2 单层神经网络的分类器 50
3.4.3 激活函数 51
3.4.4 神经网络的结构. 54
3.4.5 模型的表示能力与容量 55
3.5 深度学习的基石：反向传播算法57
3.5.1 链式法则 57
3.5.2 反向传播算法58
3.5.3 Sigmoid 函数举例58
3.6 各种优化算法的变式59
3.6.1 梯度下降法 59
3.6.2 梯度下降法的变式 62
3.7 处理数据和训练模型的技巧 . 64
3.7.1 数据预处理 64
3.7.2 权重初始化 66
3.7.3 防止过拟合 67
3.8 多层全连接神经网络实现 MNIST 手写数字分类 69
3.8.1 简单的三层全连接神经网络70
3.8.2 添加激活函数70
3.8.3 添加批标准化71
3.8.4 训练网络 71
第 4 章 卷积神经网络 76
4.1 主要任务及起源 76
4.2 卷积神经网络的原理和结构 . 77
4.2.1 卷积层80
4.2.2 池化层84
4.2.3 全连接层 85
4.2.4 卷积神经网络的基本形式. 85
4.3 PyTorch 卷积模块 . 87
4.3.1 卷积层87
4.3.2 池化层88
4.3.3 提取层结构 90
4.3.4 如何提取参数及自定义初始化 91
4.4 卷积神经网络案例分析. 92
4.4.1 LeNet. 93
4.4.2 AlexNet94
4.4.3 VGGNet 95
4.4.4 GoogLeNet . 98
4.4.5 ResNet100
4.5 再实现 MNIST 手写数字分类 . 103
4.6 图像增强的方法 105
4.7 实现 cifar10 分类 107
第 5 章 循环神经网络 111
5.1 循环神经网络111
5.1.1 问题介绍 112
5.1.2 循环神经网络的基本结构. 112
5.1.3 存在的问题 115
5.2 循环神经网络的变式：LSTM 与 GRU 116
5.2.1 LSTM. 116
5.2.2 GRU. 119
5.2.3 收敛性问题 120
5.3 循环神经网络的 PyTorch 实现 122
5.3.1 PyTorch 的循环网络模块122
5.3.2 实例介绍 127
5.4 自然语言处理的应用131
5.4.1 词嵌入131
5.4.2 词嵌入的 PyTorch 实现 133
5.4.3 N Gram 模型 133
5.4.4 单词预测的 PyTorch 实现134
5.4.5 词性判断 136
5.4.6 词性判断的 PyTorch 实现137
5.5 循环神经网络的更多应用140
5.5.1 Many to one 140
5.5.2 Many to Many（shorter）141
5.5.3 Seq2seq141
5.5.4 CNN+RNN . 142
第 6 章 生成对抗网络 144
6.1 生成模型 144
6.1.1 自动编码器 145
6.1.2 变分自动编码器. 150
6.2 生成对抗网络153
6.2.1 何为生成对抗网络 153
6.2.2 生成对抗网络的数学原理. 160
6.3 Improving GAN164
6.3.1 Wasserstein GAN. 164
6.3.2 Improving WGAN167
6.4 应用介绍 168
6.4.1 Conditional GAN. 168
6.4.2 Cycle GAN . 170
第 7 章 深度学习实战 173
7.1 实例一――猫狗大战：运用预训练卷积神经网络进行特征提取与预测 . 173
7.1.1 背景介绍 174
7.1.2 原理分析 174
7.1.3 代码实现 177
7.1.4 总结. 183
7.2 实例二――Deep Dream：探索卷积神经网络眼中的世界183
7.2.1 原理介绍 184
7.2.2 预备知识：backward . 185
7.2.3 代码实现 190
7.2.4 总结. 195
7.3 实例三――Neural-Style：使用 PyTorch 进行风格迁移196
7.3.1 背景介绍 196
7.3.2 原理分析 197
7.3.3 代码实现 199
7.3.4 总结. 205
7.4 实例四――Seq2seq：通过 RNN 实现简单的 Neural Machine Translation . 205
7.4.1 背景介绍 206
7.4.2 原理分析 206
7.4.3 代码实现 209
7.4.4 总结. 221
・ ・ ・ ・ ・ ・ (收起)第1章　深度学习介绍　　1
1.1　开始深度学习之旅　　5
1.1.1　深度前馈网络　　6
1.1.2　各种学习算法　　6
1.2　深度学习的相关术语　　10
1.3　深度学习――一场人工智能革命　　12
1.4　深度学习网络的分类　　18
1.4.1　深度生成或无监督模型　　19
1.4.2　深度判别模型　　20
1.5　小结　　22
第2章　大规模数据的分布式深度学习　　23
2.1　海量数据的深度学习　　24
2.2　大数据深度学习面临的挑战　　27
2.2.1　海量数据带来的挑战（第一个V）　　28
2.2.2　数据多样性带来的挑战（第二个V）　　28
2.2.3　数据快速处理带来的挑战（第三个V）　　29
2.2.4　数据真实性带来的挑战（第四个V）　　29
2.3　分布式深度学习和Hadoop　　29
2.3.1　Map-Reduce　　31
2.3.2　迭代Map-Reduce　　31
2.3.3　YARN　　32
2.3.4　分布式深度学习设计的重要特征　　32
2.4　深度学习的开源分布式框架Deeplearning4j　　34
2.4.1　Deeplearning4j的主要特性　　34
2.4.2　Deeplearning4j功能总结　　35
2.5　在Hadoop YARN上配置Deeplearning4j　　35
2.5.1　熟悉Deeplearning4j　　36
2.5.2　为进行分布式深度学习集成Hadoop YARN和Spark　　40
2.5.3　Spark在Hadoop YARN上的内存分配规则　　40
2.6　小结　　44
第3章　卷积神经网络　　45
3.1　卷积是什么　　46
3.2　卷积神经网络的背景　　47
3.3　卷积神经网络的基本层　　48
3.3.1　卷积神经网络深度的重要性　　49
3.3.2　卷积层　　49
3.3.3　为卷积层选择超参数　　52
3.3.4　ReLU层　　56
3.3.5　池化层　　57
3.3.6　全连接层　　58
3.4　分布式深度卷积神经网络　　58
3.4.1　最受欢迎的深度神经网络及其配置　　58
3.4.2　训练时间――深度神经网络面临的主要挑战　　59
3.4.3　将Hadoop应用于深度卷积神经网络　　59
3.5　使用Deeplearning4j构建卷积层　　61
3.5.1　加载数据　　61
3.5.2　模型配置　　62
3.5.3　训练与评估　　63
3.6　小结　　64
第4章　循环神经网络　　65
4.1　循环网络与众不同的原因　　66
4.2　循环神经网络　　67
4.2.1　展开循环计算　　68
4.2.2　循环神经网络的记忆　　69
4.2.3　架构　　70
4.3　随时间反向传播　　71
4.4　长短期记忆　　73
4.4.1　随时间深度反向传播的问题　　73
4.4.2　长短期记忆　　73
4.5　双向循环神经网络　　75
4.5.1　循环神经网络的不足　　75
4.5.2　解决方案　　76
4.6　分布式深度循环神经网络　　77
4.7　用Deeplearning4j训练循环神经网络　　77
4.8　小结　　80
第5章　受限玻尔兹曼机　　81
5.1　基于能量的模型　　82
5.2　玻尔兹曼机　　83
5.2.1　玻尔兹曼机如何学习　　84
5.2.2　玻尔兹曼机的不足　　85
5.3　受限玻尔兹曼机　　85
5.3.1　基础架构　　85
5.3.2　受限玻尔兹曼机的工作原理　　86
5.4　卷积受限玻尔兹曼机　　88
5.5　深度信念网络　　90
5.6　分布式深度信念网络　　91
5.6.1　受限玻尔兹曼机的分布式训练　　91
5.6.2　深度信念网络的分布式训练　　92
5.7　用Deeplearning4j实现受限玻尔兹曼机和深度信念网络　　94
5.7.1　受限玻尔兹曼机　　94
5.7.2　深度信念网络　　95
5.8　小结　　97
第6章　自动编码器　　98
6.1　自动编码器　　98
6.2　稀疏自动编码器　　101
6.2.1　稀疏编码　　101
6.2.2　稀疏自动编码器　　102
6.3　深度自动编码器　　104
6.3.1　训练深度自动编码器　　104
6.3.2　使用Deeplearning4j实现深度自动编码器　　107
6.4　降噪自动编码器　　108
6.4.1　降噪自动编码器的架构　　109
6.4.2　堆叠式降噪自动编码器　　109
6.4.3　使用Deeplearning4j实现堆叠式降噪自动编码器　　110
6.5　自动编码器的应用　　112
6.6　小结　　112
第7章　用Hadoop玩转深度学习　　113
7.1　Hadoop中的分布式视频解码　　114
7.2　使用Hadoop进行大规模图像处理　　116
7.3　使用Hadoop进行自然语言处理　　117
7.3.1　Web爬虫　　118
7.3.2　自然语言处理的关键词提取和模块　　118
7.3.3　从页面评估相关关键词　　118
7.4　小结　　119
参考文献　　120
・ ・ ・ ・ ・ ・ (收起)第1 章绪论.1
1.1 知识图谱简介2
1.2 深度学习的优势和挑战4
1.3 深度学习+ 知识图谱=1 .8
1.3.1 知识的表示学习9
1.3.2 知识的自动获取10
1.3.3 知识的计算应用13
1.4 本书结构14
1.5 本章总结14
第一篇世界知识图谱
第2 章世界知识的表示学习19
2.1 章节引言19
2.2 相关工作20
2.2.1 知识表示学习经典模型20
2.2.2 平移模型及其拓展模型22
2.3 基于复杂关系建模的知识表示学习25
2.3.1 算法模型.25
2.3.2 实验分析.26
2.3.3 小结32
2.4 基于关系路径建模的知识表示学习32
2.4.1 算法模型.32
2.4.2 实验分析.34
2.4.3 小结39
vi j 知识图谱与深度学习
2.5 基于属性关系建模的知识表示学习39
2.5.1 算法模型.40
2.5.2 实验分析.41
2.5.3 小结44
2.6 融合实体描述信息的知识表示学习44
2.6.1 算法模型.45
2.6.2 实验分析.47
2.6.3 小结54
2.7 融合层次类型信息的知识表示学习55
2.7.1 算法模型.55
2.7.2 实验分析.57
2.7.3 小结62
2.8 融合实体图像信息的知识表示学习62
2.8.1 算法模型.63
2.8.2 实验分析.64
2.8.3 小结68
2.9 本章总结68
第3 章世界知识的自动获取70
3.1 章节引言70
3.2 相关工作71
3.2.1 有监督的关系抽取模型71
3.2.2 远程监督的关系抽取模型.72
3.3 基于选择性注意力机制的关系抽取73
3.3.1 算法模型.74
3.3.2 实验分析.78
3.3.3 小结82
3.4 基于关系层次注意力机制的关系抽取83
3.4.1 算法模型.83
目录j vii
3.4.2 实验分析.86
3.4.3 小结89
3.5 基于选择性注意力机制的多语言关系抽取.89
3.5.1 算法模型.90
3.5.2 实验分析.93
3.5.3 小结98
3.6 引入对抗训练的多语言关系抽取98
3.6.1 算法模型.99
3.6.2 实验分析.103
3.6.3 小结106
3.7 基于知识图谱与文本互注意力机制的知识获取.106
3.7.1 算法模型.107
3.7.2 实验分析.112
3.7.3 小结117
3.8 本章总结118
第4 章世界知识的计算应用119
4.1 章节引言119
4.2 细粒度实体分类120
4.2.1 算法模型.120
4.2.2 实验分析.122
4.2.3 小结129
4.3 实体对齐129
4.3.1 算法模型.129
4.3.2 实验分析.132
4.3.3 小结135
4.4 融入知识的信息检索.136
4.4.1 算法模型.136
4.4.2 实验分析.138
4.4.3 小结143
viii j 知识图谱与深度学习
4.5 本章总结143
第二篇语言知识图谱
第5 章语言知识的表示学习147
5.1 章节引言147
5.2 相关工作148
5.2.1 词表示学习148
5.2.2 词义消歧.149
5.3 义原的表示学习149
5.3.1 算法模型.149
5.3.2 实验分析.152
5.3.3 小结155
5.4 基于义原的词表示学习156
5.4.1 算法模型.156
5.4.2 实验分析.159
5.4.3 小结164
5.5 本章总结164
第6 章语言知识的自动获取166
6.1 章节引言166
6.2 相关工作167
6.2.1 知识图谱及其构建167
6.2.2 子词和字级NLP 167
6.2.3 词表示学习及跨语言的词表示学习167
6.3 基于协同过滤和矩阵分解的义原预测168
6.3.1 算法模型.168
6.3.2 实验分析.171
6.3.3 小结175
6.4 融入中文字信息的义原预测175
6.4.1 算法模型.176
目录j ix
6.4.2 实验分析.179
6.4.3 小结183
6.5 跨语言词汇的义原预测183
6.5.1 算法模型.184
6.5.2 实验分析.188
6.5.3 小结194
6.6 本章总结194
第7 章语言知识的计算应用195
7.1 章节引言195
7.2 义原驱动的词典扩展.196
7.2.1 相关工作.196
7.2.2 任务设定.198
7.2.3 算法模型.199
7.2.4 实验分析.202
7.2.5 小结207
7.3 义原驱动的神经语言模型.207
7.3.1 相关工作.208
7.3.2 任务设定.209
7.3.3 算法模型.210
7.3.4 实验分析.213
7.3.5 小结219
7.4 本章总结219
第8 章总结与展望220
8.1 本书总结220
8.2 未来展望221
8.2.1 更全面的知识类型221
8.2.2 更复杂的知识结构222
8.2.3 更有效的知识获取223
8.2.4 更强大的知识指导223
x j 知识图谱与深度学习
8.2.5 更精深的知识推理224
8.3 结束语224
相关开源资源226
参考文献228
后记.243
・ ・ ・ ・ ・ ・ (收起)第1 章绪论.1
1.1 知识图谱简介2
1.2 深度学习的优势和挑战4
1.3 深度学习+ 知识图谱=1 .8
1.3.1 知识的表示学习9
1.3.2 知识的自动获取10
1.3.3 知识的计算应用13
1.4 本书结构14
1.5 本章总结14
第一篇世界知识图谱
第2 章世界知识的表示学习19
2.1 章节引言19
2.2 相关工作20
2.2.1 知识表示学习经典模型20
2.2.2 平移模型及其拓展模型22
2.3 基于复杂关系建模的知识表示学习25
2.3.1 算法模型.25
2.3.2 实验分析.26
2.3.3 小结32
2.4 基于关系路径建模的知识表示学习32
2.4.1 算法模型.32
2.4.2 实验分析.34
2.4.3 小结39
vi j 知识图谱与深度学习
2.5 基于属性关系建模的知识表示学习39
2.5.1 算法模型.40
2.5.2 实验分析.41
2.5.3 小结44
2.6 融合实体描述信息的知识表示学习44
2.6.1 算法模型.45
2.6.2 实验分析.47
2.6.3 小结54
2.7 融合层次类型信息的知识表示学习55
2.7.1 算法模型.55
2.7.2 实验分析.57
2.7.3 小结62
2.8 融合实体图像信息的知识表示学习62
2.8.1 算法模型.63
2.8.2 实验分析.64
2.8.3 小结68
2.9 本章总结68
第3 章世界知识的自动获取70
3.1 章节引言70
3.2 相关工作71
3.2.1 有监督的关系抽取模型71
3.2.2 远程监督的关系抽取模型.72
3.3 基于选择性注意力机制的关系抽取73
3.3.1 算法模型.74
3.3.2 实验分析.78
3.3.3 小结82
3.4 基于关系层次注意力机制的关系抽取83
3.4.1 算法模型.83
目录j vii
3.4.2 实验分析.86
3.4.3 小结89
3.5 基于选择性注意力机制的多语言关系抽取.89
3.5.1 算法模型.90
3.5.2 实验分析.93
3.5.3 小结98
3.6 引入对抗训练的多语言关系抽取98
3.6.1 算法模型.99
3.6.2 实验分析.103
3.6.3 小结106
3.7 基于知识图谱与文本互注意力机制的知识获取.106
3.7.1 算法模型.107
3.7.2 实验分析.112
3.7.3 小结117
3.8 本章总结118
第4 章世界知识的计算应用119
4.1 章节引言119
4.2 细粒度实体分类120
4.2.1 算法模型.120
4.2.2 实验分析.122
4.2.3 小结129
4.3 实体对齐129
4.3.1 算法模型.129
4.3.2 实验分析.132
4.3.3 小结135
4.4 融入知识的信息检索.136
4.4.1 算法模型.136
4.4.2 实验分析.138
4.4.3 小结143
viii j 知识图谱与深度学习
4.5 本章总结143
第二篇语言知识图谱
第5 章语言知识的表示学习147
5.1 章节引言147
5.2 相关工作148
5.2.1 词表示学习148
5.2.2 词义消歧.149
5.3 义原的表示学习149
5.3.1 算法模型.149
5.3.2 实验分析.152
5.3.3 小结155
5.4 基于义原的词表示学习156
5.4.1 算法模型.156
5.4.2 实验分析.159
5.4.3 小结164
5.5 本章总结164
第6 章语言知识的自动获取166
6.1 章节引言166
6.2 相关工作167
6.2.1 知识图谱及其构建167
6.2.2 子词和字级NLP 167
6.2.3 词表示学习及跨语言的词表示学习167
6.3 基于协同过滤和矩阵分解的义原预测168
6.3.1 算法模型.168
6.3.2 实验分析.171
6.3.3 小结175
6.4 融入中文字信息的义原预测175
6.4.1 算法模型.176
目录j ix
6.4.2 实验分析.179
6.4.3 小结183
6.5 跨语言词汇的义原预测183
6.5.1 算法模型.184
6.5.2 实验分析.188
6.5.3 小结194
6.6 本章总结194
第7 章语言知识的计算应用195
7.1 章节引言195
7.2 义原驱动的词典扩展.196
7.2.1 相关工作.196
7.2.2 任务设定.198
7.2.3 算法模型.199
7.2.4 实验分析.202
7.2.5 小结207
7.3 义原驱动的神经语言模型.207
7.3.1 相关工作.208
7.3.2 任务设定.209
7.3.3 算法模型.210
7.3.4 实验分析.213
7.3.5 小结219
7.4 本章总结219
第8 章总结与展望220
8.1 本书总结220
8.2 未来展望221
8.2.1 更全面的知识类型221
8.2.2 更复杂的知识结构222
8.2.3 更有效的知识获取223
8.2.4 更强大的知识指导223
x j 知识图谱与深度学习
8.2.5 更精深的知识推理224
8.3 结束语224
相关开源资源226
参考文献228
后记.243
・ ・ ・ ・ ・ ・ (收起)第1 章什么是推荐系统1
1.1 推荐系统的概念.1
1.1.1 推荐系统的基本概念1
1.1.2 深度学习与推荐系统4
第2 章深度神经网络.7
2.1 什么是深度学习.7
2.1.1 深度学习的三次兴起7
2.1.2 深度学习的优势9
2.2 神经网络基础11
2.2.1 神经元11
2.2.2 神经网络.12
2.2.3 反向传播.13
2.2.4 优化算法.14
2.3 卷积网络基础17
2.3.1 卷积层17
2.3.2 池化层19
2.3.3 常见的网络结构19
2.4 循环网络基础21
2.4.1 时序反向传播算法22
2.4.2 长短时记忆网络24
2.5 生成对抗基础25
2.5.1 对抗博弈.26
2.5.2 理论推导.27
2.5.3 常见的生成对抗网络29
iv j 推荐系统与深度学习
第3 章TensorFlow 平台31
3.1 什么是TensorFlow 31
3.2 TensorFlow 安装指南.33
3.2.1 Windows 环境安装.33
3.2.2 Linux 环境安装.34
3.3 TensorFlow 基础.36
3.3.1 数据流图.36
3.3.2 会话37
3.3.3 图可视化.37
3.3.4 变量37
3.3.5 占位符38
3.3.6 优化器38
3.3.7 一个简单的例子38
3.4 其他深度学习平台39
第4 章推荐系统的基础算法42
4.1 基于内容的推荐算法.42
4.1.1 基于内容的推荐算法基本流程42
4.1.2 基于内容推荐的特征提取.45
4.2 基于协同的推荐算法.47
4.2.1 基于物品的协同算法49
4.2.2 基于用户的协同算法57
4.2.3 基于用户协同和基于物品协同的区别59
4.2.4 基于矩阵分解的推荐方法.61
4.2.5 基于稀疏自编码的推荐方法.71
4.3 基于社交网络的推荐算法80
4.3.1 基于用户的推荐在社交网络中的应用81
4.3.2 node2vec 技术在社交网络推荐中的应用85
4.4 推荐系统的冷启动问题94
4.4.1 如何解决推荐系统冷启动问题94
4.4.2 深度学习技术在物品冷启动上的应用101
目录j v
第5 章混合推荐系统119
5.1 什么是混合推荐系统.119
5.1.1 混合推荐系统的意义120
5.1.2 混合推荐系统的算法分类.122
5.2 推荐系统特征处理方法125
5.2.1 特征处理方法126
5.2.2 特征选择方法134
5.3 常见的预测模型141
5.3.1 基于逻辑回归的模型141
5.3.2 基于支持向量机的模型.144
5.3.3 基于梯度提升树的模型.148
5.4 排序学习150
5.4.1 基于排序的指标来优化.150
5.4.2 L2R 算法的三种情形.152
第6 章基于深度学习的推荐模型156
6.1 基于DNN 的推荐算法156
6.2 基于DeepFM 的推荐算法163
6.3 基于矩阵分解和图像特征的推荐算法171
6.4 基于循环网络的推荐算法.174
6.5 基于生成对抗网络的推荐算法.176
6.5.1 IRGAN 的代码实现.179
第7 章推荐系统架构设计.183
7.1 推荐系统基本模型183
7.2 推荐系统常见架构185
7.2.1 基于离线训练的推荐系统架构设计185
7.2.2 面向深度学习的推荐系统架构设计191
7.2.3 基于在线训练的推荐系统架构设计194
7.2.4 面向内容的推荐系统架构设计197
7.3 推荐系统常用组件199
7.3.1 数据上报常用组件199
vi j 推荐系统与深度学习
7.3.2 离线存储常用组件200
7.3.3 离线计算常用组件200
7.3.4 在线存储常用组件201
7.3.5 模型服务常用组件201
7.3.6 实时计算常用组件201
7.4 推荐系统常见问题201
7.4.1 实时性.201
7.4.2 多样性.202
7.4.3 曝光打击和不良内容过滤.202
7.4.4 评估测试.202
后记.203
图1.1 淘宝猜你喜欢栏目2
图1.2 百度指数.4
图1.3 歌曲词嵌入模型空间向量.6
图2.1 神经网络的三次兴起8
图2.2 不同层数的神经网络拟合分界面的能力.10
图2.3 不同层数的神经网络表示能力10
图2.4 神经网络的基本结构11
图2.5 感知器算法12
图2.6 三层全连接神经网络13
图2.7 动量对比.16
图2.8 卷积运算.18
图2.9 池化层19
图2.10 LeNet 卷积结构.20
图2.11 Alex-Net 卷积结构20
图2.12 RNN 21
图2.13 LSTM 在t 时刻的内部结构24
图2.14 GAN 网络25
图3.1 TensorFlow 安装截图34
图3.2 TensorBoard 计算37
图4.1 腾讯视频APP 推荐页面.44
图4.2 截取自当当网.49
图4.3 截取自QQ 音乐APP.49
图4.4 用户购买物品记录50
图4.5 同时被购买次数矩阵C 51
图4.6 相似度计算结果1 52
图4.7 相似度计算结果2 54
viii j 推荐系统与深度学习
图4.8 相似度计算结果3 55
图4.9 截取自当当网.57
图4.10 物品的倒排索引57
图4.11 用户评分矩阵.63
图4.12 Sigma 值64
图4.13 NewData 值65
图4.14 Mydata 值65
图4.15 自编码神经网络模型72
图4.16 稀疏自编码第一个网络.73
图4.17 稀疏自编码第二个网络.74
图4.18 稀疏自编码第三个网络.75
图4.19 将三个网络组合起来75
图4.20 社交网络关系图示例81
图4.21 融入用户关系和物品关系82
图4.22 社交网络关系图示例86
图4.23 社交网络关系图示例86
图4.24 CBOW 和Skip-Gram 示例.88
图4.25 Skip-Gram 网络结构89
图4.26 CBOW 网络结构91
图4.27 word analogy 示例93
图4.28 某网站登录页面95
图4.29 QQ 互联开放注册平台1 96
图4.30 QQ 互联开放注册平台2 97
图4.31 QQ 互联应用管理页面1 97
图4.32 QQ 互联应用管理页面2 97
图4.33 QQ 互联QQ 登录功能获取97
图4.34 QQ 音乐APP 中的偏好选择98
图4.35 (a) 为每部电影被打分的分布，(b) 为每个用户打分的分布100
图4.36 (a) 为每部电影平均分分布，(b) 为每个用户平均分分布.100
图4.37 基于专家数据的CF 与基于用户数据CF 比较.101
图目录j ix
图4.38 音乐频谱示例102
图4.39 4 个流派的频谱图示例103
图4.40 CNN 音频分类结构.103
图4.41 CNN+LSTM 组合音频分类模型.104
图4.42 分类预测结果的混淆矩阵104
图4.43 模型倒数第二层128 维向量降维可视化104
图4.44 微软how-old.net 107
图4.45 SCUT-FBP 数据集示例图108
图4.46 脸部截取后的数据集示例图.108
图4.47 CNN 层数过多，误差反而较大113
图4.48 残差网络的基本结构113
图4.49 残差网络完整结构.114
图5.1 NetFlix 的实时推荐系统的架构图120
图5.2 整体式混合推荐系统125
图5.3 并行式混合推荐系统125
图5.4 流水线式混合推荐系统.125
图5.5 MDLP 特征离散化130
图5.6 ChiMerge 特征离散化.131
图5.7 层次化时间按序列特征.133
图5.8 Learn to rank 的局限153
图6.1 Wide & Deep 模型结构157
图6.2 推荐系统的召回和排序两个阶段158
图6.3 召回模型结构.159
图6.4 序列信息160
图6.5 排序模型结构.161
图6.6 不同NN 的效果162
图6.7 DeepFM 模型结构(网络左边为FM 层，右边为DNN 层).164
图6.8 FM 一阶部分165
图6.9 FM 二阶部分166
图6.10 FM/DNN/DeepFM 的比较171
x j 推荐系统与深度学习
图6.11 电影静止帧图片举例172
图6.12 Alex-Net 卷积网络.173
图6.13 左图：时间无关的推荐系统。右图：时间相关的推荐系统174
图6.14 基于循环神经网络的推荐系统175
图6.15 判别器177
图6.16 生成器178
图6.17 IRGAN 说明179
图7.1 监督学习基本模型.184
图7.2 基于离线训练的推荐系统架构设计186
图7.3 数据上报模块.187
图7.4 离线训练模块.187
图7.5 推荐系统中的存储分层.188
图7.6 在线预测的几个阶段189
图7.7 推荐系统通用性设计190
图7.8 面向深度学习的推荐系统架构设计191
图7.9 利用深度学习进行特征提取192
图7.10 参数服务器架构193
图7.11 基于在线训练的推荐系统架构设计195
图7.12 在线学习之实时特征处理196
图7.13 面向内容的推荐系统架构设计198
图7.14 用于推荐的内容池.198
图7.15 Apache Kafka 逻辑架构.200
表4.1 用户A 和B 的评分矩阵.43
表4.2 电影内容特征二进制表示45
表4.3 人脸魅力值打分不同模型的MAE 比较112
表4.4 人脸魅力值打分不同模型的MAE 比较117

表4.5 Keras 预训练好的图像分类模型118
・ ・ ・ ・ ・ ・ (收起)1 概述1
1.1 智能问答：让机器更好地服务于人 1
1.2 问答系统类型介绍 2
1.2.1 基于事实的问答系统 3
1.2.2 基于常见问题集的问答系统 3
1.2.3 开放域的问答系统 4
1.3 使用本书附带的源码程序 4
1.3.1 安装依赖软件 4
1.3.2 下载源码 5
1.3.3 执行示例程序 5
1.3.4 联系我们 6
1.4 全书结构 6
2 机器学习基础8
2.1 线性代数 8
2.1.1 标量、向量、矩阵和张量 8
2.1.2 矩阵运算 9
2.1.3 特殊类型的矩阵 10
2.1.4 线性相关 11
2.1.5 范数 12
2.2 概率论基础 12
2.2.1 随机变量 13
2.2.2 期望和方差 13
2.2.3 伯努利分布 14
2.2.4 二项分布 14
2.2.5 泊松分布 15
2.2.6 正态分布 15
2.2.7 条件概率、联合概率和全概率 17
2.2.8 先验概率与后验概率 18
2.2.9 边缘概率 18
2.2.10 贝叶斯公式 18
2.2.11 最大似然估计算法 19
2.2.12 线性回归模型 20
2.2.13 逻辑斯蒂回归模型 21
2.3 信息论基础 22
2.3.1 熵 23
2.3.2 联合熵和条件熵 23
2.3.3 相对熵与互信息 24
2.3.4 信道和信道容量 25
2.3.5 最大熵模型 26
2.3.6 信息论与机器学习 29
2.4 统计学习 29
2.4.1 输入空间、特征空间与输出空间 30
2.4.2 向量表示 30
2.4.3 数据集 31
2.4.4 从概率到函数 31
2.4.5 统计学习三要素 32
2.5 隐马尔可夫模型 33
2.5.1 随机过程和马尔可夫链 33
2.5.2 隐马尔可夫模型的定义 36
2.5.3 三个基本假设及适用场景 37
2.5.4 概率计算问题之直接计算 39
2.5.5 概率计算问题之前向算法 40
2.5.6 概率计算问题之后向算法 42
2.5.7 预测问题之维特比算法 45
2.5.8 学习问题之Baum-Welch 算法 48
2.6 条件随机场模型 52
2.6.1 超越HMM 52
2.6.2 项目实践 55
2.7 总结 59
3 自然语言处理基础60
3.1 中文自动分词 60
3.1.1 有向无环图 61
3.1.2 最大匹配算法 63
3.1.3 算法评测 69
3.1.4 由字构词的方法 72
3.2 词性标注 77
3.2.1 词性标注规范 77
3.2.2 隐马尔可夫模型词性标注 79
3.3 命名实体识别 81
3.4 上下文无关文法 82
3.4.1 原理介绍 83
3.4.2 算法浅析 83
3.5 依存关系分析 84
3.5.1 算法浅析 85
3.5.2 项目实践 92
3.5.3 小结 94
3.6 信息检索系统 95
3.6.1 什么是信息检索系统 95
3.6.2 衡量信息检索系统的关键指标 95
3.6.3 理解非结构化数据 97
3.6.4 倒排索引 98
3.6.5 处理查询 100
3.6.6 项目实践 102
3.6.7 Elasticsearch 103
3.6.8 小结 112
3.7 问答语料 113
3.7.1 WikiQA 113
3.7.2 中文版保险行业语料库InsuranceQA 113
3.8 总结 115
4 深度学习初步116
4.1 深度学习简史 116
4.1.1 感知机 116
4.1.2 寒冬和复苏 117
4.1.3 走出实验室 118
4.1.4 寒冬再临 119
4.1.5 走向大规模实际应用 119
4.2 基本架构 120
4.2.1 神经元 121
4.2.2 输入层、隐藏层和输出层 122
4.2.3 标准符号 123
4.3 神经网络是如何学习的 124
4.3.1 梯度下降 124
4.3.2 反向传播理论 127
4.3.3 神经网络全连接层的实现 130
4.3.4 使用简单神经网络实现问答任务 131
4.4 调整神经网络超参数 136
4.4.1 超参数 136
4.4.2 参考建议 137
4.5 卷积神经网络与池化 138
4.5.1 简介 138
4.5.2 卷积层的前向传播 139
4.5.3 池化层的前向传播 141
4.5.4 卷积层的实现 141
4.5.5 池化层的实现 145
4.5.6 使用卷积神经网络实现问答任务 148
4.6 循环神经网络及其变种 149
4.6.1 简介 149
4.6.2 循环神经网络 149
4.6.3 长短期记忆单元和门控循环单元 153
4.6.4 循环神经网络的实现 156
4.6.5 使用循环神经网络实现问答任务 159
4.7 简易神经网络工具包 160
5 词向量实现及应用161
5.1 语言模型 161
5.1.1 评测 162
5.1.2 ARPA 格式介绍 162
5.1.3 项目实践 163
5.2 One-hot 表示法 164
5.3 词袋模型 165
5.4 NNLM 和RNNLM 165
5.5 word2vec 168
5.5.1 C-BOW 的原理 169
5.5.2 Skip-gram 的原理 172
5.5.3 计算效率优化 174
5.5.4 项目实践 179
5.6 GloVe 189
5.6.1 GloVe 的原理 189
5.6.2 GloVe 与word2vec 的区别和联系 191
5.6.3 项目实践 193
5.7 fastText 198
5.7.1 fastText 的原理 198
5.7.2 fastText 与word2vec 的区别和联系 200
5.7.3 项目实践 201
5.8 中文近义词工具包 204
5.8.1 安装 205
5.8.2 接口 205
5.9 总结 205
6 社区问答中的QA 匹配206
6.1 社区问答任务简介 206
6.2 孪生网络模型 207
6.3 QACNN 模型 207
6.3.1 模型构建 207
6.3.2 实验结果 214
6.4 Decomposable Attention 模型 214
6.4.1 模型介绍 214
6.4.2 模型构建 216
6.5 多比较方式的比较C集成模型 216
6.5.1 模型介绍 216
6.5.2 模型构建 218
6.6 BiMPM 模型 219
6.6.1 模型介绍 219
6.6.2 模型构建 221
7 机器阅读理解222
7.1 完型填空型机器阅读理解任务 222
7.1.1 CNN/Daily Mail 数据集 222
7.1.2 Children’s Book Test（CBT）数据集 223
7.1.3 GA Reader 模型 226
7.1.4 SA Reader 模型 227
7.1.5 AoA Reader 模型 228
7.2 答案抽取型机器阅读理解任务 230
7.2.1 SQuAD 数据集 231
7.2.2 MS MARCO 数据集 232
7.2.3 TriviaQA 数据集 234
7.2.4 DuReader 数据集 235
7.2.5 BiDAF 模型 235
7.2.6 R-Net 模型 237
7.2.7 S-Net 模型 240
7.3 答案选择型机器阅读理解任务 243
7.4 展望 245
参考文献246
・ ・ ・ ・ ・ ・ (收起)第 1 章引言 1
1．1　本书面向的读者　7
1．2　深度学习的历史趋势　8
1．2．1　神经网络的众多名称和命运变迁　8
1．2．2　与日俱增的数据量　12
1．2．3　与日俱增的模型规模　13
1．2．4　与日俱增的精度、复杂度和对现实世界的冲击　15
第　1 部分应用数学与机器学习基础
第　2 章线性代数　19
2．1　标量、向量、矩阵和张量　19
2．2　矩阵和向量相乘　21
2．3　单位矩阵和逆矩阵　22
2．4　线性相关和生成子空间　23
2．5　范数　24
2．6　特殊类型的矩阵和向量　25
2．7　特征分解　26
2．8　奇异值分解　28
2．9　Moore-Penrose 伪逆　28
2．10　迹运算　29
2．11　行列式　30
2．12　实例：主成分分析　30
第3　章概率与信息论　34
3．1　为什么要使用概率　34
3．2　随机变量　35
3．3　概率分布　36
3．3．1　离散型变量和概率质量函数　36
3．3．2　连续型变量和概率密度函数　36
3．4　边缘概率　37
3．5　条件概率　37
3．6　条件概率的链式法则　38
3．7　独立性和条件独立性　38
3．8　期望、方差和协方差　38
3．9　常用概率分布　39
3．9．1　Bernoulli 分布　40
3．9．2　Multinoulli 分布　40
3．9．3　高斯分布　40
3．9．4　指数分布和Laplace 分布　41
3．9．5　Dirac 分布和经验分布　42
3．9．6　分布的混合　42
3．10　常用函数的有用性质　43
3．11　贝叶斯规则　45
3．12　连续型变量的技术细节　45
3．13　信息论　47
3．14　结构化概率模型　49
第4　章数值计算　52
4．1　上溢和下溢　52
4．2　病态条件　53
4．3　基于梯度的优化方法　53
4．4　约束优化　60
4．5　实例：线性最小二乘　61
第5　章机器学习基础　63
5．1　学习算法　63
5．1．1　任务T　63
5．1．2　性能度量P　66
5．1．3　经验E　66
5．1．4　示例：线性回归　68
5．2　容量、过拟合和欠拟合　70
5．2．1　没有免费午餐定理　73
5．2．2　正则化　74
5．3　超参数和验证集　76
5．4　估计、偏差和方差　77
5．4．1　点估计　77
5．4．2　偏差　78
5．4．3　方差和标准差　80
5．4．4　权衡偏差和方差以最小化均方误差　81
5．4．5　一致性　82
5．5　最大似然估计　82
5．5．1　条件对数似然和均方误差　84
5．5．2　最大似然的性质　84
5．6　贝叶斯统计　85
5．7　监督学习算法　88
5．7．1　概率监督学习　88
5．7．2　支持向量机　88
5．7．3　其他简单的监督学习算法　90
5．8　无监督学习算法　91
5．8．1　主成分分析　92
5．8．2　k- 均值聚类　94
5．9　随机梯度下降　94
5．10　构建机器学习算法　96
5．11　促使深度学习发展的挑战　96
5．11．1　维数灾难　97
5．11．2　局部不变性和平滑正则化　97
5．11．3　流形学习　99
第　2 部分深度网络：现代实践
第6　章深度前馈网络　105
6．1　实例：学习XOR　107
6．2　基于梯度的学习　110
6．2．1　代价函数　111
6．2．2　输出单元　113
6．3　隐藏单元　119
6．3．1　整流线性单元及其扩展　120
6．3．2　logistic sigmoid 与双曲正切函数　121
6．3．3　其他隐藏单元　122
6．4　架构设计　123
6．4．1　万能近似性质和深度　123
6．4．2　其他架构上的考虑　125
6．5　反向传播和其他的微分算法　126
6．5．1　计算图　127
6．5．2　微积分中的链式法则　127
6．5．3　递归地使用链式法则来实现反向传播　128
6．5．4　全连接MLP 中的反向传播计算　131
6．5．5　符号到符号的导数　131
6．5．6　一般化的反向传播　133
6．5．7　实例：用于MLP 训练的反向传播　135
6．5．8　复杂化　137
6．5．9　深度学习界以外的微分　137
6．5．10　高阶微分　138
6．6　历史小记　139
第7　章深度学习中的正则化　141
7．1　参数范数惩罚　142
7．1．1　L2 参数正则化　142
7．1．2　L1 正则化　144
7．2　作为约束的范数惩罚　146
7．3　正则化和欠约束问题　147
7．4　数据集增强　148
7．5　噪声鲁棒性　149
7．6　半监督学习　150
7．7　多任务学习　150
7．8　提前终止　151
7．9　参数绑定和参数共享　156
7．10　稀疏表示　157
7．11　Bagging 和其他集成方法　158
7．12　Dropout　159
7．13　对抗训练　165
7．14　切面距离、正切传播和流形正切分类器　167
第8　章深度模型中的优化　169
8．1　学习和纯优化有什么不同　169
8．1．1　经验风险最小化　169
8．1．2　代理损失函数和提前终止　170
8．1．3　批量算法和小批量算法　170
8．2　神经网络优化中的挑战　173
8．2．1　病态　173
8．2．2　局部极小值　174
8．2．3　高原、鞍点和其他平坦区域　175
8．2．4　悬崖和梯度爆炸　177
8．2．5　长期依赖　177
8．2．6　非精确梯度　178
8．2．7　局部和全局结构间的弱对应　178
8．2．8　优化的理论限制　179
8．3　基本算法　180
8．3．1　随机梯度下降　180
8．3．2　动量　181
8．3．3　Nesterov 动量　183
8．4　参数初始化策略　184
8．5　自适应学习率算法　187
8．5．1　AdaGrad　187
8．5．2　RMSProp　188
8．5．3　Adam　189
8．5．4　选择正确的优化算法　190
8．6　二阶近似方法　190
8．6．1　牛顿法　190
8．6．2　共轭梯度　191
8．6．3　BFGS　193
8．7　优化策略和元算法　194
8．7．1　批标准化　194
8．7．2　坐标下降　196
8．7．3　Polyak 平均　197
8．7．4　监督预训练　197
8．7．5　设计有助于优化的模型　199
8．7．6　延拓法和课程学习　199
第9　章卷积网络　201
9．1　卷积运算　201
9．2　动机　203
9．3　池化　207
9．4　卷积与池化作为一种无限强的先验　210
9．5　基本卷积函数的变体　211
9．6　结构化输出　218
9．7　数据类型　219
9．8　高效的卷积算法　220
9．9　随机或无监督的特征　220
9．10　卷积网络的神经科学基础　221
9．11　卷积网络与深度学习的历史　226
第　10 章序列建模：循环和递归网络　227
10．1　展开计算图　228
10．2　循环神经网络　230
10．2．1　导师驱动过程和输出循环网络　232
10．2．2　计算循环神经网络的梯度　233
10．2．3　作为有向图模型的循环网络　235
10．2．4　基于上下文的RNN 序列建模　237
10．3　双向RNN　239
10．4　基于编码{解码的序列到序列架构　240
10．5　深度循环网络　242
10．6　递归神经网络　243
10．7　长期依赖的挑战　244
10．8　回声状态网络　245
10．9　渗漏单元和其他多时间尺度的策略　247
10．9．1　时间维度的跳跃连接　247
10．9．2　渗漏单元和一系列不同时间尺度　247
10．9．3　删除连接　248
10．10　长短期记忆和其他门控RNN　248
10．10．1　LSTM　248
10．10．2　其他门控RNN　250
10．11　优化长期依赖　251
10．11．1　截断梯度　251
10．11．2　引导信息流的正则化　252
10．12　外显记忆　253
第　11 章实践方法论　256
11．1　性能度量　256
11．2　默认的基准模型　258
11．3　决定是否收集更多数据　259
11．4　选择超参数　259
11．4．1　手动调整超参数　259
11．4．2　自动超参数优化算法　262
11．4．3　网格搜索　262
11．4．4　随机搜索　263
11．4．5　基于模型的超参数优化　264
11．5　调试策略　264
11．6　示例：多位数字识别　267
第　12 章应用　269
12．1　大规模深度学习　269
12．1．1　快速的CPU 实现　269
12．1．2　GPU 实现　269
12．1．3　大规模的分布式实现　271
12．1．4　模型压缩　271
12．1．5　动态结构　272
12．1．6　深度网络的专用硬件实现　273
12．2　计算机视觉　274
12．2．1　预处理　275
12．2．2　数据集增强　277
12．3　语音识别　278
12．4　自然语言处理　279
12．4．1　n-gram　280
12．4．2　神经语言模型　281
12．4．3　高维输出　282
12．4．4　结合n-gram 和神经语言模型　286
12．4．5　神经机器翻译　287
12．4．6　历史展望　289
12．5　其他应用　290
12．5．1　推荐系统　290
12．5．2　知识表示、推理和回答　292
第3　部分深度学习研究
第　13 章线性因子模型　297
13．1　概率PCA 和因子分析　297
13．2　独立成分分析　298
13．3　慢特征分析　300
13．4　稀疏编码　301
13．5　PCA 的流形解释　304
第　14 章自编码器　306
14．1　欠完备自编码器　306
14．2　正则自编码器　307
14．2．1　稀疏自编码器　307
14．2．2　去噪自编码器　309
14．2．3　惩罚导数作为正则　309
14．3　表示能力、层的大小和深度　310
14．4　随机编码器和解码器　310
14．5　去噪自编码器详解　311
14．5．1　得分估计　312
14．5．2　历史展望　314
14．6　使用自编码器学习流形　314
14．7　收缩自编码器　317
14．8　预测稀疏分解　319
14．9　自编码器的应用　319
第　15 章表示学习　321
15．1　贪心逐层无监督预训练　322
15．2　迁移学习和领域自适应　326
15．3　半监督解释因果关系　329
15．4　分布式表示　332
15．5　得益于深度的指数增益　336
15．6　提供发现潜在原因的线索　337
第　16 章深度学习中的结构化概率模型　339
16．1　非结构化建模的挑战　339
16．2　使用图描述模型结构　342
16．2．1　有向模型　342
16．2．2　无向模型　344
16．2．3　配分函数　345
16．2．4　基于能量的模型　346
16．2．5　分离和d-分离　347
16．2．6　在有向模型和无向模型中转换　350
16．2．7　因子图　352
16．3　从图模型中采样　353
16．4　结构化建模的优势　353
16．5　学习依赖关系　354
16．6　推断和近似推断　354
16．7　结构化概率模型的深度学习方法 355第 17 章蒙特卡罗方法　359
17．1　采样和蒙特卡罗方法　359
17．1．1　为什么需要采样　359
17．1．2　蒙特卡罗采样的基础　359
17．2　重要采样　360
17．3　马尔可夫链蒙特卡罗方法　362
17．4　Gibbs 采样　365
17．5　不同的峰值之间的混合挑战　365
17．5．1　不同峰值之间通过回火来混合　367
17．5．2　深度也许会有助于混合　368
第　18 章直面配分函数　369
18．1　对数似然梯度　369
18．2　随机最大似然和对比散度　370
18．3　伪似然　375
18．4　得分匹配和比率匹配　376
18．5　去噪得分匹配　378
18．6　噪声对比估计　378
18．7　估计配分函数　380
18．7．1　退火重要采样　382
18．7．2　桥式采样　384
第　19 章近似推断　385
19．1　把推断视作优化问题　385
19．2　期望最大化　386
19．3　最大后验推断和稀疏编码　387
19．4　变分推断和变分学习　389
19．4．1　离散型潜变量　390
19．4．2　变分法　394
19．4．3　连续型潜变量　396
19．4．4　学习和推断之间的相互作用　397
19．5　学成近似推断　397
19．5．1　醒眠算法　398
19．5．2　学成推断的其他形式　398
第　20 章深度生成模型　399
20．1　玻尔兹曼机　399
20．2　受限玻尔兹曼机　400
20．2．1　条件分布　401
20．2．2　训练受限玻尔兹曼机　402
20．3　深度信念网络　402
20．4　深度玻尔兹曼机　404
20．4．1　有趣的性质　406
20．4．2　DBM 均匀场推断　406
20．4．3　DBM 的参数学习　408
20．4．4　逐层预训练　408
20．4．5　联合训练深度玻尔兹曼机　410
20．5　实值数据上的玻尔兹曼机　413
20．5．1　Gaussian-Bernoulli RBM　413
20．5．2　条件协方差的无向模型　414
20．6　卷积玻尔兹曼机　417
20．7　用于结构化或序列输出的玻尔兹曼机　418
20．8　其他玻尔兹曼机　419
20．9　通过随机操作的反向传播　419
20．10　有向生成网络　422
20．10．1　sigmoid 信念网络　422
20．10．2　可微生成器网络　423
20．10．3　变分自编码器　425
20．10．4　生成式对抗网络　426
20．10．5　生成矩匹配网络　429
20．10．6　卷积生成网络　430
20．10．7　自回归网络　430
20．10．8　线性自回归网络　430
20．10．9　神经自回归网络　431
20．10．10　NADE　432
20．11　从自编码器采样　433
20．11．1　与任意去噪自编码器相关的马尔可夫链 ．　434
20．11．2　夹合与条件采样　434
20．11．3　回退训练过程　435
20．12　生成随机网络　435
20．13　其他生成方案　436
20．14　评估生成模型　437
20．15　结论　438
参考文献　439
索引　486
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 深度学习的发展介绍 1
1.1 如何阅读本书 3
1.2 深度学习沉浮史 3
1.2.1 模拟生物大脑的疯狂远古时代 4
1.2.2 联结主义近代 5
1.2.3 百花齐放，层次结构主导，模型巨大的当代 6
1.3 Python简易教程 7
1.3.1 Anaconda搭建 7
1.3.2 IPython Notebook使用 7
1.3.3 Python基本用法 8
1.3.4 NumPy 15
1.3.5 Matplotlib 23
1.4 参考文献 25
第2章 机器学习快速入门 27
2.1 学习算法 28
2.1.1 学习任务 29
2.1.2 性能度量 30
2.1.3 学习经验 32
2.2 代价函数 33
2.2.1 均方误差函数 33
2.2.2 极大似然估计 34
2.3 梯度下降法 36
2.3.1 批量梯度下降法 38
2.3.2 随机梯度下降法 39
2.4 过拟合与欠拟合 40
2.4.1 没免费午餐理论 42
2.4.2 正则化 43
2.5 超参数与验证集 44
2.6 Softmax编码实战 46
2.6.1 编码说明 49
2.6.2 熟练使用CIFAR-10 数据集 50
2.6.3 显式循环计算损失函数及其梯度 53
2.6.4 向量化表达式计算损失函数及其梯度 56
2.6.5 最小批量梯度下降算法训练Softmax分类器 57
2.6.6 使用验证数据选择超参数 61
2.7 参考代码 68
2.8 参考文献 70
第3章 前馈神经网络 72
3.1 神经元 73
3.1.1 Sigmoid神经元 74
3.1.2 Tanh神经元 75
3.1.3 ReLU神经元 76
3.2 前馈神经网络 80
3.2.1 输出层单元 80
3.2.2 隐藏层单元 80
3.2.3 网络结构设计 81
3.3 BP算法 82
3.4 深度学习编码实战上 86
3.4.1 实现仿射传播 88
3.4.2 实现ReLU传播 91
3.4.3 组合单层神经元 93
3.4.4 实现浅层神经网络 96
3.4.5 实现深层全连接网络 101
3.5 参考代码 109
3.6 参考文献 113
第4章 深度学习正则化 115
4.1 参数范数惩罚 116
4.1.1 L2参数正则化 118
4.1.2 L1正则化 119
4.2 参数绑定与参数共享 120
4.3 噪声注入与数据扩充 120
4.4 稀疏表征 122
4.5 早停 123
4.6 Dropout 126
4.6.1 个体与集成 126
4.6.2 Dropout 127
4.7 深度学习编码实战中 129
4.7.1 Dropout传播 131
4.7.2 组合Dropout传播层 134
4.7.3 Dropout神经网络 136
4.7.4 解耦训练器trainer 138
4.7.5 解耦更新器updater 143
4.7.6 正则化实验 145
4.8 参考代码 148
4.9 参考文献 150
第5章 深度学习优化 152
5.1 神经网络优化困难 153
5.1.1 局部最优 153
5.1.2 鞍点 154
5.1.3 梯度悬崖 154
5.1.4 梯度消失或梯度爆炸 155
5.1.5 梯度不精确 156
5.1.6 优化理论的局限性 156
5.2 随机梯度下降 156
5.3 动量学习法 158
5.4 AdaGrad和RMSProp 159
5.5 Adam 160
5.6 参数初始化策略 161
5.7 批量归一化 163
5.7.1 BN算法详解 163
5.7.2 BN传播详解 165
5.8 深度学习编码实战下 166
5.8.1 Momentum 167
5.8.2 RMSProp 171
5.8.3 Adam 172
5.8.4 更新规则比较 174
5.8.5 BN前向传播 176
5.8.6 BN反向传播 180
5.8.7 使用BN的全连接网络 182
5.8.8 BN算法与权重标准差比较 188
5.9 参考代码 191
5.10 参考文献 195
第6章 卷积神经网络 196
6.1 卷积操作 197
6.2 卷积的意义 198
6.2.1 稀疏连接 199
6.2.2 参数共享 200
6.3 池化操作 201
6.4 设计卷积神经网络 204
6.4.1 跨步卷积 204
6.4.2 零填充 205
6.4.3 非共享卷积 206
6.4.4 平铺卷积 207
6.5 卷积网络编码练习 208
6.5.1 卷积前向传播 209
6.5.2 卷积反向传播 212
6.5.3 最大池化前向传播 215
6.5.4 最大池化反向传播 218
6.5.5 向量化执行 220
6.5.6 组合完整卷积层 223
6.5.7 浅层卷积网络 224
6.5.8 空间批量归一化 229
6.6 参考代码 233
6.7 参考文献 237
第7章 循环神经网络 238
7.1 循环神经网络 239
7.1.1 循环神经元展开 239
7.1.2 循环网络训练 240
7.2 循环神经网络设计 242
7.2.1 双向循环网络结构 242
7.2.2 编码-解码网络结构 243
7.2.3 深度循环网络结构 244
7.3 门控循环神经网络 245
7.3.1 LSTM 246
7.3.2 门控循环单元 249
7.4 RNN编程练习 250
7.4.1 RNN单步传播 252
7.4.2 RNN时序传播 255
7.4.3 词嵌入 258
7.4.4 RNN输出层 261
7.4.5 时序Softmax损失 262
7.4.6 RNN图片说明任务 264
7.5 LSTM编程练习 269
7.5.1 LSTM单步传播 269
7.5.2 LSTM时序传播 273
7.5.3 LSTM实现图片说明任务 276
7.6 参考代码 278
7.6.1 RNN参考代码 278
7.6.2 LSTM参考代码 282
7.7 参考文献 285
第8章 TensorFlow快速入门 287
8.1 TensorFlow介绍 288
8.2 TensorFlow 1.0安装指南 289
8.2.1 双版本切换Anaconda 289
8.2.2 安装CUDA 8.0 291
8.2.3 安装cuDNN 292
8.2.4 安装TensorFlow 293
8.2.5 验证安装 294
8.3 TensorFlow基础 295
8.3.1 Tensor 295
8.3.2 TensorFlow核心API教程 296
8.3.3 tf.train API 299
8.3.4 tf.contrib.learn 301
8.4 TensorFlow构造CNN 305
8.4.1 构建Softmax模型 305
8.4.2 使用TensorFlow训练模型 307
8.4.3 使用TensorFlow评估模型 308
8.4.4 使用TensorFlow构建卷积神经网络 308
8.5 TensorBoard快速入门 311
8.5.1 TensorBoard可视化学习 312
8.5.2 计算图可视化 316
・ ・ ・ ・ ・ ・ (收起)译者序 iv
序 vii
前言 ix
术语缩写 xxii
符号 xxvii
第 1 章 简介 1
1.1 自动语音识别：更好的沟通之桥 . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 人类之间的交流 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.2 人机交流 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 语音识别系统的基本结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 全书结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3.1 第一部分：传统声学模型 . . . . . . . . . . . . . . . . . . . . . . 6
1.3.2 第二部分：深度神经网络 . . . . . . . . . . . . . . . . . . . . . . 6
1.3.3 第三部分：语音识别中的 DNN-HMM 混合系统 . . . . . . . . . . 7
1.3.4 第四部分：深度神经网络中的表征学习 . . . . . . . . . . . . . . 7
1.3.5 第五部分：高级的深度模型 . . . . . . . . . . . . . . . . . . . . . 7
第一部分 传统声学模型 9
第 2 章 混合高斯模型 11
2.1 随机变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 高斯分布和混合高斯随机变量 . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 参数估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4 采用混合高斯分布对语音特征建模 . . . . . . . . . . . . . . . . . . . . . 16
第 3 章 隐马尔可夫模型及其变体 19
3.1 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 马尔可夫链 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.3 序列与模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.1 隐马尔可夫模型的性质 . . . . . . . . . . . . . . . . . . . . . . . . 23
3.3.2 隐马尔可夫模型的仿真 . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.3 隐马尔可夫模型似然度的计算 . . . . . . . . . . . . . . . . . . . . 24
3.3.4 计算似然度的高效算法 . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3.5 前向与后向递归式的证明 . . . . . . . . . . . . . . . . . . . . . . 27
3.4 期望最大化算法及其在学习 HMM 参数中的应用 . . . . . . . . . . . . . 28
3.4.1 期望最大化算法介绍 . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.4.2 使用 EM 算法来学习 HMM 参数――Baum-Welch 算法 . . . . . . 30
3.5 用于解码 HMM 状态序列的维特比算法 . . . . . . . . . . . . . . . . . . . 34
3.5.1 动态规划和维特比算法 . . . . . . . . . . . . . . . . . . . . . . . . 34
3.5.2 用于解码 HMM 状态的动态规划算法 . . . . . . . . . . . . . . . . 35
3.6 隐马尔可夫模型和生成语音识别模型的变体 . . . . . . . . . . . . . . . . 37
3.6.1 用于语音识别的 GMM-HMM 模型 . . . . . . . . . . . . . . . . . 38
3.6.2 基于轨迹和隐藏动态模型的语音建模和识别 . . . . . . . . . . . . 39
3.6.3 使用生成模型 HMM 及其变体解决语音识别问题 . . . . . . . . . 40
第二部分 深度神经网络 43
第 4 章 深度神经网络 45
4.1 深度神经网络框架 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.2 使用误差反向传播来进行参数训练 . . . . . . . . . . . . . . . . . . . . . 48
4.2.1 训练准则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.2.2 训练算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.3 实际应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3.1 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.3.2 模型初始化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.3.3 权重衰减 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.3.4 丢弃法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.3.5 批量块大小的选择 . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.3.6 取样随机化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3.7 惯性系数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.3.8 学习率和停止准则 . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.3.9 网络结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.3.10 可复现性与可重启性 . . . . . . . . . . . . . . . . . . . . . . . . . 62
第 5 章 高级模型初始化技术 65
5.1 受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.1.1 受限玻尔兹曼机的属性 . . . . . . . . . . . . . . . . . . . . . . . . 67
5.1.2 受限玻尔兹曼机参数学习 . . . . . . . . . . . . . . . . . . . . . . 70
5.2 深度置信网络预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.3 降噪自动编码器预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.4 鉴别性预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.5 混合预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.6 采用丢弃法的预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
第三部分 语音识别中的深度神经网络C隐马尔可夫混合模型 81
第 6 章 深度神经网络C隐马尔可夫模型混合系统 83
6.1 DNN-HMM 混合系统 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.1.1 结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.1.2 用 CD-DNN-HMM 解码 . . . . . . . . . . . . . . . . . . . . . . . . 85
6.1.3 CD-DNN-HMM 训练过程 . . . . . . . . . . . . . . . . . . . . . . . 86
6.1.4 上下文窗口的影响 . . . . . . . . . . . . . . . . . . . . . . . . . . 88
6.2 CD-DNN-HMM 的关键模块及分析 . . . . . . . . . . . . . . . . . . . . . 90
6.2.1 进行比较和分析的数据集和实验 . . . . . . . . . . . . . . . . . . 90
6.2.2 对单音素或者三音素的状态进行建模 . . . . . . . . . . . . . . . . 92
6.2.3 越深越好 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.2.4 利用相邻的语音帧 . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.2.5 预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.2.6 训练数据的标注质量的影响 . . . . . . . . . . . . . . . . . . . . . 95
6.2.7 调整转移概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.3 基于 KL 距离的隐马尔可夫模型 . . . . . . . . . . . . . . . . . . . . . . . 96
第 7 章 训练和解码的加速 99
7.1 训练加速 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
7.1.1 使用多 GPU 流水线反向传播 . . . . . . . . . . . . . . . . . . . . 100
7.1.2 异步随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.1.3 增广拉格朗日算法及乘子方向交替算法 . . . . . . . . . . . . . . 106
7.1.4 减小模型规模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1.5 其他方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.2 加速解码 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.2.1 并行计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.2.2 稀疏网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2.3 低秩近似 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.2.4 用大尺寸 DNN 训练小尺寸 DNN . . . . . . . . . . . . . . . . . . 114
7.2.5 多帧 DNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
第 8 章 深度神经网络序列鉴别性训练 117
8.1 序列鉴别性训练准则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
8.1.1 最大相互信息 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
8.1.2 增强型 MMI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.1.3 最小音素错误/状态级最小贝叶斯风险 . . . . . . . . . . . . . . . 120
8.1.4 统一的公式 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
8.2 具体实现中的考量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.2.1 词图产生 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.2.2 词图补偿 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.2.3 帧平滑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.2.4 学习率调整 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.2.5 训练准则选择 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.2.6 其他考量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.3 噪声对比估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
8.3.1 将概率密度估计问题转换为二分类设计问题 . . . . . . . . . . . . 127
8.3.2 拓展到未归一化的模型 . . . . . . . . . . . . . . . . . . . . . . . . 129
8.3.3 在深度学习网络训练中应用噪声对比估计算法 . . . . . . . . . . 130
第四部分 深度神经网络中的特征表示学习 133
第 9 章 深度神经网络中的特征表示学习 135
9.1 特征和分类器的联合学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
9.2 特征层级 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
9.3 使用随意输入特征的灵活性 . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.4 特征的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4.1 对说话人变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4.2 对环境变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . 142
9.5 对环境的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
9.5.1 对噪声的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
9.5.2 对语速变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . 147
9.6 缺乏严重信号失真情况下的推广能力 . . . . . . . . . . . . . . . . . . . . 148
第 10 章 深度神经网络和混合高斯模型的融合 151
10.1 在 GMM-HMM 系统中使用由 DNN 衍生的特征 . . . . . . . . . . . . . . 151
10.1.1 使用 Tandem 和瓶颈特征的 GMM-HMM 模型 . . . . . . . . . . . 151
10.1.2 DNN-HMM 混合系统与采用深度特征的 GMM-HMM 系统的比较 154
10.2 识别结果融合技术 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.2.1 识别错误票选降低技术（ ROVER） . . . . . . . . . . . . . . . . . 157
10.2.2 分段条件随机场（ SCARF） . . . . . . . . . . . . . . . . . . . . . 159
10.2.3 最小贝叶斯风险词图融合 . . . . . . . . . . . . . . . . . . . . . . 160
10.3 帧级别的声学分数融合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
10.4 多流语音识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
第 11 章 深度神经网络的自适应技术 165
11.1 深度神经网络中的自适应问题 . . . . . . . . . . . . . . . . . . . . . . . . 165
11.2 线性变换 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.2.1 线性输入网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.2.2 线性输出网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.3 线性隐层网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
11.4 保守训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
11.4.1 L 2 正则项 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
11.4.2 KL 距离正则项 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
11.4.3 减少每个说话人的模型开销 . . . . . . . . . . . . . . . . . . . . . 173
11.5 子空间方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
11.5.1 通过主成分分析构建子空间 . . . . . . . . . . . . . . . . . . . . . 175
11.5.2 噪声感知、说话人感知及设备感知训练 . . . . . . . . . . . . . . 176
11.5.3 张量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
11.6 DNN 说话人自适应的效果 . . . . . . . . . . . . . . . . . . . . . . . . . . 181
11.6.1 基于 KL 距离的正则化方法 . . . . . . . . . . . . . . . . . . . . . 181
11.6.2 说话人感知训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
第五部分 先进的深度学习模型 185
第 12 章 深度神经网络中的表征共享和迁移 187
12.1 多任务和迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
12.1.1 多任务学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
12.1.2 迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.2 多语言和跨语言语音识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.2.1 基于 Tandem 或瓶颈特征的跨语言语音识别 . . . . . . . . . . . . 190
12.2.2 共享隐层的多语言深度神经网络 . . . . . . . . . . . . . . . . . . 191
12.2.3 跨语言模型迁移 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
12.3 语音识别中深度神经网络的多目标学习 . . . . . . . . . . . . . . . . . . . 197
12.3.1 使用多任务学习的鲁棒语音识别 . . . . . . . . . . . . . . . . . . 197
12.3.2 使用多任务学习改善音素识别 . . . . . . . . . . . . . . . . . . . . 198
12.3.3 同时识别音素和字素（ graphemes） . . . . . . . . . . . . . . . . . 199
12.4 使用视听信息的鲁棒语音识别 . . . . . . . . . . . . . . . . . . . . . . . . 199
第 13 章 循环神经网络及相关模型 201
13.1 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
13.2 基本循环神经网络中的状态-空间公式 . . . . . . . . . . . . . . . . . . . . 203
13.3 沿时反向传播学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
13.3.1 最小化目标函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
13.3.2 误差项的递归计算 . . . . . . . . . . . . . . . . . . . . . . . . . . 205
13.3.3 循环神经网络权重的更新 . . . . . . . . . . . . . . . . . . . . . . 206
13.4 一种用于学习循环神经网络的原始对偶技术 . . . . . . . . . . . . . . . . 208
13.4.1 循环神经网络学习的难点 . . . . . . . . . . . . . . . . . . . . . . 208
13.4.2 回声状态（ Echo-State）性质及其充分条件 . . . . . . . . . . . . . 208
13.4.3 将循环神经网络的学习转化为带约束的优化问题 . . . . . . . . . 209
13.4.4 一种用于学习 RNN 的原始对偶方法 . . . . . . . . . . . . . . . . 210
13.5 结合长短时记忆单元（ LSTM）的循环神经网络 . . . . . . . . . . . . . . 212
13.5.1 动机与应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
13.5.2 长短时记忆单元的神经元架构 . . . . . . . . . . . . . . . . . . . . 213
13.5.3 LSTM-RNN 的训练 . . . . . . . . . . . . . . . . . . . . . . . . . . 214
13.6 循环神经网络的对比分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
13.6.1 信息流方向的对比：自上而下还是自下而上 . . . . . . . . . . . . 215
13.6.2 信息表征的对比：集中式还是分布式 . . . . . . . . . . . . . . . . 217
13.6.3 解释能力的对比：隐含层推断还是端到端学习 . . . . . . . . . . 218
13.6.4 参数化方式的对比：吝啬参数集合还是大规模参数矩阵 . . . . . 218
13.6.5 模型学习方法的对比：变分推理还是梯度下降 . . . . . . . . . . 219
13.6.6 识别正确率的比较 . . . . . . . . . . . . . . . . . . . . . . . . . . 220
13.7 讨论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
第 14 章 计算型网络 223
14.1 计算型网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
14.2 前向计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
14.3 模型训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
14.4 典型的计算节点 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
14.4.1 无操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . . . . 232
14.4.2 含一个操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . 232
14.4.3 含两个操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . 237
14.4.4 用来计算统计量的计算节点类型 . . . . . . . . . . . . . . . . . . 244
14.5 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
14.6 循环连接 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
14.6.1 只在循环中一个接一个地处理样本 . . . . . . . . . . . . . . . . . 249
14.6.2 同时处理多个句子 . . . . . . . . . . . . . . . . . . . . . . . . . . 251
14.6.3 创建任意的循环神经网络 . . . . . . . . . . . . . . . . . . . . . . 252
第 15 章 总结及未来研究方向 255
15.1 路线图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
15.1.1 语音识别中的深度神经网络启蒙 . . . . . . . . . . . . . . . . . . 255
15.1.2 深度神经网络训练和解码加速 . . . . . . . . . . . . . . . . . . . . 258
15.1.3 序列鉴别性训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
15.1.4 特征处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
15.1.5 自适应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
15.1.6 多任务和迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . 261
15.1.7 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
15.1.8 循环神经网络和长短时记忆神经网络 . . . . . . . . . . . . . . . . 261
15.1.9 其他深度模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2 技术前沿和未来方向 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2.1 技术前沿简析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2.2 未来方向 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
参考文献 267
・ ・ ・ ・ ・ ・ (收起)目录
前言1
第1章　工具与技术9
1.1 神经网络的类型9
1.2 数据获取19
1.3 数据预处理27
第2章　摆脱困境34
2.1 确定我们遇到的问题34
2.2 解决运行过程中的错误36
2.3 检查中间结果38
2.4 为最后一层选择正确的激活函数39
2.5 正则化和Dropout40
2.6 网络结构、批尺寸和学习率42
第3章　使用词嵌入计算文本相似性44
3.1 使用预训练的词嵌入发现词的相似性45
3.2 Word2vec数学特性47
3.3 可视化词嵌入49
3.4 在词嵌入中发现实体类51
3.5 计算类内部的语义距离55
3.6 在地图上可视化国家数据57
第4章　基于维基百科外部链接构建推荐系统58
4.1 收集数据58
4.2 训练电影嵌入62
4.3 构建电影推荐系统66
4.4 预测简单的电影属性67
第5章　按照示例文本的风格生成文本69
5.1 获取公开领域书籍文本69
5.2 生成类似莎士比亚的文本70
5.3 使用RNN编写代码74
5.4 控制输出温度76
5.5 可视化循环神经网络的活跃程度78
第6章　问题匹配80
6.1 从Stack Exchange网站获取数据80
6.2 使用Pandas探索数据82
6.3 使用Keras对文本进行特征化83
6.4 构建问答模型84
6.5 用Pandas训练模型86
6.6 检查相似性88
第7章　推荐表情符号90
7.1 构建一个简单的情感分类器90
7.2 检验一个简单的分类器93
7.3 使用卷积网络进行情感分析95
7.4 收集Twitter数据97
7.5 一个简单的表情符号预测器99
7.6 Dropout和多层窗口100
7.7 构建单词级模型102
7.8 构建你自己的嵌入104
7.9 使用循环神经网络进行分类106
7.10 可视化一致性/不一致性108
7.11 组合模型111
第8章　Sequence-to-Sequence映射113
8.1 训练一个简单的Sequence-to-Sequence模型113
8.2 从文本中提取对话115
8.3 处理开放词汇表117
8.4 训练seq2seq 聊天机器人119
第9章　复用预训练的图像识别网络123
9.1 加载预训练网络124
9.2 图像预处理124
9.3 推测图像内容126
9.4 使用Flickr API收集一组带标签的图像128
9.5 构建一个分辨猫狗的分类器129
9.6 改进搜索结果131
9.7 复训图像识别网络133
第10章　构建反向图像搜索服务137
10.1 从维基百科中获取图像137
10.2 向N维空间投影图像140
10.3 在高维空间中寻找最近邻141
10.4 探索嵌入中的局部邻域143
第11章　检测多幅图像145
11.1 使用预训练的分类器检测多个图像145
11.2 使用Faster RCNN进行目标检测149
11.3 在自己的图像上运行Faster RCNN152
第12章　图像风格155
12.1 可视化卷积神经网络激活值156
12.2 尺度和缩放159
12.3 可视化神经网络所见161
12.4 捕捉图像风格164
12.5 改进损失函数以提升图像相干性168
12.6 将风格迁移至不同图像169
12.7 风格内插171
第13章　用自编码器生成图像173
13.1 从Google Quick Draw中导入绘图174
13.2 为图像创建自编码器176
13.3 可视化自编码器结果178
13.4 从正确的分布中采样图像180
13.5 可视化变分自编码器空间183
13.6 条件变分编码器185
第14章　使用深度网络生成图标189
14.1 获得训练用的图标190
14.2 将图标转换为张量表示193
14.3 使用变分自编码器生成图标194
14.4 使用数据扩充提升自编码器的性能196
14.5 构建生成式对抗网络198
14.6 训练生成式对抗网络200
14.7 显示GAN生成的图标202
14.8 将图标编码成绘图指令204
14.9 训练RNN绘制图标205
14.10 使用RNN生成图标207
第15章　音乐与深度学习210
15.1 为音乐分类器创建训练数据集211
15.2 训练音乐风格检测器213
15.3 对混淆情况进行可视化215
15.4 为已有的音乐编制索引217
15.5 设置Spotify API219
15.6 从Spotify中收集播放列表和歌曲221
15.7 训练音乐推荐系统224
15.8 使用Word2vec模型推荐歌曲225
第16章　生产化部署机器学习系统228
16.1 使用scikit-learn最近邻计算嵌入229
16.2 使用Postgres存储嵌入230
16.3 填充和查询Postgres存储的嵌入231
16.4 在Postgres中存储高维模型233
16.5 使用Python编写微服务234
16.6 使用微服务部署Keras模型236
16.7 从Web框架中调用微服务237
16.8 Tensorflow seq2seq模型238
16.9 在浏览器中执行深度学习模型240
16.10 使用TensorFlow服务执行Keras模型243
16.11 在iOS中使用Keras模型245
・ ・ ・ ・ ・ ・ (收起)第1章 绪论 1
1.1 引言 1
1.2 本书内容 5
1.2.1 图像分类 7
1.2.2 动作识别 9
1.2.3 时序动作定位 12
1.2.4 视频 Embedding 14
1.3 本章小结 15
第2章 经典网络结构回顾 16
2.1 经典图像分类网络 16
2.1.1 LetNet-5 16
2.1.2 AlexNet 18
2.1.3 VGGNet 22
2.1.4 GoogLeNet 24
2.1.5 Inception V2/V3 27
2.1.6 ResNet 28
2.1.7 preResNet 31
2.1.8 WRN 32
2.1.9 随机深度网络 33
2.1.10 DenseNet 35
2.1.11 ResNeXt 36
2.1.12 SENet 39
2.1.13 MobileNet 41
2.1.14 MobileNet V2/V3 44
2.1.15 ShuffleNet 46
2.1.16 ShuffleNet V2 49
2.2 RNN、LSTM和GRU 51
2.2.1 RNN 51
2.2.2 梯度爆炸与梯度消失 52
2.2.3 LSTM 55
2.2.4 GRU 58
2.3 本章小结 60
第3章 基于2D卷积的动作识别 62
3.1 平均汇合 62
3.2 NetVLAD和NeXtVLAD 64
3.2.1 VLAD 65
3.2.2 NetVLAD 66
3.2.3 NeXtVLAD 71
3.2.4 NetFV和其他策略 75
3.3 利用RNN融合各帧特征 77
3.3.1 2D卷积 + RNN的基本结构 78
3.3.2 对RNN结构进行改造 80
3.4 利用3D卷积融合各帧特征 81
3.4.1 什么是3D卷积 82
3.4.2 ECO 85
3.5 双流法 87
3.5.1 什么是光流 87
3.5.2 双流法的基本网络结构 89
3.5.3 双流法的网络结构优化 91
3.6 时序稀疏采样 95
3.6.1 TSN 95
3.6.2 TSN的实现 98
3.6.3 ActionVLAD 99
3.6.4 StNet 100
3.6.5 TRN 102
3.7 利用iDT轨迹 104
3.7.1 DT和iDT 104
3.7.2 TDD 107
3.8 本章小结 108
第4章 基于3D卷积的动作识别 110
4.1 3D卷积基础网络结构 110
4.1.1 C3D 110
4.1.2 Res3D/3D ResNet 113
4.1.3 LTC 116
4.2 I3D 118
4.2.1 5类动作识别网络 118
4.2.2 2D卷积扩展为3D卷积 119
4.2.3 5类网络对比 121
4.3 3D卷积的低秩近似 123
4.3.1 低秩近似的基本原理 124
4.3.2 FSTCN 125
4.3.3 P3D 127
4.3.4 R(2+1)D 129
4.3.5 S3D 132
4.4 TSM 135
4.5 3D卷积 + RNN 137
4.6 ARTNet 139
4.7 Non-Local 141
4.7.1 Non-Local 操作 141
4.7.2 Non-Local 动作识别网络 144
4.8 SlowFast 148
4.8.1 Slow分支和Fast分支 149
4.8.2 网络结构设计 151
4.9 3D卷积神经网络超参数设计 152
4.9.1 多网格训练 152
4.9.2 X3D 154
4.10 本章小结 157
第5章 时序动作定位 159
5.1 基于滑动窗的算法 160
5.1.1 S-CNN 161
5.1.2 TURN 166
5.1.3 CBR 169
5.2 基于候选时序区间的算法 171
5.2.1 Faster R-CNN 回顾 172
5.2.2 R-C3D 175
5.2.3 TAL-Net 178
5.3 自底向上的时序动作定位算法 183
5.3.1 BSN 183
5.3.2 TSA-Net 187
5.3.3 BMN 191
5.4 对时序结构信息建模的算法 197
5.4.1 TAG 候选时序区间生成算法 198
5.4.2 SSN 网络结构 199
5.5 逐帧预测的算法 202
5.5.1 CDC层 203
5.5.2 CDC 网络结构 206
5.6 单阶段算法 208
5.6.1 SSAD 208
5.6.2 SS-TAD 212
5.6.3 GTAN 214
5.7 本章小结 217
第6章 视频Embedding 219
6.1 基于视频内容的无监督 Embedding 220
6.1.1 编码-解码网络 221
6.1.2 视频序列验证 222
6.1.3 视频和音频信息 224
6.1.4 视频和文本信息 225
6.2 Word2Vec 229
6.2.1 CBOW和Skip-Gram 229
6.2.2 分层 Softmax 234
6.2.3 负采样 239
6.3 Item2Vec 247
6.3.1 Item2Vec 基本形式 247
6.3.2 Item2Vec的改进 249
6.4 基于图的随机游走 252
6.4.1 DeepWalk 252
6.4.2 Node2Vec 254
6.5 结合一二阶相似度 257
6.5.1 LINE 258
6.5.2 SDNE 262
6.6 基于图的邻居结点 265
6.6.1 GCN 265
6.6.2 GraphSAGE 269
6.6.3 GAT 272
6.7 基于多种信息学习视频Embedding 274
6.7.1 召回模型 276
6.7.2 训练 278
6.8 本章小结 280
附录A 视频处理常用工具 281
A.1 FFmpeg 281
A.2 OpenCV 284
A.3 Decord 291
A.4 Lintel 294
参考文献 296
・ ・ ・ ・ ・ ・ (收起)第1 部分深度学习基础篇1
1 概述2
1.1 人工智能 3
1.1.1 人工智能的分类 3
1.1.2 人工智能发展史 3
1.2 机器学习 7
1.2.1 机器学习的由来 7
1.2.2 机器学习发展史 9
1.2.3 机器学习方法分类 10
1.2.4 机器学习中的基本概念 11
1.3 神经网络 12
1.3.1 神经网络发展史 13
参考文献 16
2 神经网络17
2.1 在神经科学中对生物神经元的研究 17
2.1.1 神经元激活机制 17
2.1.2 神经元的特点 18
2.2 神经元模型 19
2.2.1 线性神经元 19
2.2.2 线性阈值神经元 19
2.2.3 Sigmoid 神经元 21
2.2.4 Tanh 神经元 22
2.2.5 ReLU 22
2.2.6 Maxout 24
2.2.7 Softmax 24
2.2.8 小结 25
2.3 感知机 27
2.3.1 感知机的提出 27
2.3.2 感知机的困境 28
2.4 DNN 29
2.4.1 输入层、输出层及隐层 30
2.4.2 目标函数的选取 30
2.4.3 前向传播 32
2.4.4 后向传播 33
2.4.5 参数更新 35
2.4.6 神经网络的训练步骤 36
参考文献 36
3 初始化模型38
3.1 受限玻尔兹曼机 38
3.1.1 能量模型 39
3.1.2 带隐藏单元的能量模型 40
3.1.3 受限玻尔兹曼机基本原理 41
3.1.4 二值RBM 43
3.1.5 对比散度 45
3.2 自动编码器 47
3.2.1 稀疏自动编码器 48
3.2.2 降噪自动编码器 48
3.2.3 栈式自动编码器 49
3.3 深度信念网络 50
参考文献 52
4 卷积神经网络53
4.1 卷积算子 53
4.2 卷积的特征 56
4.3 卷积网络典型结构 59
4.3.1 基本网络结构 59
4.3.2 构成卷积神经网络的层 59
4.3.3 网络结构模式 60
4.4 卷积网络的层 61
4.4.1 卷积层 61
4.4.2 池化层 66
参考文献 67
5 循环神经网络68
5.1 循环神经网络简介 68
5.2 RNN、LSTM 和GRU 69
5.3 双向RNN 76
5.4 RNN 语言模型的简单实现 77
参考文献 80
6 深度学习优化算法81
6.1 SGD 81
6.2 Momentum 82
6.3 NAG 83
6.4 Adagrad 85
6.5 RMSProp 86
6.6 Adadelta 87
6.7 Adam 88
6.8 AdaMax 90
6.9 Nadam 90
6.10 关于优化算法的使用 92
参考文献 92
7 深度学习训练技巧94
7.1 数据预处理 94
7.2 权重初始化 95
7.3 正则化 96
7.3.1 提前终止 96
7.3.2 数据增强 96
7.3.3 L2/L1 参数正则化 98
7.3.4 集成 100
7.3.5 Dropout 101
参考文献 102
8 深度学习框架103
8.1 Theano 103
8.1.1 Theano 103
8.1.2 安装 104
8.1.3 计算图 104
8.2 Torch 105
8.2.1 概述 105
8.2.2 安装 106
8.2.3 核心结构 107
8.2.4 小试牛刀 110
8.3 PyTorch 113
8.3.1 概述 113
8.3.2 安装 113
8.3.3 核心结构 114
8.3.4 小试牛刀 114
8.4 Caffe 117
8.4.1 概述 117
8.4.2 安装 118
8.4.3 核心组件 119
8.4.4 小试牛刀 125
8.5 TensorFlow 125
8.5.1 概述 125
8.5.2 安装 126
8.5.3 核心结构 126
8.5.4 小试牛刀 127
8.6 MXNet 131
8.6.1 概述 131
8.6.2 安装 131
8.6.3 核心结构 132
8.6.4 小试牛刀 133
8.7 Keras 135
8.7.1 概述 135
8.7.2 安装 136
8.7.3 模块介绍 136
8.7.4 小试牛刀 136
参考文献 139
第2 部分计算机视觉篇140
9 计算机视觉背景141
9.1 传统计算机视觉 141
9.2 基于深度学习的计算机视觉 145
9.3 参考文献 146
10 图像分类模型147
10.1 LeNet-5 147
10.2 AlexNet 149
10.3 VGGNet 154
10.3.1 网络结构 155
10.3.2 配置 157
10.3.3 讨论 157
10.3.4 几组实验 158
10.4 GoogLeNet 159
10.4.1 NIN 161
10.4.2 GoogLeNet 的动机 161
10.4.3 网络结构细节 162
10.4.4 训练方法 164
10.4.5 后续改进版本 165
10.5 ResNet 165
10.5.1 基本思想 165
10.5.2 网络结构 167
10.6 DenseNet 169
10.7 DPN 170
参考文献 170
11 目标检测173
11.1 相关研究 175
11.1.1 选择性搜索 175
11.1.2 OverFeat 177
11.2 基于区域提名的方法 179
11.2.1 R-CNN 179
11.2.2 SPP-net 181
11.2.3 Fast R-CNN 182
11.2.4 Faster R-CNN 184
11.2.5 R-FCN 185
11.3 端到端的方法 186
11.3.1 YOLO 186
11.3.2 SSD 187
11.4 小结 188
参考文献 190
12 语义分割192
12.1 全卷积网络 193
12.1.1 FCN 193
12.1.2 DeconvNet 195
12.1.3 SegNet 197
12.1.4 DilatedConvNet 198
12.2 CRF/MRF 的使用 199
12.2.1 DeepLab 199
12.2.2 CRFasRNN 201
12.2.3 DPN 203
12.3 实例分割 205
12.3.1 Mask R-CNN 205
参考文献 206
13 图像检索的深度哈希编码208
13.1 传统哈希编码方法 208
13.2 CNNH 209
13.3 DSH 210
13.4 小结 212
参考文献 212
第3 部分语音识别篇214
14 传统语音识别基础215
14.1 语音识别简介 215
14.2 HMM 简介 216
14.2.1 HMM 是特殊的混合模型 218
14.2.2 转移概率矩阵 219
14.2.3 发射概率 220
14.2.4 Baum-Welch 算法 220
14.2.5 后验概率 224
14.2.6 前向-后向算法 224
14.3 HMM 梯度求解 227
14.3.1 梯度算法1 228
14.3.2 梯度算法2 230
14.3.3 梯度求解的重要性 234
14.4 孤立词识别 234
14.4.1 特征提取 234
14.4.2 孤立词建模 235
14.4.3 GMM-HMM 237
14.5 连续语音识别 240
14.6 Viterbi 解码 243
14.7 三音素状态聚类 245
14.8 判别式训练 248
参考文献 254
15 基于WFST 的语音解码256
15.1 有限状态机 257
15.2 WFST 及半环定义 257
15.2.1 WFST 257
15.2.2 半环（Semiring） 258
15.3 自动机操作 260
15.3.1 自动机基本操作 261
15.3.2 转换器基本操作 262
15.3.3 优化操作 265
15.4 基于WFST 的语音识别系统 277
15.4.1 声学模型WFST 279
15.4.2 三音素WFST 281
15.4.3 发音字典WFST 281
15.4.4 语言模型WFST 282
15.4.5 WFST 组合和优化 284
15.4.6 组合和优化实验 285
15.4.7 WFST 解码 286
参考文献 287
16 深度语音识别288
16.1 CD-DNN-HMM 288
16.2 TDNN 292
16.3 CTC 295
16.4 EESEN 299
16.5 Deep Speech 301
16.6 Chain 310
参考文献 313
17 CTC 解码315
17.1 序列标注 315
17.2 序列标注任务的解决办法 316
17.2.1 序列分类 316
17.2.2 分割分类 317
17.2.3 时序分类 318
17.3 隐马模型 318
17.4 CTC 基本定义 319
17.5 CTC 前向算法 321
17.6 CTC 后向算法 324
17.7 CTC 目标函数 325
17.8 CTC 解码基本原理 327
17.8.1 最大概率路径解码 327
17.8.2 前缀搜索解码 328
17.8.3 约束解码 329
参考文献 333
第4 部分自然语言处理篇334
18 自然语言处理简介335
18.1 NLP 的难点 335
18.2 NLP 的研究范围 336
19 词性标注338
19.1 传统词性标注模型 338
19.2 基于神经网络的词性标注模型 340
19.3 基于Bi-LSTM 的神经网络词性标注模型 342
参考文献 344
20 依存句法分析345
20.1 背景 346
20.2 SyntaxNet 技术要点 348
20.2.1 Transition-based 系统 349
20.2.2 “模板化” 技术 353
20.2.3 Beam Search 355
参考文献 357
21 word2vec 358
21.1 背景 359
21.1.1 词向量 359
21.1.2 统计语言模型 359
21.1.3 神经网络语言模型 362
21.1.4 Log-linear 模型 364
21.1.5 Log-bilinear 模型 365
21.1.6 层次化Log-bilinear 模型 365
21.2 CBOW 模型 366
21.3 Skip-gram 模型 369
21.4 Hierarchical Softmax 与Negative Sampling 371
21.5 fastText 372
21.6 GloVe 373
21.7 小结 374
参考文献 374
22 神经网络机器翻译376
22.1 机器翻译简介 376
22.2 神经网络机器翻译基本模型 377
22.3 基于Attention 的神经网络机器翻译 379
22.4 谷歌机器翻译系统GNMT 381
22.5 基于卷积的机器翻译 382
22.6 小结 383
参考文献 384
第5 部分深度学习研究篇385
23 Batch Normalization 386
23.1 前向与后向传播 387
23.1.1 前向传播 387
23.1.2 后向传播 390
23.2 有效性分析 393
23.2.1 内部协移 393
23.2.2 梯度流 393
23.3 使用与优化方法 395
23.4 小结 396
参考文献 396
24 Attention 397
24.1 从简单RNN 到RNN + Attention 398
24.2 Soft Attention 与Hard Attention 398
24.3 Attention 的应用 399
24.4 小结 401
参考文献 402
25 多任务学习403
25.1 背景 403
25.2 什么是多任务学习 404
25.3 多任务分类与其他分类概念的关系 406
25.3.1 二分类 406
25.3.2 多分类 407
25.3.3 多标签分类 407
25.3.4 相关关系 408
25.4 多任务学习如何发挥作用 409
25.4.1 提高泛化能力的潜在原因 410
25.4.2 多任务学习机制 410
25.4.3 后向传播多任务学习如何发现任务是相关的 412
25.5 多任务学习被广泛应用 413
25.5.1 使用未来预测现在 413
25.5.2 多种表示和度量 413
25.5.3 时间序列预测 413
25.5.4 使用不可操作特征 414
25.5.5 使用额外任务来聚焦 414
25.5.6 有序迁移 414
25.5.7 多个任务自然地出现 414
25.5.8 将输入变成输出 414
25.6 多任务深度学习应用 416
25.6.1 脸部特征点检测 416
25.6.2 DeepID2 418
25.6.3 Fast R-CNN 419
25.6.4 旋转人脸网络 420
25.6.5 实例感知语义分割的MNC 422
25.7 小结 423
参考文献 425
26 模型压缩426
26.1 模型压缩的必要性 426
26.2 较浅的网络 428
26.3 剪枝 428
26.4 参数共享 434
26.5 紧凑网络 437
26.6 二值网络 438
26.7 小结 442
参考文献 442
27 增强学习445
27.1 什么是增强学习 445
27.2 增强学习的数学表达形式 448
27.2.1 MDP 449
27.2.2 策略函数 450
27.2.3 奖励与回报 450
27.2.4 价值函数 452
27.2.5 贝尔曼方程 453
27.2.6 最优策略性质 453
27.3 用动态规划法求解增强学习问题 454
27.3.1 Agent 的目标 454
27.3.2 策略评估 455
27.3.3 策略改进 456
27.3.4 策略迭代 457
27.3.5 策略迭代的例子 458
27.3.6 价值迭代 459
27.3.7 价值迭代的例子 461
27.3.8 策略函数和价值函数的关系 462
27.4 无模型算法 462
27.4.1 蒙特卡罗法 463
27.4.2 时序差分法 465
27.4.3 Q-Learning 466
27.5 Q-Learning 的例子 467
27.6 AlphaGo 原理剖析 469
27.6.1 围棋与机器博弈 469
27.6.2 Alpha-Beta 树 472
27.6.3 MCTS 473
27.6.4 UCT 476
27.6.5 AlphaGo 的训练策略 478
27.6.6 AlphaGo 的招式搜索算法 482
27.6.7 围棋的对称性 484
27.7 AlphaGo Zero 484
参考文献 484
28 GAN 486
28.1 生成模型 486
28.2 生成对抗模型的概念 488
28.3 GAN 实战 492
28.4 InfoGAN――探寻隐变量的内涵 493
28.5 Image-Image Translation 496
28.6 WGAN（Wasserstein GAN） 499
28.6.1 GAN 目标函数的弱点 500
28.6.2 Wasserstein 度量的优势 501
28.6.3 WGAN 的目标函数 504
参考文献 505
A 本书涉及的开源资源列表 506
・ ・ ・ ・ ・ ・ (收起)1 深度学习简介1
1.1 人工智能、机器学习和深度学习 1
1.1.1 引言 1
1.1.2 人工智能、机器学习和深度学习三者的关系 2
1.2 神经网络 3
1.2.1 感知器 3
1.2.2 激活函数 5
1.2.3 损失函数 8
1.2.4 梯度下降和随机梯度下降 8
1.2.5 反向传播算法简述 11
1.2.6 其他神经网络 12
1.3 学习方法建议 13
1.3.1 网络资源 13
1.3.2 TensorFlow 官方深度学习教程 14
1.3.3 开源社区 15
1.4 TensorLayer 15
1.4.1 深度学习框架概况 15
1.4.2 TensorLayer 概括 16
1.4.3 实验环境配置 17
2 多层感知器19
2.1 McCulloch-Pitts 神经元模型 19
2.1.1 人工神经网络到底能干什么？到底在干什么 21
2.1.2 什么是激活函数？什么是偏值 22
2.2 感知器 23
2.2.1 什么是线性分类器 24
2.2.2 线性分类器有什么优缺点 26
2.2.3 感知器实例和异或问题（XOR 问题） 26
2.3 多层感知器 30
2.4 实现手写数字分类 32
2.5 过拟合 40
2.5.1 什么是过拟合 40
2.5.2 Dropout 41
2.5.3 批规范化 42
2.5.4 L1、L2 和其他正则化方法 42
2.5.5 Lp 正则化的图形化解释 44
2.6 再实现手写数字分类 46
2.6.1 数据迭代器 46
2.6.2 通过all_drop 启动与关闭Dropout 47
2.6.3 通过参数共享实现训练测试切换 50
3 自编码器54
3.1 稀疏性 54
3.2 稀疏自编码器 56
3.3 实现手写数字特征提取 59
3.4 降噪自编码器 65
3.5 再实现手写数字特征提取 68
3.6 堆栈式自编码器及其实现 72
4 卷积神经网络80
4.1 卷积原理 80
4.1.1 卷积操作 81
4.1.2 张量 84
4.1.3 卷积层 85
4.1.4 池化层 87
4.1.5 全连接层 89
4.2 经典任务 90
4.2.1 图像分类 90
4.2.2 目标检测 91
4.2.3 语义分割 94
4.2.4 实例分割 94
4.3 经典卷积网络 95
4.3.1 LeNet 95
4.3.2 AlexNet 96
4.3.3 VGGNet 96
4.3.4 GoogLeNet 98
4.3.5 ResNet 99
4.4 实现手写数字分类 100
4.5 数据增强与规范化 104
4.5.1 数据增强 104
4.5.2 批规范化 106
4.5.3 局部响应归一化 107
4.6 实现CIFAR10 分类 108
4.6.1 方法1：tl.prepro 做数据增强 108
4.6.2 方法2：TFRecord 做数据增强 114
4.7 反卷积神经网络 120
5 词的向量表达121
5.1 目的与原理 121
5.2 Word2Vec 124
5.2.1 简介 124
5.2.2 Continuous Bag-Of-Words（CBOW）模型 124
5.2.3 Skip Gram（SG）模型 129
5.2.4 Hierarchical Softmax 132
5.2.5 Negative Sampling 135
5.3 实现Word2Vec 136
5.3.1 简介 136
5.3.2 实现 136
5.4 重载预训练矩阵 144
6 递归神经网络148
6.1 为什么需要它 148
6.2 不同的RNNs 151
6.2.1 简单递归网络 151
6.2.2 回音网络 152
6.3 长短期记忆 153
6.3.1 LSTM 概括 153
6.3.2 LSTM 详解 157
6.3.3 LSTM 变种 159
6.4 实现生成句子 160
6.4.1 模型简介 160
6.4.2 数据迭代 163
6.4.3 损失函数和更新公式 164
6.4.4 生成句子及Top K 采样 167
6.4.5 接下来还可以做什么 169
7 深度增强学习171
7.1 增强学习 172
7.1.1 概述 172
7.1.2 基于价值的增强学习 173
7.1.3 基于策略的增强学习 176
7.1.4 基于模型的增强学习 177
7.2 深度增强学习 179
7.2.1 深度Q 学习 179
7.2.2 深度策略网络 181
7.3 更多参考资料 187
7.3.1 书籍 187
7.3.2 在线课程 187
8 生成对抗网络188
8.1 何为生成对抗网络 189
8.2 深度卷积对抗生成网络 190
8.3 实现人脸生成 191
8.4 还能做什么 198
9 高级实现技巧202
9.1 与其他框架对接 202
9.1.1 无参数层 203
9.1.2 有参数层 203
9.2 自定义层 204
9.2.1 无参数层 204
9.2.2 有参数层 205
9.3 建立词汇表 207
9.4 补零与序列长度 209
9.5 动态递归神经网络 210
9.6 实用小技巧 211
9.6.1 屏蔽显示 211
9.6.2 参数名字前缀 212
9.6.3 获取特定参数 213
9.6.4 获取特定层输出 213
10 实例一：使用预训练卷积网络214
10.1 高维特征表达 214
10.2 VGG 网络 215
10.3 连接TF-Slim 221
11 实例二：图像语义分割及其医学图像应用225
11.1 图像语义分割概述 225
11.1.1 传统图像分割算法简介 227
11.1.2 损失函数与评估指标 229
11.2 医学图像分割概述 230
11.3 全卷积神经网络和U-Net 网络结构 232
11.4 医学图像应用：实现脑部肿瘤分割 234
11.4.1 数据与数据增强 235
11.4.2 U-Net 网络 238
11.4.3 损失函数 239
11.4.4 开始训练 241
12 实例三：由文本生成图像244
12.1 条件生成对抗网络之GAN-CLS 245
12.2 实现句子生成花朵图片 246
13 实例四：超高分辨率复原260
13.1 什么是超高分辨率复原 260
13.2 网络结构 261
13.3 联合损失函数 264
13.4 训练网络 269
13.5 使用测试 277
14 实例五：文本反垃圾280
14.1 任务场景 280
14.2 网络结构 281
14.3 词的向量表示 282
14.4 Dynamic RNN 分类器 283
14.5 训练网络 284
14.5.1 训练词向量 284
14.5.2 文本的表示 290
14.5.3 训练分类器 291
14.5.4 模型导出 296
14.6 TensorFlow Serving 部署 299
14.7 客户端调用 301
14.8 其他常用方法 306
中英对照表及其缩写309
参考文献316
・ ・ ・ ・ ・ ・ (收起)目　　录
第　1章 深度学习简介　1
1．1　深度学习与人工智能　1
1．2　深度学习的历史渊源　2
1．2．1　从感知机到人工神经网络　3
1．2．2　深度学习时代　4
1．2．3　巨头之间的角逐　5
1．3　深度学习的影响因素　6
1．3．1　大数据　6
1．3．2　深度网络架构　7
1．3．3　GPU　11
1．4　深度学习为什么如此成功　11
1．4．1　特征学习（representation learning）　11
1．4．2　迁移学习（transfer learning）　12
1．5　小结　13
参考文献　14
第　2章 PyTorch简介　15
2．1　PyTorch安装　15
2．2　初识PyTorch　15
2．2．1　与Python的完美融合　16
2．2．2　张量计算　16
2．2．3　动态计算图　20
2．3　PyTorch实例：预测房价　27
2．3．1　准备数据　27
2．3．2　模型设计　28
2．3．3　训练　29
2．3．4　预测　31
2．3．5　术语汇总　32
2．4　小结　33
第3章　单车预测器：你的第 一个
神经网络　35
3．1　共享单车的烦恼　35
3．2　单车预测器1．0　37
3．2．1　神经网络简介　37
3．2．2　人工神经元　38
3．2．3　两个隐含层神经元　40
3．2．4　训练与运行　42
3．2．5　失败的神经预测器　43
3．2．6　过拟合　48
3．3　单车预测器2．0　49
3．3．1　数据的预处理过程　49
3．3．2　构建神经网络　52
3．3．3　测试神经网络　55
3．4　剖析神经网络Neu　57
3．5　小结　61
3．6　Q&A　61
第4章　机器也懂感情――中文情绪
分类器　63
4．1　神经网络分类器　64
4．1．1　如何用神经网络做分类　64
4．1．2　分类问题的损失函数　66
4．2　词袋模型分类器　67
4．2．1　词袋模型简介　68
4．2．2　搭建简单文本分类器　69
4．3　程序实现　70
4．3．1　数据获取　70
4．3．2　数据处理　74
4．3．3　文本数据向量化　75
4．3．4　划分数据集　76
4．3．5　建立神经网络　78
4．4　运行结果　80
4．5　剖析神经网络　81
4．6　小结　85
4．7　Q&A　85
第5章　手写数字识别器――认识卷积
神经网络　87
5．1　什么是卷积神经网络　88
5．1．1　手写数字识别任务的CNN
网络及运算过程　88
5．1．2　卷积运算操作　90
5．1．3　池化操作　96
5．1．4　立体卷积核　97
5．1．5　超参数与参数　98
5．1．6　其他说明　99
5．2　手写数字识别器　100
5．2．1　数据准备　100
5．2．2　构建网络　103
5．2．3　运行模型　105
5．2．4　测试模型　106
5．3　剖析卷积神经网络　107
5．3．1　第 一层卷积核与特征图　107
5．3．2　第二层卷积核与特征图　109
5．3．3　卷积神经网络的健壮性试验　110
5．4　小结　112
5．5　Q&A　112
5．6　扩展阅读　112
第6章　手写数字加法机――迁移学习　113
6．1　什么是迁移学习　114
6．1．1　迁移学习的由来　114
6．1．2　迁移学习的分类　115
6．1．3　迁移学习的意义　115
6．1．4　如何用神经网络实现迁移
学习　116
6．2　应用案例：迁移学习如何抗击贫困　118
6．2．1　背景介绍　118
6．2．2　方法探寻　119
6．2．3　迁移学习方法　120
6．3　蚂蚁还是蜜蜂：迁移大型卷积神经
网络　121
6．3．1　任务描述与初步尝试　121
6．3．2　ResNet与模型迁移　122
6．3．3　代码实现　123
6．3．4　结果分析　127
6．3．5　更多的模型与数据　128
6．4　手写数字加法机　128
6．4．1　网络架构　128
6．4．2　代码实现　129
6．4．3　训练与测试　136
6．4．4　结果　138
6．4．5　大规模实验　138
6．5　小结　143
6．6　实践项目：迁移与效率　143
第7章　你自己的Prisma――图像
风格迁移　145
7．1　什么是风格迁移　145
7．1．1　什么是风格　145
7．1．2　风格迁移的涵义　146
7．2　风格迁移技术发展简史　147
7．2．1　神经网络之前的风格迁移　147
7．2．2　特定风格的实现　148
7．3　神经网络风格迁移　149
7．3．1　神经网络风格迁移的优势　150
7．3．2　神经网络风格迁移的基本
思想　150
7．3．3　卷积神经网络的选取　151
7．3．4　内容损失　152
7．3．5　风格损失　152
7．3．6　风格损失原理分析　153
7．3．7　损失函数与优化　156
7．4　神经网络风格迁移实战　157
7．4．1　准备工作　157
7．4．2　建立风格迁移网络　159
7．4．3　风格迁移训练　162
7．5　小结　165
7．6　扩展阅读　165
第8章　人工智能造假术――图像生成
与对抗学习　166
8．1　反卷积与图像生成　169
8．1．1　CNN回顾　169
8．1．2　反卷积操作　171
8．1．3　反池化过程　173
8．1．4　反卷积与分数步伐　174
8．1．5　输出图像尺寸公式　175
8．1．6　批正则化技术　176
8．2　图像生成实验1――最小均方误差
模型　177
8．2．1　模型思路　177
8．2．2　代码实现　178
8．2．3　运行结果　182
8．3　图像生成实验2――生成器-识别器
模型　184
8．3．1　生成器-识别器模型的实现　184
8．3．2　对抗样本　187
8．4　图像生成实验3――生成对抗网络
GAN　190
8．4．1　GAN的总体架构　191
8．4．2　程序实现　192
8．4．3　结果展示　195
8．5　小结　197
8．6　Q&A　197
8．7　扩展阅读　198
第9章　词汇的星空――神经语言模型
与Word2Vec　199
9．1　词向量技术介绍　199
9．1．1　初识词向量　199
9．1．2　传统编码方式　200
9．2　NPLM：神经概率语言模型　201
9．2．1　NPLM的基本思想　202
9．2．2　NPLM的运作过程详解　202
9．2．3　读取NPLM中的词向量　205
9．2．4　NPLM的编码实现　206
9．2．5　运行结果　209
9．2．6　NPLM的总结与局限　211
9．3　Word2Vec　211
9．3．1　CBOW模型和Skip-gram模型的结构　211
9．3．2　层级软最大　213
9．3．3　负采样　213
9．3．4　总结及分析　214
9．4　Word2Vec的应用　214
9．4．1　在自己的语料库上训练Word2Vec词向量　214
9．4．2　调用现成的词向量　216
9．4．3　女人-男人＝皇后-国王　218
9．4．4　使用向量的空间位置进行词对词翻译　220
9．4．5　Word2Vec小结　221
9．5　小结　221
9．5　Q&A　222
第　10章 LSTM作曲机――序列生成
模型　224
10．1　序列生成问题　224
10．2　RNN与LSTM　225
10．2．1　RNN　226
10．2．2　LSTM　231
10．3　简单01序列的学习问题　235
10．3．1　RNN的序列学习　236
10．3．2　LSTM的序列学习　245
10．4　LSTM作曲机　248
10．4．1　MIDI文件　248
10．4．2　数据准备　249
10．4．3　模型结构　249
10．4．4　代码实现　250
10．5　小结　259
10．6　Q&A　259
10．7　扩展阅读　259
・ ・ ・ ・ ・ ・ (收起)第1章　编程和数学基础 1
1.1　Python快速入门 1
1.1.1　快速安装Python 1
1.1.2　Python基础 2
1.1.3　Python中的常见运算 5
1.1.4　Python控制语句 7
1.1.5　Python常用容器类型 10
1.1.6　Python常用函数 16
1.1.7　类和对象 22
1.1.8　Matplotlib入门 24
1.2　张量库NumPy 33
1.2.1　什么是张量 33
1.2.2　创建ndarray对象 37
1.2.3　ndarray数组的索引和切片 53
1.2.4　张量的计算 57
1.3　微积分 63
1.3.1　函数 64
1.3.2　四则运算和复合运算 66
1.3.3　极限和导数 69
1.3.4　导数的四则运算和链式法则 72
1.3.5　计算图、正向计算和反向传播求导 74
1.3.6　多变量函数的偏导数与梯度 75
1.3.7　向量值函数的导数与Jacobian矩阵 78
1.3.8　积分 83
1.4　概率基础 84
1.4.1　概率 84
1.4.2　条件概率、联合概率、全概率公式、贝叶斯公式 86
1.4.3　随机变量 88
1.4.4　离散型随机变量的概率分布 89
1.4.5　连续型随机变量的概率密度 91
1.4.6　随机变量的分布函数 93
1.4.7　期望、方差、协方差、协变矩阵 95
第2章　梯度下降法 99
2.1　函数极值的必要条件 99
2.2　梯度下降法基础 101
2.3　梯度下降法的参数优化策略 108
2.3.1　Momentum法 108
2.3.2　AdaGrad法 110
2.3.3　AdaDelta法 112
2.3.4　RMSprop法 114
2.3.5　Adam法 115
2.4　梯度验证 117
2.4.1　比较数值梯度和分析梯度 117
2.4.2　通用的数值梯度 118
2.5　分离梯度下降法与参数优化策略 119
2.5.1　参数优化器 119
2.5.2　接受参数优化器的梯度下降法 120
第3章　线性回归、逻辑回归和softmax回归 122
3.1　线性回归 122
3.1.1　餐车利润问题 122
3.1.2　机器学习与人工智能 123
3.1.3　什么是线性回归 126
3.1.4　用正规方程法求解线性回归问题 127
3.1.5　用梯度下降法求解线性回归问题 129
3.1.6　调试学习率 133
3.1.7　梯度验证 135
3.1.8　预测 135
3.1.9　多特征线性回归 136
3.2　数据的规范化 143
3.2.1　预测大坝出水量 143
3.2.2　数据的规范化过程 147
3.3　模型的评估 149
3.3.1　欠拟合和过拟合 149
3.3.2　验证集和测试集 153
3.3.3　学习曲线 155
3.3.4　偏差和方差 160
3.4　正则化 165
3.5　逻辑回归 168
3.5.1　逻辑回归基础 169
3.5.2　逻辑回归的NumPy实现 173
3.5.3　实战：鸢尾花分类的NumPy实现 178
3.6　softmax回归 180
3.6.1　spiral数据集 180
3.6.2　softmax函数 181
3.6.3　softmax回归模型 186
3.6.4　多分类交叉熵损失 188
3.6.5　通过加权和计算交叉熵损失 191
3.6.6　softmax回归的梯度计算 191
3.6.7　softmax回归的梯度下降法的实现 197
3.6.8　spiral数据集的softmax回归模型 197
3.7　批梯度下降法和随机梯度下降法 199
3.7.1　MNIST手写数字集 199
3.7.2　用部分训练样本训练逻辑回归模型 201
3.7.3　批梯度下降法 202
3.7.4　随机梯度下降法 207
第4章　神经网络 209
4.1　神经网络概述 209
4.1.1　感知机和神经元 209
4.1.2　激活函数 213
4.1.3　神经网络与深度学习 216
4.1.4　多个样本的正向计算 221
4.1.5　输出 224
4.1.6　损失函数 224
4.1.7　基于数值梯度的神经网络训练 229
4.2　反向求导 235
4.2.1　正向计算和反向求导 235
4.2.2　计算图 237
4.2.3　损失函数关于输出的梯度 239
4.2.4　2层神经网络的反向求导 242
4.2.5　2层神经网络的Python实现 247
4.2.6　任意层神经网络的反向求导 252
4.3　实现一个简单的深度学习框架 256
4.3.1　神经网络的训练过程 256
4.3.2　网络层的代码实现 257
4.3.3　网络层的梯度检验 260
4.3.4　神经网络的类 261
4.3.5　神经网络的梯度检验 263
4.3.6　基于深度学习框架的MNIST手写数字识别 266
4.3.7　改进的通用神经网络框架：分离加权和与激活函数 268
4.3.8　独立的参数优化器 276
4.3.9　fashion-mnist的分类训练 279
4.3.10　读写模型参数 282
第5章　改进神经网络性能的基本技巧 285
5.1　数据处理 285
5.1.1　数据增强 285
5.1.2　规范化 289
5.1.3　特征工程 289
5.2　参数调试 296
5.2.1　权重初始化 296
5.2.2　优化参数 301
5.3　批规范化 301
5.3.1　什么是批规范化 301
5.3.2　批规范化的反向求导 303
5.3.3　批规范化的代码实现 304
5.4　正则化 310
5.4.1　权重正则化 310
5.4.2　Dropout 312
5.4.3　早停法 316
5.5　梯度爆炸和梯度消失 317
第6章　卷积神经网络 318
6.1　卷积入门 319
6.1.1　什么是卷积 319
6.1.2　一维卷积 325
6.1.3　二维卷积 326
6.1.4　多通道输入和多通道输出 338
6.1.5　池化 341
6.2　卷积神经网络概述 344
6.2.1　全连接神经元和卷积神经元 345
6.2.2　卷积层和卷积神经网络 346
6.2.3　卷积层和池化层的反向求导及代码实现 349
6.2.4　卷积神经网络的代码实现 361
6.3　卷积的矩阵乘法 364
6.3.1　一维卷积的矩阵乘法 364
6.3.2　二维卷积的矩阵乘法 365
6.3.3　一维卷积反向求导的矩阵乘法 371
6.3.4　二维卷积反向求导的矩阵乘法 373
6.4　基于坐标索引的快速卷积 377
6.5　典型卷积神经网络结构 393
6.5.1　LeNet-5 393
6.5.2　AlexNet 394
6.5.3　VGG 395
6.5.4　残差网络 396
6.5.5　Inception网络 398
6.5.6　NiN 399
第7章　循环神经网络 403
7.1　序列问题和模型 403
7.1.1　股票价格预测问题 404
7.1.2　概率序列模型和语言模型 405
7.1.3　自回归模型 406
7.1.4　生成自回归数据 406
7.1.5　时间窗方法 408
7.1.6　时间窗采样 409
7.1.7　时间窗方法的建模和训练 409
7.1.8　长期预测和短期预测 410
7.1.9　股票价格预测的代码实现 412
7.1.10　k-gram语言模型 415
7.2　循环神经网络基础 416
7.2.1　无记忆功能的非循环神经网络 417
7.2.2　具有记忆功能的循环神经网络 418
7.3　穿过时间的反向传播 421
7.4　单层循环神经网络的实现 425
7.4.1　初始化模型参数 425
7.4.2　正向计算 425
7.4.3　损失函数 427
7.4.4　反向求导 427
7.4.5　梯度验证 429
7.4.6　梯度下降训练 432
7.4.7　序列数据的采样 433
7.4.8　序列数据的循环神经网络训练和预测 441
7.5　循环神经网络语言模型和文本的生成 448
7.5.1　字符表 448
7.5.2　字符序列样本的采样 450
7.5.3　模型的训练和预测 452
7.6　循环神经网络中的梯度爆炸和梯度消失 455
7.7　长短期记忆网络 456
7.7.1　LSTM的神经元 457
7.7.2　LSTM的反向求导 460
7.7.3　LSTM的代码实现 461
7.7.4　LSTM的变种 469
7.8　门控循环单元 470
7.8.1　门控循环单元的工作原理 470
7.8.2　门控循环单元的代码实现 472
7.9　循环神经网络的类及其实现 475
7.9.1　用类实现循环神经网络 475
7.9.2　循环神经网络单元的类实现 483
7.10　多层循环神经网络和双向循环神经网络 491
7.10.1　多层循环神经网络 491
7.10.2　多层循环神经网络的训练和预测 497
7.10.3　双向循环神经网络 500
7.11　Seq2Seq模型 506
7.11.1　机器翻译概述 507
7.11.2　Seq2Seq模型的实现 508
7.11.3　字符级的Seq2Seq模型 516
7.11.4　基于Word2Vec的Seq2Seq模型 522
7.11.5　基于词嵌入层的Seq2Seq模型 533
7.11.6　注意力机制 541
第8章　生成模型 552
8.1　生成模型概述 552
8.2　自动编码器 556
8.2.1　什么是自动编码器 557
8.2.2　稀疏编码器 559
8.2.3　自动编码器的代码实现 560
8.3　变分自动编码器 563
8.3.1　什么是变分自动编码器 563
8.3.2　变分自动编码器的损失函数 564
8.3.3　变分自动编码器的参数重采样 565
8.3.4　变分自动编码器的反向求导 565
8.3.5　变分自动编码器的代码实现 566
8.4　生成对抗网络 571
8.4.1　生成对抗网络的原理 573
8.4.2　生成对抗网络训练过程的代码实现 577
8.5　生成对抗网络建模实例 579
8.5.1　一组实数的生成对抗网络建模 579
8.5.2　二维坐标点的生成对抗网络建模 585
8.5.3　MNIST手写数字集的生成对抗网络建模 590
8.5.4　生成对抗网络的训练技巧 594
8.6　生成对抗网络的损失函数及其概率解释 594
8.6.1　生成对抗网络的损失函数的全局最优解 594
8.6.2　Kullback-Leibler散度和Jensen-Shannon散度 595
8.6.3　生成对抗网络的最大似然解释 598
8.7　改进的损失函数――Wasserstein GAN 599
8.7.1　Wasserstein GAN的原理 599
8.7.2　Wasserstein GAN的代码实现 603
8.8　深度卷积对抗网络 605
8.8.1　一维转置卷积 606
8.8.2　二维转置卷积 609
8.8.3　卷积对抗网络的代码实现 612
参考文献 617
・ ・ ・ ・ ・ ・ (收起)第一部分　元编程基础技术
第1章 基本技巧　3
1.1　元函数与type_traits　3
1.1.1　元函数介绍　3
1.1.2　类型元函数　4
1.1.3　各式各样的元函数　6
1.1.4　type_traits　7
1.1.5　元函数与宏　7
1.1.6　本书中元函数的命名方式　8
1.2　模板型模板参数与容器模板　8
1.2.1　模板作为元函数的输入　9
1.2.2　模板作为元函数的输出　9
1.2.3　容器模板　10
1.3　顺序、分支与循环代码的编写　12
1.3.1　顺序执行的代码　12
1.3.2　分支执行的代码　13
1.3.3　循环执行的代码　19
1.3.4　小心：实例化爆炸与编译崩溃　21
1.3.5　分支选择与短路逻辑　23
1.4　奇特的递归模板式　24
1.5　小结　25
1.6　练习　26
第2章 异类词典与policy模板　28
2.1　具名参数简介　28
2.2　异类词典　30
2.2.1　模块的使用方式　30
2.2.2　键的表示　32
2.2.3　异类词典的实现　34
2.2.4　VarTypeDict的性能简析　41
2.2.5　用std::tuple作为缓存　41
2.3　policy模板　42
2.3.1　policy介绍　42
2.3.2　定义policy与policy对象（模板）　45
2.3.3　使用policy　47
2.3.4　背景知识：支配与虚继承　49
2.3.5　policy对象与policy支配结构　50
2.3.6　policy选择元函数　52
2.3.7　使用宏简化policy对象的声明　57
2.4　小结　58
2.5　练习　58
第二部分 深度学习框架
第3章　深度学习概述　63
3.1　深度学习简介　63
3.1.1　从机器学习到深度学习　64
3.1.2　各式各样的人工神经网络　65
3.1.3　深度学习系统的组织与训练　68
3.2　本书所实现的框架：MetaNN　70
3.2.1　从矩阵计算工具到深度学习框架　70
3.2.2　MetaNN介绍　71
3.2.3　本书将要讨论的内容　72
3.2.4　本书不会涉及的主题　75
3.3　小结　75
第4章　类型体系与基本数据类型　76
4.1　类型体系　77
4.1.1　类型体系介绍　77
4.1.2　迭代器分类体系　78
4.1.3　将标签作为模板参数　80
4.1.4　MetaNN的类型体系　81
4.1.5　与类型体系相关的元函数　82
4.2　设计理念　84
4.2.1　支持不同的计算设备与计算单元　84
4.2.2　存储空间的分配与维护　85
4.2.3　浅拷贝与写操作检测　88
4.2.4　底层接口扩展　89
4.2.5　类型转换与求值　91
4.2.6　数据接口规范　92
4.3　标量　92
4.3.1　类模板的声明　93
4.3.2　基于CPU的特化版本　94
4.3.3　标量的主体类型　95
4.4　矩阵　96
4.4.1　Matrix类模板　96
4.4.2　特殊矩阵：平凡矩阵、全零矩阵与独热向量　101
4.4.3　引入新的矩阵类　104
4.5　列表　105
4.5.1　Batch模板　105
4.5.2　Array模板　108
4.5.3　重复与Duplicate模板　113
4.6　小结　116
4.7　练习　116
第5章　运算与表达式模板　119
5.1　表达式模板简介　119
5.2　MetaNN运算模板的设计思想　122
5.2.1　Add模板的问题　122
5.2.2　运算模板的行为分析　122
5.3　运算分类　124
5.4　辅助模板　125
5.4.1　辅助类模板OperElementType_/OperDeviceType_　125
5.4.2　辅助类模板OperXXX_　126
5.4.3　辅助类模板OperCateCal　126
5.4.4　辅助类模板OperOrganizer　128
5.4.5　辅助类模板OperSeq　130
5.5　运算模板的框架　131
5.5.1　运算模板的类别标签　131
5.5.2　UnaryOp的定义　132
5.6　运算实现示例　133
5.6.1　Sigmoid运算　133
5.6.2　Add运算　136
5.6.3　转置运算　139
5.6.4　折叠运算　141
5.7　MetaNN已支持的运算列表　141
5.7.1　一元运算　141
5.7.2　二元运算　142
5.7.3　三元运算　144
5.8　运算的折衷与局限性　144
5.8.1　运算的折衷　144
5.8.2　运算的局限性　145
5.9　小结　146
5.10　练习　146
第6章　基本层　148
6.1　层的设计理念　148
6.1.1　层的介绍　148
6.1.2　层对象的构造　150
6.1.3　参数矩阵的初始化与加载　151
6.1.4　正向传播　152
6.1.5　存储中间结果　154
6.1.6　反向传播　154
6.1.7　参数矩阵的更新　155
6.1.8　参数矩阵的获取　155
6.1.9　层的中性检测　156
6.2　层的辅助逻辑　156
6.2.1　初始化模块　156
6.2.2　DynamicData类模板　161
6.2.3　层的常用policy对象　166
6.2.4　InjectPolicy元函数　168
6.2.5　通用I/O结构　168
6.2.6　通用操作函数　169
6.3　层的具体实现　170
6.3.1　AddLayer　170
6.3.2　ElementMulLayer　172
6.3.3　BiasLayer　176
6.4　MetaNN已实现的基本层　181
6.5　小结　183
6.6　练习　184
第7章　复合层与循环层　185
7.1　复合层的接口与设计理念　186
7.1.1　基本结构　186
7.1.2　结构描述语法　187
7.1.3　policy的继承关系　188
7.1.4　policy的修正　189
7.1.5　复合层的构造函数　190
7.1.6　一个完整的复合层构造示例　190
7.2　policy继承与修正逻辑的实现　191
7.2.1　policy继承逻辑的实现　191
7.2.2　policy修正逻辑的实现　194
7.3　ComposeTopology的实现　195
7.3.1　功能介绍　195
7.3.2　拓扑排序算法介绍　195
7.3.3　ComposeTopology包含的主要步骤　196
7.3.4　结构描述子句与其划分　196
7.3.5　结构合法性检查　198
7.3.6　拓扑排序的实现　200
7.3.7　子层实例化元函数　203
7.4　ComposeKernel的实现　207
7.4.1　类模板的声明　208
7.4.2　子层对象管理　208
7.4.3　参数获取、梯度收集与中性检测　211
7.4.4　参数初始化与加载　212
7.4.5　正向传播　214
7.4.6　反向传播　221
7.5　复合层实现示例　221
7.6　循环层　222
7.6.1　GruStep　222
7.6.2　构建RecurrentLayer类模板　224
7.6.3　RecurrentLayer的使用　230
7.7　小结　230
7.8　练习　230
第8章　求值与优化　233
8.1　MetaNN的求值模型　234
8.1.1　运算的层次结构　234
8.1.2　求值子系统的模块划分　235
8.2　基本求值逻辑　242
8.2.1　主体类型的求值接口　242
8.2.2　非主体基本数据类型的求值　243
8.2.3　运算模板的求值　245
8.2.4　DyanmicData与求值　248
8.3　求值过程的优化　249
8.3.1　避免重复计算　249
8.3.2　同类计算合并　250
8.3.3　多运算协同优化　251
8.4　小结　258
8.5　练习　259
后记―方家休见笑，吾道本艰难　260
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序
前言
第1章　打造深度学习工具箱1
1.1　TensorFlow1
1.1.1　安装1
1.1.2　使用举例3
1.2　TFLearn3
1.3　PaddlePaddle4
1.3.1　安装5
1.3.2　使用举例6
1.4　Karas7
1.5　本章小结9
第2章　卷积神经网络10
2.1　传统的图像分类算法10
2.2　基于CNN的图像分类算法11
2.2.1　局部连接11
2.2.2　参数共享13
2.2.3　池化15
2.2.4　典型的CNN结构及实现16
2.2.5　AlexNet的结构及实现19
2.2.6　VGG的结构及实现24
2.3　基于CNN的文本处理29
2.3.1　典型的CNN结构30
2.3.2　典型的CNN代码实现30
2.4　本章小结32
第3章　循环神经网络33
3.1　循环神经算法概述34
3.2　单向循环神经网络结构与实现36
3.3　双向循环神经网络结构与实现38
3.4　循环神经网络在序列分类的应用41
3.5　循环神经网络在序列生成的应用42
3.6　循环神经网络在序列标记的应用43
3.7　循环神经网络在序列翻译的应用44
3.8　本章小结46
第4章　基于OpenSOC的机器学习框架47
4.1　OpenSOC框架47
4.2　数据源系统48
4.3　数据收集层53
4.4　消息系统层57
4.5　实时处理层60
4.6　存储层62
4.6.1　HDFS62
4.6.2　HBase64
4.6.3　Elasticsearch65
4.7　分析处理层66
4.8　计算系统67
4.9　实战演练72
4.10　本章小结77
第5章　验证码识别78
5.1　数据集79
5.2　特征提取80
5.3　模型训练与验证81
5.3.1　K近邻算法81
5.3.2　支持向量机算法81
5.3.3　深度学习算法之MLP82
5.3.4　深度学习算法之CNN83
5.4　本章小结87
第6章　垃圾邮件识别88
6.1　数据集89
6.2　特征提取90
6.2.1　词袋模型90
6.2.2　TF-IDF模型93
6.2.3　词汇表模型95
6.3　模型训练与验证97
6.3.1　朴素贝叶斯算法97
6.3.2　支持向量机算法100
6.3.3　深度学习算法之MLP101
6.3.4　深度学习算法之CNN102
6.3.5　深度学习算法之RNN106
6.4　本章小结108
第7章　负面评论识别109
7.1　数据集110
7.2　特征提取112
7.2.1　词袋和TF-IDF模型112
7.2.2　词汇表模型114
7.2.3　Word2Vec模型和Doc2Vec模型115
7.3　模型训练与验证119
7.3.1　朴素贝叶斯算法119
7.3.2　支持向量机算法122
7.3.3　深度学习算法之MLP123
7.3.4　深度学习算法之CNN124
7.4　本章小结127
第8章　骚扰短信识别128
8.1　数据集129
8.2　特征提取130
8.2.1　词袋和TF-IDF模型130
8.2.2　词汇表模型131
8.2.3　Word2Vec模型和Doc2Vec模型132
8.3　模型训练与验证134
8.3.1　朴素贝叶斯算法134
8.3.2　支持向量机算法136
8.3.3　XGBoost算法137
8.3.4　深度学习算法之MLP140
8.4　本章小结141
第9章　Linux后门检测142
9.1　数据集142
9.2　特征提取144
9.3　模型训练与验证145
9.3.1　朴素贝叶斯算法145
9.3.2　XGBoost算法146
9.3.3　深度学习算法之多层感知机148
9.4　本章小结149
第10章　用户行为分析与恶意行为检测150
10.1　数据集151
10.2　特征提取152
10.2.1　词袋和TF-IDF模型152
10.2.2　词袋和N-Gram模型154
10.2.3　词汇表模型155
10.3　模型训练与验证156
10.3.1　朴素贝叶斯算法156
10.3.2　XGBoost算法157
10.3.3　隐式马尔可夫算法159
10.3.4　深度学习算法之MLP164
10.4　本章小结166
第11章　WebShell检测167
11.1　数据集168
11.1.1　WordPress168
11.1.2　PHPCMS170
11.1.3　phpMyAdmin170
11.1.4　Smarty171
11.1.5　Yii171
11.2　特征提取172
11.2.1　词袋和TF-IDF模型172
11.2.2　opcode和N-Gram模型174
11.2.3　opcode调用序列模型180
11.3　模型训练与验证181
11.3.1　朴素贝叶斯算法181
11.3.2　深度学习算法之MLP182
11.3.3　深度学习算法之CNN184
11.4　本章小结188
第12章　智能扫描器189
12.1　自动生成XSS攻击载荷190
12.1.1　数据集190
12.1.2　特征提取194
12.1.3　模型训练与验证195
12.2　自动识别登录界面198
12.2.1　数据集198
12.2.2　特征提取199
12.2.3　模型训练与验证201
12.3　本章小结203
第13章　DGA域名识别204
13.1　数据集206
13.2　特征提取207
13.2.1　N-Gram模型207
13.2.2　统计特征模型208
13.2.3　字符序列模型210
13.3　模型训练与验证210
13.3.1　朴素贝叶斯算法210
13.3.2　XGBoost算法212
13.3.3　深度学习算法之多层感知机215
13.3.4　深度学习算法之RNN218
13.4　本章小结221
第14章　恶意程序分类识别222
14.1　数据集223
14.2　特征提取226
14.3　模型训练与验证228
14.3.1　支持向量机算法228
14.3.2　XGBoost算法229
14.3.3　深度学习算法之多层感知机230
14.4　本章小结231
第15章　反信用卡欺诈232
15.1　数据集232
15.2　特征提取234
15.2.1　标准化234
15.2.2　标准化和降采样234
15.2.3　标准化和过采样236
15.3　模型训练与验证239
15.3.1　朴素贝叶斯算法239
15.3.2　XGBoost算法243
15.3.3　深度学习算法之多层感知机247
15.4　本章小结251
・ ・ ・ ・ ・ ・ (收起)第1章 数据科学概述 1
1.1　挑战　2
1.1.1　工程实现的挑战　2
1.1.2　模型搭建的挑战　3
1.2　机器学习　5
1.2.1　机器学习与传统编程　5
1.2.2　监督式学习和非监督式学习　8
1.3　统计模型　8
1.4　关于本书　10
第2章 Python安装指南与简介：告别空谈　12
2.1　Python简介　13
2.1.1　什么是Python　15
2.1.2　Python在数据科学中的地位　16
2.1.3　不可能绕过的第三方库　17
2.2　Python安装　17
2.2.1　Windows下的安装　18
2.2.2　Mac下的安装　21
2.2.3　Linux下的安装　24
2.3　Python上手实践　26
2.3.1　Python shell　26
2.3.2　第 一个Python程序：Word Count　28
2.3.3　Python编程基础　30
2.3.4　Python的工程结构　34
2.4　本章小结　35
第3章　数学基础：恼人但又不可或缺的知识　36
3.1　矩阵和向量空间　37
3.1.1　标量、向量与矩阵　37
3.1.2　特殊矩阵　39
3.1.3　矩阵运算　39
3.1.4　代码实现　42
3.1.5　向量空间　44
3.2　概率：量化随机　46
3.2.1　定义概率：事件和概率空间　47
3.2.2　条件概率：信息的价值　48
3.2.3　随机变量：两种不同的随机　50
3.2.4　正态分布：殊途同归　52
3.2.5　P-value：自信的猜测　53
3.3　微积分　55
3.3.1　导数和积分：位置、速度　55
3.3.2　极限：变化的终点　57
3.3.3　复合函数：链式法则　58
3.3.4　多元函数：偏导数　59
3.3.5　极值与最值：最优选择　59
3.4　本章小结　61
第4章　线性回归：模型之母　62
4.1　一个简单的例子　64
4.1.1　从机器学习的角度看这个问题　66
4.1.2　从统计学的角度看这个问题　69
4.2　上手实践：模型实现　73
4.2.1　机器学习代码实现　74
4.2.2　统计方法代码实现　77
4.3　模型陷阱　82
4.3.1　过度拟合：模型越复杂越好吗　84
4.3.2　模型幻觉之统计学方案：假设检验　87
4.3.3　模型幻觉之机器学习方案：惩罚项　89
4.3.4　比较两种方案　92
4.4　模型持久化　92
4.4.1　模型的生命周期　93
4.4.2　保存模型　93
4.5　本章小结　96
第5章　逻辑回归：隐藏因子　97
5.1　二元分类问题：是与否　98
5.1.1　线性回归：为何失效　98
5.1.2　窗口效应：看不见的才是关键　100
5.1.3　逻辑分布：胜者生存　102
5.1.4　参数估计之似然函数：统计学角度　104
5.1.5　参数估计之损失函数：机器学习角度　104
5.1.6　参数估计之最终预测：从概率到选择　106
5.1.7　空间变换：非线性到线性　106
5.2　上手实践：模型实现　108
5.2.1　初步分析数据：直观印象　108
5.2.2　搭建模型　113
5.2.3　理解模型结果　116
5.3　评估模型效果：孰优孰劣　118
5.3.1　查准率与查全率　119
5.3.2　ROC曲线与AUC　123
5.4　多元分类问题：超越是与否　127
5.4.1　多元逻辑回归：逻辑分布的威力　128
5.4.2　One-vs.-all：从二元到多元　129
5.4.3　模型实现　130
5.5　非均衡数据集　132
5.5.1　准确度悖论　132
5.5.2　一个例子　133
5.5.3　解决方法　135
5.6　本章小结　136
第6章　工程实现：计算机是怎么算的　138
6.1　算法思路：模拟滚动　139
6.2　数值求解：梯度下降法　141
6.3　上手实践：代码实现　142
6.3.1　TensorFlow基础　143
6.3.2　定义模型　148
6.3.3　梯度下降　149
6.3.4　分析运行细节　150
6.4　更优化的算法：随机梯度下降法　153
6.4.1　算法细节　153
6.4.2　代码实现　154
6.4.3　两种算法比较　156
6.5　本章小结　158
第7章　计量经济学的启示：他山之石　159
7.1　定量与定性：变量的数学运算合理吗　161
7.2　定性变量的处理　162
7.2.1　虚拟变量　162
7.2.2　上手实践：代码实现　164
7.2.3　从定性变量到定量变量　168
7.3　定量变量的处理　170
7.3.1　定量变量转换为定性变量　171
7.3.2　上手实践：代码实现　171
7.3.3　基于卡方检验的方法　173
7.4　显著性　175
7.5　多重共线性：多变量的烦恼　176
7.5.1　多重共线性效应　176
7.5.2　检测多重共线性　180
7.5.3　解决方法　185
7.5.4　虚拟变量陷阱　188
7.6　内生性：变化来自何处　191
7.6.1　来源　192
7.6.2　内生性效应　193
7.6.3　工具变量　195
7.6.4　逻辑回归的内生性　198
7.6.5　模型的联结　200
7.7　本章小结　201
第8章　监督式学习： 目标明确　202
8.1　支持向量学习机　203
8.1.1　直观例子　204
8.1.2　用数学理解直观　205
8.1.3　从几何直观到最优化问题　207
8.1.4　损失项　209
8.1.5　损失函数与惩罚项　210
8.1.6　Hard margin 与soft margin比较　211
8.1.7　支持向量学习机与逻辑回归：隐藏的假设　213
8.2　核函数　216
8.2.1　空间变换：从非线性到线性　216
8.2.2　拉格朗日对偶　218
8.2.3　支持向量　220
8.2.4　核函数的定义：优化运算　221
8.2.5　常用的核函数　222
8.2.6　Scale variant　225
8.3　决策树　227
8.3.1　决策规则　227
8.3.2　评判标准　229
8.3.3　代码实现　231
8.3.4　决策树预测算法以及模型的联结　231
8.3.5　剪枝　235
8.4　树的集成　238
8.4.1　随机森林　238
8.4.2　Random forest embedding　239
8.4.3　GBTs之梯度提升　241
8.4.4　GBTs之算法细节　242
8.5　本章小结　244
第9章　生成式模型：量化信息的价值　246
9.1　贝叶斯框架　248
9.1.1　蒙提霍尔问题　248
9.1.2　条件概率　249
9.1.3　先验概率与后验概率　251
9.1.4　参数估计与预测公式　251
9.1.5　贝叶斯学派与频率学派　252
9.2　朴素贝叶斯　254
9.2.1　特征提取：文字到数字　254
9.2.2　伯努利模型　256
9.2.3　多项式模型　258
9.2.4　TF-IDF　259
9.2.5　文本分类的代码实现　260
9.2.6　模型的联结　265
9.3　判别分析　266
9.3.1　线性判别分析　267
9.3.2　线性判别分析与逻辑回归比较　269
9.3.3　数据降维　270
9.3.4　代码实现　273
9.3.5　二次判别分析　275
9.4　隐马尔可夫模型　276
9.4.1　一个简单的例子　276
9.4.2　马尔可夫链　278
9.4.3　模型架构　279
9.4.4　中文分词：监督式学习　280
9.4.5　中文分词之代码实现　282
9.4.6　股票市场：非监督式学习　284
9.4.7　股票市场之代码实现　286
9.5　本章小结　289
第10章 非监督式学习：聚类与降维　290
10.1　K-means　292
10.1.1　模型原理　292
10.1.2　收敛过程　293
10.1.3　如何选择聚类个数　295
10.1.4　应用示例　297
10.2　其他聚类模型　298
10.2.1　混合高斯之模型原理　299
10.2.2　混合高斯之模型实现　300
10.2.3　谱聚类之聚类结果　303
10.2.4　谱聚类之模型原理　304
10.2.5　谱聚类之图片分割　307
10.3　Pipeline　308
10.4　主成分分析　309
10.4.1　模型原理　310
10.4.2　模型实现　312
10.4.3　核函数　313
10.4.4　Kernel PCA的数学原理　315
10.4.5　应用示例　316
10.5　奇异值分解　317
10.5.1　定义　317
10.5.2　截断奇异值分解　317
10.5.3　潜在语义分析　318
10.5.4　大型推荐系统　320
10.6　本章小结　323
第11章 分布式机器学习：集体力量　325
11.1　Spark简介　327
11.1.1　Spark安装　328
11.1.2　从MapReduce到Spark　333
11.1.3　运行Spark　335
11.1.4　Spark DataFrame　336
11.1.5　Spark的运行架构　339
11.2　最优化问题的分布式解法　341
11.2.1　分布式机器学习的原理　341
11.2.2　一个简单的例子　342
11.3　大数据模型的两个维度　344
11.3.1　数据量维度　344
11.3.2　模型数量维度　346
11.4　开源工具的另一面　348
11.4.1　一个简单的例子　349
11.4.2　开源工具的阿喀琉斯之踵　351
11.5　本章小结　351
第12章 神经网络：模拟人的大脑　353
12.1　神经元　355
12.1.1　神经元模型　355
12.1.2　Sigmoid神经元与二元逻辑回归　356
12.1.3　Softmax函数与多元逻辑回归　358
12.2　神经网络　360
12.2.1　图形表示　360
12.2.2　数学基础　361
12.2.3　分类例子　363
12.2.4　代码实现　365
12.2.5　模型的联结　369
12.3　反向传播算法　370
12.3.1　随机梯度下降法回顾　370
12.3.2　数学推导　371
12.3.3　算法步骤　373
12.4　提高神经网络的学习效率　373
12.4.1　学习的原理　373
12.4.2　激活函数的改进　375
12.4.3　参数初始化　378
12.4.4　不稳定的梯度　380
12.5　本章小结　381
第13章 深度学习：继续探索　383
13.1　利用神经网络识别数字　384
13.1.1　搭建模型　384
13.1.2　防止过拟合之惩罚项　386
13.1.3　防止过拟合之dropout　387
13.1.4　代码实现　389
13.2　卷积神经网络　394
13.2.1　模型结构之卷积层　395
13.2.2　模型结构之池化层　397
13.2.3　模型结构之完整结构　399
13.2.4　代码实现　400
13.2.5　结构真的那么重要吗　405
13.3　其他深度学习模型　406
13.3.1　递归神经网络　406
13.3.2　长短期记忆　407
13.3.3　非监督式学习　409
13.4　本章小结　411
・ ・ ・ ・ ・ ・ (收起)基础篇
第1章 深度学习概述 2
1.1 深度学习发展简史 2
1.2 有监督学习 4
1.2.1 图像分类 4
1.2.2 目标检测 6
1.2.3 人脸识别 10
1.2.4 语音识别 13
1.3 无监督学习 18
1.3.1 无监督学习概述 18
1.3.2 生成对抗网络 18
1.4 强化学习 21
1.4.1 AlphaGo 21
1.4.2 AlphaGo Zero 23
1.5 小结 25
参考文献 25
第2章 深度神经网络 27
2.1 神经元 27
2.2 感知机 30
2.3 前向传递 32
2.3.1 前向传递的流程 32
2.3.2 激活函数 33
2.3.3 损失函数 37
2.4 后向传递 40
2.4.1 后向传递的流程 40
2.4.2 梯度下降 40
2.4.3 参数修正 42
2.5 防止过拟合 44
2.5.1 dropout 44
2.5.2 正则化 45
2.6 小结 46
第3章 卷积神经网络 47
3.1 卷积层 48
3.1.1 valid 卷积 48
3.1.2 full 卷积 50
3.1.3 same 卷积 51
3.2 池化层 52
3.3 反卷积 53
3.4 感受野 55
3.5 卷积网络实例 56
3.5.1 Lenet-5 56
3.5.2 AlexNet 59
3.5.3 VGGNet 62
3.5.4 GoogLeNet 64
3.5.5 ResNet 72
3.5.6 MobileNet 73
3.6 小结 76
进阶篇
第4章 两阶段目标检测方法 78
4.1 R-CNN 78
4.1.1 算法流程 79
4.1.2 训练过程 80
4.2 SPP-Net 83
4.2.1 网络结构 84
4.2.2 空间金字塔池化 84
4.3 Fast R-CNN 86
4.3.1 感兴趣区域池化层 86
4.3.2 网络结构 88
4.3.3 全连接层计算加速 89
4.3.4 目标分类 90
4.3.5 边界框回归 91
4.3.6 训练过程 93
4.4 Faster R-CNN 96
4.4.1 网络结构 97
4.4.2 RPN 98
4.4.3 训练过程 104
4.5 R-FCN 106
4.5.1 R-FCN 网络结构 107
4.5.2 位置敏感的分数图 108
4.5.3 位置敏感的RoI 池化 109
4.5.4 R-FCN 损失函数 110
4.5.5 Caffe 网络模型解析 111
4.6 Mask R-CNN 115
4.6.1 实例分割简介 115
4.6.2 COCO 数据集的像素级标注 116
4.6.3 网络结构 117
4.6.4 U-Net 121
4.6.5 SegNet 122
4.7 小结 123
第5章 单阶段目标检测方法 124
5.1 SSD 124
5.1.1 default box 125
5.1.2 网络结构 125
5.1.3 Caffe 网络模型解析 126
5.1.4 训练过程 134
5.2 RetinaNet 136
5.2.1 FPN 136
5.2.2 聚焦损失函数 138
5.3 RefineDet 139
5.3.1 网络模型 140
5.3.2 Caffe 网络模型解析 142
5.3.3 训练过程 151
5.4 YOLO 152
5.4.1 YOLO v1 152
5.4.2 YOLO v2 155
5.4.3 YOLO v3 157
5.5 目标检测算法应用 159
5.5.1 高速公路坑洞检测 159
5.5.2 息肉检测 160
5.6 小结 162
应用篇
第6章 肋骨骨折检测 164
6.1 国内外研究现状 165
6.2 解决方案 166
6.3 预处理 166
6.4 肋骨骨折检测 167
6.5 实验结果分析 168
6.6 小结 170
参考文献 171
第7章 肺结节检测 172
7.1 国内外研究现状 172
7.1.1 肺结节可疑位置推荐算法 173
7.1.2 假阳性肺结节抑制算法 173
7.2 总体框架 174
7.2.1 肺结节数据集 174
7.2.2 肺结节检测难点 175
7.2.3 算法框架 175
7.3 肺结节可疑位置推荐算法 176
7.3.1 CT图像的预处理 177
7.3.2 肺结节分割算法 178
7.3.3 优化方法 180
7.3.4 推断方法 182
7.4 可疑肺结节定位算法 183
7.5 实验结果与分析 184
7.5.1 实验结果 184
7.5.2 改进点效果分析 184
7.6 假阳性肺结节抑制算法 186
7.6.1 假阳性肺结节抑制网络 186
7.6.2 优化策略 190
7.6.3 推断策略 192
7.7 实验结果与分析 192
7.7.1 实验结果 193
7.7.2 改进点效果分析 193
7.7.3 可疑位置推荐与假阳抑制算法整合 194
7.8 小结 195
参考文献 195
第8章 车道线检测 198
8.1 国内外研究现状 198
8.2 主要研究内容 200
8.2.1 总体解决方案 200
8.2.2 各阶段概述 201
8.3 车道线检测系统的设计与实现 204
8.3.1 车道线图像数据标注与筛选 205
8.3.2 车道线图片预处理 206
8.3.3 车道线分割模型训练 211
8.3.4 车道线检测 220
8.3.5 车道线检测结果 224
8.4 车道线检测系统的性能测试 224
8.4.1 车道线检测质量测试 224
8.4.2 车道线检测时间测试 226
8.5 小结 227
参考文献 227
第9章 交通视频分析 229
9.1 国内外研究现状 230
9.2 主要研究内容 231
9.2.1 总体设计 231
9.2.2 精度和性能要求 232
9.3 交通视频分析 232
9.3.1 车辆检测和车牌检测 233
9.3.2 车牌识别功能设计详解 235
9.3.3 车辆品牌及颜色的识别 243
9.3.4 目标跟踪设计详解 244
9.4 系统测试 247
9.4.1 车辆检测 248
9.4.2 车牌检测 251
9.4.3 车牌识别 253
9.4.4 车辆品牌识别 256
9.4.5 目标跟踪 259
9.5 小结 259
参考文献 260
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序
前言
第1章　打造深度学习工具箱1
1.1　TensorFlow1
1.1.1　安装1
1.1.2　使用举例3
1.2　TFLearn3
1.3　PaddlePaddle4
1.3.1　安装5
1.3.2　使用举例6
1.4　Karas7
1.5　本章小结9
第2章　卷积神经网络10
2.1　传统的图像分类算法10
2.2　基于CNN的图像分类算法11
2.2.1　局部连接11
2.2.2　参数共享13
2.2.3　池化15
2.2.4　典型的CNN结构及实现16
2.2.5　AlexNet的结构及实现19
2.2.6　VGG的结构及实现24
2.3　基于CNN的文本处理29
2.3.1　典型的CNN结构30
2.3.2　典型的CNN代码实现30
2.4　本章小结32
第3章　循环神经网络33
3.1　循环神经算法概述34
3.2　单向循环神经网络结构与实现36
3.3　双向循环神经网络结构与实现38
3.4　循环神经网络在序列分类的应用41
3.5　循环神经网络在序列生成的应用42
3.6　循环神经网络在序列标记的应用43
3.7　循环神经网络在序列翻译的应用44
3.8　本章小结46
第4章　基于OpenSOC的机器学习框架47
4.1　OpenSOC框架47
4.2　数据源系统48
4.3　数据收集层53
4.4　消息系统层57
4.5　实时处理层60
4.6　存储层62
4.6.1　HDFS62
4.6.2　HBase64
4.6.3　Elasticsearch65
4.7　分析处理层66
4.8　计算系统67
4.9　实战演练72
4.10　本章小结77
第5章　验证码识别78
5.1　数据集79
5.2　特征提取80
5.3　模型训练与验证81
5.3.1　K近邻算法81
5.3.2　支持向量机算法81
5.3.3　深度学习算法之MLP82
5.3.4　深度学习算法之CNN83
5.4　本章小结87
第6章　垃圾邮件识别88
6.1　数据集89
6.2　特征提取90
6.2.1　词袋模型90
6.2.2　TF-IDF模型93
6.2.3　词汇表模型95
6.3　模型训练与验证97
6.3.1　朴素贝叶斯算法97
6.3.2　支持向量机算法100
6.3.3　深度学习算法之MLP101
6.3.4　深度学习算法之CNN102
6.3.5　深度学习算法之RNN106
6.4　本章小结108
第7章　负面评论识别109
7.1　数据集110
7.2　特征提取112
7.2.1　词袋和TF-IDF模型112
7.2.2　词汇表模型114
7.2.3　Word2Vec模型和Doc2Vec模型115
7.3　模型训练与验证119
7.3.1　朴素贝叶斯算法119
7.3.2　支持向量机算法122
7.3.3　深度学习算法之MLP123
7.3.4　深度学习算法之CNN124
7.4　本章小结127
第8章　骚扰短信识别128
8.1　数据集129
8.2　特征提取130
8.2.1　词袋和TF-IDF模型130
8.2.2　词汇表模型131
8.2.3　Word2Vec模型和Doc2Vec模型132
8.3　模型训练与验证134
8.3.1　朴素贝叶斯算法134
8.3.2　支持向量机算法136
8.3.3　XGBoost算法137
8.3.4　深度学习算法之MLP140
8.4　本章小结141
第9章　Linux后门检测142
9.1　数据集142
9.2　特征提取144
9.3　模型训练与验证145
9.3.1　朴素贝叶斯算法145
9.3.2　XGBoost算法146
9.3.3　深度学习算法之多层感知机148
9.4　本章小结149
第10章　用户行为分析与恶意行为检测150
10.1　数据集151
10.2　特征提取152
10.2.1　词袋和TF-IDF模型152
10.2.2　词袋和N-Gram模型154
10.2.3　词汇表模型155
10.3　模型训练与验证156
10.3.1　朴素贝叶斯算法156
10.3.2　XGBoost算法157
10.3.3　隐式马尔可夫算法159
10.3.4　深度学习算法之MLP164
10.4　本章小结166
第11章　WebShell检测167
11.1　数据集168
11.1.1　WordPress168
11.1.2　PHPCMS170
11.1.3　phpMyAdmin170
11.1.4　Smarty171
11.1.5　Yii171
11.2　特征提取172
11.2.1　词袋和TF-IDF模型172
11.2.2　opcode和N-Gram模型174
11.2.3　opcode调用序列模型180
11.3　模型训练与验证181
11.3.1　朴素贝叶斯算法181
11.3.2　深度学习算法之MLP182
11.3.3　深度学习算法之CNN184
11.4　本章小结188
第12章　智能扫描器189
12.1　自动生成XSS攻击载荷190
12.1.1　数据集190
12.1.2　特征提取194
12.1.3　模型训练与验证195
12.2　自动识别登录界面198
12.2.1　数据集198
12.2.2　特征提取199
12.2.3　模型训练与验证201
12.3　本章小结203
第13章　DGA域名识别204
13.1　数据集206
13.2　特征提取207
13.2.1　N-Gram模型207
13.2.2　统计特征模型208
13.2.3　字符序列模型210
13.3　模型训练与验证210
13.3.1　朴素贝叶斯算法210
13.3.2　XGBoost算法212
13.3.3　深度学习算法之多层感知机215
13.3.4　深度学习算法之RNN218
13.4　本章小结221
第14章　恶意程序分类识别222
14.1　数据集223
14.2　特征提取226
14.3　模型训练与验证228
14.3.1　支持向量机算法228
14.3.2　XGBoost算法229
14.3.3　深度学习算法之多层感知机230
14.4　本章小结231
第15章　反信用卡欺诈232
15.1　数据集232
15.2　特征提取234
15.2.1　标准化234
15.2.2　标准化和降采样234
15.2.3　标准化和过采样236
15.3　模型训练与验证239
15.3.1　朴素贝叶斯算法239
15.3.2　XGBoost算法243
15.3.3　深度学习算法之多层感知机247
15.4　本章小结251
・ ・ ・ ・ ・ ・ (收起)第1章　编程和数学基础 1
1.1　Python快速入门 1
1.1.1　快速安装Python 1
1.1.2　Python基础 2
1.1.3　Python中的常见运算 5
1.1.4　Python控制语句 7
1.1.5　Python常用容器类型 10
1.1.6　Python常用函数 16
1.1.7　类和对象 22
1.1.8　Matplotlib入门 24
1.2　张量库NumPy 33
1.2.1　什么是张量 33
1.2.2　创建ndarray对象 37
1.2.3　ndarray数组的索引和切片 53
1.2.4　张量的计算 57
1.3　微积分 63
1.3.1　函数 64
1.3.2　四则运算和复合运算 66
1.3.3　极限和导数 69
1.3.4　导数的四则运算和链式法则 72
1.3.5　计算图、正向计算和反向传播求导 74
1.3.6　多变量函数的偏导数与梯度 75
1.3.7　向量值函数的导数与Jacobian矩阵 78
1.3.8　积分 83
1.4　概率基础 84
1.4.1　概率 84
1.4.2　条件概率、联合概率、全概率公式、贝叶斯公式 86
1.4.3　随机变量 88
1.4.4　离散型随机变量的概率分布 89
1.4.5　连续型随机变量的概率密度 91
1.4.6　随机变量的分布函数 93
1.4.7　期望、方差、协方差、协变矩阵 95
第2章　梯度下降法 99
2.1　函数极值的必要条件 99
2.2　梯度下降法基础 101
2.3　梯度下降法的参数优化策略 108
2.3.1　Momentum法 108
2.3.2　AdaGrad法 110
2.3.3　AdaDelta法 112
2.3.4　RMSprop法 114
2.3.5　Adam法 115
2.4　梯度验证 117
2.4.1　比较数值梯度和分析梯度 117
2.4.2　通用的数值梯度 118
2.5　分离梯度下降法与参数优化策略 119
2.5.1　参数优化器 119
2.5.2　接受参数优化器的梯度下降法 120
第3章　线性回归、逻辑回归和softmax回归 122
3.1　线性回归 122
3.1.1　餐车利润问题 122
3.1.2　机器学习与人工智能 123
3.1.3　什么是线性回归 126
3.1.4　用正规方程法求解线性回归问题 127
3.1.5　用梯度下降法求解线性回归问题 129
3.1.6　调试学习率 133
3.1.7　梯度验证 135
3.1.8　预测 135
3.1.9　多特征线性回归 136
3.2　数据的规范化 143
3.2.1　预测大坝出水量 143
3.2.2　数据的规范化过程 147
3.3　模型的评估 149
3.3.1　欠拟合和过拟合 149
3.3.2　验证集和测试集 153
3.3.3　学习曲线 155
3.3.4　偏差和方差 160
3.4　正则化 165
3.5　逻辑回归 168
3.5.1　逻辑回归基础 169
3.5.2　逻辑回归的NumPy实现 173
3.5.3　实战：鸢尾花分类的NumPy实现 178
3.6　softmax回归 180
3.6.1　spiral数据集 180
3.6.2　softmax函数 181
3.6.3　softmax回归模型 186
3.6.4　多分类交叉熵损失 188
3.6.5　通过加权和计算交叉熵损失 191
3.6.6　softmax回归的梯度计算 191
3.6.7　softmax回归的梯度下降法的实现 197
3.6.8　spiral数据集的softmax回归模型 197
3.7　批梯度下降法和随机梯度下降法 199
3.7.1　MNIST手写数字集 199
3.7.2　用部分训练样本训练逻辑回归模型 201
3.7.3　批梯度下降法 202
3.7.4　随机梯度下降法 207
第4章　神经网络 209
4.1　神经网络概述 209
4.1.1　感知机和神经元 209
4.1.2　激活函数 213
4.1.3　神经网络与深度学习 216
4.1.4　多个样本的正向计算 221
4.1.5　输出 224
4.1.6　损失函数 224
4.1.7　基于数值梯度的神经网络训练 229
4.2　反向求导 235
4.2.1　正向计算和反向求导 235
4.2.2　计算图 237
4.2.3　损失函数关于输出的梯度 239
4.2.4　2层神经网络的反向求导 242
4.2.5　2层神经网络的Python实现 247
4.2.6　任意层神经网络的反向求导 252
4.3　实现一个简单的深度学习框架 256
4.3.1　神经网络的训练过程 256
4.3.2　网络层的代码实现 257
4.3.3　网络层的梯度检验 260
4.3.4　神经网络的类 261
4.3.5　神经网络的梯度检验 263
4.3.6　基于深度学习框架的MNIST手写数字识别 266
4.3.7　改进的通用神经网络框架：分离加权和与激活函数 268
4.3.8　独立的参数优化器 276
4.3.9　fashion-mnist的分类训练 279
4.3.10　读写模型参数 282
第5章　改进神经网络性能的基本技巧 285
5.1　数据处理 285
5.1.1　数据增强 285
5.1.2　规范化 289
5.1.3　特征工程 289
5.2　参数调试 296
5.2.1　权重初始化 296
5.2.2　优化参数 301
5.3　批规范化 301
5.3.1　什么是批规范化 301
5.3.2　批规范化的反向求导 303
5.3.3　批规范化的代码实现 304
5.4　正则化 310
5.4.1　权重正则化 310
5.4.2　Dropout 312
5.4.3　早停法 316
5.5　梯度爆炸和梯度消失 317
第6章　卷积神经网络 318
6.1　卷积入门 319
6.1.1　什么是卷积 319
6.1.2　一维卷积 325
6.1.3　二维卷积 326
6.1.4　多通道输入和多通道输出 338
6.1.5　池化 341
6.2　卷积神经网络概述 344
6.2.1　全连接神经元和卷积神经元 345
6.2.2　卷积层和卷积神经网络 346
6.2.3　卷积层和池化层的反向求导及代码实现 349
6.2.4　卷积神经网络的代码实现 361
6.3　卷积的矩阵乘法 364
6.3.1　一维卷积的矩阵乘法 364
6.3.2　二维卷积的矩阵乘法 365
6.3.3　一维卷积反向求导的矩阵乘法 371
6.3.4　二维卷积反向求导的矩阵乘法 373
6.4　基于坐标索引的快速卷积 377
6.5　典型卷积神经网络结构 393
6.5.1　LeNet-5 393
6.5.2　AlexNet 394
6.5.3　VGG 395
6.5.4　残差网络 396
6.5.5　Inception网络 398
6.5.6　NiN 399
第7章　循环神经网络 403
7.1　序列问题和模型 403
7.1.1　股票价格预测问题 404
7.1.2　概率序列模型和语言模型 405
7.1.3　自回归模型 406
7.1.4　生成自回归数据 406
7.1.5　时间窗方法 408
7.1.6　时间窗采样 409
7.1.7　时间窗方法的建模和训练 409
7.1.8　长期预测和短期预测 410
7.1.9　股票价格预测的代码实现 412
7.1.10　k-gram语言模型 415
7.2　循环神经网络基础 416
7.2.1　无记忆功能的非循环神经网络 417
7.2.2　具有记忆功能的循环神经网络 418
7.3　穿过时间的反向传播 421
7.4　单层循环神经网络的实现 425
7.4.1　初始化模型参数 425
7.4.2　正向计算 425
7.4.3　损失函数 427
7.4.4　反向求导 427
7.4.5　梯度验证 429
7.4.6　梯度下降训练 432
7.4.7　序列数据的采样 433
7.4.8　序列数据的循环神经网络训练和预测 441
7.5　循环神经网络语言模型和文本的生成 448
7.5.1　字符表 448
7.5.2　字符序列样本的采样 450
7.5.3　模型的训练和预测 452
7.6　循环神经网络中的梯度爆炸和梯度消失 455
7.7　长短期记忆网络 456
7.7.1　LSTM的神经元 457
7.7.2　LSTM的反向求导 460
7.7.3　LSTM的代码实现 461
7.7.4　LSTM的变种 469
7.8　门控循环单元 470
7.8.1　门控循环单元的工作原理 470
7.8.2　门控循环单元的代码实现 472
7.9　循环神经网络的类及其实现 475
7.9.1　用类实现循环神经网络 475
7.9.2　循环神经网络单元的类实现 483
7.10　多层循环神经网络和双向循环神经网络 491
7.10.1　多层循环神经网络 491
7.10.2　多层循环神经网络的训练和预测 497
7.10.3　双向循环神经网络 500
7.11　Seq2Seq模型 506
7.11.1　机器翻译概述 507
7.11.2　Seq2Seq模型的实现 508
7.11.3　字符级的Seq2Seq模型 516
7.11.4　基于Word2Vec的Seq2Seq模型 522
7.11.5　基于词嵌入层的Seq2Seq模型 533
7.11.6　注意力机制 541
第8章　生成模型 552
8.1　生成模型概述 552
8.2　自动编码器 556
8.2.1　什么是自动编码器 557
8.2.2　稀疏编码器 559
8.2.3　自动编码器的代码实现 560
8.3　变分自动编码器 563
8.3.1　什么是变分自动编码器 563
8.3.2　变分自动编码器的损失函数 564
8.3.3　变分自动编码器的参数重采样 565
8.3.4　变分自动编码器的反向求导 565
8.3.5　变分自动编码器的代码实现 566
8.4　生成对抗网络 571
8.4.1　生成对抗网络的原理 573
8.4.2　生成对抗网络训练过程的代码实现 577
8.5　生成对抗网络建模实例 579
8.5.1　一组实数的生成对抗网络建模 579
8.5.2　二维坐标点的生成对抗网络建模 585
8.5.3　MNIST手写数字集的生成对抗网络建模 590
8.5.4　生成对抗网络的训练技巧 594
8.6　生成对抗网络的损失函数及其概率解释 594
8.6.1　生成对抗网络的损失函数的全局最优解 594
8.6.2　Kullback-Leibler散度和Jensen-Shannon散度 595
8.6.3　生成对抗网络的最大似然解释 598
8.7　改进的损失函数――Wasserstein GAN 599
8.7.1　Wasserstein GAN的原理 599
8.7.2　Wasserstein GAN的代码实现 603
8.8　深度卷积对抗网络 605
8.8.1　一维转置卷积 606
8.8.2　二维转置卷积 609
8.8.3　卷积对抗网络的代码实现 612
参考文献 617
・ ・ ・ ・ ・ ・ (收起)基础篇
第1章 深度学习概述 2
1.1 深度学习发展简史 2
1.2 有监督学习 4
1.2.1 图像分类 4
1.2.2 目标检测 6
1.2.3 人脸识别 10
1.2.4 语音识别 13
1.3 无监督学习 18
1.3.1 无监督学习概述 18
1.3.2 生成对抗网络 18
1.4 强化学习 21
1.4.1 AlphaGo 21
1.4.2 AlphaGo Zero 23
1.5 小结 25
参考文献 25
第2章 深度神经网络 27
2.1 神经元 27
2.2 感知机 30
2.3 前向传递 32
2.3.1 前向传递的流程 32
2.3.2 激活函数 33
2.3.3 损失函数 37
2.4 后向传递 40
2.4.1 后向传递的流程 40
2.4.2 梯度下降 40
2.4.3 参数修正 42
2.5 防止过拟合 44
2.5.1 dropout 44
2.5.2 正则化 45
2.6 小结 46
第3章 卷积神经网络 47
3.1 卷积层 48
3.1.1 valid 卷积 48
3.1.2 full 卷积 50
3.1.3 same 卷积 51
3.2 池化层 52
3.3 反卷积 53
3.4 感受野 55
3.5 卷积网络实例 56
3.5.1 Lenet-5 56
3.5.2 AlexNet 59
3.5.3 VGGNet 62
3.5.4 GoogLeNet 64
3.5.5 ResNet 72
3.5.6 MobileNet 73
3.6 小结 76
进阶篇
第4章 两阶段目标检测方法 78
4.1 R-CNN 78
4.1.1 算法流程 79
4.1.2 训练过程 80
4.2 SPP-Net 83
4.2.1 网络结构 84
4.2.2 空间金字塔池化 84
4.3 Fast R-CNN 86
4.3.1 感兴趣区域池化层 86
4.3.2 网络结构 88
4.3.3 全连接层计算加速 89
4.3.4 目标分类 90
4.3.5 边界框回归 91
4.3.6 训练过程 93
4.4 Faster R-CNN 96
4.4.1 网络结构 97
4.4.2 RPN 98
4.4.3 训练过程 104
4.5 R-FCN 106
4.5.1 R-FCN 网络结构 107
4.5.2 位置敏感的分数图 108
4.5.3 位置敏感的RoI 池化 109
4.5.4 R-FCN 损失函数 110
4.5.5 Caffe 网络模型解析 111
4.6 Mask R-CNN 115
4.6.1 实例分割简介 115
4.6.2 COCO 数据集的像素级标注 116
4.6.3 网络结构 117
4.6.4 U-Net 121
4.6.5 SegNet 122
4.7 小结 123
第5章 单阶段目标检测方法 124
5.1 SSD 124
5.1.1 default box 125
5.1.2 网络结构 125
5.1.3 Caffe 网络模型解析 126
5.1.4 训练过程 134
5.2 RetinaNet 136
5.2.1 FPN 136
5.2.2 聚焦损失函数 138
5.3 RefineDet 139
5.3.1 网络模型 140
5.3.2 Caffe 网络模型解析 142
5.3.3 训练过程 151
5.4 YOLO 152
5.4.1 YOLO v1 152
5.4.2 YOLO v2 155
5.4.3 YOLO v3 157
5.5 目标检测算法应用 159
5.5.1 高速公路坑洞检测 159
5.5.2 息肉检测 160
5.6 小结 162
应用篇
第6章 肋骨骨折检测 164
6.1 国内外研究现状 165
6.2 解决方案 166
6.3 预处理 166
6.4 肋骨骨折检测 167
6.5 实验结果分析 168
6.6 小结 170
参考文献 171
第7章 肺结节检测 172
7.1 国内外研究现状 172
7.1.1 肺结节可疑位置推荐算法 173
7.1.2 假阳性肺结节抑制算法 173
7.2 总体框架 174
7.2.1 肺结节数据集 174
7.2.2 肺结节检测难点 175
7.2.3 算法框架 175
7.3 肺结节可疑位置推荐算法 176
7.3.1 CT图像的预处理 177
7.3.2 肺结节分割算法 178
7.3.3 优化方法 180
7.3.4 推断方法 182
7.4 可疑肺结节定位算法 183
7.5 实验结果与分析 184
7.5.1 实验结果 184
7.5.2 改进点效果分析 184
7.6 假阳性肺结节抑制算法 186
7.6.1 假阳性肺结节抑制网络 186
7.6.2 优化策略 190
7.6.3 推断策略 192
7.7 实验结果与分析 192
7.7.1 实验结果 193
7.7.2 改进点效果分析 193
7.7.3 可疑位置推荐与假阳抑制算法整合 194
7.8 小结 195
参考文献 195
第8章 车道线检测 198
8.1 国内外研究现状 198
8.2 主要研究内容 200
8.2.1 总体解决方案 200
8.2.2 各阶段概述 201
8.3 车道线检测系统的设计与实现 204
8.3.1 车道线图像数据标注与筛选 205
8.3.2 车道线图片预处理 206
8.3.3 车道线分割模型训练 211
8.3.4 车道线检测 220
8.3.5 车道线检测结果 224
8.4 车道线检测系统的性能测试 224
8.4.1 车道线检测质量测试 224
8.4.2 车道线检测时间测试 226
8.5 小结 227
参考文献 227
第9章 交通视频分析 229
9.1 国内外研究现状 230
9.2 主要研究内容 231
9.2.1 总体设计 231
9.2.2 精度和性能要求 232
9.3 交通视频分析 232
9.3.1 车辆检测和车牌检测 233
9.3.2 车牌识别功能设计详解 235
9.3.3 车辆品牌及颜色的识别 243
9.3.4 目标跟踪设计详解 244
9.4 系统测试 247
9.4.1 车辆检测 248
9.4.2 车牌检测 251
9.4.3 车牌识别 253
9.4.4 车辆品牌识别 256
9.4.5 目标跟踪 259
9.5 小结 259
参考文献 260
・ ・ ・ ・ ・ ・ (收起)序言
前言
如何使用本书 ――写在第十次印刷之际
主要符号表
第1章 绪论
1.1 引言
1.2 基本术语
1.3 假设空间
1.4 归纳偏好
1.5 发展历程
1.6 应用现状
1.7 阅读材料
习题
参考文献
休息一会儿
第2章 模型评估与选择
2.1 经验误差与过拟合
2.2 评估方法
2.3 性能度量
2.4 比较检验
2.5 偏差与方差
2.6 阅读材料
习题
参考文献
休息一会儿
第3章 线性模型
3.1 基本形式
3.2 线性回归
3.3 对数几率回归
3.4 线性判别分析
3.5 多分类学习
3.6 类别不平衡问题
3.7 阅读材料
习题
参考文献
休息一会儿
第4章 决策树
4.1 基本流程
4.2 划分选择
4.3 剪枝处理
4.4 连续与缺失值
4.5 多变量决策树
4.6 阅读材料
习题
参考文献
休息一会儿
第5章 神经网络
5.1 神经元模型
5.2 感知机与多层网络
5.3 误差逆传播算法
5.4 全局最小与局部极小
5.5 其他常见神经网络
5.6 深度学习
5.7 阅读材料
习题
参考文献
休息一会儿
第6章 支持向量机
6.1 间隔与支持向量
6.2 对偶问题
6.3 核函数
6.4 软间隔与正则化
6.5 支持向量回归
6.6 核方法
6.7 阅读材料
习题
参考文献
休息一会儿
第7章 贝叶斯分类器
7.1 贝叶斯决策论
7.2 极大似然估计
7.3 朴素贝叶斯分类器
7.4 半朴素贝叶斯分类器
7.5 贝叶斯网
7.6 EM算法
7.7 阅读材料
习题
参考文献
休息一会儿
第8章 集成学习
8.1 个体与集成
8.2 Boosting
8.3 Bagging与随机森林
8.4 结合策略
8.5 多样性
8.6 阅读材料
习题
参考文献
休息一会儿
第9章 聚类
9.1 聚类任务
9.2 性能度量
9.3 距离计算
9.4 原型聚类
9.5 密度聚类
9.6 层次聚类
9.7 阅读材料
习题
参考文献
休息一会儿
第10章 降维与度量学习
10.1 k近邻学习
10.2 低维嵌入
10.3 主成分分析
10.4 归纳偏好
10.5 流形学习
10.6 度量学习
10.7 阅读材料
习题
参考文献
休息一会儿
第11章 特征选择与稀疏学习
11.1 子集搜索与评价
11.2 过滤式选择
11.3包裹式选择
11.4 嵌入式选择与L1正则化
11.5 稀疏表示与字典学习
11.6 压缩感知
11.7 阅读材料
习题
参考文献
休息一会儿
第12章 计算学习理论
12.1 基础知识
12.2 PAC学习
12.3 有限假设空间
12.4 VC维
12.5 Rademacher复杂度
12.6 稳定性
12.7 阅读材料
习题
参考文献
休息一会儿
第13章 半监督学习
13.1 未标记样本
13.2 生成式方法
13.3 半监督SVM
13.4 图半监督学习
13.5 基于分歧的方法
13.6 半监督聚类
13.7 阅读材料
习题
参考文献
休息一会儿
第14章 概率图模型
14.1 隐马尔可夫模型
14.2 马尔可夫随机场
14.3 条件随机场
14.4学习与推断
14.5 近似推断
14.6 话题模型
14.7 阅读材料
习题
参考文献
休息一会儿
第15章 规则学习
15.1 基本概念
15.2 序贯覆盖
15.3 剪枝优化
15.4 一阶规则学习
15.5 归纳逻辑程序设计
15.6 阅读材料
习题
参考文献
休息一会儿
第16章 强化学习
16.1 任务与奖赏
16.2 K-摇臂赌博机
16.3 有模型学习
16.4 免模型学习
16.5 值函数近似
16.6 模仿学习
16.7 阅读材料
习题
参考文献
休息一会儿
附录
A 矩阵
B 优化
C 概率分布
后记
・ ・ ・ ・ ・ ・ (收起)前言 1
第一部分 机器学习的基础知识 11
第 1 章 机器学习概览
13 1.1 什么是机器学习 14
1.2 为什么使用机器学习 14
1.3 机器学习的应用示例 16
1.4 机器学习系统的类型 18
1.5 机器学习的主要挑战 32
1.6 测试与验证 38
1.7 练习题 40
第 2 章 端到端的机器学习项目 42
2.1 使用真实数据
42 2.2 观察大局 44
2.3 获取数据 48
2.4 从数据探索和可视化中获得洞见 60
2.5 机器学习算法的数据准备 66
2.6 选择和训练模型 74
2.7 微调模型 77
2.8 启动、监控和维护你的系统 .82
2.9 试试看 84
2.10 练习题 84
第 3 章 分类 86
3.1 MNIST 86
3.2 训练二元分类器 88
3.3 性能测量 89
3.4 多类分类器 99
3.5 误差分析 101
3.6 多标签分类 104
3.7 多输出分类 105
3.8 练习题 107
第 4 章 训练模型 108
4.1 线性回归 109
4.2 梯度下降 113
4.3 多项式回归 122
4.4 学习曲线 124
4.5 正则化线性模型 127
4.6 逻辑回归 134
4.7 练习题 141
第 5 章 支持向量机 143
5.1 线性 SVM 分类 143
5.2 非线性 SVM 分类 146
5.3 SVM 回归 151
5.4 工作原理 152
5.5 练习题 160
第 6 章 决策树 162
6.1 训练和可视化决策树 162
6.2 做出预测 163
6.3 估计类概率 165
6.4 CART 训练算法 166
6.5 计算复杂度 166
6.6 基尼不纯度或熵 167
6.7 正则化超参数 167
6.8 回归 168
6.9 不稳定性 170
6.10 练习题 172
第 7 章 集成学习和随机森林 173
7.1 投票分类器 173
7.2 bagging 和 pasting 176
7.3 随机补丁和随机子空间 179
7.4 随机森林 180
7.5 提升法 182
7.6 堆叠法 190
7.7 练习题 192
第 8 章 降维 193
8.1 维度的诅咒 194
8.2 降维的主要方法 195
8.3 PCA 198
8.4 内核 PCA . 204
8.5 LLE 206
8.6 其他降维技术 208
8.7 练习题 209
第 9 章 无监督学习技术 211
9.1 聚类 212
9.2 高斯混合模型 232
9.3 练习题 245
第二部分 神经网络与深度学习 247
第 10 章 Keras 人工神经网络简介 249
10.1 从生物神经元到人工神经元 250
10.2 使用 Keras 实现 MLP 262
10.3 微调神经网络超参数 284
10.4 练习题 290
第 11 章 训练深度神经网络 293
11.1 梯度消失与梯度爆炸问题 293
11.2 重用预训练层 305
11.3 更快的优化器 310
11.4 通过正则化避免过拟合 321
11.5 总结和实用指南 327
11.6 练习题 329
第 12 章 使用 TensorFlow 自定义模型和训练 330
12.1 TensorFlow 快速浏览 330
12.2 像 NumPy 一样使用 TensorFlow 333
12.3 定制模型和训练算法 338
12.4 TensorFlow 函数和图 356
12.5 练习题 360
第 13 章 使用 TensorFlow 加载和预处理数据 362
13.1 数据 API 363
13.2 TFRecord 格式 372
13.3 预处理输入特征 377
13.4 TF Transform 385
13.5 TensorFlow 数据集项目 386
13.6 练习题 388
第 14 章 使用卷积神经网络的深度计算机视觉 390
14.1 视觉皮层的架构 390
14.2 卷积层 392
14.3 池化层 399
14.4 CNN 架构 402
14.5 使用 Keras 实现 ResNet-34 CNN 416
14.6 使用 Keras 的预训练模型 417
14.7 迁移学习的预训练模型 418
14.8 分类和定位 421
14.9 物体检测 422
14.10 语义分割 428
14.11 练习题 431
第 15 章 使用 RNN 和 CNN 处理序列 432
15.1 循环神经元和层 432
15.2 训练 RNN 436
15.3 预测时间序列 437
15.4 处理长序列 444
15.5 练习题 453
第 16 章 使用 RNN 和注意力机制进行自然语言处理 455
16.1 使用字符 RNN 生成莎士比亚文本 456
16.2 情感分析 464
16.3 神经机器翻译的编码器 - 解码器网络 470
16.4 注意力机制 476
16.5 最近语言模型的创新 486
16.6 练习题 ... 488
第 17 章 使用自动编码器和 GAN 的表征学习和生成学习 489
17.1 有效的数据表征 490
17.2 使用不完整的线性自动编码器执行 PCA 491
17.3 堆叠式自动编码器 493
17.4 卷积自动编码器 499
17.5 循环自动编码器 500
17.6 去噪自动编码器 501
17.7 稀疏自动编码器 502
17.8 变分自动编码器 505
17.9 生成式对抗网络 510
17.10 练习题 522
第 18 章 强化学习 523
18.1 学习优化奖励 524
18.2 策略搜索 525
18.3 OpenAI Gym 介绍 526
18.4 神经网络策略 529
18.5 评估动作：信用分配问题 531
18.6 策略梯度 532
18.7 马尔可夫决策过程 536
18.8 时序差分学习 540
18.9 Q 学习 540
18.10 实现深度 Q 学习 544
18.11 深度 Q 学习的变体 547
18.12 TF-Agents 库 550
18.13 一些流行的 RL 算法概述 568
18.14 练习题 569
第 19 章 大规模训练和部署TensorFlow 模型 571
19.1 为 TensorFlow 模型提供服务 572
19.2 将模型部署到移动端或嵌入式设备 586
19.3 使用 GPU 加速计算 589
19.4 跨多个设备的训练模型 600
19.5 练习题 613
19.6 致谢 613
附录 A 课后练习题解答 .....614
附录 B 机器学习项目清单 ..642
附录 C SVM 对偶问题 647
附录 D 自动微分 .650
附录 E 其他流行的人工神经网络架构 ...656
附录 F 特殊数据结构..663
附录 G TensorFlow 图 ......669
・ ・ ・ ・ ・ ・ (收起)引子：AI 菜鸟的挑战―100 天上线智能预警系统
第壹 课 机器学习快速上手路径―唯有实战
1．1 机器学习的家族谱
1．1．1 新手入门机器学习的3 个好消息
1．1．2 机器学习*是从数据中发现规律
1．1．3 机器学习的类别―监督学习及其他
1．1．4 机器学习的重要分支―深度学习
1．1．5 机器学习新热点―强化学习
1．1．6 机器学习的两大应用场景―回归与分类
1．1．7 机器学习的其他应用场景
1．2 快捷的云实战学习模式
1．2．1 在线学习平台上的机器学习课程
1．2．2 用Jupyter Notebook 直接实战
1．2．3 用Google Colab 开发第壹个机器学习程序
1．2．4 在Kaggle 上参与机器学习竞赛
1．2．5 在本机上“玩”机器学习
1．3 基本机器学习术语
1．3．1 特征
1．3．2 标签
1．3．3 模型
1．4 Python 和机器学习框架
1．4．1 为什么选择用Python
1．4．2 机器学习和深度学习框架
1．5 机器学习项目实战架构
1．5．1 第壹 个环节：问题定义
1．5．2 第2 个环节：数据的收集和预处理
1．5．3 第3 个环节：选择机器学习模型
1．5．4 第4 个环节：训练机器，确定参数
1．5．5 第5 个环节：超参数调试和性能优化
1．6 本课内容小结
1．7 课后练习
第2 课 数学和Python 基础知识―*天搞定
2．1 函数描述了事物间的关系
2．1．1 什么是函数
2．1．2 机器学习中的函数
2．2 捕捉函数的变化趋势
2．2．1 连续性是求导的前提条件
2．2．2 通过求导发现y 如何随x 而变
2．2．3 凸函数有一个全局*低点
2．3 梯度下降是机器学习的动力之源
2．3．1 什么是梯度
2．3．2 梯度下降：下山的隐喻
2．3．3 梯度下降有什么用
2．4 机器学习的数据结构―张量
2．4．1 张量的轴、阶和形状
2．4．2 标量―0D（阶）张量
2．4．3 向量―1D（阶）张量
2．4．4 矩阵―2D（阶）张量
2．4．5 序列数据 ―3D（阶）张量
2．4．6 图像数据 ―4D（阶）张量
2．4．7 视频数据―5D（阶）张量
2．4．8 数据的维度和空间的维度
2．5 Python 的张量运算
2．5．1 机器学习中张量的创建
2．5．2 通过索引和切片访问张量中的数据
2．5．3 张量的整体操作和逐元素运算
2．5．4 张量的变形和转置
2．5．5 Python 中的广播
2．5．6 向量和矩阵的点积运算
2．6 机器学习的几何意义
2．6．1 机器学习的向量空间
2．6．2 深度学习和数据流形
2．7 概率与统计研究了随机事件的规律
2．7．1 什么是概率
2．7．2 正态分布
2．7．3 标准差和方差
2．8 本课内容小结
2．9 课后练习
第3 课 线性回归―预测网店的销售额
3．1 问题定义：小冰的网店广告该如何投放
3．2 数据的收集和预处理
3．2．1 收集网店销售额数据
3．2．2 数据读取和可视化
3．2．3 数据的相关分析
3．2．4 数据的散点图
3．2．5 数据集清洗和规范化
3．2．6 拆分数据集为训练集和测试集
3．2．7 把数据归一化
3．3 选择机器学习模型
3．3．1 确定线性回归模型
3．3．2 假设（预测）函数―h （x ）
3．3．3 损失（误差）函数―L （w ，b ）
3．4 通过梯度下降找到*佳参数
3．4．1 训练机器要有正确的方向
3．4．2 凸函数确保有*小损失点
3．4．3 梯度下降的实现
3．4．4 学习速率也很重要
3．5 实现一元线性回归模型并调试超参数
3．5．1 权重和偏置的初始值
3．5．2 进行梯度下降
3．5．3 调试学习速率
3．5．4 调试迭代次数
3．5．5 在测试集上进行预测
3．5．6 用轮廓图描绘L 、w 和b 的关系
3．6 实现多元线性回归模型
3．6．1 向量化的点积运算
3．6．2 多变量的损失函数和梯度下降
3．6．3 构建一个线性回归函数模型
3．6．4 初始化权重并训练机器
3．7 本课内容小结
3．8 课后练习
第4 课 逻辑回归―给病患和鸢尾花分类
4．1 问题定义：判断客户是否患病
4．2 从回归问题到分类问题
4．2．1 机器学习中的分类问题
4．2．2 用线性回归+ 阶跃函数完成分类
4．2．3 通过Sigmiod 函数进行转换
4．2．4 逻辑回归的假设函数
4．2．5 逻辑回归的损失函数
4．2．6 逻辑回归的梯度下降
4．3 通过逻辑回归解决二元分类问题
4．3．1 数据的准备与分析
4．3．2 建立逻辑回归模型
4．3．3 开始训练机器
4．3．4 测试分类结果
4．3．5 绘制损失曲线
4．3．6 直接调用Sklearn 库
4．3．7 哑特征的使用
4．4 问题定义：确定鸢尾花的种类
4．5 从二元分类到多元分类
4．5．1 以一对多
4．5．2 多元分类的损失函数
4．6 正则化、欠拟合和过拟合
4．6．1 正则化
4．6．2 欠拟合和过拟合
4．6．3 正则化参数
4．7 通过逻辑回归解决多元分类问题
4．7．1 数据的准备与分析
4．7．2 通过Sklearn 实现逻辑回归的多元分类
4．7．3 正则化参数―C 值的选择
4．8 本课内容小结
4．9 课后练习
第5 课 深度神经网络―找出可能流失的客户
5．1 问题定义：咖哥接手的金融项目
5．2 神经网络的原理
5．2．1 神经网络极简史
5．2．2 传统机器学习算法的局限性
5．2．3 神经网络的优势
5．3 从感知器到单隐层网络
5．3．1 感知器是*基本的神经元
5．3．2 假设空间要能覆盖特征空间
5．3．3 单神经元特征空间的局限性
5．3．4 分层：加入一个网络隐层
5．4 用Keras 单隐层网络预测客户流失率
5．4．1 数据的准备与分析
5．4．2 先尝试逻辑回归算法
5．4．3 单隐层神经网络的Keras 实现
5．4．4 训练单隐层神经网络
5．4．5 训练过程的图形化显示
5．5 分类数据不平衡问题：只看准确率够用吗
5．5．1 混淆矩阵、精que率、召回率和F1 分数
5．5．2 使用分类报告和混淆矩阵
5．5．3 特征缩放的魔力
5．5．4 阈值调整、欠采样和过采样
5．6 从单隐层神经网络到深度神经网络
5．6．1 梯度下降：正向传播和反向传播
5．6．2 深度神经网络中的一些可调超参数
5．6．3 梯度下降优化器
5．6．4 激活函数：从Sigmoid 到ReLU
5．6．5 损失函数的选择
5．6．6 评估指标的选择
5．7 用Keras 深度神经网络预测客户流失率
5．7．1 构建深度神经网络
5．7．2 换一换优化器试试
5．7．3 神经网络正则化：添加Dropout 层
5．8 深度神经网络的调试及性能优化
5．8．1 使用回调功能
5．8．2 使用TensorBoard
5．8．3 神经网络中的过拟合
5．8．4 梯度消失和梯度bao炸
5．9 本课内容小结
5．10 课后练习
第6课 卷积神经网络―识别狗狗的图像
6．1 问题定义：有趣的狗狗图像识别
6．2 卷积网络的结构
6．3 卷积层的原理
6．3．1 机器通过“模式”进行图像识别
6．3．2 平移不变的模式识别
6．3．3 用滑动窗口抽取局部特征
6．3．4 过滤器和响应通道
6．3．5 对特征图进行卷积运算
6．3．6 模式层级结构的形成
6．3．7 卷积过程中的填充和步幅
6．4 池化层的功能
6．5 用卷积网络给狗狗图像分类
6．5．1 图像数据的读入
6．5．2 构建简单的卷积网络
6．5．3 训练网络并显示误差和准确率
6．6 卷积网络性能优化
6．6．1 第壹招：更新优化器并设置学习速率
6．6．2 第2招：添加Dropout 层
6．6．3 “大杀器”：进行数据增强
6．7 卷积网络中特征通道的可视化
6．8 各种大型卷积网络模型
6．8．1 经典的VGGNet
6．8．2 采用Inception 结构的GoogLeNet
6．8．3 残差网络ResNet
6．9 本课内容小结
6．10 课后练习
第7 课 循环神经网络―鉴定留言及探索系外行星
7．1 问题定义：鉴定评论文本的情感属性
7．2 循环神经网络的原理和结构
7．2．1 什么是序列数据
7．2．2 前馈神经网络处理序列数据的局限性
7．2．3 循环神经网络处理序列问题的策略
7．2．4 循环神经网络的结构
7．3 原始文本如何转化成向量数据
7．3．1 文本的向量化：分词
7．3．2 通过One-hot 编码分词
7．3．3 词嵌入
7．4 用SimpleRNN 鉴定评论文本
7．4．1 用Tokenizer 给文本分词
7．4．2 构建包含词嵌入的SimpleRNN
7．4．3 训练网络并查看验证准确率
7．5 从SimpleRNN 到LSTM
7．5．1 SimpleRNN 的局限性
7．5．2 LSTM 网络的记忆传送带
7．6 用LSTM 鉴定评论文本
7．7 问题定义：太阳系外哪些恒星有行星环绕
7．8 用循环神经网络处理时序问题
7．8．1 时序数据的导入与处理
7．8．2 建模：CNN 和RNN 的组合
7．8．3 输出阈值的调整
7．8．4 使用函数式API
7．9 本课内容小结
7．10 课后练习
第8 课 经典算法“宝刀未老”
8．1 K *近邻
8．2 支持向量机
8．3 朴素贝叶斯
8．4 决策树
8．4．1 熵和特征节点的选择
8．4．2 决策树的深度和剪枝
8．5 随机森林
8．6 如何选择*佳机器学习算法
8．7 用网格搜索超参数调优
8．8 本课内容小结
8．9 课后练习
第9 课 集成学习“笑傲江湖”
9．1 偏差和方差―机器学习性能优化的风向标
9．1．1 目标：降低偏差与方差
9．1．2 数据集大小对偏差和方差的影响
9．1．3 预测空间的变化带来偏差和方差的变化
9．2 Bagging 算法―多个基模型的聚合
9．2．1 决策树的聚合
9．2．2 从树的聚合到随机森林
9．2．3 从随机森林到ji端随机森林
9．2．4 比较决策树、树的聚合、随机森林、ji端随机森林的效率
9．3 Boosting 算法―锻炼弱模型的“肌肉”
9．3．1 AdaBoost 算法
9．3．2 梯度提升算法
9．3．3 XGBoost 算法
9．3．4 Bagging 算法与Boosting 算法的不同之处
9．4 Stacking/Blending 算法―以预测结果作为新特征
9．4．1 Stacking 算法
9．4．2 Blending 算法
9．5 Voting/Averaging 算法―集成基模型的预测结果
9．5．1 通过Voting 进行不同算法的集成
9．5．2 通过Averaging 集成不同算法的结果
9．6 本课内容小结
9．7 课后练习
第壹0 课 监督学习之外―其他类型的机器学习
10．1 无监督学习―聚类
10．1．1 K 均值算法
10．1．2 K 值的选取：手肘法
10．1．3 用聚类辅助理解营销数据
10．2 无监督学习―降维
10．2．1 PCA 算法
10．2．2 通过PCA 算法进行图像特征采样
10．3 半监督学习
10．3．1 自我训练
10．3．2 合作训练
10．3．3 半监督聚类
10．4 自监督学习
10．4．1 潜隐空间
10．4．2 自编码器
10．4．3 变分自编码器
10．5 生成式学习
10．5．1 机器学习的生成式
10．5．2 生成式对抗网络
10．6 本课内容小结
10．7 课后练习
第壹1 课 强化学习实战―咖哥的冰湖挑战
11．1 问题定义：帮助智能体完成冰湖挑战
11．2 强化学习基础知识
11．2．1 延迟满足
11．2．2 更复杂的环境
11．2．3 强化学习中的元素
11．2．4 智能体的视角
11．3 强化学习基础算法Q-Learning 详解
11．3．1 迷宫游戏的示例
11．3．2 强化学习中的局部*优
11．3．3 ε -Greedy 策略
11．3．4 Q-Learning 算法的伪代码
11．4 用Q-Learning 算法来解决冰湖挑战问题
11．4．1 环境的初始化
11．4．2 Q-Learning 算法的实现
11．4．3 Q-Table 的更新过程
11．5 从Q-Learning 算法到SARSA算法
11．5．1 异策略和同策略
11．5．2 SARSA 算法的实现
11．6 用SARSA 算法来解决冰湖挑战问题
11．7 Deep Q Network 算法：用深度网络实现Q-Learning
11．8 本课内容小结
11．9 课后练习
尾声：如何实现机器学习中的知识迁移及持续性的学习
练习答案
・ ・ ・ ・ ・ ・ (收起)目 录
草 1 蒲 血 督 学 习
第 1 章 机器学习及监督学习概论 3
1.1 机器学习 3
1.2 机器学习的分类 5
1.2.1 基本分类 5
1.2.2 按模型分类 10
1.2.3 按算法分类 11
1.2.4 按技巧分类 12
1.3 机器学习方法主要素 13
1.3.1 模型 13
1.3.2 策略 14
1.3.3 算法 16
1.4 模型评估与模型选择 17
1.4.1 训练误差与测试误差 17
1.4.2 过拟合与模型选择 18
1.5 正则化与交叉验证 20
1.5.1 正则化 20
1.5.2 交叉验证 20
1.6 泛化能力 21
1.6.1 泛化误差 21
1.6.2 泛化误差上界 22
1.7 生成模型与判别模型 24
1.8 监督学习应用 24
1.8.1 分类问题 24
1.8.2 标注问题 26
1.8.3 回归问题 27
本章概要 28
继续阅读 29
习题 29
参考文献 29
VIII 机器学习方法
第 2 章 感知机 30
2.1 感知机模型 30
2.2 感知机学习策略 31
2.2.1 数据集的线性可分性 31
2.2.2 感知机学习策略 31
2.3 感知机学习算法 32
2.3.1 感知机学习算法的原始形式 33
2.3.2 算法的收敛性 35
2.3.3 感知机学习算法的对偶形式 37
本章概要 39
继续阅读 40
习题 40
参考文献 40
第 3 章 k 近邻法 41
3.1 k 近邻算法 41
3.2 k 近邻模型 42
3.2.1 模型 42
3.2.2 距离度量 42
3.2.3 k 值的选择 43
3.2.4 分类决策规则 44
3.3 k 近邻法的实现：kd 树 44
3.3.1 构造 kd 树 45
3.3.2 搜索 kd 树 46
本章概要 48
继续阅读 48
习题 48
参考文献 49
第 4 章 朴素贝叶斯法 50
4.1 朴素贝叶斯法的学习与分类 50
4.1.1 基本方法 50
4.1.2 后验概率最大化的含义 51
4.2 朴素贝叶斯法的参数估计 52
4.2.1 极大似然估计 52
4.2.2 学习与分类算法 53
4.2.3 贝叶斯估计 54
本章概要 55
继续阅读 56
目录 IX
习题 56
参考文献 56
第 5 章 决策树 57
5.1 决策树模型与学习 57
5.1.1 决策树模型 57
5.1.2 决策树与 if-then 规则 58
5.1.3 决策树与条件概率分布 58
5.1.4 决策树学习 58
5.2 特征选择 60
5.2.1 特征选择问题 60
5.2.2 信息增益 61
5.2.3 信息增益比 64
5.3 决策树的生成 64
5.3.1 ID3 算法 65
5.3.2 C4.5 的生成算法 66
5.4 决策树的剪枝 66
5.5 CART 算法 68
5.5.1 CART 生成 69
5.5.2 CART 剪枝 72
本章概要 74
继续阅读 75
习题 75
参考文献 75
第 6 章 逻辑斯谛回归与最大烟模型 77
6.1 逻辑斯谛回归模型 77
6.1.1 逻辑斯谛分布 77
6.1.2 二项逻辑斯谛回归模型 78
6.1.3 模型参数估计 79
6.1.4 多项逻辑斯谛回归 79
6.2 最大煽模型 80
6.2.1 最大煽原理 80
6.2.2 最大煽模型的定义 82
6.2.3 最大煽模型的学习 83
6.2.4 极大似然估计 86
6.3 模型学习的最优化算法 87
6.3.1 改进的迭代尺度法 87
6.3.2 拟牛顿法 90
X 机器学习方法
本章概要 91
继续阅读 92
习题 92
参考文献 93
第 7 章 支持向量机 94
7.1 线性可分支持向量机与硬间隔最大化 94
7.1.1 线性可分支持向量机 94
7.1.2 函数间隔和儿何间隔 96
7.1.3 间隔最大化 97
7.1.4 学习的对偶算法 101
7.2 线性支持向量机与软间隔最大化 106
7.2.1 线性支持向量机 106
7.2.2 学习的对偶算法 107
7.2.3 支持向量 110
7.2.4 合页损失函数 111
7.3 非线性支持向量机与核函数 112
7.3.1 核技巧 112
7.3.2 正定核 115
7.3.3 常用核函数 118
7.3.4 非线性支持向量分类机 120
7.4 序列最小最优化算法 121
7.4.1 两个变量二次规划的求解方法 122
7.4.2 变量的选择方法 124
7.4.3 SMO 算法 126
本章概要 127
继续阅读 129
习题 129
参考文献 129
第 8 章 Boosting 131
AdaBoost 算法 131
8.1.1 Boosting 的基本思路 131
AdaBoost 算法 132
AdaBoost 的例子 134
8.2 AdaBoost 算法的训练误差分析 135
8.3 AdaBoost 算法的解释 137
8.3.1 前向分步算法 137
8.3.2 前向分步算法与 AdaBoost 138
目录 XI
8.4 提升树 140
8.4.1 提升树模型 140
8.4.2 提升树算法 140
8.4.3 梯度提升 144
本章概要 145
继续阅读 146
习题 146
参考文献 146
第 9 章 EM 算法及其推广 148
9.1 EM 算法的引入 148
9.1.1 EM 算法 148
9.1.2 EM 算法的导出 151
9.1.3 EM 算法在无监督学习中的应用 153
9.2 EM 算法的收敛性 153
9.3 EM 算法在高斯混合模型学习中的应用 154
9.3.1 高斯混合模型 155
9.3.2 高斯混合模型参数估计的 EM 算法 155
9.4 EM 算法的推广 158
9.4.1 F 函数的极大-极大算法 158
9.4.2 GEM 算法 160
本章概要 161
继续阅读 162
习题 162
参考文献 162
第 10 章 隐马尔可夫模型 163
10.1 隐马尔可夫模型的基本概念 163
10.1.1 隐马尔可夫模型的定义 163
10.1.2 观测序列的生成过程 166
10.1.3 隐马尔可夫模型的 3 个基本问题 166
10.2 概率计算算法 166
10.2.1 直接计算法 166
10.2.2 前向算法 167
10.2.3 后向算法 169
10.2.4 一些概率与期望值的计算 170
10.3 学习算法 172
10.3.1 监督学习方法 172
10.3.2 Baum-Welch 算法 172
XII 机器学习方法
10.3.3 Baum-Welch 模型参数估计公式 174
10.4 预测算法 175
10.4.1 近似算法 175
10.4.2 维特比算法 176
本章概要 179
继续阅读 179
习题 180
参考文献 180
第 11 章 条件随机场 181
11.1 概率无向图模型 181
11.1.1 模型定义 181
11.1.2 概率无向图模型的因子分解 183
11.2 条件随机场的定义与形式 184
11.2.1 条件随机场的定义 184
11.2.2 条件随机场的参数化形式 185
11.2.3 条件随机场的简化形式 186
11.2.4 条件随机场的矩阵形式 187
11.3 条件随机场的概率计算问题 189
11.3.1 前向-后向算法 189
11.3.2 概率计算 189
11.3.3 期望值的计算 190
11.4 条件随机场的学习算法 191
11.4.1 改进的迭代尺度法 191
11.4.2 拟牛顿法 194
11.5 条件随机场的预测算法 195
本章概要 197
继续阅读 198
习题 198
参考文献 199
第 12 章 监督学习方法总结 200
草 2 蒲 元元血血督学学习习
第 13 章 无监督学习概论 207
13.1 无监督学习基本原理 207
13.2 基本问题 208
13.3 机器学习主要素 210
13.4 无监督学习方法 210
目录 XIII
本章概要 214
继续阅读 215
参考文献 215
第 14 章 聚类方法 216
14.1 聚类的基本概念 216
14.1.1 相似度或距离 216
14.1.2 类或簇 219
14.1.3 类与类之间的距离 220
14.2 层次聚类 220
14.3 k 均值聚类 222
14.3.1 模型 222
14.3.2 策略 223
14.3.3 算法 224
14.3.4 算法特性 225
本章概要 226
继续阅读 227
习题 227
参考文献 227
第 15 章 奇异值分解 229
15.1 奇异值分解的定义与性质 229
15.1.1 定义与定理 229
15.1.2 紧奇异值分解与截断奇异值分解 233
15.1.3 儿何解释 235
15.1.4 主要性质 237
15.2 奇异值分解的计算 238
15.3 奇异值分解与矩阵近似 241
15.3.1 弗罗贝尼乌斯范数 241
15.3.2 矩阵的最优近似 242
15.3.3 矩阵的外积展开式 245
本章概要 247
继续阅读 248
习题 248
参考文献 249
第 16 章 主成分分析 250
16.1 总体主成分分析 250
16.1.1 基本想法 250
XIV 机器学习方法
16.1.2 定义和导出 252
16.1.3 主要性质 253
16.1.4 主成分的个数 257
16.1.5 规范化变量的总体主成分 260
16.2 样本主成分分析 260
16.2.1 样本主成分的定义和性质 261
16.2.2 相关矩阵的特征值分解算法 263
16.2.3 数据矩阵的奇异值分解算法 265
本章概要 267
继续阅读 269
习题 269
参考文献 269
第 17 章 潜在语义分析 271
17.1 单词向量空间与话题向量空间 271
17.1.1 单词向量空间 271
17.1.2 话题向量空间 273
17.2 潜在语义分析算法 276
17.2.1 矩阵奇异值分解算法 276
17.2.2 例子 278
17.3 非负矩阵分解算法 279
17.3.1 非负矩阵分解 279
17.3.2 潜在语义分析模型 280
17.3.3 非负矩阵分解的形式化 280
17.3.4 算法 281
本章概要 283
继续阅读 284
习题 284
参考文献 285
第 18 章 概率潜在语义分析 286
18.1 概率潜在语义分析模型 286
18.1.1 基本想法 286
18.1.2 生成模型 287
18.1.3 共现模型 288
18.1.4 模型性质 289
18.2 概率潜在语义分析的算法 291
本章概要 293
继续阅读 294
目录 XV
习题 294
参考文献 295
第 19 章 马尔可夫链蒙特卡罗法 296
19.1 蒙特卡罗法 296
19.1.1 随机抽样 296
19.1.2 数学期望估计 297
19.1.3 积分计算 298
19.2 马尔可夫链 299
19.2.1 基本定义 299
19.2.2 离散状态马尔可夫链 300
19.2.3 连续状态马尔可夫链 305
19.2.4 马尔可夫链的性质 306
19.3 马尔可夫链蒙特卡罗法 310
19.3.1 基本想法 310
19.3.2 基本步骤 311
19.3.3 马尔可夫链蒙特卡罗法与统计学习 311
Metropolis-Hastings 算法 312
19.4.1 基本原理 312
Metropolis-Hastings 算法 315
单分量 Metropolis-Hastings 算法 315
19.5 吉布斯抽样 316
19.5.1 基本原理 316
19.5.2 吉布斯抽样算法 318
19.5.3 抽样计算 319
本章概要 320
继续阅读 321
习题 321
参考文献 322
第 20 章 潜在狄利克雷分配 324
20.1 狄利克雷分布 324
20.1.1 分布定义 324
20.1.2 共辄先验 327
20.2 潜在狄利克雷分自模型 328
20.2.1 基本想法 328
20.2.2 模型定义 329
20.2.3 概率图模型 331
20.2.4 随机变量序列的可交换性 332
XVI 机器学习方法
20.2.5 概率公式 332
20.3 LDA 的吉布斯抽样算法 333
20.3.1 基本想法 333
20.3.2 算法的主要部分 334
20.3.3 算法的后处理 336
20.3.4 算法 337
20.4 LDA 的变分 EM 算法 338
20.4.1 变分推理 338
20.4.2 变分 EM 算法 339
20.4.3 算法推导 340
20.4.4 算法总结 346
本章概要 346
继续阅读 348
习题 348
参考文献 348
第 21 章 PageRank 算法 349
PageRank 的定义 349
21.1.1 基本想法 349
21.1.2 有向图和随机游走模型 350
21.1.3 PageRank 的基本定义 352
21.1.4 PageRank 的一般定义 354
PageRank 的计算 355
21.2.1 迭代算法 355
21.2.2 幕法 357
21.2.3 代数算法 361
本章概要 362
继续阅读 363
习题 363
参考文献 364
第 22 章 无监督学习方法总结 365
22.1 无监督学习方法的关系和特点 365
22.1.1 各种方法之间的关系 365
22.1.2 无监督学习方法 366
22.1.3 基础机器学习方法 366
22.2 话题模型之间的关系和特点 367
参考文献 368
目录 XVII
草 3 蒲 深 反 学 习
第 23 章 前馈神经网络 371
23.1 前馈神经网络的模型 371
23.1.1 前馈神经网络定义 372
23.1.2 前馈神经网络的例子 381
23.1.3 前馈神经网络的表示能力 386
23.2 前馈神经网络的学习算法 389
23.2.1 前馈神经网络学习 389
23.2.2 前馈神经网络学习的优化算法 391
23.2.3 反向传播算法 393
23.2.4 在计算图上的实现 397
23.2.5 算法的实现技巧 401
23.3 前馈神经网络学习的正则化 406
23.3.1 深度学习中的正则化 406
23.3.2 早停法 406
23.3.3 暂返法 408
本章概要 410
继续阅读 413
习题 413
参考文献 414
第 24 章 卷积神经网络 415
24.1 卷积神经网络的模型 415
24.1.1 背景 415
24.1.2 卷积 416
24.1.3 汇聚 424
24.1.4 卷积神经网络 427
24.1.5 卷积神经网络性质 430
24.2 卷积神经网络的学习算法 432
24.2.1 卷积导数 432
24.2.2 反向传播算法 433
24.3 图像分类中的应用 436
24.3.1 AlexNet 436
24.3.2 残差网络 437
本章概要 441
继续阅读 443
习题 443
参考文献 445
XVIII 机器学习方法
第 25 章 循环神经网络 447
25.1 简单循环神经网络 447
25.1.1 模型 447
25.1.2 学习算法 450
25.2 常用循环神经网络 454
25.2.1 长短期记忆网络 454
25.2.2 门控循环单元网络 457
25.2.3 深度循环神经网络 458
25.2.4 双向循环神经网络 459
25.3 自然语言生成中的应用 460
25.3.1 词向量 460
25.3.2 语言模型与语言生成 463
本章概要 465
继续阅读 467
习题 467
参考文献 468
第 26 章 序列到序列模型 469
26.1 序列到序列基本模型 469
26.1.1 序列到序列学习 469
26.1.2 基本模型 471
RNN Search 模型 472
26.2.1 注意力 472
26.2.2 模型定义 474
26.2.3 模型特点 475
Transformer 模型 475
26.3.1 模型架构 476
26.3.2 模型特点 482
本章概要 483
继续阅读 486
习题 486
参考文献 486
第 27 章 预训练语言模型 488
27.1 GPT 模型 488
27.1.1 预训练语言模型 488
27.1.2 模型和学习 490
27.2 BERT 模型 493
27.2.1 去噪自动编码器 493
27.2.2 模型和学习 495
目录 XIX
27.2.3 模型特点 499
本章概要 500
继续阅读 502
习题 502
参考文献 502
第 28 章 生成对抗网络 504
28.1 GAN 基本模型 504
28.1.1 模型 504
28.1.2 学习算法 506
28.1.3 理论分析 507
28.2 图像生成中的应用 508
28.2.1 转置卷积 509
28.2.2 DCGAN 511
本章概要 513
继续阅读 514
习题 514
参考文献 515
第 29 章 深度学习方法总结 516
29.1 深度学习的模型 516
29.2 深度学习的方法 518
29.3 深度学习的优化算法 520
29.4 深度学习的优缺点 522
参考文献 523
附录 A 梯度下降法 524
附录 B 牛顿法和拟牛顿法 526
附录 C 拉格朗日对偶性 531
附录 D 矩阵的基本子空间 534
附录 E KL 散度的定义和狄利克雷分布的性质 537
附录 F 软最大化函数的偏导数和交叉烟损失函数的偏导数 539
索引 541
・ ・ ・ ・ ・ ・ (收起)目录
前言 ix
第 1 章　引言 1
1.1　为何选择机器学习 1
1.1.1　机器学习能够解决的问题 2
1.1.2　熟悉任务和数据 4
1.2　为何选择Python 4
1.3　scikit-learn 4
1.4　必要的库和工具 5
1.4.1　Jupyter Notebook 6
1.4.2　NumPy 6
1.4.3　SciPy 6
1.4.4　matplotlib 7
1.4.5　pandas 8
1.4.6　mglearn 9
1.5　Python 2 与Python 3 的对比 9
1.6　本书用到的版本 10
1.7　第 一个应用：鸢尾花分类 11
1.7.1　初识数据 12
1.7.2　衡量模型是否成功：训练数据与测试数据 14
1.7.3　要事第 一：观察数据 15
1.7.4　构建第 一个模型：k 近邻算法 16
1.7.5　做出预测 17
1.7.6　评估模型 18
1.8　小结与展望 19
第 2 章　监督学习 21
2.1　分类与回归 21
2.2　泛化、过拟合与欠拟合 22
2.3　监督学习算法 24
2.3.1　一些样本数据集 25
2.3.2　k 近邻 28
2.3.3　线性模型 35
2.3.4　朴素贝叶斯分类器 53
2.3.5　决策树 54
2.3.6　决策树集成 64
2.3.7　核支持向量机 71
2.3.8　神经网络（深度学习） 80
2.4　分类器的不确定度估计 91
2.4.1　决策函数 91
2.4.2　预测概率 94
2.4.3　多分类问题的不确定度 96
2.5　小结与展望 98
第3 章　无监督学习与预处理 100
3.1　无监督学习的类型 100
3.2　无监督学习的挑战 101
3.3　预处理与缩放 101
3.3.1　不同类型的预处理 102
3.3.2　应用数据变换 102
3.3.3　对训练数据和测试数据进行相同的缩放 104
3.3.4　预处理对监督学习的作用 106
3.4　降维、特征提取与流形学习 107
3.4.1　主成分分析 107
3.4.2　非负矩阵分解 120
3.4.3　用t-SNE 进行流形学习 126
3.5　聚类 130
3.5.1　k 均值聚类 130
3.5.2　凝聚聚类 140
3.5.3　DBSCAN 143
3.5.4　聚类算法的对比与评估 147
3.5.5　聚类方法小结 159
3.6　小结与展望 159
第4 章　数据表示与特征工程 161
4.1　分类变量 161
4.1.1　One-Hot 编码（虚拟变量） 162
4.1.2　数字可以编码分类变量 166
4.2　分箱、离散化、线性模型与树 168
4.3　交互特征与多项式特征 171
4.4　单变量非线性变换 178
4.5　自动化特征选择 181
4.5.1　单变量统计 181
4.5.2　基于模型的特征选择 183
4.5.3　迭代特征选择 184
4.6　利用专家知识 185
4.7　小结与展望 192
第5 章　模型评估与改进 193
5.1　交叉验证 194
5.1.1　scikit-learn 中的交叉验证 194
5.1.2　交叉验证的优点 195
5.1.3　分层k 折交叉验证和其他策略 196
5.2　网格搜索 200
5.2.1　简单网格搜索 201
5.2.2　参数过拟合的风险与验证集 202
5.2.3　带交叉验证的网格搜索 203
5.3　评估指标与评分 213
5.3.1　牢记目标 213
5.3.2　二分类指标 214
5.3.3　多分类指标 230
5.3.4　回归指标 232
5.3.5　在模型选择中使用评估指标 232
5.4　小结与展望 234
第6 章　算法链与管道 236
6.1　用预处理进行参数选择 237
6.2　构建管道 238
6.3　在网格搜索中使用管道 239
6.4　通用的管道接口 242
6.4.1　用make_pipeline 方便地创建管道 243
6.4.2　访问步骤属性 244
6.4.3　访问网格搜索管道中的属性 244
6.5　网格搜索预处理步骤与模型参数 246
6.6　网格搜索选择使用哪个模型 248
6.7　小结与展望 249
第7 章　处理文本数据 250
7.1　用字符串表示的数据类型 250
7.2　示例应用：电影评论的情感分析 252
7.3　将文本数据表示为词袋 254
7.3.1　将词袋应用于玩具数据集 255
7.3.2　将词袋应用于电影评论 256
7.4　停用词 259
7.5　用tf-idf 缩放数据 260
7.6　研究模型系数 263
7.7　多个单词的词袋（n 元分词） 263
7.8　分词、词干提取与词形还原 267
7.9　主题建模与文档聚类 270
7.10　小结与展望 277
第8 章　全书总结 278
8.1　处理机器学习问题 278
8.2　从原型到生产 279
8.3　测试生产系统 280
8.4　构建你自己的估计器 280
8.5　下一步怎么走 281
8.5.1　理论 281
8.5.2　其他机器学习框架和包 281
8.5.3　排序、推荐系统与其他学习类型 282
8.5.4　概率建模、推断与概率编程 282
8.5.5　神经网络 283
8.5.6　推广到更大的数据集 283
8.5.7　磨练你的技术 284
8.6　总结 284
关于作者 285
关于封面 285
・ ・ ・ ・ ・ ・ (收起)序（王斌 小米AI 实验室主任、NLP 首席科学家）
前言
主要符号表
第1章 绪论
式(1.1)
式(1.2)
第2章 模型评估与选择
式(2.20)
式(2.21)
式(2.27)
式(2.41)
附注
参考文献
第3章 线性模型
式(3.5)
式(3.6)
式(3.7)
式(3.10)
式(3.27)
式(3.30)
式(3.32)
式(3.37)
式(3.38)
式(3.39)
式(3.43)
式(3.44)
式(3.45)
第4章 决策树
式(4.1)
式(4.2)
式(4.6)
式(4.7)
式(4.8)
附注
参考文献
第5章 神经网络
式(5.2)
式(5.10)
式(5.12)
式(5.13)
式(5.14)
式(5.15)
ii j 目录
式(5.20)
式(5.22)
式(5.23)
式(5.24)
附注
参考文献
第6章 支持向量机
式(6.9)
式(6.10)
式(6.11)
式(6.13)
式(6.35)
式(6.37)
式(6.38)
式(6.39)
式(6.40)
式(6.41)
式(6.52)
式(6.60)
式(6.62)
式(6.63)
式(6.65)
式(6.66)
式(6.67)
式(6.70)
附注
参考文献
第7章 贝叶斯分类器
式(7.5)
式(7.6)
式(7.12)
式(7.13)
式(7.19)
式(7.20)
式(7.24)
式(7.25)
式(7.27)
式(7.34)
附注
参考文献
第8章 集成学习
式(8.1)
式(8.2)
式(8.3)
式(8.4)
式(8.5)
式(8.6)
式(8.7)
式(8.8)
式(8.9)
式(8.10)
式(8.11)
式(8.12)
式(8.13)
式(8.14)
式(8.16)
式(8.17)
式(8.18)
式(8.19)
式(8.20)
式(8.21)
式(8.22)
式(8.23)
式(8.24)
式(8.25)
式(8.26)
式(8.27)
式(8.28)
式(8.29)
式(8.30)
式(8.31)
式(8.32)
式(8.33)
式(8.34)
式(8.35)
式(8.36)
第9章 聚类
式(9.5)
式(9.6)
式(9.7)
式(9.8)
式(9.33)
式(9.34)
式(9.35)
式(9.38)
第10章 降维与度量学习
式(10.1)
式(10.2)
式(10.3)
式(10.4)
式(10.5)
式(10.6)
式(10.10)
式(10.14)
式(10.17)
式(10.24)
式(10.28)
式(10.31)
第11章 特征选择与稀疏学习
式(11.1)
式(11.2)
式(11.5)
式(11.6)
式(11.7)
式(11.10)
式(11.11)
式(11.12)
式(11.13)
式(11.14)
式(11.15)
式(11.16)
式(11.17)
式(11.18)
第12章 计算学习理论
式(12.1)
式(12.2)
式(12.3)
式(12.4)
式(12.5)
式(12.7)
式(12.9)
式(12.10)
式(12.11)
式(12.12)
式(12.13)
式(12.14)
式(12.15)
式(12.16)
式(12.17)
式(12.18)
式(12.19)
式(12.20)
式(12.21)
式(12.22)
式(12.23)
式(12.24)
式(12.25)
式(12.26)
式(12.27)
式(12.28)
式(12.29)
式(12.30)
式(12.31)
式(12.32)
式(12.34)
式(12.36)
式(12.37)
式(12.38)
式(12.39)
式(12.40)
式(12.41)
式(12.42)
式(12.43)
式(12.44)
式(12.45)
式(12.46)
式(12.52)
式(12.53)
式(12.57)
式(12.58)
式(12.59)
式(12.60)
参考文献
第13章 半监督学习
式(13.1)
式(13.2)
式(13.3)
式(13.4)
式(13.5)
式(13.6)
式(13.7)
式(13.8)
式(13.9)
式(13.12)
式(13.13)
式(13.14)
式(13.15)
式(13.16)
式(13.17)
式(13.20)
第14章 概率图模型
式(14.1)
式(14.2)
式(14.3)
式(14.4)
式(14.7)
式(14.8)
式(14.9)
式(14.10)
式(14.14)
式(14.15)
式(14.16)
式(14.17)
式(14.18)
式(14.19)
式(14.20)
式(14.22)
式(14.26)
式(14.27)
式(14.28)
式(14.29)
式(14.30)
式(14.31)
式(14.32)
式(14.33)
式(14.34)
式(14.35)
式(14.36)
式(14.37)
式(14.38)
式(14.39)
式(14.40)
式(14.41)
式(14.42)
式(14.43)
式(14.44)
第15章 规则学习
式(15.2)
式(15.3)
式(15.6)
式(15.7)
式(15.9)
式(15.10)
式(15.11)
式(15.12)
式(15.13)
式(15.14)
式(15.16)
第16章 强化学习
式(16.2)
式(16.3)
式(16.4)
式(16.7)
式(16.8)
式(16.10)
式(16.14)
式(16.16)
式(16.31)
・ ・ ・ ・ ・ ・ (收起)序
前言
第1章 概率思想：构建理论基础 1
1.1 理论基石：条件概率、独立性与贝叶斯 1
1.1.1 从概率到条件概率 1
1.1.2 条件概率的具体描述 2
1.1.3 条件概率的表达式分析 3
1.1.4 两个事件的独立性 4
1.1.5 从条件概率到全概率公式 5
1.1.6 聚焦贝叶斯公式 6
1.1.7 本质内涵：由因到果，由果推因 7
1.2 事件的关系：深入理解独立性 8
1.2.1 重新梳理两个事件的独立性 8
1.2.2 不相容与独立性 8
1.2.3 条件独立 9
1.2.4 独立与条件独立 11
1.2.5 独立重复实验 11
第2章 变量分布：描述随机世界 13
2.1 离散型随机变量：分布与数字特征 13
2.1.1 从事件到随机变量 13
2.1.2 离散型随机变量及其要素 14
2.1.3 离散型随机变量的分布列 15
2.1.4 分布列和概率质量函数 16
2.1.5 二项分布及二项随机变量 17
2.1.6 几何分布及几何随机变量 21
2.1.7 泊松分布及泊松随机变量 24
2.2 连续型随机变量：分布与数字特征 27
2.2.1 概率密度函数 27
2.2.2 连续型随机变量区间概率的计算 29
2.2.3 连续型随机变量的期望与方差 29
2.2.4 正态分布及正态随机变量 30
2.2.5 指数分布及指数随机变量 33
2.2.6 均匀分布及其随机变量 35
2.3 多元随机变量（上）：联合、边缘与条件 38
2.3.1 实验中引入多个随机变量 38
2.3.2 联合分布列 38
2.3.3 边缘分布列 39
2.3.4 条件分布列 40
2.3.5 集中梳理核心的概率理论 44
2.4 多元随机变量（下）：独立与相关 46
2.4.1 随机变量与事件的独立性 46
2.4.2 随机变量之间的独立性 47
2.4.3 独立性示例 48
2.4.4 条件独立的概念 48
2.4.5 独立随机变量的期望和方差 50
2.4.6 随机变量的相关性分析及量化方法 52
2.4.7 协方差及协方差矩阵 52
2.4.8 相关系数的概念 54
2.5 多元随机变量实践：聚焦多元正态分布 55
2.5.1 再谈相关性：基于二元标准正态分布 55
2.5.2 二元一般正态分布 57
2.5.3 聚焦相关系数 60
2.5.4 独立和相关性的关系 64
2.6 多元高斯分布：参数特征和几何意义 66
2.6.1 从一元分布到多元分布 66
2.6.2 多元高斯分布的参数形式 67
2.6.3 二元高斯分布的具体示例 68
2.6.4 多元高斯分布的几何特征 71
2.6.5 二元高斯分布几何特征实例分析 74
第3章 参数估计：探寻最大可能 77
3.1 极限思维：大数定律与中心极限定理 77
3.1.1 一个背景话题 77
3.1.2 大数定律 78
3.1.3 大数定律的模拟 80
3.1.4 中心极限定理 83
3.1.5 中心极限定理的工程意义 84
3.1.6 中心极限定理的模拟 85
3.1.7 大数定律的应用：蒙特卡罗方法 86
3.2 推断未知：统计推断的基本框架 89
3.2.1 进入统计学 89
3.2.2 统计推断的例子 90
3.2.3 统计推断中的一些重要概念 91
3.2.4 估计量的偏差与无偏估计 92
3.2.5 总体均值的估计 93
3.2.6 总体方差的估计 95
3.3 极大似然估计 100
3.3.1 极大似然估计法的引例 100
3.3.2 似然函数的由来 102
3.3.3 极大似然估计的思想 103
3.3.4 极大似然估计值的计算 105
3.3.5 简单极大似然估计案例 106
3.3.6 高斯分布参数的极大似然估计 107
3.4 含有隐变量的参数估计问题 110
3.4.1 参数估计问题的回顾 110
3.4.2 新情况：场景中含有隐变量 111
3.4.3 迭代法：解决含有隐变量情形的抛硬币问题 112
3.4.4 代码实验 115
3.5 概率渐增：EM算法的合理性 118
3.5.1 EM算法的背景介绍 119
3.5.2 先抛出EM算法的迭代公式 119
3.5.3 EM算法为什么是有效的 120
3.6 探索EM公式的底层逻辑与由来 123
3.6.1 EM公式中的E步和M步 124
3.6.2 剖析EM算法的由来 124
3.7 探索高斯混合模型：EM 迭代实践 127
3.7.1 高斯混合模型的引入 128
3.7.2 从混合模型的角度看内部机理 129
3.7.3 高斯混合模型的参数估计 131
3.8 高斯混合模型的参数求解 132
3.8.1 利用 EM 迭代模型参数的思路 132
3.8.2 参数估计示例 136
3.8.3 高斯混合模型的应用场景 139
第4章 随机过程：聚焦动态特征 145
4.1 由静向动：随机过程导引 145
4.1.1 随机过程场景举例1：博彩 146
4.1.2 随机过程场景举例2：股价的变化 150
4.1.3 随机过程场景举例3：股价变化过程的展现 152
4.1.4 两类重要的随机过程概述 154
4.2 状态转移：初识马尔可夫链 155
4.2.1 马尔可夫链三要素 155
4.2.2 马尔可夫性：灵魂特征 156
4.2.3 转移概率和状态转移矩阵 157
4.2.4 马尔可夫链性质的总结 158
4.2.5 一步到达与多步转移的含义 159
4.2.6 多步转移与矩阵乘法 160
4.2.7 路径概率问题 163
4.3 变与不变：马尔可夫链的极限与稳态 164
4.3.1 极限与初始状态无关的情况 164
4.3.2 极限依赖于初始状态的情况 165
4.3.3 吸收态与收敛分析 167
4.3.4 可达与常返 168
4.3.5 周期性问题 171
4.3.6 马尔可夫链的稳态分析和判定 172
4.3.7 稳态的求法 174
4.4 隐马尔可夫模型：明暗两条线 176
4.4.1 从马尔可夫链到隐马尔可夫模型 176
4.4.2 典型实例1：盒子摸球实验 177
4.4.3 典型实例2：小宝宝的日常生活 180
4.4.4 隐马尔可夫模型的外在表征 181
4.4.5 推动模型运行的内核三要素 182
4.4.6 关键性质：齐次马尔可夫性和观测独立性 183
4.5 概率估计：隐马尔可夫模型观测序列描述 183
4.5.1 隐马尔可夫模型的研究内容 183
4.5.2 模型研究问题的描述 185
4.5.3 一个直观的思路 186
4.5.4 更优的方法：前向概率算法 187
4.5.5 概率估计实践 190
4.5.6 代码实践 192
4.6 状态解码：隐马尔可夫模型隐状态揭秘 194
4.6.1 隐状态解码问题的描述 194
4.6.2 最大路径概率与维特比算法 195
4.6.3 应用维特比算法进行解码 197
4.6.4 维特比算法的案例实践 199
4.6.5 代码实践 202
4.7 连续域上的无限维：高斯过程 204
4.7.1 高斯过程的一个实际例子 205
4.7.2 高斯过程的核心要素和严谨描述 206
4.7.3 径向基函数的代码演示 207
4.7.4 高斯过程回归原理详解 208
4.7.5 高斯过程回归代码演示 210
第5章 统计推断：贯穿近似策略 215
5.1 统计推断的基本思想和分类 215
5.1.1 统计推断的根源和场景 215
5.1.2 后验分布：推断过程的关注重点 216
5.1.3 精确推断和近似推断 216
5.1.4 确定性近似：变分推断概述 217
5.2 随机近似方法 219
5.2.1 蒙特卡罗方法的理论支撑 219
5.2.2 随机近似的核心：蒙特卡罗 220
5.2.3 接受-拒绝采样的问题背景 221
5.2.4 接受-拒绝采样的方法和步骤 221
5.2.5 接受-拒绝采样的实践 222
5.2.6 接受-拒绝采样方法背后的内涵挖掘 225
5.2.7 重要性采样 226
5.2.8 两种采样方法的问题及思考 227
5.3 采样绝佳途径：借助马尔可夫链的稳态性质 228
5.3.1 马尔可夫链回顾 228
5.3.2 核心：马尔可夫链的平稳分布 229
5.3.3 马尔可夫链进入稳态的转移过程 231
5.3.4 稳态及转移过程演示 231
5.3.5 马尔可夫链稳态的价值和意义 235
5.3.6 基于马尔可夫链进行采样的原理分析 236
5.3.7 采样过程实践与分析 238
5.3.8 一个显而易见的问题和难点 242
5.4 马尔可夫链-蒙特卡罗方法详解 242
5.4.1 稳态判定：细致平稳条件 243
5.4.2 Metropolis-Hastings采样方法的原理 244
5.4.3 如何理解随机游走叠加接受概率 245
5.4.4 如何实现随机游走叠加接受概率 247
5.4.5 建议转移概率矩阵Q的设计 247
5.4.6 Metropolis-Hastings方法的步骤和代码演示 251
5.5 Gibbs采样方法简介 253
5.5.1 Gibbs方法核心流程 253
5.5.2 Gibbs采样的合理性 255
5.5.3 Gibbs采样代码实验 256
・ ・ ・ ・ ・ ・ (收起)第1章　初识Python与Jupyter 1
1.1　Python概要 2
1.1.1　为什么要学习Python 2
1.1.2　Python中常用的库 2
1.2　Python的版本之争 4
1.3　安装Anaconda 5
1.3.1　Linux环境下的Anaconda安装 5
1.3.2　conda命令的使用 6
1.3.3　Windows环境下的Anaconda安装 7
1.4　运行Python 11
1.4.1　验证Python 11
1.4.2　Python版本的Hello World 12
1.4.3　Python的脚本文件 13
1.4.4　代码缩进 15
1.4.5　代码注释 17
1.5　Python中的内置函数 17
1.6　文学化编程―Jupyter 20
1.6.1　Jupyter的由来 20
1.6.2　Jupyter的安装 21
1.6.3　Jupyter的使用 23
1.6.4　Markdown编辑器 26
1.7　Jupyter中的魔法函数 31
1.7.1　%lsmagic函数 31
1.7.2　%matplotlib inline函数 32
1.7.3　%timeit函数 32
1.7.4　%%writefile函数 33
1.7.5　其他常用的魔法函数 34
1.7.6　在Jupyter中执行shell命令 35
1.8　本章小结 35
1.9　思考与提高 36
第2章　数据类型与程序控制结构 40
2.1　为什么需要不同的数据类型 41
2.2　Python中的基本数据类型 42
2.2.1　数值型（Number） 42
2.2.2　布尔类型（Boolean） 45
2.2.3　字符串型（String） 45
2.2.4　列表（List） 49
2.2.5　元组（Tuple） 59
2.2.6　字典（Dictionary） 62
2.2.7　集合（Set） 65
2.3　程序控制结构 67
2.3.1　回顾那段难忘的历史 67
2.3.2 顺序结构 69
2.3.3 选择结构 70
2.3.4 循环结构 74
2.4　高效的推导式 80
2.4.1　列表推导式 80
2.4.2　字典推导式 83
2.4.3　集合推导式 83
2.5　本章小结 84
2.6　思考与提高 84
第3章　自建Python模块与第三方模块 90
3.1　导入Python标准库 91
3.2　编写自己的模块 93
3.3　模块的搜索路径 97
3.4　创建模块包 100
3.5　常用的内建模块 103
3.5.1 collection模块 103
3.5.2　datetime模块 110
3.5.3　json模块 115
3.5.4　random模块 118
3.6　本章小结 121
3.7　思考与提高 122
第4章　Python函数 124
4.1　Python中的函数 125
4.1.1　函数的定义 125
4.1.2　函数返回多个值 127
4.1.3 函数文档的构建 128
4.2　函数参数的“花式”传递 132
4.2.1　关键字参数 132
4.2.2　可变参数 133
4.2.3　默认参数 136
4.2.4　参数序列的打包与解包 138
4.2.5　传值还是传引用 142
4.3　函数的递归 146
4.3.1 感性认识递归 146
4.3.2 思维与递归思维 148
4.3.3 递归调用的函数 149
4.4　函数式编程的高阶函数 151
4.4.1　lambda表达式 152
4.4.2 filter()函数 153
4.4.3 map()函数 155
4.4.4 reduce()函数 157
4.4.5 sorted()函数 158
4.5 本章小结 159
4.6 思考与提高 160
第5章　Python高级特性 165
5.1　面向对象程序设计 166
5.1.1　面向过程与面向对象之辩 166
5.1.2 类的定义与使用 169
5.1.3 类的继承 173
5.2　生成器与迭代器 176
5.2.1 生成器 176
5.2.2 迭代器 183
5.3 文件操作 187
5.3.1 打开文件 187
5.3.2 读取一行与读取全部行 191
5.3.3 写入文件 193
5.4　异常处理 193
5.4.1 感性认识程序中的异常 194
5.4.2 异常处理的三步走 195
5.5 错误调试 197
5.5.1 利用print()输出观察变量 197
5.5.2 assert断言 198
5.6　本章小结 201
5.7　思考与提高 202
第6章　NumPy向量计算 204
6.1　为何需要NumPy 205
6.2　如何导入NumPy 205
6.3 生成NumPy数组 206
6.3.1　利用序列生成 206
6.3.2　利用特定函数生成 207
6.3.3 Numpy数组的其他常用函数 209
6.4　N维数组的属性 212
6.5 NumPy数组中的运算 215
6.5.1 向量运算 216
6.5.2 算术运算 216
6.5.3 逐元素运算与张量点乘运算 218
6.6　爱因斯坦求和约定 222
6.6.1　不一样的标记法 222
6.6.2　NumPy中的einsum()方法 224
6.7 NumPy中的“轴”方向 231
6.8 操作数组元素 234
6.8.1 通过索引访问数组元素 234
6.8.2 NumPy中的切片访问 236
6.8.3 二维数组的转置与展平 238
6.9 NumPy中的广播 239
6.10 NumPy数组的高级索引 242
6.10.1 “花式”索引 242
6.10.2 布尔索引 247
6.11 数组的堆叠操作 249
6.11.1 水平方向堆叠hstack() 250
6.11.2 垂直方向堆叠vstack() 251
6.11.3 深度方向堆叠hstack() 252
6.11.4 列堆叠与行堆叠 255
6.11.5 数组的分割操作 257
6.12 NumPy中的随机数模块 264
6.13 本章小结 266
6.14 思考与提高 267
第7章　Pandas数据分析 271
7.1 Pandas简介 272
7.2 Pandas的安装 272
7.3 Series类型数据 273
7.3.1 Series的创建 273
7.3.2 Series中的数据访问 277
7.3.3 Series中的向量化操作与布尔索引 280
7.3.4 Series中的切片操作 283
7.3.5 Series中的缺失值 284
7.3.6 Series中的删除与添加操作 286
7.3.7 Series中的name属性 288
7.4 DataFrame 类型数据 289
7.4.1 构建DataFrame 289
7.4.2 访问DataFrame中的列与行 293
7.4.3 DataFrame中的删除操作 298
7.4.4 DataFrame中的“轴”方向 301
7.4.5 DataFrame中的添加操作 303
7.5 基于Pandas的文件读取与分析 310
7.5.1 利用Pandas读取文件 311
7.5.2 DataFrame中的常用属性 312
7.5.3 DataFrame中的常用方法 314
7.5.4 DataFrame的条件过滤 318
7.5.5 DataFrame的切片操作 320
7.5.6 DataFrame的排序操作 323
7.5.7 Pandas的聚合和分组运算 325
7.5.8 DataFrame的透视表 334
7.5.9 DataFrame的类SQL操作 339
7.5.10 DataFrame中的数据清洗方法 341
7.6 泰坦尼克幸存者数据预处理 342
7.6.1 数据集简介 342
7.6.2 数据集的拼接 344
7.6.3 缺失值的处理 350
7.7 本章小结 353
7.8 思考与提高 353
第8章　Matplotlib与Seaborn可视化分析 365
8.1 Matplotlib与图形绘制 366
8.2 绘制简单图形 366
8.3 pyplot的高级功能 371
8.3.1 添加图例与注释 371
8.3.2 设置图形标题及坐标轴 374
8.3.3 添加网格线 378
8.3.4 绘制多个子图 380
8.3.5 Axes与Subplot的区别 382
8.4 散点图 388
8.5 条形图与直方图 392
8.5.1 垂直条形图 392
8.5.2 水平条形图 394
8.5.3 并列条形图 395
8.5.4 叠加条形图 400
8.5.5 直方图 402
8.6 饼图 407
8.7 箱形图 409
8.8 误差条 411
8.9 绘制三维图形 413
8.10 与Pandas协作绘图―以谷歌流感趋势数据为例 416
8.10.1 谷歌流感趋势数据描述 416
8.10.2 导入数据与数据预处理 417
8.10.3 绘制时序曲线图 421
8.10.4 选择合适的数据可视化表达 423
8.10.5 基于条件判断的图形绘制 427
8.10.6 绘制多个子图 430
8.11 惊艳的Seaborn 431
8.11.1 pairplot（对图） 432
8.11.2 heatmap（热力图） 434
8.11.3 boxplot（箱形图） 436
8.11.4 violin plot（小提琴图） 442
8.11.5 Density Plot（密度图） 446
8.12 本章小结 450
8.13 思考与提高 450
第9章　机器学习初步 459
9.1 机器学习定义 460
9.1.1　什么是机器学习 460
9.1.2 机器学习的三个步骤 461
9.1.3 传统编程与机器学习的差别 464
9.1.4 为什么机器学习不容易 465
9.2　监督学习 467
9.2.1　感性认识监督学习 467
9.2.2　监督学习的形式化描述 468
9.2.3 损失函数 470
9.3　非监督学习 471
9.4　半监督学习 473
9.5 机器学习的哲学视角 474
9.6 模型性能评估 476
9.6.1 经验误差与测试误差 476
9.6.2 过拟合与欠拟合 477
9.6.3 模型选择与数据拟合 479
9.7 性能度量 480
9.7.1　二分类的混淆矩阵 480
9.7.2 查全率、查准率与F1分数 481
9.7.3 P-R曲线 484
9.7.4 ROC曲线 485
9.7.5 AUC 489
9.8　本章小结 489
9.9　思考与提高 490
第10章　sklearn与经典机器学习算法 492
10.1 机器学习的利器―sklearn 493
10.1.1 sklearn简介 494
10.1.2 sklearn的安装 496
10.2 线性回归 497
10.2.1　线性回归的概念 497
10.2.2 使用sklearn实现波士顿房价预测 499
10.3　k-近邻算法 516
10.3.1 算法简介 516
10.3.2　k值的选取 518
10.3.3　特征数据的归一化 519
10.3.4　邻居距离的度量 521
10.3.5 分类原则的制定 522
10.3.6　基于sklearn的k-近邻算法实战 522
10.4 Logistic回归 527
10.4.1 为什么需要Logistic回归 527
10.4.2 Logistic源头初探 529
10.4.3 Logistic回归实战 532
10.5 神经网络学习算法 536
10.5.1 人工神经网络的定义 537
10.5.2 神经网络中的“学习”本质 537
10.5.3 神经网络结构的设计 540
10.5.4 利用sklearn搭建多层神经网络 541
10.6 非监督学习的代表―k均值聚类 550
10.6.1 聚类的基本概念 551
10.6.2 簇的划分 552
10.6.3 k均值聚类算法核心 552
10.6.4 k均值聚类算法优缺点 554
10.6.5 基于sklearn的k均值聚类算法实战 555
10.7 本章小结 561
10.8 思考与提高 562
・ ・ ・ ・ ・ ・ (收起)赛题一 工业蒸汽量预测
1 赛题理解 2
1.1 赛题背景 2
1.2 赛题目标 2
1.3 数据概览 2
1.4 评估指标 3
1.5 赛题模型 4
2 数据探索 6
2.1 理论知识 6
2.1.1 变量识别 6
2.1.2 变量分析 6
2.1.3 缺失值处理 10
2.1.4 异常值处理 11
2.1.5 变量转换 14
2.1.6 新变量生成 15
2.2 赛题数据探索 16
2.2.1 导入工具包 16
2.2.2 读取数据 16
2.2.3 查看数据 16
2.2.4 可视化数据分布 18
2.2.5 查看特征变量的相关性 26
3 特征工程 33
3.1 特征工程的重要性和处理 33
3.2 数据预处理和特征处理 33
3.2.1 数据预处理 33
3.2.2 特征处理 34
3.3 特征降维 38
3.3.1 特征选择 39
3.3.2 线性降维 44
3.4 赛题特征工程 45
3.4.1 异常值分析 45
3.4.2 最大值和最小值的
归一化 46
3.4.3 查看数据分布 47
3.4.4 特征相关性 48
3.4.5 特征降维 48
3.4.6 多重共线性分析 49
3.4.7 PCA处理 50
4 模型训练 52
4.1 回归及相关模型 52
4.1.1 回归的概念 52
4.1.2 回归模型训练和预测 52
4.1.3 线性回归模型 52
4.1.4 K近邻回归模型 54
4.1.5 决策树回归模型 55
4.1.6 集成学习回归模型 58
4.2 赛题模型训练 61
4.2.1 导入相关库 61
4.2.2 切分数据 62
4.2.3 多元线性回归 62
4.2.4 K近邻回归 62
4.2.5 随机森林回归 63
4.2.6 LGB模型回归 63
5 模型验证 64
5.1 模型评估的概念和方法 64
5.1.1 欠拟合与过拟合 64
5.1.2 模型的泛化与正则化 68
5.1.3 回归模型的评估指标和
调用方法 70
5.1.4 交叉验证 72
5.2 模型调参 75
5.2.1 调参 75
5.2.2 网格搜索 76
5.2.3 学习曲线 77
5.2.4 验证曲线 78
5.3 赛题模型验证和调参 78
5.3.1 模型过拟合与欠拟合 78
5.3.2 模型正则化 81
5.3.3 模型交叉验证 82
5.3.4 模型超参空间及调参 85
5.3.5 学习曲线和验证曲线 89
6 特征优化 93
6.1 特征优化的方法 93
6.1.1 合成特征 93
6.1.2 特征的简单变换 93
6.1.3 用决策树创造新特征 94
6.1.4 特征组合 94
6.2 赛题特征优化 96
6.2.1 导入数据 96
6.2.2 特征构造方法 96
6.2.3 特征构造函数 96
6.2.4 特征降维处理 96
6.2.5 模型训练和评估 97
7 模型融合 100
7.1 模型优化 100
7.1.1 模型学习曲线 100
7.1.2 模型融合提升技术 100
7.1.3 预测结果融合策略 102
7.1.4 其他提升方法 105
7.2 赛题模型融合 106
7.2.1 导入工具包 106
7.2.2 获取训练数据和测试
数据 106
7.2.3 模型评价函数 107
7.2.4 采用网格搜索训练
模型 107
7.2.5 单一模型预测效果 109
7.2.6 模型融合Boosting方法 115
7.2.7 多模型预测Bagging
方法 118
7.2.8 多模型融合Stacking
方法 119
7.2.9 模型验证 127
7.2.10 使用lr_reg和lgb_reg
进行融合预测 127

赛题二 天猫用户重复购买预测
1 赛题理解 130
1.1 赛题背景 130
1.2 数据介绍 131
1.3 评估指标 133
1.4 赛题分析 134
2 数据探索 137
2.1 理论知识 137
2.1.1 缺失数据处理 137
2.1.2 不均衡样本 138
2.1.3 常见的数据分布 141
2.2 赛题数据探索 144
2.2.1 导入工具包 145
2.2.2 读取数据 145
2.2.3 数据集样例查看 145
2.2.4 查看数据类型和数据
大小 146
2.2.5 查看缺失值 147
2.2.6 观察数据分布 148
2.2.7 探查影响复购的各种
因素 150
3 特征工程 155
3.1 特征工程介绍 155
3.1.1 特征工程的概念 155
3.1.2 特征归一化 155
3.1.3 类别型特征的转换 156
3.1.4 高维组合特征的处理 156
3.1.5 组合特征 157
3.1.6 文本表示模型 157
3.2 赛题特征工程思路 158
3.3 赛题特征工程构造 160
3.3.1 工具导入 160
3.3.2 数据读取 160
3.3.3 对数据进行内存压缩 161
3.3.4 数据处理 163
3.3.5 定义特征统计函数 164
3.3.6 提取统计特征 166
3.3.7 利用Countvector和
TF-IDF提取特征 170
3.3.8 嵌入特征 170

3.3.9 Stacking分类特征 171
4 模型训练 179
4.1 分类的概念 179
4.2 分类相关模型 179
4.2.1 逻辑回归分类模型 179
4.2.2 K近邻分类模型 180
4.2.3 高斯贝叶斯分类模型 182
4.2.4 决策树分类模型 182
4.2.5 集成学习分类模型 183
5 模型验证 186
5.1 模型验证指标 186
5.1.1 准确度 186
5.1.2 查准率和查全率 188
5.1.3 F1值 189
5.1.4 分类报告 189
5.1.5 混淆矩阵 189
5.1.6 ROC 190
5.1.7 AUC曲线 190
5.2 赛题模型验证和评估 190
5.2.1 基础代码 190
5.2.2 简单验证 191
5.2.3 设置交叉验证方式 192
5.2.4 模型调参 194
5.2.5 混淆矩阵 195
5.2.6 不同的分类模型 198
5.2.7 自己封装模型 205
6 特征优化 211
6.1 特征选择技巧 211
6.2 赛题特征优化 213
6.2.1 基础代码 213
6.2.2 缺失值补全 213
6.2.3 特征选择 213
赛题三 O2O优惠券预测
1 赛题理解 222
1.1 赛题介绍 222
1.2 赛题分析 223
2 数据探索 225
2.1 理论知识 225
2.1.1 数据探索的定义 225
2.1.2 数据探索的目的 226
2.1.3 相关Python包 226
2.2 初步的数据探索 226
2.2.1 数据读取 226
2.2.2 数据查看 227
2.2.3 数据边界探索 231
2.2.4 训练集与测试集的
相关性 232
2.2.5 数据统计 236
2.3 数据分布 238
2.3.1 对文本数据的数值化
处理 238
2.3.2 数据分布可视化 242
3 特征工程 246
3.1 赛题特征工程思路 246
3.2 赛题特征构建 248
3.2.1 工具函数 248
3.2.2 特征群生成函数 250
3.2.3 特征集成函数 256
3.2.4 特征输出 257
3.3 对特征进行探索 260
3.3.1 特征读取函数 260
3.3.2 特征总览 261
3.3.3 查看特征的分布 262
3.3.4 特征相关性分析 265
4 模型训练 266
4.1 模型训练与评估 266
4.2 不同算法模型的性能对比 271
4.2.1 朴素贝叶斯 271
4.2.2 逻辑回归 271
4.2.3 决策树 272
4.2.4 随机森林 272
4.2.5 XGBoost 273
4.2.6 LightGBM 274
4.2.7 不同特征效果对比 274
4.3 结果输出 274
5 模型验证 276
5.1 评估指标 276
5.2 交叉验证 276
5.3 模型比较 279
5.4 验证结果可视化 282
5.5 结果分析 289
5.6 模型调参 290
5.7 实际方案 292
6 提交结果 299
6.1 整合及输出结果 299
6.2 结果提交及线上验证 302
赛题四 阿里云安全恶意程序检测
1 赛题理解 306
1.1 赛题介绍 306
1.2 赛题分析 307
2 数据探索 310
2.1 训练集数据探索 310
2.1.1 数据特征类型 310
2.1.2 数据分布 311
2.1.3 缺失值 312
2.1.4 异常值 312
2.1.5 标签分布 313
2.2 测试集数据探索 314
2.2.1 数据信息 314
2.2.2 缺失值 315
2.2.3 数据分布 315
2.2.4 异常值 315
2.3 数据集联合分析 316
2.3.1 file_id分析 316
2.3.2 API分析 317
3 特征工程与基线模型 318
3.1 特征工程概述 318
3.1.1 特征工程介绍 318
3.1.2 构造特征 318
3.1.3 特征选择 319
3.2 构造线下验证集 319
3.2.1 评估穿越 319
3.2.2 训练集和测试集的特征
差异性 320
3.2.3 训练集和测试集的分布
差异性 320
3.3 基线模型 320
3.3.1 数据读取 320
3.3.2 特征工程 321

3.3.3 基线构建 322
3.3.4 特征重要性分析 324
3.3.5 模型测试 325
4 高阶数据探索 326
4.1 变量分析 326
4.2 高阶数据探索实战 329
4.2.1 数据读取 329
4.2.2 多变量交叉探索 329
5 特征工程进阶与方案优化 343
5.1 pivot特征构建 343
5.1.1 pivot特征 343
5.1.2 pivot特征构建时间 343
5.1.3 pivot特征构建细节和
特点 343
5.2 业务理解和结果分析 344
5.2.1 结合模型理解业务 344
5.2.2 多分类问题预测结果
分析 344
5.3 特征工程进阶实践 344
5.3.1 特征工程基础部分 344
5.3.2 特征工程进阶部分 348
5.3.3 基于LightGBM的模型
验证 349
5.3.4 模型结果分析 351
5.3.5 模型测试 354
6 优化技巧与解决方案升级 355
6.1 优化技巧：Python处理大数据
的技巧 355
6.1.1 内存管理控制 355
6.1.2 加速数据处理的技巧 356
6.1.3 其他开源工具包 356
6.2 深度学习解决方案：TextCNN
建模 358
6.2.1 问题转化 358
6.2.2 TextCNN建模 358
6.2.3 数据预处理 360
6.2.4 TextCNN网络结构 361
6.2.5 TextCNN训练和测试 362
6.2.6 结果提交 364
7 开源方案学习 365
・ ・ ・ ・ ・ ・ (收起)第一部分 通用流程
第　1章 问题建模　2
1.1　评估指标　3
1.1.1　分类指标　4
1.1.2　回归指标　7
1.1.3　排序指标　9
1.2　样本选择　10
1.2.1　数据去噪　11
1.2.2　采样　12
1.2.3　原型选择和训练集选择　13
1.3　交叉验证　14
1.3.1　留出法　14
1.3.2　K折交叉验证　15
1.3.3　自助法　16
参考文献　17
第　2章 特征工程　18
2.1　特征提取　18
2.1.1　探索性数据分析　19
2.1.2　数值特征　20
2.1.3　类别特征　22
2.1.4　时间特征　24
2.1.5　空间特征　25
2.1.6　文本特征　25
2.2　特征选择　27
2.2.1　过滤方法　28
2.2.2　封装方法　31
2.2.3　嵌入方法　31
2.2.4　小结　32
2.2.5　工具介绍　33
参考文献　33
第3章　常用模型　35
3.1　逻辑回归　35
3.1.1　逻辑回归原理　35
3.1.2　逻辑回归应用　38
3.2　场感知因子分解机　39
3.2.1　因子分解机原理　39
3.2.2　场感知因子分解机原理　40
3.2.3　场感知因子分解机的应用　41
3.3　梯度提升树　42
3.3.1　梯度提升树原理　42
3.3.2　梯度提升树的应用　44
参考文献　44
第4章　模型融合　45
4.1　理论分析　46
4.1.1　融合收益　46
4.1.2　模型误差 分歧分解　46
4.1.3　模型多样性度量　48
4.1.4　多样性增强　49
4.2　融合方法　50
4.2.1　平均法　50
4.2.2　投票法　52
4.2.3　Bagging　54
4.2.4　Stacking　55
4.2.5　小结　56
参考文献　57
第二部分　数据挖掘
第5章　用户画像　60
5.1　什么是用户画像　60
5.2　用户画像数据挖掘　63
5.2.1　画像数据挖掘整体架构　63
5.2.2　用户标识　65
5.2.3　特征数据　67
5.2.4　样本数据　68
5.2.5　标签建模　69
5.3　用户画像应用　83
5.3.1　用户画像实时查询系统　83
5.3.2　人群画像分析系统　87
5.3.3　其他系统　90
5.3.4　线上应用效果　91
5.4　小结　91
参考文献　91
第6章　POI实体链接　92
6.1　问题的背景与难点　92
6.2　国内酒店POI实体链接解决方案　94
6.2.1　酒店POI实体链接　94
6.2.2　数据清洗　96
6.2.3　特征生成　97
6.2.4　模型选择与效果评估　100
6.2.5　索引粒度的配置　101
6.3　其他场景的策略调整　101
6.4　小结　103
第7章　评论挖掘　104
7.1　评论挖掘的背景　104
7.1.1　评论挖掘的粒度　105
7.1.2　评论挖掘的维度　105
7.1.3　评论挖掘的整合思考　106
7.2　评论标签提取　106
7.2.1　数据的获取及预处理　107
7.2.2　无监督的标签提取方法　109
7.2.3　基于深度学习的标签提取方法　111
7.3　标签情感分析　113
7.3.1　评论标签情感分析的特殊性　113
7.3.2　基于深度学习的情感分析方法　115
7.3.3　评论标签情感分析的后续优 化与思考　118
7.4　评论挖掘的未来应用及实践　119
7.5　小结　119
参考文献　119
第三部分　搜索和推荐
第8章　O2O场景下的查询理解与 用户引导　122
8.1　现代搜索引擎原理　123
8.2　精确理解查询　124
8.2.1　用户查询意图的定义与识别　125
8.2.2　查询实体识别与结构化　129
8.2.3　召回策略的变迁　130
8.2.4　查询改写　131
8.2.5　词权重与相关性计算　134
8.2.6　类目相关性与人工标注　135
8.2.7　查询理解小结　136
8.3　引导用户完成搜索　137
8.3.1　用户引导的产品定义与衡量 标准　137
8.3.2　搜索前的引导――查询词 推荐　140
8.3.3　搜索中的引导――查询补全　143
8.3.4　搜索后的引导――相关搜索　145
8.3.5　效率提升与效果提升　145
8.3.6　用户引导小结　149
8.4　小结　149
参考文献　150
第9章　O2O场景下排序的特点　152
9.1　系统概述　154
9.2　在线排序服务　154
9.3　多层正交A/B测试　155
9.4　特征获取　155
9.5　离线调研系统　156
9.6　特征工程　156
9.7　排序模型　157
9.8　场景化排序　160
9.9　小结　165
第　10章 推荐在O2O场景的应用　166
10.1　典型的O2O推荐场景　166
10.2　O2O推荐场景特点　167
10.2.1　O2O场景的地理位置因素　168
10.2.2　O2O场景的用户历史行为　168
10.2.3　O2O场景的实时推荐　169
10.3　美团推荐实践――推荐框架　169
10.4　美团推荐实践――推荐召回　170
10.4.1　基于协同过滤的召回　171
10.4.2　基于位置的召回　171
10.4.3　基于搜索查询的召回　172
10.4.4　基于图的召回　172
10.4.5　基于实时用户行为的召回　172
10.4.6　替补策略　172
10.5　美团推荐实践――推荐排序　173
10.5.1　排序特征　173
10.5.2　排序样本　174
10.5.3　排序模型　175
10.6　推荐评价指标　176
参考文献　176
第四部分　计算广告
第　11章 O2O场景下的广告营销　178
11.1　O2O场景下的广告业务特点　178
11.2　商户、用户和平台三者利益平衡　180
11.2.1　商户效果感知　180
11.2.2　用户体验　181
11.2.3　平台收益　182
11.3　O2O广告机制设计　183
11.3.1　广告位设定　183
11.3.2　广告召回机制　183
11.3.3　广告排序机制　184
11.4　O2O推送广告　187
11.5　O2O广告系统工具　190
11.5.1　面向开发人员的系统工具　190
11.5.2　面向广告主和运营人员的 工具　192
11.6　小结　194
参考文献　194
第　12章 用户偏好和损失建模　196
12.1　如何定义用户偏好　196
12.1.1　什么是用户偏好　196
12.1.2　如何衡量用户偏好　196
12.1.3　对不同POI 的偏好　197
12.1.4　用户对 POI 偏好的衡量　197
12.2　广告价值与偏好损失的兑换　198
12.2.1　优化目标　199
12.2.2　模型建模　199
12.3　Pairwise 模型学习　201
12.3.1　GBRank　202
12.3.2　RankNet　204
参考文献　205
第五部分　深度学习
第　13章 深度学习概述　208
13.1　深度学习技术发展历程　209
13.2　深度学习基础结构　211
13.3　深度学习研究热点　216
13.3.1　基于深度学习的生成式模型　216
13.3.2　深度强化学习　218
参考文献　219
第　14章 深度学习在文本领域的应用　220
14.1　基于深度学习的文本匹配　221
14.2　基于深度学习的排序模型　231
14.2.1　排序模型简介　231
14.2.2　深度学习排序模型的演进　232
14.2.3　美团的深度学习排序模型 尝试　235
14.3　小结　237
参考文献　237
第　15章 深度学习在计算机视觉中的 应用　238
15.1　基于深度学习的OCR　238
15.1.1　OCR技术发展历程　239
15.1.2　基于深度学习的文字检测　244
15.1.3　基于序列学习的文字识别　248
15.1.4　小结　251
15.2　基于深度学习的图像智能审核　251
15.2.1　基于深度学习的水印检测　252
15.2.2　明星脸识别　254
15.2.3　色情图片检测　257
15.2.4　场景分类　257
15.3　基于深度学习的图像质量排序　259
15.3.1　图像美学质量评价　260
15.3.2　面向点击预测的图像质量 评价　260
15.4　小结　263
参考文献　264
第六部分　算法工程
第　16章 大规模机器学习　268
16.1　并行计算编程技术　268
16.1.1　向量化　269
16.1.2　多核并行OpenMP　270
16.1.3　GPU编程　272
16.1.4　多机并行MPI　273
16.1.5　并行编程技术小结　276
16.2　并行计算模型　276
16.2.1　BSP　277
16.2.2　SSP　279
16.2.3　ASP　280
16.2.4　参数服务器　281
16.3　并行计算案例　284
16.3.1　XGBoost并行库Rabit　284
16.3.2　MXNet并行库PS-Lite　286
16.4　美团并行计算机器学习平台　287
参考文献　289
第　17章 特征工程和实验平台　290
17.1　特征平台　290
17.1.1　特征生产　290
17.1.2　特征上线　293
17.1.3　在线特征监控　301
17.2　实验管理平台　302
17.2.1　实验平台概述　302
17.2.2　美团实验平台――Gemini　304
・ ・ ・ ・ ・ ・ (收起)第1章 向量和向量空间 1
1.1 向量 2
1.1.1 描述向量 3
1.1.2 向量的加法 10
1.1.3 向量的数量乘法 12
1.2 向量空间 14
1.2.1 什么是向量空间 14
1.2.2 线性组合 16
1.2.3 线性无关 17
1.2.4 子空间 23
1.3 基和维数 25
1.3.1 极大线性无关组 25
1.3.2 基 26
1.3.3 维数 32
1.4 内积空间 34
1.4.1 什么是内积空间 34
1.4.2 点积和欧几里得空间 36
1.5 距离和角度 38
1.5.1 距离 38
1.5.2 基于距离的分类 43
1.5.3 范数和正则化 46
1.5.4 角度 49
1.6 非欧几何 51
第2章 矩阵 54
2.1 基础知识 55
2.1.1 什么是矩阵 55
2.1.2 初等变换 59
2.1.3 矩阵加法 62
2.1.4 数量乘法 63
2.1.5 矩阵乘法 65
2.2 线性映射 70
2.2.1 理解什么是线性 70
2.2.2 线性映射 72
2.2.3 矩阵与线性映射 76
2.2.4 齐次坐标系 79
2.3 矩阵的逆和转置 85
2.3.1 逆矩阵 85
2.3.2 转置矩阵 89
2.3.3 矩阵LU分解 91
2.4 行列式 94
2.4.1 计算方法和意义 94
2.4.2 线性方程组 98
2.5 矩阵的秩 102
2.6 稀疏矩阵 107
2.6.1 生成稀疏矩阵 107
2.6.2 稀疏矩阵压缩 108
2.7 图与矩阵 112
2.7.1 图的基本概念 112
2.7.2 邻接矩阵 114
2.7.3 关联矩阵 119
2.7.4 拉普拉斯矩阵 120
第3章 特征值和特征向量 122
3.1 基本概念 123
3.1.1 定义 123
3.1.2 矩阵的迹 127
3.1.3 一般性质 128
3.2 应用示例 129
3.2.1 动力系统微分方程 129
3.2.2 马尔科夫矩阵 131
3.3 相似矩阵 135
3.3.1 相似变换 137
3.3.2 几何理解 141
3.3.3 对角化 144
3.4 正交和投影 150
3.4.1 正交集和标准正交基 150
3.4.2 正交矩阵 154
3.4.3 再探对称矩阵 156
3.4.4 投影 159
3.5 矩阵分解 163
3.5.1 QR分解 163
3.5.2 特征分解 167
3.5.3 奇异值分解 172
3.5.4 数据压缩 178
3.5.5 降噪 182
3.6 最小二乘法（1） 184
3.6.1 正规方程 184
3.6.2 线性回归（1） 186
第4章 向量分析 191
4.1 向量的代数运算 192
4.1.1 叉积 192
4.1.2 张量和外积 196
4.2 向量微分 199
4.2.1 函数及其导数 199
4.2.2 偏导数 201
4.2.3 梯度 206
4.2.4 矩阵导数 211
4.3 最优化方法 215
4.3.1 简单的线性规划 215
4.3.2 最小二乘法（2） 218
4.3.3 梯度下降法 221
4.3.4 线性回归（2） 226
4.3.5 牛顿法 228
4.4 反向传播算法 229
4.4.1 神经网络 230
4.4.2 参数学习 234
4.4.3 损失函数 248
4.4.4 激活函数 253
4.4.5 理论推导 258
第5章 概率 263
5.1 基本概念 264
5.1.1 试验和事件 264
5.1.2 理解概率 266
5.1.3 条件概率 269
5.2 贝叶斯定理 272
5.2.1 事件的独立性 273
5.2.2 全概率公式 274
5.2.3 理解贝叶斯定理 276
5.3 随机变量和概率分布 279
5.3.1 随机变量 279
5.3.2 离散型随机变量的分布 281
5.3.3 连续型随机变量的分布 295
5.3.4 多维随机变量及分布 307
5.3.5 条件概率分布 312
5.4 随机变量的和 317
5.4.1 离散型随机变量的和 317
5.4.2 连续型随机变量的和 318
5.5 随机变量的数字特征 321
5.5.1 数学期望 321
5.5.2 方差和协方差 326
5.5.3 计算相似度 337
5.5.4 协方差矩阵 343
第6章 数理统计 346
6.1 样本和抽样 347
6.1.1 总体和样本 347
6.1.2 统计量 348
6.2 点估计 353
6.2.1 最大似然估计 354
6.2.2 线性回归（3） 358
6.2.3 最大后验估计 362
6.2.4 估计的选择标准 365
6.3 区间估计 368
6.4 参数检验 373
6.4.1 基本概念 374
6.4.2 正态总体均值的假设检验 378
6.4.3 正态总体方差的假设检验 384
6.4.4 p值检验 385
6.4.5 用假设检验比较模型 388
6.5 非参数检验 391
6.5.1 拟合优度检验 391
6.5.2 列联表检验 394
第7章 信息与熵 399
7.1 度量信息 399
7.2 信息熵 402
7.3 联合熵和条件熵 406
7.4 相对熵和交叉熵 409
7.5 互信息 414
7.6 连续分布 416
附录 419
后记 436
・ ・ ・ ・ ・ ・ (收起)目　录

第一部分　分类
第1章　机器学习基础　　2
1.1 　何谓机器学习　　3
1.1.1 　传感器和海量数据　　4
1.1.2 　机器学习非常重要　　5
1.2 　关键术语　　5
1.3 　机器学习的主要任务　　7
1.4 　如何选择合适的算法　　8
1.5 　开发机器学习应用程序的步骤　　9
1.6 　Python语言的优势　　10
1.6.1 　可执行伪代码　　10
1.6.2 　Python比较流行　　10
1.6.3 　Python语言的特色　　11
1.6.4 　Python语言的缺点　　11
1.7 　NumPy函数库基础　　12
1.8 　本章小结　　13
第2章　k-近邻算法 　　15
2.1 　k-近邻算法概述　　15
2.1.1 　准备：使用Python导入数据　　17
2.1.2 　从文本文件中解析数据　　19
2.1.3 　如何测试分类器　　20
2.2 　示例：使用k-近邻算法改进约会网站的配对效果　　20
2.2.1 　准备数据：从文本文件中解析数据　　21
2.2.2 　分析数据：使用Matplotlib创建散点图　　23
2.2.3 　准备数据：归一化数值　　25
2.2.4 　测试算法：作为完整程序验证分类器　　26
2.2.5 　使用算法：构建完整可用系统　　27
2.3 　示例：手写识别系统　　28
2.3.1 　准备数据：将图像转换为测试向量　　29
2.3.2 　测试算法：使用k-近邻算法识别手写数字　　30
2.4 　本章小结　　31
第3章　决策树 　　32
3.1 　决策树的构造　　33
3.1.1 　信息增益　　35
3.1.2 　划分数据集　　37
3.1.3 　递归构建决策树　　39
3.2 　在Python中使用Matplotlib注解绘制树形图　　42
3.2.1 　Matplotlib注解　　43
3.2.2 　构造注解树　　44
3.3 　测试和存储分类器　　48
3.3.1 　测试算法：使用决策树执行分类　　49
3.3.2 　使用算法：决策树的存储　　50
3.4 　示例：使用决策树预测隐形眼镜类型　　50
3.5 　本章小结　　52
第4章　基于概率论的分类方法：朴素贝叶斯 　　53
4.1 　基于贝叶斯决策理论的分类方法　　53
4.2 　条件概率　　55
4.3 　使用条件概率来分类　　56
4.4 　使用朴素贝叶斯进行文档分类　　57
4.5 　使用Python进行文本分类　　58
4.5.1 　准备数据：从文本中构建词向量　　58
4.5.2 　训练算法：从词向量计算概率　　60
4.5.3 　测试算法：根据现实情况修改分类器　　62
4.5.4 　准备数据：文档词袋模型　　64
4.6 　示例：使用朴素贝叶斯过滤垃圾邮件　　64
4.6.1 　准备数据：切分文本　　65
4.6.2 　测试算法：使用朴素贝叶斯进行交叉验证　　66
4.7 　示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向　　68
4.7.1 　收集数据：导入RSS源　　68
4.7.2 　分析数据：显示地域相关的用词　　71
4.8 　本章小结　　72
第5章　Logistic回归 　　73
5.1 　基于Logistic回归和Sigmoid函数的分类　　74
5.2 　基于最优化方法的最佳回归系数确定　　75
5.2.1 　梯度上升法　　75
5.2.2 　训练算法：使用梯度上升找到最佳参数　　77
5.2.3 　分析数据：画出决策边界　　79
5.2.4 　训练算法：随机梯度上升　　80
5.3 　示例：从疝气病症预测病马的死亡率　　85
5.3.1 　准备数据：处理数据中的缺失值　　85
5.3.2 　测试算法：用Logistic回归进行分类　　86
5.4 　本章小结　　88
第6章　支持向量机　　89
6.1 　基于最大间隔分隔数据　　89
6.2 　寻找最大间隔　　91
6.2.1 　分类器求解的优化问题　　92
6.2.2 　SVM应用的一般框架　　93
6.3 　SMO高效优化算法　　94
6.3.1 　Platt的SMO算法　　94
6.3.2 　应用简化版SMO算法处理小规模数据集　　94
6.4 　利用完整Platt SMO算法加速优化　　99
6.5 　在复杂数据上应用核函数　　105
6.5.1 　利用核函数将数据映射到高维空间　　106
6.5.2 　径向基核函数　　106
6.5.3 　在测试中使用核函数　　108
6.6 　示例：手写识别问题回顾　　111
6.7 　本章小结　　113
第7章　利用AdaBoost元算法提高分类
性能 　　115
7.1 　基于数据集多重抽样的分类器　　115
7.1.1 　bagging：基于数据随机重抽样的分类器构建方法　　116
7.1.2 　boosting　　116
7.2 　训练算法：基于错误提升分类器的性能　　117
7.3 　基于单层决策树构建弱分类器　　118
7.4 　完整AdaBoost算法的实现　　122
7.5 　测试算法：基于AdaBoost的分类　　124
7.6 　示例：在一个难数据集上应用AdaBoost　　125
7.7 　非均衡分类问题　　127
7.7.1 　其他分类性能度量指标：正确率、召回率及ROC曲线　　128
7.7.2 　基于代价函数的分类器决策控制　　131
7.7.3 　处理非均衡问题的数据抽样方法　　132
7.8 　本章小结　　132
第二部分　利用回归预测数值型数据
第8章　预测数值型数据：回归 　　136
8.1 　用线性回归找到最佳拟合直线　　136
8.2 　局部加权线性回归　　141
8.3 　示例：预测鲍鱼的年龄　　145
8.4 　缩减系数来“理解”数据　　146
8.4.1 　岭回归　　146
8.4.2 　lasso　　148
8.4.3 　前向逐步回归　　149
8.5 　权衡偏差与方差　　152
8.6 　示例：预测乐高玩具套装的价格　　153
8.6.1 　收集数据：使用Google购物的API　　153
8.6.2 　训练算法：建立模型　　155
8.7 　本章小结　　158
第9章　树回归　　159
9.1 　复杂数据的局部性建模　　159
9.2 　连续和离散型特征的树的构建　　160
9.3 　将CART算法用于回归　　163
9.3.1 　构建树　　163
9.3.2 　运行代码　　165
9.4 　树剪枝　　167
9.4.1 　预剪枝　　167
9.4.2 　后剪枝　　168
9.5 　模型树　　170
9.6 　示例：树回归与标准回归的比较　　173
9.7 　使用Python的Tkinter库创建GUI　　176
9.7.1 　用Tkinter创建GUI　　177
9.7.2 　集成Matplotlib和Tkinter　　179
9.8 　本章小结　　182
第三部分　无监督学习
第10章　利用K-均值聚类算法对未标注数据分组　　184
10.1 　K-均值聚类算法　　185
10.2 　使用后处理来提高聚类性能　　189
10.3 　二分K-均值算法　　190
10.4 　示例：对地图上的点进行聚类　　193
10.4.1 　Yahoo! PlaceFinder API　　194
10.4.2 　对地理坐标进行聚类　　196
10.5 　本章小结　　198
第11章　使用Apriori算法进行关联分析　　200
11.1 　关联分析　　201
11.2 　Apriori原理　　202
11.3 　使用Apriori算法来发现频繁集　　204
11.3.1 　生成候选项集　　204
11.3.2 　组织完整的Apriori算法　　207
11.4 　从频繁项集中挖掘关联规则　　209
11.5 　示例：发现国会投票中的模式　　212
11.5.1 　收集数据：构建美国国会投票记录的事务数据集　　213
11.5.2 　测试算法：基于美国国会投票记录挖掘关联规则　　219
11.6 　示例：发现毒蘑菇的相似特征　　220
11.7 　本章小结　　221
第12章　使用FP-growth算法来高效发现频繁项集　　223
12.1 　FP树：用于编码数据集的有效方式　　224
12.2 　构建FP树　　225
12.2.1 　创建FP树的数据结构　　226
12.2.2 　构建FP树　　227
12.3 　从一棵FP树中挖掘频繁项集　　231
12.3.1 　抽取条件模式基　　231
12.3.2 　创建条件FP树　　232
12.4 　示例：在Twitter源中发现一些共现词　　235
12.5 　示例：从新闻网站点击流中挖掘　　238
12.6 　本章小结　　239
第四部分　其他工具
第13章　利用PCA来简化数据　　242
13.1 　降维技术　　242
13.2 　PCA　　243
13.2.1 　移动坐标轴　　243
13.2.2 　在NumPy中实现PCA　　246
13.3 　示例：利用PCA对半导体制造数据降维　　248
13.4 　本章小结　　251
第14章　利用SVD简化数据　　252
14.1 　SVD的应用　　252
14.1.1 　隐性语义索引　　253
14.1.2 　推荐系统　　253
14.2 　矩阵分解　　254
14.3 　利用Python实现SVD　　255
14.4 　基于协同过滤的推荐引擎　　257
14.4.1 　相似度计算　　257
14.4.2 　基于物品的相似度还是基于用户的相似度？　　260
14.4.3 　推荐引擎的评价　　260
14.5 　示例：餐馆菜肴推荐引擎　　260
14.5.1 　推荐未尝过的菜肴　　261
14.5.2 　利用SVD提高推荐的效果　　263
14.5.3 　构建推荐引擎面临的挑战　　265
14.6 　基于SVD的图像压缩　　266
14.7 　本章小结　　268
第15章　大数据与MapReduce　　270
15.1 　MapReduce：分布式计算的框架　　271
15.2 　Hadoop流　　273
15.2.1 　分布式计算均值和方差的mapper　　273
15.2.2 　分布式计算均值和方差的reducer　　274
15.3 　在Amazon网络服务上运行Hadoop程序　　275
15.3.1 　AWS上的可用服务　　276
15.3.2 　开启Amazon网络服务之旅　　276
15.3.3 　在EMR上运行Hadoop作业　　278
15.4 　MapReduce上的机器学习　　282
15.5 　在Python中使用mrjob来自动化MapReduce　　283
15.5.1 　mrjob与EMR的无缝集成　　283
15.5.2 　mrjob的一个MapReduce脚本剖析　　284
15.6 　示例：分布式SVM的Pegasos算法　　286
15.6.1 　Pegasos算法　　287
15.6.2 　训练算法：用mrjob实现MapReduce版本的SVM　　288
15.7 　你真的需要MapReduce吗？　　292
15.8 　本章小结　　292
附录A 　Python入门　　294
附录B 　线性代数　　303
附录C 　概率论复习　　309
附录D 　资源　　312
索引　　313
版权声明　　316
・ ・ ・ ・ ・ ・ (收起)第1章　Spark的环境搭建与运行　　1
1.1　Spark的本地安装与配置　　2
1.2　Spark集群　　3
1.3　Spark编程模型　　4
1.3.1　SparkContext类与SparkConf 类　　4
1.3.2　Spark shell　　5
1.3.3　弹性分布式数据集　　6
1.3.4　广播变量和累加器　　10
1.4　Spark Scala编程入门　　11
1.5　Spark Java编程入门　　14
1.6　Spark Python编程入门　　17
1.7　在Amazon EC2上运行Spark　　18
1.8　小结　　23
第2章　设计机器学习系统　　24
2.1　MovieStream介绍　　24
2.2　机器学习系统商业用例　　25
2.2.1　个性化　　26
2.2.2　目标营销和客户细分　　26
2.2.3　预测建模与分析　　26
2.3　机器学习模型的种类　　27
2.4　数据驱动的机器学习系统的组成　　27
2.4.1　数据获取与存储　　28
2.4.2　数据清理与转换　　28
2.4.3　模型训练与测试回路　　29
2.4.4　模型部署与整合　　30
2.4.5　模型监控与反馈　　30
2.4.6　批处理或实时方案的选择　　31
2.5　机器学习系统架构　　31
2.6　小结　　33
第3章　Spark上数据的获取、处理与准备　　34
3.1　获取公开数据集　　35
3.2　探索与可视化数据　　37
3.2.1　探索用户数据　　38
3.2.2　探索电影数据　　41
3.2.3　探索评级数据　　43
3.3　处理与转换数据　　46
3.4　从数据中提取有用特征　　48
3.4.1　数值特征　　48
3.4.2　类别特征　　49
3.4.3　派生特征　　50
3.4.4　文本特征　　51
3.4.5　正则化特征　　55
3.4.6　用软件包提取特征　　56
3.5　小结　　57
第4章　构建基于Spark的推荐引擎　　58
4.1　推荐模型的分类　　59
4.1.1　基于内容的过滤　　59
4.1.2　协同过滤　　59
4.1.3　矩阵分解　　60
4.2　提取有效特征　　64
4.3　训练推荐模型　　67
4.3.1　使用MovieLens 100k数据集训练模型　　67
4.3.2　使用隐式反馈数据训练模型　　68
4.4　使用推荐模型　　69
4.4.1　用户推荐　　69
4.4.2　物品推荐　　72
4.5　推荐模型效果的评估　　75
4.5.1　均方差　　75
4.5.2　K值平均准确率　　77
4.5.3　使用MLlib内置的评估函数　　81
4.6　小结　　82
第5章　Spark构建分类模型　　83
5.1　分类模型的种类　　85
5.1.1　线性模型　　85
5.1.2　朴素贝叶斯模型　　89
5.1.3　决策树　　90
5.2　从数据中抽取合适的特征　　91
5.3　训练分类模型　　93
5.4　使用分类模型　　95
5.5　评估分类模型的性能　　96
5.5.1　预测的正确率和错误率　　96
5.5.2　准确率和召回率　　97
5.5.3　ROC曲线和AUC　　99
5.6　改进模型性能以及参数调优　　101
5.6.1　特征标准化　　101
5.6.2　其他特征　　104
5.6.3　使用正确的数据格式　　106
5.6.4　模型参数调优　　107
5.7　小结　　115
第6章　Spark构建回归模型　　116
6.1　回归模型的种类　　116
6.1.1　最小二乘回归　　117
6.1.2　决策树回归　　117
6.2　从数据中抽取合适的特征　　118
6.3　回归模型的训练和应用　　123
6.4　评估回归模型的性能　　125
6.4.1　均方误差和均方根误差　　125
6.4.2　平均绝对误差　　126
6.4.3　均方根对数误差　　126
6.4.4　R-平方系数　　126
6.4.5　计算不同度量下的性能　　126
6.5　改进模型性能和参数调优　　127
6.5.1　变换目标变量　　128
6.5.2　模型参数调优　　132
6.6　小结　　140
第7章　Spark构建聚类模型　　141
7.1　聚类模型的类型　　142
7.1.1　K-均值聚类　　142
7.1.2　混合模型　　146
7.1.3　层次聚类　　146
7.2　从数据中提取正确的特征　　146
7.3　训练聚类模型　　150
7.4　使用聚类模型进行预测　　151
7.5　评估聚类模型的性能　　155
7.5.1　内部评价指标　　155
7.5.2　外部评价指标　　156
7.5.3　在MovieLens数据集计算性能　　156
7.6　聚类模型参数调优　　156
7.7　小结　　158
第8章　Spark应用于数据降维　　159
8.1　降维方法的种类　　160
8.1.1　主成分分析　　160
8.1.2　奇异值分解　　160
8.1.3　和矩阵分解的关系　　161
8.1.4　聚类作为降维的方法　　161
8.2　从数据中抽取合适的特征　　162
8.3　训练降维模型　　169
8.4　使用降维模型　　172
8.4.1　在LFW数据集上使用PCA投影数据　　172
8.4.2　PCA和SVD模型的关系　　173
8.5　评价降维模型　　174
8.6　小结　　176
第9章　Spark高级文本处理技术　　177
9.1　处理文本数据有什么特别之处　　177
9.2　从数据中抽取合适的特征　　177
9.2.1　短语加权表示　　178
9.2.2　特征哈希　　179
9.2.3　从20新闻组数据集中提取TF-IDF特征　　180
9.3　使用TF-IDF模型　　192
9.3.1　20 Newsgroups数据集的文本相似度和TF-IDF特征　　192
9.3.2　基于20 Newsgroups数据集使用TF-IDF训练文本分类器　　194
9.4　评估文本处理技术的作用　　196
9.5　Word2Vec 模型　　197
9.6　小结　　200
第10章　Spark Streaming在实时机器学习上的应用　　201
10.1　在线学习　　201
10.2　流处理　　202
10.2.1　Spark Streaming介绍　　202
10.2.2　使用Spark Streaming缓存和容错　　205
10.3　创建Spark Streaming应用　　206
10.3.1　消息生成端　　207
10.3.2　创建简单的流处理程序　　209
10.3.3　流式分析　　211
10.3.4　有状态的流计算　　213
10.4　使用Spark Streaming进行在线学习　　215
10.4.1　流回归　　215
10.4.2　一个简单的流回归程序　　216
10.4.3　流K-均值　　220
10.5　在线模型评估　　221
10.6　小结　　224
・ ・ ・ ・ ・ ・ (收起)第1 章一元函数微积分1
1.1 极限与连续. . . . . . . . . . . . . . 1
1.1.1 可数集与不可数集. . . . . . . . 1
1.1.2 数列的极限. . . . . . . . . . . . 3
1.1.3 函数的极限. . . . . . . . . . . . 7
1.1.4 函数的连续性与间断点. . . . . 9
1.1.5 上确界与下确界. . . . . . . . . 11
1.1.6 李普希茨连续性. . . . . . . . . 12
1.1.7 无穷小量. . . . . . . . . . . . . 13
1.2 导数与微分. . . . . . . . . . . . . . 14
1.2.1 一阶导数. . . . . . . . . . . . . 14
1.2.2 机器学习中的常用函数. . . . . 20
1.2.3 高阶导数. . . . . . . . . . . . . 22
1.2.4 微分. . . . . . . . . . . . . . . . 24
1.2.5 导数与函数的单调性. . . . . . 25
1.2.6 极值判别法则. . . . . . . . . . 26
1.2.7 导数与函数的凹凸性. . . . . . 28
1.3 微分中值定理. . . . . . . . . . . . . 29
1.3.1 罗尔中值定理. . . . . . . . . . 29
1.3.2 拉格朗日中值定理. . . . . . . . 29
1.3.3 柯西中值定理. . . . . . . . . . 31
1.4 泰勒公式. . . . . . . . . . . . . . . . 31
1.5 不定积分. . . . . . . . . . . . . . . . 33
1.5.1 不定积分的定义与性质. . . . . 33
1.5.2 换元积分法. . . . . . . . . . . . 35
1.5.3 分部积分法. . . . . . . . . . . . 36
1.6 定积分. . . . . . . . . . . . . . . . . 37
1.6.1 定积分的定义与性质. . . . . . 38
1.6.2 牛顿-莱布尼茨公式. . . . . . . 39
1.6.3 定积分的计算. . . . . . . . . . 40
1.6.4 变上限积分. . . . . . . . . . . . 41
1.6.5 定积分的应用. . . . . . . . . . 42
1.6.6 广义积分. . . . . . . . . . . . . 44
1.7 常微分方程. . . . . . . . . . . . . . 45
1.7.1 基本概念. . . . . . . . . . . . . 45
1.7.2 一阶线性微分方程. . . . . . . . 46
第2 章线性代数与矩阵论49
2.1 向量及其运算. . . . . . . . . . . . . 49
2.1.1 基本概念. . . . . . . . . . . . . 49
2.1.2 基本运算. . . . . . . . . . . . . 51
2.1.3 向量的范数. . . . . . . . . . . . 53
2.1.4 解析几何. . . . . . . . . . . . . 55
2.1.5 线性相关性. . . . . . . . . . . . 57
2.1.6 向量空间. . . . . . . . . . . . . 58
2.1.7 应用――线性回归. . . . . . . . 61
2.1.8 应用――线性分类器与支持
向量机. . . . . . . . . . . . . . 62
2.2 矩阵及其运算. . . . . . . . . . . . . 65
2.2.1 基本概念. . . . . . . . . . . . . 65
2.2.2 基本运算. . . . . . . . . . . . . 67
2.2.3 逆矩阵. . . . . . . . . . . . . . 72
2.2.4 矩阵的范数. . . . . . . . . . . . 78
2.2.5 应用――人工神经网络. . . . . 78
2.2.6 线性变换. . . . . . . . . . . . . 81
2.3 行列式. . . . . . . . . . . . . . . . . 82
2.3.1 行列式的定义与性质. . . . . . 83
2.3.2 计算方法. . . . . . . . . . . . . 91
2.4 线性方程组. . . . . . . . . . . . . . 92
2.4.1 高斯消元法. . . . . . . . . . . . 92
2.4.2 齐次方程组. . . . . . . . . . . . 93
2.4.3 非齐次方程组. . . . . . . . . . 95
2.5 特征值与特征向量. . . . . . . . . . 97
2.5.1 特征值与特征向量. . . . . . . . 97
2.5.2 相似变换. . . . . . . . . . . . . 105
2.5.3 正交变换. . . . . . . . . . . . . 106
2.5.4 QR 算法. . . . . . . . . . . . . . 110
2.5.5 广义特征值. . . . . . . . . . . . 112
2.5.6 瑞利商. . . . . . . . . . . . . . 112
2.5.7 谱范数与特征值的关系. . . . . 114
2.5.8 条件数. . . . . . . . . . . . . . 114
2.5.9 应用――谱归一化与谱正则化. . . . . . . . . . . . . . . . . 115
2.6 二次型. . . . . . . . . . . . . . . . . 116
2.6.1 基本概念. . . . . . . . . . . . . 116
2.6.2 正定二次型与正定矩阵. . . . . 116
2.6.3 标准型. . . . . . . . . . . . . . 119
2.7 矩阵分解. . . . . . . . . . . . . . . . 121
2.7.1 楚列斯基分解. . . . . . . . . . 121
2.7.2 QR 分解. . . . . . . . . . . . . . 123
2.7.3 特征值分解. . . . . . . . . . . . 127
2.7.4 奇异值分解. . . . . . . . . . . . 128
第3 章多元函数微积分133
3.1 偏导数. . . . . . . . . . . . . . . . . 133
3.1.1 一阶偏导数. . . . . . . . . . . . 133
3.1.2 高阶偏导数. . . . . . . . . . . . 134
3.1.3 全微分. . . . . . . . . . . . . . 136
3.1.4 链式法则. . . . . . . . . . . . . 136
3.2 梯度与方向导数. . . . . . . . . . . . 138
3.2.1 梯度. . . . . . . . . . . . . . . . 138
3.2.2 方向导数. . . . . . . . . . . . . 139
3.2.3 应用――边缘检测与HOG
特征. . . . . . . . . . . . . . . . 139
3.3 黑塞矩阵. . . . . . . . . . . . . . . . 140
3.3.1 黑塞矩阵的定义与性质. . . . . 141
3.3.2 凹凸性. . . . . . . . . . . . . . 141
3.3.3 极值判别法则. . . . . . . . . . 143
3.3.4 应用――最小二乘法. . . . . . . 145
3.4 雅可比矩阵. . . . . . . . . . . . . . 146
3.4.1 雅可比矩阵的定义和性质. . . . 146
3.4.2 链式法则的矩阵形式. . . . . . 148
3.5 向量与矩阵求导. . . . . . . . . . . . 150
3.5.1 常用求导公式. . . . . . . . . . 150
3.5.2 应用――反向传播算法. . . . . 154
3.6 微分算法. . . . . . . . . . . . . . . . 156
3.6.1 符号微分. . . . . . . . . . . . . 156
3.6.2 数值微分. . . . . . . . . . . . . 157
3.6.3 自动微分. . . . . . . . . . . . . 158
3.7 泰勒公式. . . . . . . . . . . . . . . . 159
3.8 多重积分. . . . . . . . . . . . . . . . 161
3.8.1 二重积分. . . . . . . . . . . . . 161
3.8.2 三重积分. . . . . . . . . . . . . 164
3.8.3 n 重积分. . . . . . . . . . . . . 167
3.9 无穷级数. . . . . . . . . . . . . . . . 170
3.9.1 常数项级数. . . . . . . . . . . . 170
3.9.2 函数项级数. . . . . . . . . . . . 173
第4 章最优化方法176
4.1 基本概念. . . . . . . . . . . . . . . . 176
4.1.1 问题定义. . . . . . . . . . . . . 177
4.1.2 迭代法的基本思想. . . . . . . . 179
4.2 一阶优化算法. . . . . . . . . . . . . 180
4.2.1 梯度下降法. . . . . . . . . . . . 180
4.2.2 最速下降法. . . . . . . . . . . . 183
4.2.3 梯度下降法的改进. . . . . . . . 184
4.2.4 随机梯度下降法. . . . . . . . . 186
4.2.5 应用――人工神经网络. . . . . 187
4.3 二阶优化算法. . . . . . . . . . . . . 188
4.3.1 牛顿法. . . . . . . . . . . . . . 188
4.3.2 拟牛顿法. . . . . . . . . . . . . 189
4.4 分治法. . . . . . . . . . . . . . . . . 193
4.4.1 坐标下降法. . . . . . . . . . . . 193
4.4.2 SMO 算法. . . . . . . . . . . . . 194
4.4.3 分阶段优化. . . . . . . . . . . . 195
4.4.4 应用――logistic 回归. . . . . . 196
4.5 凸优化问题. . . . . . . . . . . . . . 198
4.5.1 数值优化算法面临的问题. . . . 198
4.5.2 凸集. . . . . . . . . . . . . . . . 199
4.5.3 凸优化问题及其性质. . . . . . 200
4.5.4 机器学习中的凸优化问题. . . . 201
4.6 带约束的优化问题. . . . . . . . . . 202
4.6.1 拉格朗日乘数法. . . . . . . . . 202
4.6.2 应用――线性判别分析. . . . . 204
4.6.3 拉格朗日对偶. . . . . . . . . . 205
4.6.4 KKT 条件. . . . . . . . . . . . . 208
4.6.5 应用――支持向量机. . . . . . . 209
4.7 多目标优化问题. . . . . . . . . . . . 213
4.7.1 基本概念. . . . . . . . . . . . . 213
4.7.2 求解算法. . . . . . . . . . . . . 215
4.7.3 应用――多目标神经结构搜
索. . . . . . . . . . . . . . . . . 215
4.8 泛函极值与变分法. . . . . . . . . . 216
4.8.1 泛函与变分. . . . . . . . . . . . 217
4.8.2 欧拉―拉格朗日方程. . . . . . 218
4.8.3 应用――证明两点之间直线
最短. . . . . . . . . . . . . . . . 220
4.9 目标函数的构造. . . . . . . . . . . . 221
4.9.1 有监督学习. . . . . . . . . . . . 221
4.9.2 无监督学习. . . . . . . . . . . . 224
4.9.3 强化学习. . . . . . . . . . . . . 225
第5 章概率论228
5.1 随机事件与概率. . . . . . . . . . . . 229
5.1.1 随机事件概率. . . . . . . . . . 229
5.1.2 条件概率. . . . . . . . . . . . . 233
5.1.3 全概率公式. . . . . . . . . . . . 234
5.1.4 贝叶斯公式. . . . . . . . . . . . 235
5.1.5 条件独立. . . . . . . . . . . . . 236
5.2 随机变量. . . . . . . . . . . . . . . . 236
5.2.1 离散型随机变量. . . . . . . . . 236
5.2.2 连续型随机变量. . . . . . . . . 237
5.2.3 数学期望. . . . . . . . . . . . . 240
5.2.4 方差与标准差. . . . . . . . . . 242
5.2.5 Jensen 不等式. . . . . . . . . . . 243
5.3 常用概率分布. . . . . . . . . . . . . 244
5.3.1 均匀分布. . . . . . . . . . . . . 244
5.3.2 伯努利分布. . . . . . . . . . . . 246
5.3.3 二项分布. . . . . . . . . . . . . 247
5.3.4 多项分布. . . . . . . . . . . . . 248
5.3.5 几何分布. . . . . . . . . . . . . 249
5.3.6 正态分布. . . . . . . . . . . . . 250
5.3.7 t 分布. . . . . . . . . . . . . . . 252
5.3.8 应用――颜色直方图. . . . . . . 253
5.3.9 应用――贝叶斯分类器. . . . . 254
5.4 分布变换. . . . . . . . . . . . . . . . 254
5.4.1 随机变量函数. . . . . . . . . . 254
5.4.2 逆变换采样算法. . . . . . . . . 256
5.5 随机向量. . . . . . . . . . . . . . . . 258
5.5.1 离散型随机向量. . . . . . . . . 258
5.5.2 连续型随机向量. . . . . . . . . 260
5.5.3 数学期望. . . . . . . . . . . . . 261
5.5.4 协方差. . . . . . . . . . . . . . 262
5.5.5 常用概率分布. . . . . . . . . . 265
5.5.6 分布变换. . . . . . . . . . . . . 268
5.5.7 应用――高斯混合模型. . . . . 269
5.6 极限定理. . . . . . . . . . . . . . . . 271
5.6.1 切比雪夫不等式. . . . . . . . . 271
5.6.2 大数定律. . . . . . . . . . . . . 271
5.6.3 中心极限定理. . . . . . . . . . 273
5.7 参数估计. . . . . . . . . . . . . . . . 273
5.7.1 最大似然估计. . . . . . . . . . 274
5.7.2 最大后验概率估计. . . . . . . . 276
5.7.3 贝叶斯估计. . . . . . . . . . . . 278
5.7.4 核密度估计. . . . . . . . . . . . 278
5.7.5 应用――logistic 回归. . . . . . 280
5.7.6 应用――EM 算法. . . . . . . . 282
5.7.7 应用――Mean Shift 算法. . . . 286
5.8 随机算法. . . . . . . . . . . . . . . . 288
5.8.1 基本随机数生成算法. . . . . . 288
5.8.2 遗传算法. . . . . . . . . . . . . 290
5.8.3 蒙特卡洛算法. . . . . . . . . . 293
5.9 采样算法. . . . . . . . . . . . . . . . 295
5.9.1 拒绝采样. . . . . . . . . . . . . 296
5.9.2 重要性采样. . . . . . . . . . . . 297
第6 章信息论298
6.1 熵与联合熵. . . . . . . . . . . . . . 298
6.1.1 信息量与熵. . . . . . . . . . . . 298
6.1.2 熵的性质. . . . . . . . . . . . . 300
6.1.3 应用――决策树. . . . . . . . . 302
6.1.4 联合熵. . . . . . . . . . . . . . 303
6.2 交叉熵. . . . . . . . . . . . . . . . . 305
6.2.1 交叉熵的定义. . . . . . . . . . 306
6.2.2 交叉熵的性质. . . . . . . . . . 306
6.2.3 应用――softmax 回归. . . . . . 307
6.3 Kullback-Leibler 散度. . . . . . . . . 309
6.3.1 KL 散度的定义. . . . . . . . . . 309
6.3.2 KL 散度的性质. . . . . . . . . . 311
6.3.3 与交叉熵的关系. . . . . . . . . 312
6.3.4 应用――流形降维. . . . . . . . 312
6.3.5 应用――变分推断. . . . . . . . 313
6.4 Jensen-Shannon 散度. . . . . . . . . 316
6.4.1 JS 散度的定义. . . . . . . . . . 316
6.4.2 JS 散度的性质. . . . . . . . . . 316
6.4.3 应用――生成对抗网络. . . . . 317
6.5 互信息. . . . . . . . . . . . . . . . . 320
6.5.1 互信息的定义. . . . . . . . . . 320
6.5.2 互信息的性质. . . . . . . . . . 321
6.5.3 与熵的关系. . . . . . . . . . . . 322
6.5.4 应用――特征选择. . . . . . . . 323
6.6 条件熵. . . . . . . . . . . . . . . . . 324
6.6.1 条件熵定义. . . . . . . . . . . . 324
6.6.2 条件熵的性质. . . . . . . . . . 325
6.6.3 与熵以及互信息的关系. . . . . 325
6.7 总结. . . . . . . . . . . . . . . . . . 326
第7 章随机过程328
7.1 马尔可夫过程. . . . . . . . . . . . . 328
7.1.1 马尔可夫性. . . . . . . . . . . . 329
7.1.2 马尔可夫链的基本概念. . . . . 330
7.1.3 状态的性质与分类. . . . . . . . 333
7.1.4 平稳分布与极限分布. . . . . . 337
7.1.5 细致平衡条件. . . . . . . . . . 342
7.1.6 应用――隐马尔可夫模型. . . . 343
7.1.7 应用――强化学习. . . . . . . . 345
7.2 马尔可夫链采样算法. . . . . . . . . 348
7.2.1 基本马尔可夫链采样. . . . . . 349
7.2.2 MCMC 采样算法. . . . . . . . . 349
7.2.3 Metropolis-Hastings 算法. . . . . 351
7.2.4 Gibbs 算法. . . . . . . . . . . . 353
7.3 高斯过程. . . . . . . . . . . . . . . . 355
7.3.1 高斯过程性质. . . . . . . . . . 355
7.3.2 高斯过程回归. . . . . . . . . . 355
7.3.3 应用――贝叶斯优化. . . . . . . 358
第8 章图论363
8.1 图的基本概念. . . . . . . . . . . . . 363
8.1.1 基本概念. . . . . . . . . . . . . 363
8.1.2 应用――计算图与自动微分. . . 365
8.1.3 应用――概率图模型. . . . . . . 370
8.1.4 邻接矩阵与加权度矩阵. . . . . 371
8.1.5 应用――样本集的相似度图. . . 372
8.2 若干特殊的图. . . . . . . . . . . . . 373
8.2.1 联通图. . . . . . . . . . . . . . 373
8.2.2 二部图. . . . . . . . . . . . . . 374
8.2.3 应用――受限玻尔兹曼机. . . . 374
8.2.4 有向无环图. . . . . . . . . . . . 376
8.2.5 应用――神经结构搜索. . . . . 376
8.3 重要的算法. . . . . . . . . . . . . . 380
8.3.1 遍历算法. . . . . . . . . . . . . 380
8.3.2 最短路径算法. . . . . . . . . . 381
8.3.3 拓扑排序算法. . . . . . . . . . 382
8.4 谱图理论. . . . . . . . . . . . . . . . 384
8.4.1 拉普拉斯矩阵. . . . . . . . . . 385
8.4.2 归一化拉普拉斯矩阵. . . . . . 388
8.4.3 应用――流形降维. . . . . . . . 390
・ ・ ・ ・ ・ ・ (收起)绪论　机器学习概述　　1
第1章　机器学习的构成要素　　9
1.1　任务：可通过机器学习解决的问题　　9
1.1.1　探寻结构　　11
1.1.2　性能评价　　13
1.2　模型：机器学习的输出　　14
1.2.1　几何模型　　14
1.2.2　概率模型　　17
1.2.3　逻辑模型　　22
1.2.4　分组模型与评分模型　　26
1.3　特征：机器学习的马达　　26
1.3.1　特征的两种用法　　28
1.3.2　特征的构造与变换　　29
1.3.3　特征之间的交互　　32
1.4　总结与展望　　33
第2章　两类分类及相关任务　　37
2.1　分类　　39
2.1.1　分类性能的评价　　40
2.1.2　分类性能的可视化　　43
2.2　评分与排序　　46
2.2.1　排序性能的评价及可视化　　48
2.2.2　将排序器转化为分类器　　52
2.3　类概率估计　　54
2.3.1　类概率估计量　　55
2.3.2　将排序器转化为概率估计子　　57
2.4　小结与延伸阅读　　59
第3章　超越两类分类　　61
3.1　处理多类问题　　61
3.1.1　多类分类　　61
3.1.2　多类得分及概率　　65
3.2　回归　　68
3.3　无监督学习及描述性学习　　70
3.3.1　预测性聚类与描述性聚类　　71
3.2.2　其他描述性模型　　74
3.4　小结与延伸阅读　　76
第4章　概念学习　　77
4.1　假设空间　　78
4.1.1　最小一般性　　79
4.1.2　内部析取　　82
4.2　通过假设空间的路径　　84
4.2.1　最一般相容假设　　86
4.2.2　封闭概念　　87
4.3　超越合取概念　　88
4.4　可学习性　　92
4.5　小结与延伸阅读　　94
第5章　树模型　　97
5.1　决策树　　100
5.2　排序与概率估计树　　103
5.3　作为减小方差的树学习方法　　110
5.3.1　回归树　　110
5.3.2　聚类树　　113
5.4　小结与延伸阅读　　115
第6章　规则模型　　117
6.1　学习有序规则列表　　117
6.2　学习无序规则集　　124
6.2.1　用于排序和概率估计的规则集　　128
6.2.2　深入探究规则重叠　　130
6.3　描述性规则学习　　131
6.3.1　用于子群发现的规则学习　　131
6.3.2　关联规则挖掘　　135
6.4　一阶规则学习　　139
6.5　小结与延伸阅读　　143
第7章　线性模型　　145
7.1　最小二乘法　　146
7.1.1　多元线性回归　　150
7.1.2　正则化回归　　153
7.1.3　利用最小二乘回归实现分类　　153
7.2　感知机　　155
7.3　支持向量机　　158
7.4　从线性分类器导出概率　　164
7.5　超越线性的核方法　　168
7.6　小结与延伸阅读　　170
第8章　基于距离的模型　　173
8.1　距离测度的多样性　　173
8.2　近邻与范例　　178
8.3　最近邻分类器　　182
8.4　基于距离的聚类　　184
8.4.1　K均值算法　　186
8.4.2　K中心点聚类　　187
8.4.3　silhouette　　188
8.5　层次聚类　　190
8.6　从核函数到距离　　194
8.7　小结与延伸阅读　　195
第9章　概率模型　　197
9.1　正态分布及其几何意义　　200
9.2　属性数据的概率模型　　205
9.2.1　利用朴素贝叶斯模型实现分类　　206
9.2.2　训练朴素贝叶斯模型　　209
9.3　通过优化条件似然实现鉴别式学习　　211
9.4　含隐变量的概率模型　　214
9.4.1　期望最大化算法　　215
9.4.2　高斯混合模型　　216
9.5　基于压缩的模型　　218
9.6　小结与延伸阅读　　220
第10章　特征　　223
10.1　特征的类型　　223
10.1.1　特征上的计算　　223
10.1.2　属性特征、有序特征及数量特征　　227
10.1.3　结构化特征　　228
10.2　特征变换　　229
10.2.1　阈值化与离散化　　229
10.2.2　归一化与标定　　234
10.2.3　特征缺失　　239
10.3　特征的构造与选择　　240
10.4　小结与延伸阅读　　243
第11章　模型的集成　　245
11.1　Bagging与随机森林　　246
11.2　Boosting　　247
11.3　集成学习进阶　　250
11.3.1　偏差、方差及裕量　　250
11.3.2　其他集成方法　　251
11.3.3　元学习　　252
11.4　小结与延伸阅读　　252
第12章　机器学习的实验　　255
12.1　度量指标的选择　　256
12.2　量指标的获取　　258
12.3　如何解释度量指标　　260
12.4　小结与延伸阅读　　264
后记　路在何方　　267
记忆要点　　269
参考文献　　271
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
缩写和符号
术语
第0章 导言1
0.1 什么是神经网络1
0.2 人类大脑4
0.3 神经元模型7
0.4 被看作有向图的神经网络10
0.5 反馈11
0.6 网络结构13
0.7 知识表示14
0.8 学习过程20
0.9 学习任务22
0.10 结束语27
注释和参考文献27
第1章 Rosenblatt感知器28
1.1 引言28
1.2 感知器28
1.3 感知器收敛定理29
1.4 高斯环境下感知器与贝叶斯分类器的关系33
1.5 计算机实验：模式分类36
1.6 批量感知器算法38
1.7 小结和讨论39
注释和参考文献39
习题40
第2章 通过回归建立模型28
2.1 引言41
2.2 线性回归模型：初步考虑41
2.3 参数向量的最大后验估计42
2.4 正则最小二乘估计和MAP估计之间的关系46
2.5 计算机实验：模式分类47
2.6 最小描述长度原则48
2.7 固定样本大小考虑50
2.8 工具变量方法53
2.9 小结和讨论54
注释和参考文献54
习题55
第3章 最小均方算法56
3.1 引言56
3.2 LMS算法的滤波结构56
3.3 无约束最优化：回顾58
3.4 维纳滤波器61
3.5 最小均方算法63
3.6 用马尔可夫模型来描画LMS算法和维纳滤波器的偏差64
3.7 朗之万方程：布朗运动的特点65
3.8 Kushner直接平均法66
3.9 小学习率参数下统计LMS学习理论67
3.10 计算机实验Ⅰ：线性预测68
3.11 计算机实验Ⅱ：模式分类69
3.12 LMS算法的优点和局限71
3.13 学习率退火方案72
3.14 小结和讨论73
注释和参考文献74
习题74
第4章 多层感知器77
4.1 引言77
4.2 一些预备知识78
4.3 批量学习和在线学习79
4.4 反向传播算法81
4.5 异或问题89
4.6 改善反向传播算法性能的试探法90
4.7 计算机实验：模式分类94
4.8 反向传播和微分95
4.9 Hessian矩阵及其在在线学习中的规则96
4.10 学习率的最优退火和自适应控制98
4.11 泛化102
4.12 函数逼近104
4.13 交叉验证107
4.14 复杂度正则化和网络修剪109
4.15 反向传播学习的优点和局限113
4.16 作为最优化问题看待的监督学习117
4.17 卷积网络126
4.18 非线性滤波127
4.19 小规模和大规模学习问题131
4.20 小结和讨论136
注释和参考文献137
习题138
第5章 核方法和径向基函数网络144
5.1 引言144
5.2 模式可分性的Cover定理144
5.3 插值问题148
5.4 径向基函数网络150
5.5 K-均值聚类152
5.6 权向量的递归最小二乘估计153
5.7 RBF网络的混合学习过程156
5.8 计算机实验：模式分类157
5.9 高斯隐藏单元的解释158
5.10 核回归及其与RBF网络的关系160
5.11 小结和讨论162
注释和参考文献164
习题165
第6章 支持向量机168
6.1 引言168
6.2 线性可分模式的最优超平面168
6.3 不可分模式的最优超平面173
6.4 使用核方法的支持向量机176
6.5 支持向量机的设计178
6.6 XOR问题179
6.7 计算机实验:模式分类181
6.8 回归：鲁棒性考虑184
6.9 线性回归问题的最优化解184
6.10 表示定理和相关问题187
6.11 小结和讨论191
注释和参考文献192
习题193
第7章 正则化理论197
7.1 引言197
7.2 良态问题的Hadamard条件198
7.3 Tikhonov正则化理论198
7.4 正则化网络205
7.5 广义径向基函数网络206
7.6 再论正则化最小二乘估计209
7.7 对正则化的附加要点211
7.8 正则化参数估计212
7.9 半监督学习215
7.10 流形正则化：初步的考虑216
7.11 可微流形217
7.12 广义正则化理论220
7.13 光谱图理论221
7.14 广义表示定理222
7.15 拉普拉斯正则化最小二乘算法223
7.16 用半监督学习对模式分类的实验225
7.17 小结和讨论227
注释和参考文献228
习题229
第8章 主分量分析232
8.1 引言232
8.2 自组织原则232
8.3 自组织的特征分析235
8.4 主分量分析：扰动理论235
8.5 基于Hebb的最大特征滤波器241
8.6 基于Hebb的主分量分析247
8.7 计算机实验：图像编码251
8.8 核主分量分析252
8.9 自然图像编码中的基本问题256
8.10 核Hebb算法257
8.11 小结和讨论260
注释和参考文献262
习题264
第9章 自组织映射268
9.1 引言268
9.2 两个基本的特征映射模型269
9.3 自组织映射270
9.4 特征映射的性质275
9.5 计算机实验Ⅰ：利用SOM解网格动力学问题280
9.6 上下文映射281
9.7 分层向量量化283
9.8 核自组织映射285
9.9 计算机实验Ⅱ：利用核SOM解点阵动力学问题290
9.10 核SOM和相对熵之间的关系291
9.11 小结和讨论293
注释和参考文献294
习题295
第10章 信息论学习模型299
10.1 引言299
10.2 熵300
10.3 最大熵原则302
10.4 互信息304
10.5 相对熵306
10.6 系词308
10.7 互信息作为最优化的目标函数310
10.8 最大互信息原则311
10.9 最大互信息和冗余减少314
10.10 空间相干特征316
10.11 空间非相干特征318
10.12 独立分量分析320
10.13 自然图像的稀疏编码以及与ICA编码的比较324
10.14 独立分量分析的自然梯度学习326
10.15 独立分量分析的最大似然估计332
10.16 盲源分离的最大熵学习334
10.17 独立分量分析的负熵最大化337
10.18 相关独立分量分析342
10.19 速率失真理论和信息瓶颈347
10.20 数据的最优流形表达350
10.21 计算机实验：模式分类354
10.22 小结和讨论354
注释和参考文献356
习题361
第11章 植根于统计力学的随机方法366
11.1 引言366
11.2 统计力学367
11.3 马尔可夫链368
11.4 Metropolis算法374
11.5 模拟退火375
11.6 Gibbs抽样377
11.7 Boltzmann机378
11.8 logistic信度网络382
11.9 深度信度网络383
11.10 确定性退火385
11.11 和EM算法的类比389
11.12 小结和讨论390
注释和参考文献390
习题392
第12章 动态规划396
12.1 引言396
12.2 马尔可夫决策过程397
12.3 Bellman最优准则399
12.4 策略迭代401
12.5 值迭代402
12.6 逼近动态规划：直接法406
12.7 时序差分学习406
12.8 Q学习410
12.9 逼近动态规划：非直接法412
12.10 最小二乘策略评估414
12.11 逼近策略迭代417
12.12 小结和讨论419
注释和参考文献421
习题422
第13章 神经动力学425
13.1 引言425
13.2 动态系统426
13.3 平衡状态的稳定性428
13.4 吸引子432
13.5 神经动态模型433
13.6 作为递归网络范例的吸引子操作435
13.7 Hopfield模型435
13.8 Cohen-Grossberg定理443
13.9 盒中脑状态模型445
13.10 奇异吸引子和混沌448
13.11 混沌过程的动态重构452
13.12 小结和讨论455
注释和参考文献457
习题458
第14章 动态系统状态估计的贝叶斯滤波461
14.1 引言461
14.2 状态空间模型462
14.3 卡尔曼滤波器464
14.4 发散现象及平方根滤波469
14.5 扩展的卡尔曼滤波器474
14.6 贝叶斯滤波器477
14.7 数值积分卡尔曼滤波器:基于卡尔曼滤波器480
14.8 粒子滤波器484
14.9 计算机实验：扩展的卡尔曼滤波器和粒子滤波器对比评价490
14.10 大脑功能建模中的
卡尔曼滤波493
14.11 小结和讨论494
注释和参考文献496
习题497
第15章 动态驱动递归网络501
15.1 引言501
15.2 递归网络体系结构502
15.3 通用逼近定理505
15.4 可控性和可观测性507
15.5 递归网络的计算能力510
15.6 学习算法511
15.7 通过时间的反向传播512
15.8 实时递归学习515
15.9 递归网络的消失梯度519
15.10 利用非线性逐次状态估计的递归网络监督学习框架521
15.11 计算机实验：Mackay-Glass吸引子的动态重构526
15.12 自适应考虑527
15.13 实例学习：应用于神经控制的模型参考529
15.14 小结和讨论530
注释和参考文献533
习题534
参考文献538
・ ・ ・ ・ ・ ・ (收起)推荐序
译者序
前言
致谢
关于技术评审人
第1章　机器学习简介 1
1.1　机器学习的起源 2
1.2　机器学习的使用与滥用 3
1.3　机器如何学习 5
1.3.1　抽象化和知识表达 6
1.3.2　一般化 7
1.3.3　评估学习的成功性 9
1.4　将机器学习应用于数据中的步骤 9
1.5　选择机器学习算法 10
1.5.1　考虑输入的数据 10
1.5.2　考虑机器学习算法的类型 11
1.5.3　为数据匹配合适的算法 13
1.6　使用R进行机器学习 13
1.7　总结 17
第2章　数据的管理和理解 18
2.1　R数据结构 18
2.2　向量 19
2.3　因子 20
2.3.1　列表 21
2.3.2　数据框 22
2.3.3　矩阵和数组 24
2.4　用R管理数据 25
2.4.1　保存和加载R数据结构 25
2.4.2　用CSV文件导入和保存数据 26
2.4.3　从SQL数据库导入数据 27
2.5　探索和理解数据 28
2.5.1　探索数据的结构 29
2.5.2　探索数值型变量 29
2.5.3　探索分类变量 37
2.5.4　探索变量之间的关系 39
2.6　总结 42
第3章　懒惰学习――使用近邻分类 44
3.1　理解使用近邻进行分类 45
3.1.1　kNN算法 45
3.1.2　为什么kNN算法是懒惰的 51
3.2　用kNN算法诊断乳腺癌 51
3.2.1　第1步――收集数据 51
3.2.2　第2步――探索和准备数据 52
3.2.3　第3步――基于数据训练模型 55
3.2.4　第4步――评估模型的性能 57
3.2.5　第5步――提高模型的性能 58
3.3　总结 60
第4章　概率学习――朴素贝叶斯分类 61
4.1　理解朴素贝叶斯 61
4.1.1　贝叶斯方法的基本概念 62
4.1.2　朴素贝叶斯算法 65
4.2　例子――基于贝叶斯算法的手机垃圾短信过滤 70
4.2.1　第1步――收集数据 70
4.2.2　第2步――探索和准备数据 71
4.2.3　数据准备――处理和分析文本数据 72
4.2.4　第3步――基于数据训练模型 78
4.2.5　第4步――评估模型的性能 79
4.2.6　第5步――提升模型的性能 80
4.3　总结 81
第5章　分而治之――应用决策树和规则进行分类 82
5.1　理解决策树 82
5.1.1　分而治之 83
5.1.2　C5.0决策树算法 86
5.2　例子――使用C5.0决策树识别高风险银行贷款 89
5.2.1　第1步――收集数据 89
5.2.2　第2步――探索和准备数据 89
5.2.3　第3步――基于数据训练模型 92
5.2.4　第4步――评估模型的性能 95
5.2.5　第5步――提高模型的性能 95
5.3　理解分类规则 98
5.3.1　独立而治之 99
5.3.2　单规则（1R）算法 101
5.3.3　RIPPER算法 103
5.3.4　来自决策树的规则 105
5.4　例子――应用规则学习识别有毒的蘑菇 105
5.4.1　第1步――收集数据 106
5.4.2　第2步――探索和准备数据 106
5.4.3　第3步――基于数据训练模型 107
5.4.4　第4步――评估模型的性能 109
5.4.5　第5步――提高模型的性能 109
5.5　总结 111
第6章　预测数值型数据――回归方法 113
6.1　理解回归 113
6.1.1　简单线性回归 115
6.1.2　普通最小二乘估计 117
6.1.3　相关系数 118
6.1.4　多元线性回归 120
6.2　例子――应用线性回归预测医疗费用 122
6.2.1　第1步――收集数据 122
6.2.2　第2步――探索和准备数据 123
6.2.3　第3步――基于数据训练模型 127
6.2.4　第4步――评估模型的性能 129
6.2.5　第5步――提高模型的性能 130
6.3　理解回归树和模型树 133
6.4　例子――用回归树和模型树估计葡萄酒的质量 135
6.4.1　第1步――收集数据 135
6.4.2　第2步――探索和准备数据 136
6.4.3　第3步――基于数据训练模型 137
6.4.4　第4步――评估模型的性能 140
6.4.5　第5步――提高模型的性能 142
6.5　总结 144
第7章　黑箱方法――神经网络和支持向量机 146
7.1　理解神经网络 146
7.1.1　从生物神经元到人工神经元 148
7.1.2　激活函数 148
7.1.3　网络拓扑 151
7.1.4　用后向传播训练神经网络 153
7.2　用人工神经网络对混凝土的强度进行建模 154
7.2.1　第1步――收集数据 154
7.2.2　第2步――探索和准备数据 155
7.2.3　第3步――基于数据训练模型 156
7.2.4　第4步――评估模型的性能 158
7.2.5　第5步――提高模型的性能 159
7.3　理解支持向量机 160
7.3.1　用超平面分类 161
7.3.2　寻找最大间隔 161
7.3.3　对非线性空间使用核函数 164
7.4　用支持向量机进行光学字符识别 165
7.4.1　第1步――收集数据 166
7.4.2　第2步――探索和准备数据 166
7.4.3　第3步――基于数据训练模型 167
7.4.4　第4步――评估模型的性能 169
7.4.5　第5步――提高模型的性能 170
7.5　总结 171
第8章　探寻模式――基于关联规则的购物篮分析 172
8.1　理解关联规则 172
8.2　例子――用关联规则确定经常一起购买的食品杂货 176
8.2.1　第1步――收集数据 176
8.2.2　第2步――探索和准备数据 177
8.2.3　第3步――基于数据训练模型 183
8.2.4　第4步――评估模型的性能 184
8.2.5　第5步――提高模型的性能 187
8.3　总结 189
第9章　寻找数据的分组――k均值聚类 191
9.1　理解聚类 191
9.1.1　聚类――一种机器学习任务 192
9.1.2　k均值聚类算法 193
9.1.3　用k均值聚类探寻青少年市场细分 198
9.1.4　第1步――收集数据 198
9.1.5　第2步――探索和准备数据 199
9.1.6　第3步――基于数据训练模型 202
9.1.7　第4步――评估模型的性能 204
9.1.8　第5步――提高模型的性能 206
9.2　总结 207
第10章　模型性能的评价 208
10.1　度量分类方法的性能 208
10.1.1　在R中处理分类预测数据 209
10.1.2　深入探讨混淆矩阵 211
10.1.3　使用混淆矩阵度量性能 212
10.1.4　准确度之外的其他性能评价指标 214
10.1.5　性能权衡的可视化 221
10.2　评估未来的性能 224
10.2.1　保持法 225
10.2.2　交叉验证 226
10.2.3　自助法抽样 229
10.3　总结 229
第11章　提高模型的性能 231
11.1　调整多个模型来提高性能 231
11.2　使用元学习来提高模型的性能 239
11.2.1　理解集成学习 239
11.2.2　bagging 241
11.2.3　boosting 243
11.2.4　随机森林 244
11.3　总结 248
第12章　其他机器学习主题 249
12.1　分析专用数据 250
12.1.1　用RCurl添加包从网上获取数据 250
12.1.2　用XML添加包读/写XML格式数据 250
12.1.3　用rjson添加包读/写JSON 251
12.1.4　用xlsx添加包读/写Microsoft Excel电子表格 251
12.1.5　生物信息学数据 251
12.1.6　社交网络数据和图数据 252
12.2　提高R语言的性能 252
12.2.1　处理非常大的数据集 253
12.2.2　使用并行处理来加快学习过程 254
12.2.3　GPU计算 257
12.2.4　部署最优的学习算法 257
12.3　总结 258
・ ・ ・ ・ ・ ・ (收起)第1章　Python机器学习入门　　1
1.1 　梦之队：机器学习与Python　　1
1.2 　这本书将教给你什么（以及不会教什么）　　2
1.3 　遇到困难的时候怎么办　　3
1.4 　开始　　4
1.4.1 　NumPy、SciPy和Matplotlib简介　　4
1.4.2 　安装Python　　5
1.4.3 　使用NumPy和SciPy智能高效地处理数据　　5
1.4.4 　学习NumPy　　5
1.4.5 　学习SciPy　　9
1.5 　我们第一个（极小的）机器学习应用　　10
1.5.1 　读取数据　　10
1.5.2 　预处理和清洗数据　　11
1.5.3 　选择正确的模型和学习算法　　12
1.6 　小结　　20
第2章　如何对真实样本分类　　22
2.1 　Iris数据集　　22
2.1.1 　第一步是可视化　　23
2.1.2 　构建第一个分类模型　　24
2.2 　构建更复杂的分类器　　28
2.3 　更复杂的数据集和更复杂的分类器　　29
2.3.1 　从Seeds数据集中学习　　29
2.3.2 　特征和特征工程　　30
2.3.3 　最邻近分类　　30
2.4 　二分类和多分类　　33
2.5 　小结　　34
第3章　聚类：寻找相关的帖子　　35
3.1 　评估帖子的关联性　　35
3.1.1 　不应该怎样　　36
3.1.2 　应该怎样　　36
3.2 　预处理：用相近的公共词语个数来衡量相似性　　37
3.2.1 　将原始文本转化为词袋　　37
3.2.2 　统计词语　　38
3.2.3 　词语频次向量的归一化　　40
3.2.4 　删除不重要的词语　　41
3.2.5 　词干处理　　42
3.2.6 　停用词兴奋剂　　44
3.2.7 　我们的成果和目标　　45
3.3 　聚类　　46
3.3.1 　K均值　　46
3.3.2 　让测试数据评估我们的想法　　49
3.3.3 　对帖子聚类　　50
3.4 　解决我们最初的难题　　51
3.5 　调整参数　　54
3.6 　小结　　54
第4章　主题模型　　55
4.1 　潜在狄利克雷分配（LDA）　　55
4.2 　在主题空间比较相似度　　59
4.3 　选择主题个数　　64
4.4 　小结　　65
第5章　分类：检测劣质答案　　67
5.1 　路线图概述　　67
5.2 　学习如何区分出优秀的答案　　68
5.2.1 　调整样本　　68
5.2.2 　调整分类器　　68
5.3 　获取数据　　68
5.3.1 　将数据消减到可处理的程度　　69
5.3.2 　对属性进行预选择和处理　　70
5.3.3 　定义什么是优质答案　　71
5.4 　创建第一个分类器　　71
5.4.1 　从k邻近（kNN）算法开始　　71
5.4.2 　特征工程　　72
5.4.3 　训练分类器　　73
5.4.4 　评估分类器的性能　　74
5.4.5 　设计更多的特征　　74
5.5 　决定怎样提升效果　　77
5.5.1 　偏差?方差及其折中　　77
5.5.2 　解决高偏差　　78
5.5.3 　解决高方差　　78
5.5.4 　高偏差或低偏差　　78
5.6 　采用逻辑回归　　81
5.6.1 　一点数学和一个小例子　　81
5.6.2 　在帖子分类问题上应用逻辑回归　　83
5.7 　观察正确率的背后：准确率和召回率　　84
5.8 　为分类器瘦身　　87
5.9 　出货　　88
5.10 　小结　　88
第6章　分类II：情感分析　　89
6.1 　路线图概述　　89
6.2 　获取推特（Twitter）数据　　89
6.3 　朴素贝叶斯分类器介绍　　90
6.3.1 　了解贝叶斯定理　　90
6.3.2 　朴素　　91
6.3.3 　使用朴素贝叶斯进行分类　　92
6.3.4 　考虑未出现的词语和其他古怪情况　　94
6.3.5 　考虑算术下溢　　95
6.4 　创建第一个分类器并调优　　97
6.4.1 　先解决一个简单问题　　97
6.4.2 　使用所有的类　　99
6.4.3 　对分类器的参数进行调优　　101
6.5 　清洗推文　　104
6.6 　将词语类型考虑进去　　106
6.6.1 　确定词语的类型　　106
6.6.2 　用SentiWordNet成功地作弊　　108
6.6.3 　我们第一个估算器　　110
6.6.4 　把所有东西融合在一起　　111
6.7 　小结　　112
第7章　回归：推荐　　113
7.1 　用回归预测房价　　113
7.1.1 　多维回归　　116
7.1.2 　回归里的交叉验证　　116
7.2 　惩罚式回归　　117
7.2.1 　L1和L2惩罚　　117
7.2.2 　在Scikit-learn中使用Lasso或弹性网　　118
7.3 　P大于N的情形　　119
7.3.1 　基于文本的例子　　120
7.3.2 　巧妙地设置超参数（hyperparameter）　　121
7.3.3 　评分预测和推荐　　122
7.4 　小结　　126
第8章　回归：改进的推荐　　127
8.1 　改进的推荐　　127
8.1.1 　使用二值推荐矩阵　　127
8.1.2 　审视电影的近邻　　129
8.1.3 　组合多种方法　　130
8.2 　购物篮分析　　132
8.2.1 　获取有用的预测　　133
8.2.2 　分析超市购物篮　　134
8.2.3 　关联规则挖掘　　136
8.2.4 　更多购物篮分析的高级话题　　137
8.3 　小结　　138
第9章　分类III：音乐体裁分类　　139
9.1 　路线图概述　　139
9.2 　获取音乐数据　　139
9.3 　观察音乐　　140
9.4 　用FFT构建第一个分类器　　143
9.4.1 　增加实验敏捷性　　143
9.4.2 　训练分类器　　144
9.4.3 　在多分类问题中用混淆矩阵评估正确率　　144
9.4.4 　另一种方式评估分类器效果：受试者工作特征曲线（ROC）　　146
9.5 　用梅尔倒频谱系数（MFCC）提升分类效果　　148
9.6 　小结　　152
第10章　计算机视觉：模式识别　　154
10.1 　图像处理简介　　154
10.2 　读取和显示图像　　155
10.2.1 　图像处理基础　　156
10.2.2 　加入椒盐噪声　　161
10.2.3 　模式识别　　163
10.2.4 　计算图像特征　　163
10.2.5 　设计你自己的特征　　164
10.3 　在更难的数据集上分类　　166
10.4 　局部特征表示　　167
10.5 　小结　　170
第11章　降维　　171
11.1 　路线图　　171
11.2 　选择特征　　172
11.2.1 　用筛选器检测冗余特征　　172
11.2.2 　用封装器让模型选择特征　　178
11.3 　其他特征选择方法　　180
11.4 　特征抽取　　181
11.4.1 　主成分分析（PCA）　　181
11.4.2 　PCA的局限性以及LDA会有什么帮助　　183
11.5 　多维标度法（MDS）　　184
11.6 　小结　　187
第12章　大数据　　188
12.1 　了解大数据　　188
12.2 　用Jug程序包把你的处理流程分解成几个任务　　189
12.2.1 　关于任务　　189
12.2.2 　复用部分结果　　191
12.2.3 　幕后的工作原理　　192
12.2.4 　用Jug分析数据　　192
12.3 　使用亚马逊Web服务（AWS）　　194
12.3.1 　构建你的第一台机器　　195
12.3.2 　用starcluster自动创建集群　　199
12.4 　小结　　202
附录A 　更多机器学习知识　　203
A.1 　在线资源　　203
A.2 　参考书　　203
A.2.1 　问答网站　　203
A.2.2 　博客　　204
A.2.3 　数据资源　　205
A.2.4 　竞争日益加剧　　205
A.3 　还剩下什么　　205
A.4 　小结　　206
索引　　207
・ ・ ・ ・ ・ ・ (收起)第1章 引言 1
1.1 学习与智能优化：燎原之火 1
1.2 寻找黄金和寻找伴侣 3
1.3 需要的只是数据 5
1.4 超越传统的商业智能 5
1.5 LION方法的实施 6
1.6 “动手”的方法 6
第2章 懒惰学习：最近邻方法 9
第3章 学习需要方法 14
3.1 从已标记的案例中学习：最小化和泛化 16
3.2 学习、验证、测试 18
3.3 不同类型的误差 21
第一部分 监督学习
第4章 线性模型 26
4.1 线性回归 27
4.2 处理非线性函数关系的技巧 28
4.3 用于分类的线性模型 29
4.4 大脑是如何工作的 30
4.5 线性模型为何普遍，为何成功 31
4.6 最小化平方误差和 32
4.7 数值不稳定性和岭回归 34
第5章 广义线性最小二乘法 37
5.1 拟合的优劣和卡方分布 38
5.2 最小二乘法与最大似然估计 42
5.2.1 假设检验 42
5.2.2 交叉验证 44
5.3 置信度的自助法 44
第6章 规则、决策树和森林 50
6.1 构造决策树 52
6.2 民主与决策森林 56
第7章 特征排序及选择 59
7.1 特征选择：情境 60
7.2 相关系数 62
7.3 相关比 63
7.4 卡方检验拒绝统计独立性 64
7.5 熵和互信息 64
第8章 特定非线性模型 67
8.1 logistic 回归 67
8.2 局部加权回归 69
8.3 用LASSO来缩小系数和选择输入值 72
第9章 神经网络：多层感知器 76
9.1 多层感知器 78
9.2 通过反向传播法学习 80
9.2.1 批量和bold driver反向传播法 81
9.2.2 在线或随机反向传播 82
9.2.3 训练多层感知器的高级优化 83
第10章 深度和卷积网络 84
10.1 深度神经网络 85
10.1.1 自动编码器 86
10.1.2 随机噪声、屏蔽和课程 88
10.2 局部感受野和卷积网络 89
第11章 统计学习理论和支持向量机 94
11.1 经验风险最小化 96
11.1.1 线性可分问题 98
11.1.2 不可分问题 100
11.1.3 非线性假设 100
11.1.4 用于回归的支持向量 101
第12章 最小二乘法和健壮内核机器 103
12.1 最小二乘支持向量机分类器 104
12.2 健壮加权最小二乘支持向量机 106
12.3 通过修剪恢复稀疏 107
12.4 算法改进：调谐QP、原始版本、无补偿 108
第13章 机器学习中的民主 110
13.1 堆叠和融合 111
13.2 实例操作带来的多样性：装袋法和提升法 113
13.3 特征操作带来的多样性 114
13.4 输出值操作带来的多样性：纠错码 115
13.5 训练阶段随机性带来的多样性 115
13.6 加性logistic回归 115
13.7 民主有助于准确率－拒绝的折中 118
第14章 递归神经网络和储备池计算 121
14.1 递归神经网络 122
14.2 能量极小化霍普菲尔德网络 124
14.3 递归神经网络和时序反向传播 126
14.4 递归神经网络储备池学习 127
14.5 超限学习机 128
第二部分 无监督学习和聚类
第15章 自顶向下的聚类：K均值 132
15.1 无监督学习的方法 134
15.2 聚类：表示与度量 135
15.3 硬聚类或软聚类的K均值方法 137
第16章 自底向上（凝聚）聚类 142
16.1 合并标准以及树状图 142
16.2 适应点的分布距离：马氏距离 144
16.3 附录：聚类的可视化 146
第17章 自组织映射 149
17.1 将实体映射到原型的人工皮层 150
17.2 使用成熟的自组织映射进行分类 153
第18章 通过线性变换降维（投影） 155
18.1 线性投影 156
18.2 主成分分析 158
18.3 加权主成分分析：结合坐标和关系 160
18.4 通过比值优化进行线性判别 161
18.5 费希尔线性判别分析 163
第19章 通过非线性映射可视化图与网络 165
19.1 最小应力可视化 166
19.2 一维情况：谱图绘制 168
19.3 复杂图形分布标准 170
第20章 半监督学习 174
20.1 用部分无监督数据进行学习 175
20.1.1 低密度区域中的分离 177
20.1.2 基于图的算法 177
20.1.3 学习度量 179
20.1.4 集成约束和度量学习 179
第三部分 优化：力量之源
第21章 自动改进的局部方法 184
21.1 优化和学习 185
21.2 基于导数技术的一维情况 186
21.2.1 导数可以由割线近似 190
21.2.2 一维最小化 191
21.3 求解高维模型（二次正定型） 191
21.3.1 梯度与最速下降法 194
21.3.2 共轭梯度法 196
21.4 高维中的非线性优化 196
21.4.1 通过线性查找的全局收敛 197
21.4.2 解决不定黑塞矩阵 198
21.4.3 与模型信赖域方法的关系 199
21.4.4 割线法 200
21.4.5 缩小差距：二阶方法与线性复杂度 201
21.5 不涉及导数的技术：反馈仿射振荡器 202
21.5.1 RAS：抽样区域的适应性 203
21.5.2 为健壮性和多样化所做的重复 205
第22章 局部搜索和反馈搜索优化 211
22.1 基于扰动的局部搜索 212
22.2 反馈搜索优化：搜索时学习 215
22.3 基于禁忌的反馈搜索优化 217
第23章 合作反馈搜索优化 222
23.1 局部搜索过程的智能协作 223
23.2 CoRSO：一个政治上的类比 224
23.3 CoRSO的例子：RSO与RAS合作 226
第24章 多目标反馈搜索优化 232
24.1 多目标优化和帕累托最优 233
24.2 脑－计算机优化：循环中的用户 235
第四部分 应用精选
第25章 文本和网页挖掘 240
25.1 网页信息检索与组织 241
25.1.1 爬虫 241
25.1.2 索引 242
25.2 信息检索与排名 244
25.2.1 从文档到向量：向量－空间模型 245
25.2.2 相关反馈 247
25.2.3 更复杂的相似性度量 248
25.3 使用超链接来进行网页排名 250
25.4 确定中心和权威：HITS 254
25.5 聚类 256
第26章 协同过滤和推荐 257
26.1 通过相似用户结合评分 258
26.2 基于矩阵分解的模型 260
参考文献 263
索引 269
・ ・ ・ ・ ・ ・ (收起)序言一
序言二
前　言
作者介绍
第1章　绪论/ 1
1.1　人工智能及其飞速发展/ 2
1.2　大规模、分布式机器学习/ 4
1.3　本书的安排/ 6
参考文献/ 7
第2章　机器学习基础/ 9
2.1　机器学习的基本概念/ 10
2.2　机器学习的基本流程/ 13
2.3　常用的损失函数/ 16
2.3.1　Hinge损失函数/ 16
2.3.2　指数损失函数/ 16
2.3.3　交叉熵损失函数/ 17
2.4　常用的机器学习模型/ 18
2.4.1　线性模型/ 18
2.4.2　核方法与支持向量机/ 18
2.4.3　决策树与Boosting/ 21
2.4.4　神经网络/ 23
2.5　常用的优化方法/ 32
2.6　机器学习理论/ 33
2.6.1　机器学习算法的泛化误差/ 34
2.6.2　泛化误差的分解/ 34
2.6.3　基于容度的估计误差的上界/ 35
2.7　总结/ 36
参考文献/ 36
第3章　分布式机器学习框架/ 41
3.1　大数据与大模型的挑战/ 42
3.2　分布式机器学习的基本流程/ 44
3.3　数据与模型划分模块/ 46
3.4　单机优化模块/ 48
3.5　通信模块/ 48
3.5.1　通信的内容/ 48
3.5.2　通信的拓扑结构/ 49
3.5.3　通信的步调/ 51
3.5.4　通信的频率/ 52
3.6　数据与模型聚合模块/ 53
3.7　分布式机器学习理论/ 54
3.8　分布式机器学习系统/ 55
3.9　总结/ 56
参考文献/ 57
第4章　单机优化之确定性算法/ 61
4.1　基本概述/ 62
4.1.1　机器学习的优化框架/ 62
4.1.2　优化算法的分类和发展历史/ 65
4.2　一阶确定性算法/ 67
4.2.1　梯度下降法/ 67
4.2.2　投影次梯度下降法/ 69
4.2.3　近端梯度下降法/ 70
4.2.4　Frank-Wolfe算法/ 71
4.2.5　Nesterov加速法/ 72
4.2.6　坐标下降法/ 75
4.3　二阶确定性算法/ 75
4.3.1　牛顿法/ 76
4.3.2　拟牛顿法/ 77
4.4　对偶方法/ 78
4.5　总结/ 81
参考文献/ 8
第5章　单机优化之随机算法/ 85
5.1　基本随机优化算法/ 86
5.1.1　随机梯度下降法/ 86
5.1.2　随机坐标下降法/ 88
5.1.3　随机拟牛顿法/ 91
5.1.4　随机对偶坐标上升法/ 93
5.1.5　小结/ 95
5.2　随机优化算法的改进/ 96
5.2.1　方差缩减方法/ 96
5.2.2　算法组合方法/ 100
5.3　非凸随机优化算法/ 101
5.3.1　Ada系列算法/ 102
5.3.2　非凸理论分析/ 104
5.3.3　逃离鞍点问题/ 106
5.3.4　等级优化算法/ 107
5.4　总结/ 109
参考文献/ 109
第6章　数据与模型并行/ 113
6.1　基本概述/ 114
6.2　计算并行模式/ 117
6.3　数据并行模式/ 119
6.3.1　数据样本划分/ 120
6.3.2　数据维度划分/ 123
6.4　模型并行模式/ 123
6.4.1　线性模型/ 123
6.4.2　神经网络/ 127
6.5　总结/ 133
参考文献/ 133
第7章　通信机制/ 135
7.1　基本概述/ 136
7.2　通信的内容/ 137
7.2.1　参数或参数的更新/ 137
7.2.2　计算的中间结果/ 137
7.2.3　讨论/ 138
7.3　通信的拓扑结构/ 139
7.3.1　基于迭代式MapReduce/AllReduce的通信拓扑/ 140
7.3.2　基于参数服务器的通信拓扑/ 142
7.3.3　基于数据流的通信拓扑/ 143
7.3.4　讨论/ 145
7.4　通信的步调/ 145
7.4.1　同步通信/ 146
7.4.2　异步通信/ 147
7.4.3　同步和异步的平衡/ 148
7.4.4　讨论/ 150
7.5　通信的频率/ 150
7.5.1　时域滤波/ 150
7.5.2　空域滤波/ 153
7.5.3　讨论/ 155
7.6　总结/ 156
参考文献/ 156
第8章　数据与模型聚合/ 159
8.1　基本概述/ 160
8.2　基于模型加和的聚合方法/ 160
8.2.1　基于全部模型加和的聚合/ 160
8.2.2　基于部分模型加和的聚合/ 162
8.3　基于模型集成的聚合方法/ 167
8.3.1　基于输出加和的聚合/ 168
8.3.2　基于投票的聚合/ 171
8.4　总结/ 174
参考文献/ 174
第9章　分布式机器学习算法/ 177
9.1　基本概述/ 178
9.2　同步算法/ 179
9.2.1　同步SGD方法/ 179
9.2.2　模型平均方法及其改进/ 182
9.2.3　ADMM算法/ 183
9.2.4　弹性平均SGD算法/ 185
9.2.5　讨论/ 186
9.3　异步算法/ 187
9.3.1　异步SGD/ 187
9.3.2　Hogwild!算法/ 189
9.3.3　Cyclades算法/ 190
9.3.4　带延迟处理的异步算法/ 192
9.3.5　异步方法的进一步加速/ 199
9.3.6　讨论/ 199
9.4　同步和异步的对比与融合/ 199
9.4.1　同步和异步算法的实验对比/ 199
9.4.2　同步和异步的融合/ 201
9.5　模型并行算法/ 203
9.5.1　DistBelief/ 203
9.5.2　AlexNet/ 204
9.6　总结/ 205
参考文献/ 205
第10章　分布式机器学习理论/ 209
10.1　基本概述/ 210
10.2　收敛性分析/ 210
10.2.1　优化目标和算法/ 211
10.2.2　数据和模型并行/ 213
10.2.3　同步和异步/ 215
10.3　加速比分析/ 217
10.3.1　从收敛速率到加速比/ 218
10.3.2　通信量的下界/ 219
10.4　泛化分析/ 221
10.4.1　优化的局限性/ 222
10.4.2　具有更好泛化能力的非凸优化算法/ 224
10.5　总结/ 226
参考文献/ 226
第11章　分布式机器学习系统/ 229
11.1　基本概述/ 230
11.2　基于IMR的分布式机器学习系统/ 231
11.2.1　IMR和Spark/ 231
11.2.2　Spark MLlib/ 234
11.3　基于参数服务器的分布式机器学习系统/ 236
11.3.1　参数服务器/ 236
11.3.2　Multiverso参数服务器/ 237
11.4　基于数据流的分布式机器学习系统/ 241
11.4.1　数据流/ 241
11.4.2　TensorFlow数据流系统/ 243
11.5　实战比较/ 248
11.6　总结/ 252
参考文献/ 252
第12章　结语/ 255
12.1　全书总结/ 256
12.2　未来展望/ 257
索引/ 260
・ ・ ・ ・ ・ ・ (收起)前言1
第一部分 机器学习基础
第1章 机器学习概览11
什么是机器学习12
为什么要使用机器学习12
机器学习系统的种类15
监督式/无监督式学习16
批量学习和在线学习21
基于实例与基于模型的学习24
机器学习的主要挑战29
训练数据的数量不足29
训练数据不具代表性30
质量差的数据32
无关特征32
训练数据过度拟合33
训练数据拟合不足34
退后一步35
测试与验证35
练习37
第2章 端到端的机器学习项目39
使用真实数据39
观察大局40
框架问题41
选择性能指标42
检查假设45
获取数据45
创建工作区45
下载数据48
快速查看数据结构49
创建测试集52
从数据探索和可视化中获得洞见56
将地理数据可视化57
寻找相关性59
试验不同属性的组合61
机器学习算法的数据准备62
数据清理63
处理文本和分类属性65
自定义转换器67
特征缩放68
转换流水线68
选择和训练模型70
培训和评估训练集70
使用交叉验证来更好地进行评估72
微调模型74
网格搜索74
随机搜索76
集成方法76
分析最佳模型及其错误76
通过测试集评估系统77
启动、监控和维护系统78
试试看79
练习79
第3章 分类80
MNIST80
训练一个二元分类器82
性能考核83
使用交叉验证测量精度83
混淆矩阵84
精度和召回率86
精度/召回率权衡87
ROC曲线90
多类别分类器93
错误分析95
多标签分类98
多输出分类99
练习100
第4章 训练模型102
线性回归103
标准方程104
计算复杂度106
梯度下降107
批量梯度下降110
随机梯度下降112
小批量梯度下降114
多项式回归115
学习曲线117
正则线性模型121
岭回归121
套索回归123
弹性网络125
早期停止法126
逻辑回归127
概率估算127
训练和成本函数128
决策边界129
Softmax回归131
练习134
第5章 支持向量机136
线性SVM分类136
软间隔分类137
非线性SVM分类139
多项式核140
添加相似特征141
高斯RBF核函数142
计算复杂度143
SVM回归144
工作原理145
决策函数和预测146
训练目标146
二次规划148
对偶问题149
核化SVM149
在线SVM151
练习152
第6章 决策树154
决策树训练和可视化154
做出预测155
估算类别概率157
CART训练算法158
计算复杂度158
基尼不纯度还是信息熵159
正则化超参数159
回归161
不稳定性162
练习163
第7章 集成学习和随机森林165
投票分类器165
bagging和pasting168
Scikit-Learn的bagging和pasting169
包外评估170
Random Patches和随机子空间171
随机森林172
极端随机树173
特征重要性173
提升法174
AdaBoost175
梯度提升177
堆叠法181
练习184
第8章 降维185
维度的诅咒186
数据降维的主要方法187
投影187
流形学习189
PCA190
保留差异性190
主成分191
低维度投影192
使用Scikit-Learn192
方差解释率193
选择正确数量的维度193
PCA压缩194
增量PCA195
随机PCA195
核主成分分析196
选择核函数和调整超参数197
局部线性嵌入199
其他降维技巧200
练习201
第二部分 神经网络和深度学习
第9章 运行TensorFlow205
安装207
创建一个计算图并在会话中执行208
管理图209
节点值的生命周期210
TensorFlow中的线性回归211
实现梯度下降211
手工计算梯度212
使用自动微分212
使用优化器214
给训练算法提供数据214
保存和恢复模型215
用TensorBoard来可视化图和训练曲线216
命名作用域219
模块化220
共享变量222
练习225
第10章 人工神经网络简介227
从生物神经元到人工神经元227
生物神经元228
具有神经元的逻辑计算229
感知器230
多层感知器和反向传播233
用TensorFlow的高级API来训练MLP236
使用纯TensorFlow训练DNN237
构建阶段237
执行阶段240
使用神经网络241
微调神经网络的超参数242
隐藏层的个数242
每个隐藏层中的神经元数243
激活函数243
练习244
第11章 训练深度神经网络245
梯度消失/爆炸问题245
Xavier初始化和He初始化246
非饱和激活函数248
批量归一化250
梯度剪裁254
重用预训练图层255
重用TensorFlow模型255
重用其他框架的模型256
冻结低层257
缓存冻结层257
调整、丢弃或替换高层258
模型动物园258
无监督的预训练259
辅助任务中的预训练260
快速优化器261
Momentum优化261
Nesterov梯度加速262
AdaGrad263
RMSProp265
Adam优化265
学习速率调度267
通过正则化避免过度拟合269
提前停止269
1和2正则化269
dropout270
最大范数正则化273
数据扩充274
实用指南275
练习276
第12章 跨设备和服务器的分布式TensorFlow279
一台机器上的多个运算资源280
安装280
管理GPU RAM282
在设备上操作284
并行执行287
控制依赖288
多设备跨多服务器288
开启一个会话290
master和worker服务290
分配跨任务操作291
跨多参数服务器分片变量291
用资源容器跨会话共享状态292
使用TensorFlow队列进行异步通信294
直接从图中加载数据299
在TensorFlow集群上并行化神经网络305
一台设备一个神经网络305
图内与图间复制306
模型并行化308
数据并行化309
练习314
第13章 卷积神经网络31
・ ・ ・ ・ ・ ・ (收起)译者序
前言
第1章　引言1
1.1　应用与问题1
1.2　定义与术语2
1.3　交叉验证4
1.4　学习情境5
1.5　本书概览6
第2章　PAC学习框架8
2.1　PAC学习模型8
2.2　对有限假设集的学习保证――一致的情况12
2.3　对有限假设集的学习保证――不一致的情况16
2.4　泛化性18
2.4.1　确定性与随机性情境18
2.4.2　贝叶斯误差与噪声19
2.4.3　估计误差与近似误差19
2.4.4　模型选择20
2.5　文献评注21
2.6　习题22
第3章　Rademacher复杂度和VC-维25
3.1　Rademacher复杂度25
3.2　生长函数29
3.3　VC-维31
3.4　下界36
3.5　文献评注41
3.6　习题42
第4章　支持向量机47
4.1　线性分类47
4.2　可分情况下的支持向量机48
4.2.1　原始优化问题48
4.2.2　支持向量49
4.2.3　对偶优化问题50
4.2.4　留一法51
4.3　不可分情况下的支持向量机52
4.3.1　原始优化问题53
4.3.2　支持向量54
4.3.3　对偶优化问题55
4.4　间隔理论56
4.5　文献评注62
4.6　习题62
第5章　核方法65
5.1　引言65
5.2　正定对称核67
5.2.1　定义67
5.2.2　再生核希尔伯特空间69
5.2.3　性质70
5.3　基于核的算法73
5.3.1　具有PDS核的SVM73
5.3.2　表示定理74
5.3.3　学习保证75
5.4　负定对称核76
5.5　序列核78
5.5.1　加权转换器79
5.5.2　有理核82
5.6　文献评注85
5.7　习题85
第6章　boosting89
6.1　引言89
6.2　AdaBoost算法90
6.2.1　经验误差的界92
6.2.2　与坐标下降的关系93
6.2.3　与逻辑回归的关系94
6.2.4　实践中的标准使用方式95
6.3　理论结果95
6.3.1　基于VC-维的分析96
6.3.2　基于间隔的分析96
6.3.3　间隔最大化100
6.3.4　博弈论解释101
6.4　讨论103
6.5　文献评注104
6.6　习题105
第7章　在线学习108
7.1　引言108
7.2　有专家建议的预测109
7.2.1　错误界和折半算法109
7.2.2　加权多数算法110
7.2.3　随机加权多数算法111
7.2.4　指数加权平均算法114
7.3　线性分类117
7.3.1　感知机算法117
7.3.2　Winnow算法122
7.4　在线到批处理的转换124
7.5　与博弈论的联系127
7.6　文献评注127
7.7　习题128
第8章　多分类133
8.1　多分类问题133
8.2　泛化界134
8.3　直接型多分类算法139
8.3.1　多分类SVM139
8.3.2　多分类boosting算法140
8.3.3　决策树141
8.4　类别分解型多分类算法144
8.4.1　一对多144
8.4.2　一对一145
8.4.3　纠错编码146
8.5　结构化预测算法148
8.6　文献评注149
8.7　习题150
第9章　排序152
9.1　排序问题152
9.2　泛化界153
9.3　使用SVM进行排序155
9.4　RankBoost156
9.4.1　经验误差界158
9.4.2　与坐标下降的关系159
9.4.3　排序问题集成算法的间隔界160
9.5　二部排序161
9.5.1　二部排序中的boosting算法162
9.5.2　ROC曲线下面积164
9.6　基于偏好的情境165
9.6.1　两阶段排序问题166
9.6.2　确定性算法167
9.6.3　随机性算法168
9.6.4　关于其他损失函数的扩展168
9.7　讨论169
9.8　文献评注170
9.9　习题171
第10章　回归172
10.1　回归问题172
10.2　泛化界173
10.2.1　有限假设集173
10.2.2　Rademacher复杂度界174
10.2.3　伪维度界175
10.3　回归算法177
10.3.1　线性回归178
10.3.2　核岭回归179
10.3.3　支持向量回归182
10.3.4　Lasso186
10.3.5　组范数回归算法188
10.3.6　在线回归算法189
10.4　文献评注190
10.5　习题190
第11章　算法稳定性193
11.1　定义193
11.2　基于稳定性的泛化保证194
11.3　基于核的正则化算法的稳定性196
11.3.1　应用于回归算法：SVR和KRR198
11.3.2　应用于分类算法：SVM200
11.3.3　讨论200
11.4　文献评述201
11.5　习题201
第12章　降维203
12.1　主成分分析204
12.2　核主成分分析205
12.3　KPCA和流形学习206
12.3.1　等距映射206
12.3.2　拉普拉斯特征映射207
12.3.3　局部线性嵌入207
12.4　Johnson-Lindenstrauss引理208
12.5　文献评注210
12.6　习题210
第13章　学习自动机和语言212
13.1　引言212
13.2　有限自动机213
13.3　高效精确学习214
13.3.1　被动学习214
13.3.2　通过查询学习215
13.3.3　通过查询学习自动机216
13.4　极限下的识别220
13.5　文献评注224
13.6　习题225
第14章　强化学习227
14.1　学习情境227
14.2　马尔可夫决策过程模型228
14.3　策略229
14.3.1　定义229
14.3.2　策略值229
14.3.3　策略评估230
14.3.4　最优策略230
14.4　规划算法231
14.4.1　值迭代231
14.4.2　策略迭代233
14.4.3　线性规划235
14.5　学习算法235
14.5.1　随机逼近236
14.5.2　TD（0）算法239
14.5.3　Q-学习算法240
14.5.4　SARSA242
14.5.5　TD（λ）算法242
14.5.6　大状态空间243
14.6　文献评注244
结束语245
附录A　线性代数回顾246
附录B　凸优化251
附录C　概率论回顾257
附录D　集中不等式264
附录E　符号273
索引274
参考文献
・ ・ ・ ・ ・ ・ (收起)序言
第 一章 机器学习概述 1
1．1 机器学习简介 1
1．1．1 机器学习简史 1
1．1．2 机器学习主要流派 2
1．2 机器学习、人工智能和数据挖掘 4
1．2．1 什么是人工智能 4
1．2．2 机器学习、人工智能与数据挖掘 5
1．3 典型机器学习应用领域 5
1．4 机器学习算法 12
1．5 机器学习的一般流程 20
第 二章 机器学习基本方法 23
2．1 统计分析 23
2．1．1 统计基础 23
2．1．2 常见概率分布 29
2．1．3 参数估计 31
2．1．4 假设检验 33
2．1．5 线性回归 33
2．1．6 Logistics回归 37
2．1．7 判别分析 38
2．1．8 非线性模型 39
2．2 高维数据降维 40
2．2．1 主成分分析 40
2．2．2 线性判别分析 43
2．2．3 局部线性嵌入 47
2．3 特征工程 48
2．3．1 特征构造 48
2．3．2 特征选择 49
2．3．3 特征提取 50
2．4 模型训练 50
2．4．1 模型训练常见术语 50
2．4．2 训练数据收集 51
2．5 可视化分析 52
2．5．1 可视化分析的作用 52
2．5．2 可视化分析方法 53
2．5．3 可视化分析常用工具 54
2．5．4 常见的可视化图表 56
2．5．5 可视化分析面临的挑战 62
第三章 决策树与分类算法 64
3．1 决策树算法 64
3．1．1 分支处理 66
3．1．2 连续属性离散化 72
3．1．3 过拟合问题 74
3．1．4 分类效果评价 78
3．2 集成学习 83
3．2．1 装袋法 83
3．2．2 提升法 84
3．2．3 GBDT 86
3．2．4 随机森林 87
3．3 决策树应用 89
第四章 聚类分析 95
4．1 聚类分析概念 95
4．1．1 聚类方法分类 95
4．1．2 良好聚类算法的特征 97
4．2 聚类分析的度量 97
4．2．1 外部指标 98
4．2．2 内部指标 99
4．3 基于划分的方法 101
4．3．1 k-均值算法 101
4．3．2 k-medoids算法 106
4．3．3 k-prototype算法 107
4．4 基于密度聚类 107
4．4．1 DBSCAN算法 108
4．4．2 OPTICS算法 110
4．4．3 DENCLUE算法 111
4．5 基于层次的聚类 116
4．5．1 BIRCH聚类 117
4．5．2 CURE算法 120
4．6 基于网格的聚类 122
4．7 基于模型的聚类 123
4．7．1 概率模型聚类 123
4．7．2 模糊聚类 129
4．7．3 Kohonen神经网络聚类 129
第五章 文本分析 137
5．1 文本分析介绍 137
5．2 文本特征提取及表示 138
5．2．1 TF-IDF 138
5．2．2 信息增益 139
5．2．3 互信息 139
5．2．4 卡方统计量 140
5．2．5 词嵌入 141
5．2．6 语言模型 142
5．2．7 向量空间模型 144
5．3 知识图谱 146
5．3．1 知识图谱相关概念 147
5．3．2 知识图谱的存储 147
5．3．3 知识图谱挖掘与计算 148
5．3．4 知识图谱的构建过程 150
5．4 词法分析 155
5．4．1 文本分词 156
5．4．2 命名实体识别 159
5．4．3 词义消歧 160
5．5 句法分析 161
5．6 语义分析 163
5．7 文本分析应用 164
5．7．1 文本分类 164
5．7．2 信息抽取 167
5．7．3 问答系统 168
5．7．4 情感分析 169
5．7．5 自动摘要 171
第六章 神经网络 173
6．1 神经网络介绍 173
6．1．1 前馈神经网络 173
6．1．2 反馈神经网络 176
6．1．3 自组织神经网络 179
6．2 神经网络相关概念 180
6．2．1 激活函数 180
6．2．2 损失函数 184
6．2．3 学习率 185
6．2．4 过拟合 188
6．2．5 模型训练中的问题 189
6．2．6 神经网络效果评价 192
6．3 神经网络应用 192
第七章 贝叶斯网络 197
7．1 贝叶斯理论概述 197
7．1．1 贝叶斯方法的基本观点 197
7．1．2 贝叶斯网络的应用 198
7．2 贝叶斯概率基础 198
7．2．1 概率论 198
7．2．2 贝叶斯概率 199
7．3 朴素贝叶斯分类模型 200
7．4 贝叶斯网络 203
7．5 贝叶斯网络的应用 209
7．5．1 中文分词 210
7．5．2 机器翻译 210
7．5．3 故障诊断 211
7．5．4 疾病诊断 211
第八章 支持向量机 215
8．1 支持向量机模型 215
8．1．1 核函数 215
8．1．2 模型原理分析 216
8．2 支持向量机应用 219
第九章 进化计算 226
9．1 遗传算法的基础 226
9．1．1 基因重组（交叉）与基因突变 227
9．1．2 遗传算法实现技术 228
9．1．3 遗传算法案例 234
9．2 蚁群算法 237
9．2．1 蚁群算法应用案例 238
9．3 蜂群算法简介 239
9．3．1 蜂群算法应用案例 241
第十章 分布式机器学习 245
10．1 分布式机器学习基础 245
10．1．1 参数服务器 245
10．1．2 分布式并行计算类型 246
10．2 分布式机器学习框架 247
10．3 并行决策树 254
10．4 并行k-均值算法 255
第十一章 深度学习 258
11．1 卷积神经网络 258
11．1．1 卷积神经网络的整体结构 259
11．1．2 常见卷积神经网络 262
11．2 循环神经网络 271
11．2．1 RNN基本原理 271
11．2．2 长短期记忆网络 274
11．2．3 门限循环单元 277
11．3 深度学习流行框架 278
第十二章 高等级深度学习 281
12．1 高等级卷积神经网络 281
12．1．1 目标检测与追踪 281
12．1．2 目标分割 295
12．2 高等级循环神经网络应用 301
12．2．1 Encoder-Decoder模型 301
12．2．2 注意力模型 301
12．2．3 LSTM高等级应用 302
12．3 无监督式深度学习 307
12．3．1 深度信念网络 307
12．3．2 自动编码器网络 309
12．3．3 生成对抗网络模型 312
12．4 强化学习 316
12．4．1 增强学习基础 316
12．4．2 深度增强学习 318
12．5 迁移学习 321
12．6 对偶学习 324
第十三章 推荐系统 327
13．1 推荐系统介绍 327
13．1．1 推荐系统的应用场景 327
13．2 推荐系统通用模型 329
13．2．1 推荐系统结构 329
13．2．2 基于内容的推荐 330
13．2．3 基于协同过滤的推荐算法 331
13．2．4 基于图的模型 334
13．2．5 基于关联规则的推荐 335
13．2．6 基于知识的推荐 341
13．2．7 基于标签的推荐 342
13．3 推荐系统评测 343
13．3．1 评测方法 343
13．3．2 评测指标 345
13．4 推荐系统常见问题 349
13．4．1 冷启动问题 349
13．4．2 推荐系统注意事项 351
13．5 推荐系统实例 352
第十四章 实验 364
14．1 华为FusionInsight产品平台介绍 364
14．2 银行定期存款业务预测 365
14．2．1 上传银行客户及存贷款数据 366
14．2．2 准备存款业务分析工作区 367
14．2．3 创建数据挖掘流程 368
14．2．4 定期存款业务模型保存和应用 375
14．3 客户分群 378
14．3．1 分析业务需求 379
14．3．2 上传客户信息数据 381
14．3．3 准备客户分群工作区 382
14．3．4 创建数据挖掘流程 383
14．3．5 客户分群模型保存和应用 392
・ ・ ・ ・ ・ ・ (收起)第1章 基础知识 1
1.1　准备数据 1
1.1.1　数据格式 1
1.1.2　变量类型 2
1.1.3　变量选择 3
1.1.4　特征工程 3
1.1.5　缺失数据 4
1.2　选择算法 4
1.2.1　无监督学习 5
1.2.2　监督学习 6
1.2.3　强化学习 7
1.2.4　注意事项 7
1.3　参数调优 7
1.4　评价模型 9
1.4.1　分类指标 9
1.4.2　回归指标 10
1.4.3　验证 10
1.5　小结 11
第2章　k均值聚类 13
2.1　找出顾客群 13
2.2　示例：影迷的性格特征 13
2.3　定义群组 16
2.3.1　有多少个群组 16
2.3.2　每个群组中有谁 17
2.4　局限性 18
2.5　小结 19
第3章　主成分分析 21
3.1　食物的营养成分 21
3.2　主成分 22
3.3　示例：分析食物种类 24
3.4　局限性 27
3.5　小结 29
第4章　关联规则 31
4.1　发现购买模式 31
4.2　支持度、置信度和提升度 31
4.3　示例：分析杂货店的销售数据 33
4.4　先验原则 35
4.4.1　寻找具有高支持度的项集 36
4.4.2　寻找具有高置信度或高提升度的关联规则 37
4.5　局限性 37
4.6　小结 37
第5章　社会网络分析 39
5.1　展现人际关系 39
5.2　示例：国际贸易 40
5.3　Louvain方法 42
5.4　PageRank算法 43
5.5　局限性 46
5.6　小结 47
第6章　回归分析 49
6.1　趋势线 49
6.2　示例：预测房价 49
6.3　梯度下降法 52
6.4　回归系数 54
6.5　相关系数 55
6.6　局限性 56
6.7　小结 57
第7章　k最近邻算法和异常检测 59
7.1　食品检测 59
7.2　物以类聚，人以群分 60
7.3　示例：区分红白葡萄酒 61
7.4　异常检测 62
7.5　局限性 63
7.6　小结 63
第8章　支持向量机 65
8.1　医学诊断 65
8.2　示例：预测心脏病 65
8.3　勾画最佳分界线 66
8.4　局限性 69
8.5　小结 69
第9章　决策树 71
9.1　预测灾难幸存者 71
9.2　示例：逃离泰坦尼克号 72
9.3　生成决策树 73
9.4　局限性 74
9.5　小结 75
第10章　随机森林 77
10.1　集体智慧 77
10.2　示例：预测犯罪行为 77
10.3　集成模型 81
10.4　自助聚集法 82
10.5　局限性 83
10.6　小结 84
第11章　神经网络 85
11.1　建造人工智能大脑 85
11.2　示例：识别手写数字 86
11.3　神经网络的构成 89
11.4　激活规则 91
11.5　局限性 92
11.6　小结 94
第12章　A/B测试和多臂老虎机 95
12.1　初识A/B测试 95
12.2　A/B测试的局限性 95
12.3　epsilon递减策略 96
12.4　示例：多臂老虎机 97
12.5　胜者为先 99
12.6　epsilon递减策略的局限性 99
12.7　小结 100
附录A　无监督学习算法概览 101
附录B　监督学习算法概览 102
附录C　调节参数列表 103
附录D　更多评价指标 104
术语表 107
关于作者 114
・ ・ ・ ・ ・ ・ (收起)推荐序
作者序
致谢
译者序
关于本书
作者简介
关于封面插图
第1部分机器学习工作流程
第1章什么是机器学习
1.1理解机器学习
1.2使用数据进行决策
1.2.1传统方法
1.2.2机器学习方法
1.2.3机器学习的五大优势
1.2.4面临的挑战
1.3跟踪机器学习流程：从数据到部署
1.3.1数据集合和预处理
1.3.2数据构建模型
1.3.3模型性能评估
1.3.4模型性能优化
1.4提高模型性能的高级技巧
1.4.1数据预处理和特征工程
1.4.2用在线算法持续改进模型
1.4.3具有数据量和速度的规模化模型
1.5总结
1.6本章术语
第2章实用数据处理
2.1起步：数据收集
2.1.1应包含哪些特征
2.1.2如何获得目标变量的真实值
2.1.3需要多少训练数据
2.1.4训练集是否有足够的代表性
2.2数据预处理
2.2.1分类特征
2.2.2缺失数据处理
2.2.3简单特征工程
2.2.4数据规范化
2.3数据可视化
2.3.1马赛克图
2.3.2盒图
2.3.3密度图
2.3.4散点图
2.4总结
2.5本章术语
第3章建模和预测
3.1基础机器学习建模
3.1.1寻找输入和目标间的关系
3.1.2寻求好模型的目的
3.1.3建模方法类型
3.1.4有监督和无监督学习
3.2分类：把数据预测到桶中
3.2.1构建分类器并预测
3.2.2非线性数据与复杂分类
3.2.3多类别分类
3.3回归：预测数值型数据
3.3.1构建回归器并预测
3.3.2对复杂的非线性数据进行回归
3.4总结
3.5本章术语
第4章模型评估与优化
4.1模型泛化：评估新数据的预测准确性
4.1.1问题：过度拟合与乐观模型
4.1.2解决方案：交叉验证
4.1.3交叉验证的注意事项
4.2分类模型评估
4.2.1分类精度和混淆矩阵
4.2.2准确度权衡与ROC曲线
4.2.3多类别分类
4.3回归模型评估
4.3.1使用简单回归性能指标
4.3.2检验残差
4.4参数调整优化模型
4.4.1机器学习算法和它们的调整参数
4.4.2网格搜索
4.5总结
4.6本章术语
第5章基础特征工程
5.1动机：为什么特征工程很有用
5.1.1什么是特征工程
5.1.2使用特征工程的5个原因
5.1.3特征工程与领域专业知识
5.2基本特征工程过程
5.2.1实例：事件推荐
5.2.2处理日期和时间特征
5.2.3处理简单文本特征
5.3特征选择
5.3.1前向选择和反向消除
5.3.2数据探索的特征选择
5.3.3实用特征选择实例
5.4总结
5.5本章术语
第2部分实 际 应 用
第6章案例：NYC出租车数据
6.1数据：NYC出租车旅程和收费信息
6.1.1数据可视化
6.1.2定义问题并准备数据
6.2建模
6.2.1基本线性模型
6.2.2非线性分类器
6.2.3包含分类特征
6.2.4包含日期-时间特征
6.2.5模型的启示
6.3总结
6.4本章术语
第7章高级特征工程
7.1高级文本特征
7.1.1词袋模型
7.1.2主题建模
7.1.3内容拓展
7.2图像特征
7.2.1简单图像特征
7.2.2提取物体和形状
7.3时间序列特征
7.3.1时间序列数据的类型
7.3.2时间序列数据的预测
7.3.3经典时间序列特征
7.3.4事件流的特征工程
7.4总结
7.5本章术语
第8章NLP高级案例：电影评论情感预测
8.1研究数据和应用场景
8.1.1数据集初探
8.1.2检查数据
8.1.3应用场景有哪些
8.2提取基本NLP特征并构建初始模型
8.2.1词袋特征
8.2.2用朴素贝叶斯算法构建模型
8.2.3tf-idf算法规范词袋特征
8.2.4优化模型参数
8.3高级算法和模型部署的考虑
8.3.1word2vec特征
8.3.2随机森林模型
8.4总结
8.5本章术语
第9章扩展机器学习流程
9.1扩展前需考虑的问题
9.1.1识别关键点
9.1.2选取训练数据子样本代替扩展性
9.1.3可扩展的数据管理系统
9.2机器学习建模流程扩展
9.3预测扩展
9.3.1预测容量扩展
9.3.2预测速度扩展
9.4总结
9.5本章术语
第10章案例：数字显示广告
10.1显示广告
10.2数字广告数据
10.3特征工程和建模策略
10.4数据大小和形状
10.5奇异值分解
10.6资源估计和优化
10.7建模
10.8K近邻算法
10.9随机森林算法
10.10其他实用考虑
10.11总结
10.12本章术语
10.13摘要和结论
附录常用机器学习算法
名词术语中英文对照
・ ・ ・ ・ ・ ・ (收起)第 1 章 机器学习是什么――机器学习定义 .. 1
引言.. 2
1.1 数据. 5
1.1.1 结构型与非结构型数据 5
1.1.2 原始数据与加工.. 7
1.1.3 样本内数据与样本外数据 . 9
1.2 机器学习类别 9
1.2.1 有监督学习 10
1.2.2 无监督学习 10
1.2.3 半监督学习. 11
1.2.4 增强学习 11
1.2.5 深度学习 11
1.2.6 迁移学习.. 12
1.3 性能度量. 12
1.3.1 误差函数.. 13
1.3.2 回归度量.. 14
1.3.3 分类度量.. 15
1.4 总结.. 19
参考资料.. 20
第 2 章 机器学习可行吗――计算学习理论 22
引言 23
2.1 基础知识. 25
2.1.1 二分类. 25
2.1.2 对分 26
2.1.3 增长函数.. 29
2.1.4 突破点. 30
2.2 核心推导. 31
2.2.1 机器学习可行条件 . 31
2.2.2 从已知推未知. 33
2.2.3 从民意调查到机器学习 35
2.2.4 从单一到有限. 36
2.2.5 从有限到无限. 37
2.2.6 从无限到有限. 38
2.3 结论应用. 39
2.3.1 VC 不等式 39
2.3.2 VC 维度 .. 40
2.3.3 模型复杂度 40
2.3.4 样本复杂度 41
2.4 总结.. 42
参考资料.. 43
技术附录.. 43
第 3 章 机器学习怎么学――模型评估选择 47
引言 48
3.1 模型评估. 52
3.2 训练误差和测试误差. 52
3.2.1 训练误差.. 52
3.2.2 真实误差.. 54
3.2.3 测试误差.. 57
3.2.4 学习理论.. 57
3.3 验证误差和交叉验证误差. 60
3.3.1 验证误差.. 60
3.3.2 交叉验证误差. 61
3.3.3 学习理论.. 62
3.4 误差剖析. 64
3.4.1 误差来源.. 64
3.4.2 偏差―方差权衡 66
3.5 模型选择. 67
3.6 总结.. 70
参考资料.. 71
技术附录.. 71
第 4 章 线性回归 73
引言 74
4.1 基础知识. 75
4.1.1 标量微积分 75
4.1.2 向量微积分 76
4.2 模型介绍. 77
4.2.1 核心问题.. 77
4.2.2 通用线性回归模型 . 83
4.2.3 特征缩放.. 84
4.2.4 学习率设定 86
4.2.5 数值算法比较. 87
4.2.6 代码实现.. 89
4.3 总结.. 90
参考资料.. 90
第 5 章 对率回归 92
引言 93
5.1 基础内容. 94
5.1.1 联系函数.. 94
5.1.2 函数绘图.. 95
5.2 模型介绍. 96
5.2.1 核心问题.. 96
5.2.2 查准和查全. 102
5.2.3 类别不平衡. 104
5.2.4 线性不可分. 105
5.2.5 多分类问题. 106
5.2.6 代码实现 109
5.3 总结. 110
参考资料. 111
第 6 章 正则化回归 . 112
引言. 113
6.1 基础知识 114
6.1.1 等值线图. 114
6.1.2 坐标下降. 116
6.2 模型介绍 116
6.2.1 核心问题. 116
6.2.2 模型对比 122
6.2.3 最佳模型 125
6.2.4 代码实现 126
6.3 总结 126
参考资料 127
第 7 章 支持向量机 . 128
引言 129
7.1 基础知识.. 133
7.1.1 向量初体验. 133
7.1.2 拉格朗日量. 136
7.1.3 原始和对偶. 137
7.2 模型介绍.. 138
7.2.1 硬间隔 SVM 原始问题. 138
7.2.2 硬间隔 SVM 对偶问题. 144
7.2.3 软间隔 SVM 原始问题. 148
7.2.4 软间隔 SVM 对偶问题. 150
7.2.5 空间转换 151
7.2.6 核技巧. 155
7.2.7 核 SVM . 158
7.2.8 SMO 算法 .. 159
7.2.9 模型选择 161
7.3 总结 162
参考资料 164
技术附录 164
第 8 章 朴素贝叶斯 . 170
引言 171
8.1 基础知识.. 174
8.1.1 两种概率学派.. 174
8.1.2 两种独立类别.. 174
8.1.3 两种学习算法.. 175
8.1.4 两种估计方法.. 176
8.1.5 两类概率分布.. 177
8.2 模型介绍.. 179
8.2.1 问题剖析 179
8.2.2 朴素贝叶斯算法 182
8.2.3 多元伯努利模型 183
8.2.4 多项事件模型.. 184
8.2.5 高斯判别分析模型 . 184
8.2.6 多分类问题. 186
8.2.7 拉普拉斯校正.. 187
8.2.8 最大似然估计和最大后验估计 . 188
8.3 总结 190
参考资料 191
技术附录 191
第 9 章 决策树 . 195
引言 196
9.1 基础知识.. 198
9.1.1 多数规则 198
9.1.2 熵和条件熵. 198
9.1.3 信息增益和信息增益比 . 200
9.1.4 基尼指数 201
9.2 模型介绍.. 201
9.2.1 二分类决策树.. 201
9.2.2 多分类决策树.. 209
9.2.3 连续值分裂. 210
9.2.4 欠拟合和过拟合. 211
9.2.5 预修剪和后修剪 212
9.2.6 数据缺失 215
9.2.7 代码实现 218
9.3 总结 219
参考资料 219
第 10 章 人工神经网络 220
引言 221
10.1 基本知识 223
10.1.1 转换函数 223
10.1.2 单输入单层单输出神经网络 . 224
10.1.3 多输入单层单输出神经网络 . 224
10.1.4 多输入单层多输出神经网络 . 225
10.1.5 多输入多层多输出神经网络 . 225
10.2 模型应用 227
10.2.1 创建神经网络模型 .. 227
10.2.2 回归应用 230
10.2.3 分类应用 238
第 11 章 正向/反向传播 246
引言 247
11.1 基础知识 250
11.1.1 神经网络元素 250
11.1.2 链式法则 254
11.2 算法介绍 254
11.2.1 正向传播 254
11.2.2 梯度下降 257
11.2.3 反向传播 258
11.2.4 代码实现 262
11.3 总结 268
参考资料 268
技术附录 269
第 12 章 集成学习. 272
引言 273
12.1 结合假设 277
12.1.1 语文和数学. 277
12.1.2 准确和多样. 278
12.1.3 独裁和民主. 279
12.1.4 学习并结合. 279
12.2 装袋法. 280
12.2.1 基本概念 280
12.2.2 自助采样 280
12.2.3 结合假设 281
12.3 提升法. 282
12.3.1 基本概念 282
12.3.2 最优加权 283
12.3.3 结合假设 285
12.4 集成方式 286
12.4.1 同质学习器. 286
12.4.2 异质学习器. 286
12.5 总结 288
参考资料 288
第 13 章 随机森林和提升树 . 289
引言 290
13.1 基础知识 293
13.1.1 分类回归树. 293
13.1.2 前向分布算法 294
13.1.3 置换检验 295
13.2 模型介绍 296
13.2.1 随机森林 296
13.2.2 提升树.. 302
13.2.3 代码实现 306
13.3 总结 307
参考资料 307
第 14 章 极度梯度提升 309
引言 310
14.1 基础知识. 311
14.1.1 树的重定义 311
14.1.2 树的复杂度. 313
14.2 模型介绍 313
14.2.1 XGB 简介. 313
14.2.2 XGB 的泛化度 314
14.2.3 XGB 的精确度 315
14.2.4 XGB 的速度.. 318
14.2.5 代码实现 324
14.3 总结 325
参考资料 326
第 15 章 本书总结. 327
15.1 正交策略 328
15.2 单值评估指标.. 330
15.3 偏差和方差. 332
15.3.1 理论定义 332
15.3.2 实用定义 334
15.3.3 最优误差 335
15.3.4 两者权衡 336
15.3.5 学习曲线 336
结语 339
・ ・ ・ ・ ・ ・ (收起)前言
第1章　机器学习概述 1
1.1　什么是机器学习 1
1.2　机器学习的几个需求层次 3
1.3　机器学习的基本原理 5
1.4　机器学习的基本概念 7
1.4.1　书中用到的术语介绍 7
1.4.2　机器学习的基本模式 11
1.4.3　优化方法 12
1.5　机器学习问题分类 14
1.6　常用的机器学习算法 15
1.7　机器学习算法的性能衡量指标 16
1.8　数据对算法结果的影响 18
第2章　机器学习所需的环境 20
2.1　常用环境 20
2.2　Python简介 21
2.2.1　Python的安装 23
2.2.2　Python的基本用法 24
2.3　Numpy简介 25
2.3.1　Numpy的安装 26
2.3.2　Numpy的基本用法 26
2.4　Scikit-Learn简介 27
2.4.1　Scikit-Learn的安装 28
2.4.2　Scikit-Learn的基本用法 28
2.5　Pandas简介 29
2.5.1　Pandas的安装 30
2.5.2　Pandas的基本用法 31
第3章　线性回归算法 33
3.1　线性回归：“钢铁直男”解决回归问题的正确方法 33
3.1.1　用于预测未来的回归问题 35
3.1.2　怎样预测未来 38
3.1.3　线性方程的“直男”本性 40
3.1.4　最简单的回归问题―线性回归问题 44
3.2　线性回归的算法原理 46
3.2.1　线性回归算法的基本思路 46
3.2.2　线性回归算法的数学解析 48
3.2.3　线性回归算法的具体步骤 53
3.3　在Python中使用线性回归算法 54
3.4　线性回归算法的使用场景 60
第4章　Logistic回归分类算法 61
4.1　Logistic回归：换上“S型曲线马甲”的线性回归 61
4.1.1　分类问题：选择困难症患者的自我救赎 63
4.1.2　Logistic函数介绍 66
4.1.3　此回归非彼回归：“LR”辨析 70
4.2　Logistic回归的算法原理 71
4.2.1　Logistic回归算法的基本思路 71
4.2.2　Logistic回归算法的数学解析 74
4.2.3　Logistic回归算法的具体步骤 78
4.3　在Python中使用Logistic回归算法 78
4.4　Logistic回归算法的使用场景 81
第5章　KNN分类算法 82
5.1　KNN分类算法：用多数表决进行分类 82
5.1.1　用“同类相吸”的办法解决分类问题 84
5.1.2　KNN分类算法的基本方法：多数表决 86
5.1.3　表决权问题 89
5.1.4　KNN的具体含义 89
5.2　KNN分类的算法原理 90
5.2.1　KNN分类算法的基本思路 90
5.2.2　KNN分类算法的数学解析 93
5.2.3　KNN分类算法的具体步骤 94
5.3　在Python中使用KNN分类算法 95
5.4　KNN分类算法的使用场景 96
第6章　朴素贝叶斯分类算法 98
6.1　朴素贝叶斯：用骰子选择 98
6.1.1　从统计角度看分类问题 99
6.1.2　贝叶斯公式的基本思想 102
6.1.3　用贝叶斯公式进行选择 104
6.2　朴素贝叶斯分类的算法原理 106
6.2.1　朴素贝叶斯分类算法的基本思路 106
6.2.2　朴素贝叶斯分类算法的数学解析 108
6.2.3　朴素贝叶斯分类算法的具体步骤 111
6.3　在Python中使用朴素贝叶斯分类算法 111
6.4　朴素贝叶斯分类算法的使用场景 112
第7章　决策树分类算法 114
7.1　决策树分类：用“老朋友”if-else进行选择 114
7.1.1　程序员的选择观：if-else 116
7.1.2　如何种植一棵有灵魂的“树” 118
7.1.3　决策条件的选择艺术 119
7.1.4　决策树的剪枝问题 122
7.2　决策树分类的算法原理 125
7.2.1　决策树分类算法的基本思路 125
7.2.2　决策树分类算法的数学解析 127
7.2.3　决策树分类算法的具体步骤 133
7.3　在Python中使用决策树分类算法 134
7.4　决策树分类算法的使用场景 135
第8章　支持向量机分类算法 137
8.1　支持向量机：线性分类器的“王者” 137
8.1.1　距离是不同类别的天然间隔 139
8.1.2　何为“支持向量” 140
8.1.3　从更高维度看“线性不可分” 142
8.2　支持向量机分类的算法原理 146
8.2.1　支持向量机分类算法的基本思路 146
8.2.2　支持向量机分类算法的数学解析 150
8.2.3　支持向量机分类算法的具体步骤 153
8.3　在Python中使用支持向量机分类算法 154
8.4　支持向量机分类算法的使用场景 156
第9章　K-means聚类算法 157
9.1　用投票表决实现“物以类聚” 157
9.1.1　聚类问题就是“物以类聚”的实施问题 159
9.1.2　用“K”来决定归属类别 162
9.1.3　度量“相似”的距离 164
9.1.4　聚类问题中的多数表决 165
9.2　K-means聚类的算法原理 168
9.2.1　K-means聚类算法的基本思路 168
9.2.2　K-means聚类算法的数学解析 169
9.2.3　K-means聚类算法的具体步骤 170
9.3　在Python中使用K-means聚类算法 171
9.4　K-means聚类算法的使用场景 172
第10章　神经网络分类算法 174
10.1　用神经网络解决分类问题 174
10.1.1　神经元的“内心世界” 177
10.1.2　从神经元看分类问题 180
10.1.3　神经网络的“细胞”：人工神经元 181
10.1.4　构成网络的魔力 184
10.1.5　神经网络与深度学习 188
10.2　神经网络分类的算法原理 188
10.2.1　神经网络分类算法的基本思路 188
10.2.2　神经网络分类算法的数学解析 190
10.2.3　神经网络分类算法的具体步骤 193
10.3　在Python中使用神经网络分类算法 194
10.4　神经网络分类算法的使用场景 195
第11章　集成学习方法 197
11.1　集成学习方法：三个臭皮匠赛过诸葛亮 197
11.1.1　集成学习方法与经典机器学习算法的关系 198
11.1.2　集成学习的主要思想 199
11.1.3　几种集成结构 200
11.2　集成学习方法的具体实现方式 202
11.2.1　Bagging算法 202
11.2.2　Boosting算法 202
11.2.3　Stacking算法 202
11.3　在Python中使用集成学习方法 203
11.4　集成学习方法的使用场景 205
・ ・ ・ ・ ・ ・ (收起)第1章　机器学习应用快速入门　　1
1.1　机器学习与数据科学　　1
1.1.1　机器学习能够解决的问题　　2
1.1.2　机器学习应用流程　　3
1.2　数据与问题定义　　4
1.3　数据收集　　5
1.3.1　发现或观察数据　　5
1.3.2　生成数据　　6
1.3.3　采样陷阱　　7
1.4　数据预处理　　7
1.4.1　数据清洗　　8
1.4.2　填充缺失值　　8
1.4.3　剔除异常值　　8
1.4.4　数据转换　　9
1.4.5　数据归约　　10
1.5　无监督学习　　10
1.5.1　查找相似项目　　10
1.5.2　聚类　　12
1.6　监督学习　　13
1.6.1　分类　　14
1.6.2　回归　　16
1.7　泛化与评估　　18
1.8　小结　　21
第2章　面向机器学习的Java库与平台　　22
2.1　Java环境　　22
2.2　机器学习库　　23
2.2.1　Weka　　23
2.2.2　Java机器学习　　25
2.2.3　Apache Mahout　　26
2.2.4　Apache Spark　　27
2.2.5　Deeplearning4j　　28
2.2.6　MALLET　　29
2.2.7　比较各个库　　30
2.3　创建机器学习应用　　31
2.4　处理大数据　　31
2.5　小结　　33
第3章　基本算法――分类、回归和聚类　　34
3.1　开始之前　　34
3.2　分类　　35
3.2.1　数据　　35
3.2.2　加载数据　　36
3.2.3　特征选择　　37
3.2.4　学习算法　　38
3.2.5　对新数据分类　　40
3.2.6　评估与预测误差度量　　41
3.2.7　混淆矩阵　　41
3.2.8　选择分类算法　　42
3.3　回归　　43
3.3.1　加载数据　　43
3.3.2　分析属性　　44
3.3.3　创建与评估回归模型　　45
3.3.4　避免常见回归问题的小技巧　　48
3.4　聚类　　49
3.4.1　聚类算法　　49
3.4.2　评估　　50
3.5　小结　　51
第4章　利用集成方法预测客户关系　　52
4.1　客户关系数据库　　52
4.1.1　挑战　　53
4.1.2　数据集　　53
4.1.3　评估　　54
4.2　最基本的朴素贝叶斯分类器基准　　55
4.2.1　获取数据　　55
4.2.2　加载数据　　56
4.3　基准模型　　58
4.3.1　评估模型　　58
4.3.2　实现朴素贝叶斯基准线　　59
4.4　使用集成方法进行高级建模　　60
4.4.1　开始之前　　60
4.4.2　数据预处理　　61
4.4.3　属性选择　　62
4.4.4　模型选择　　63
4.4.5　性能评估　　66
4.5　小结　　66
第5章　关联分析　　67
5.1　购物篮分析　　67
5.2　关联规则学习　　69
5.2.1　基本概念　　69
5.2.2　Apriori算法　　71
5.2.3　FP-增长算法　　71
5.2.4　超市数据集　　72
5.3　发现模式　　73
5.3.1　Apriori算法　　73
5.3.2　FP-增长算法　　74
5.4　在其他领域中的应用　　75
5.4.1　医疗诊断　　75
5.4.2　蛋白质序列　　75
5.4.3　人口普查数据　　76
5.4.4　客户关系管理　　76
5.4.5　IT运营分析　　76
5.5　小结　　77
第6章　使用Apache Mahout制作推荐引擎　　78
6.1　基本概念　　78
6.1.1　关键概念　　79
6.1.2　基于用户与基于项目的分析　　79
6.1.3　计算相似度的方法　　80
6.1.4　利用与探索　　81
6.2　获取Apache Mahout　　81
6.3　创建一个推荐引擎　　84
6.3.1　图书评分数据集　　84
6.3.2　加载数据　　84
6.3.3　协同过滤　　89
6.4　基于内容的过滤　　97
6.5　小结　　97
第7章　欺诈与异常检测　　98
7.1　可疑与异常行为检测　　98
7.2　可疑模式检测　　99
7.3　异常模式检测　　100
7.3.1　分析类型　　100
7.3.2　事务分析　　101
7.3.3　规划识别　　101
7.4　保险理赔欺诈检测　　101
7.4.1　数据集　　102
7.4.2　为可疑模式建模　　103
7.5　网站流量异常检测　　107
7.5.1　数据集　　107
7.5.2　时序数据中的异常检测　　108
7.6　小结　　113
第8章　利用Deeplearning4j进行图像识别　　114
8.1　图像识别简介　　114
8.2　图像分类　　120
8.2.1　Deeplearning4j　　120
8.2.2　MNIST数据集　　121
8.2.3　加载数据　　121
8.2.4　创建模型　　122
8.3　小结　　128
第9章　利用手机传感器进行行为识别　　129
9.1　行为识别简介　　129
9.1.1　手机传感器　　130
9.1.2　行为识别流水线　　131
9.1.3　计划　　132
9.2　从手机收集数据　　133
9.2.1　安装Android Studio　　133
9.2.2　加载数据采集器　　133
9.2.3　收集训练数据　　136
9.3　创建分类器　　138
9.3.1　减少假性转换　　140
9.3.2　将分类器嵌入移动应用　　142
9.4　小结　　143
第10章　利用Mallet进行文本挖掘――主题模型与垃圾邮件检测　　144
10.1　文本挖掘简介　　144
10.1.1　主题模型　　145
10.1.2　文本分类　　145
10.2　安装Mallet　　146
10.3　使用文本数据　　147
10.3.1　导入数据　　149
10.3.2　对文本数据做预处理　　150
10.4　为BBC新闻做主题模型　　152
10.4.1　BBC数据集　　152
10.4.2　建模　　153
10.4.3　评估模型　　155
10.4.4　重用模型　　156
10.5　垃圾邮件检测　　157
10.5.1　垃圾邮件数据集　　158
10.5.2　特征生成　　159
10.5.3　训练与测试模型　　160
10.6　小结　　161
第11章　机器学习进阶　　162
11.1　现实生活中的机器学习　　162
11.1.1　噪声数据　　162
11.1.2　类不平衡　　162
11.1.3　特征选择困难　　163
11.1.4　模型链　　163
11.1.5　评价的重要性　　163
11.1.6　从模型到产品　　164
11.1.7　模型维护　　164
11.2　标准与标记语言　　165
11.2.1　CRISP-DM　　165
11.2.2　SEMMA方法　　166
11.2.3　预测模型标记语言　　166
11.3　云端机器学习　　167
11.4　Web资源与比赛　　168
11.4.1　数据集　　168
11.4.2　在线课程　　169
11.4.3　比赛　　170
11.4.4　网站与博客　　170
11.4.5　场馆与会议　　171
11.5　小结　　171
・ ・ ・ ・ ・ ・ (收起)第1部分 背景知识
第1章 机器学习概述 3
1.1 背景 3
1.2 发展现状 6
1.2.1 数据现状 6
1.2.2 机器学习算法现状 8
1.3 机器学习基本概念 12
1.3.1 机器学习流程 12
1.3.2 数据源结构 14
1.3.3 算法分类 16
1.3.4 过拟合问题 18
1.3.5 结果评估 20
1.4 本章小结 22
第2部分 算法流程
第2章 场景解析 25
2.1 数据探查 25
2.2 场景抽象 27
2.3 算法选择 29
2.4 本章小结 31
第3章 数据预处理 32
3.1 采样 32
3.1.1 随机采样 32
3.1.2 系统采样 34
3.1.3 分层采样 35
3.2 归一化 36
3.3 去除噪声 39
3.4 数据过滤 42
3.5 本章小结 43
第4章 特征工程 44
4.1 特征抽象 44
4.2 特征重要性评估 49
4.3 特征衍生 53
4.4 特征降维 57
4.4.1 特征降维的基本概念 57
4.4.2 主成分分析 59
4.5 本章小结 62
第5章 机器学习算法――常规算法 63
5.1 分类算法 63
5.1.1 K近邻 63
5.1.2 朴素贝叶斯 68
5.1.3 逻辑回归 74
5.1.4 支持向量机 81
5.1.5 随机森林 87
5.2 聚类算法 94
5.2.1 K-means 97
5.2.2 DBSCAN 103
5.3 回归算法 109
5.4 文本分析算法 112
5.4.1 分词算法――Hmm 112
5.4.2 TF-IDF 118
5.4.3 LDA 122
5.5 推荐类算法 127
5.6 关系图算法 133
5.6.1 标签传播 134
5.6.2 Dijkstra最短路径 138
5.7 本章小结 145
第6章 机器学习算法――深度学习 146
6.1 深度学习概述 146
6.1.1 深度学习的发展 147
6.1.2 深度学习算法与传统
算法的比较 148
6.2 深度学习的常见结构 152
6.2.1 深度神经网络 152
6.2.2 卷积神经网络 153
6.2.3 循环神经网络 156
6.3 本章小结 157
第3部分 工具介绍
第7章 常见机器学习工具介绍 161
7.1 概述 161
7.2 单机版机器学习工具 163
7.2.1 SPSS 163
7.2.2 R语言 167
7.2.3 工具对比 172
7.3 开源分布式机器学习工具 172
7.3.1 Spark MLib 172
7.3.2 TensorFlow 179
7.4 企业级云机器学习工具 190
7.4.1 亚马逊AWS ML 191
7.4.2 阿里云机器学习PAI 196
7.5 本章小结 205
第4部分 实战应用
第8章 业务解决方案 209
8.1 心脏病预测 209
8.1.1 场景解析 209
8.1.2 实验搭建 211
8.1.3 小结 216
8.2 商品推荐系统 216
8.2.1 场景解析 217
8.2.2 实验搭建 218
8.2.3 小结 220
8.3 金融风控案例 220
8.3.1 场景解析 221
8.3.2 实验搭建 222
8.3.3 小结 225
8.4 新闻文本分析 225
8.4.1 场景解析 225
8.4.2 实验搭建 226
8.4.3 小结 230
8.5 农业贷款发放预测 230
8.5.1 场景解析 230
8.5.2 实验搭建 232
8.5.3 小结 236
8.6 雾霾天气成因分析 236
8.6.1 场景解析 237
8.6.2 实验搭建 238
8.6.3 小结 243
8.7 图片识别 243
8.7.1 场景解析 243
8.7.2 实验搭建 245
8.7.3 小结 253
8.8 本章小结 253
第5部分 知识图谱
第9章 知识图谱 257
9.1 未来数据采集 257
9.2 知识图谱的概述 259
9.3 知识图谱开源
工具 261
9.4 本章小结 264
参考文献 265
・ ・ ・ ・ ・ ・ (收起)第一部分 场景化机器学习
第1章 机器学习如何应用于业务　2
1.1 为什么我们的业务系统如此糟糕　3
1.2 为什么如今自动化很重要　5
1.2.1 什么是生产率　6
1.2.2 机器学习如何提高生产率　6
1.3 机器如何做出决策　7
1.3.1 人：是否基于规则　7
1.3.2 你能相信一个基于模式的答案吗　8
1.3.3 机器学习如何能提升你的业务系统　8
1.4 机器能帮Karen做决策吗　9
1.4.1 目标变量　10
1.4.2 特征　10
1.5 机器如何学习　10
1.6 在你的公司落实使用机器学习进行决策　13
1.7 工具　14
1.7.1 AWS和SageMaker是什么，它们如何帮助你　14
1.7.2 Jupyter笔记本是什么　15
1.8 配置SageMaker为解决第2~7章中的场景做准备　15
1.9 是时候行动了　16
1.10 小结　16
第二部分 公司机器学习的六个场景
第2章 你是否应该将采购订单发送给技术审批人　18
2.1 决策　18
2.2 数据　19
2.3 开始你的训练过程　20
2.4 运行Jupyter笔记本并进行预测　21
2.4.1 第一部分：加载并检查数据　24
2.4.2 第二部分：将数据转换为正确的格式　27
2.4.3 第三部分：创建训练集、验证集和测试集　30
2.4.4 第四部分：训练模型　32
2.4.5 第五部分：部署模型　33
2.4.6 第六部分：测试模型　34
2.5 删除端点并停止你的笔记本实例　35
2.5.1 删除端点　36
2.5.2 停止笔记本实例　37
2.6 小结　38
第3章 你是否应该致电客户以防客户流失　39
3.1 你在决策什么　40
3.2 处理流程　40
3.3 准备数据集　41
3.3.1 转换操作1：标准化数据　42
3.3.2 转换操作2：计算周与周之间的变化　43
3.4 XGBoost基础　43
3.4.1 XGBoost的工作原理　43
3.4.2 机器学习模型如何确定函数的AUC的好坏　45
3.5 准备构建模型　47
3.5.1 将数据集上传到S3　47
3.5.2 在SageMaker上设置笔记本　48
3.6 构建模型　49
3.6.1 第一部分：加载并检查数据　50
3.6.2 第二部分：将数据转换为正确的格式　52
3.6.3 第三部分：创建训练集、验证集和测试集　53
3.6.4 第四部分：训练模型　55
3.6.5 第五部分：部署模型　57
3.6.6 第六部分：测试模型　57
3.7 删除端点并停止笔记本实例　60
3.7.1 删除端点　60
3.7.2 停止笔记本实例　60
3.8 检查以确保端点已被删除　60
3.9 小结　61
第4章 你是否应该将事件上报给支持团队　62
4.1 你在决策什么　62
4.2 处理流程　63
4.3 准备数据集　63
4.4 NLP　65
4.4.1 生成词向量　65
4.4.2 决定每组包含多少单词　67
4.5 BlazingText及其工作原理　68
4.6 准备构建模型　69
4.6.1 将数据集上传到S3　69
4.6.2 在SageMaker上设置笔记本　70
4.7 构建模型　70
4.7.1 第一部分：加载并检查数据　71
4.7.2 第二部分：将数据转换为正确的格式　74
4.7.3 第三部分：创建训练集和验证集　76
4.7.4 第四部分：训练模型　77
4.7.5 第五部分：部署模型　79
4.7.6 第六部分：测试模型　79
4.8 删除端点并停止你的笔记本实例　80
4.8.1 删除端点　80
4.8.2 停止笔记本实例　80
4.9 检查以确保端点已被删除　81
4.10 小结　81
第5章 你是否应该质疑供应商发送给你的发票　82
5.1 你在决策什么　82
5.2 处理流程　84
5.3 准备数据集　85
5.4 什么是异常　86
5.5 监督机器学习与无监督机器学习　87
5.6 随机裁剪森林及其工作原理　88
5.6.1 样本1　88
5.6.2 样本2　90
5.7 准备构建模型　94
5.7.1 将数据集上传到S3　94
5.7.2 在SageMaker上设置笔记本　94
5.8 构建模型　95
5.8.1 第一部分：加载并检查数据　96
5.8.2 第二部分：将数据转换为正确的格式　99
5.8.3 第三部分：创建训练集和验证集　100
5.8.4 第四部分：训练模型　100
5.8.5 第五部分：部署模型　101
5.8.6 第六部分：测试模型　102
5.9 删除端点并停止笔记本实例　104
5.9.1 删除端点　104
5.9.2 停止笔记本实例　104
5.10 检查以确保端点已被删除　105
5.11 小结　105
第6章 预测你公司的每月能耗　106
6.1 你在决策什么　106
6.1.1 时间序列数据介绍　107
6.1.2 Kiara的时间序列数据：每日能耗　109
6.2 加载处理时间序列数据的Jupyter笔记本　109
6.3 准备数据集：绘制时间序列数据　111
6.3.1 通过循环展示数据列　113
6.3.2 创建多个图表　114
6.4 神经网络是什么　116
6.5 准备构建模型　116
6.5.1 将数据集上传到S3　117
6.5.2 在SageMaker上设置笔记本　117
6.6 构建模型　117
6.6.1 第一部分：加载并检查数据　118
6.6.2 第二部分：将数据转换为正确的格式　119
6.6.3 第三部分：创建训练集和测试集　122
6.6.4 第四部分：训练模型　125
6.6.5 第五部分：部署模型　128
6.6.6 第六部分：进行预测并绘制结果　128
6.7 删除端点并停止你的笔记本实例　132
6.7.1 删除端点　133
6.7.2 停止笔记本实例　133
6.8 检查以确保端点已被删除　133
6.9 小结　134
第7 章 优化你公司的每月能耗预测　135
7.1 DeepAR对周期性事件的处理能力　135
7.2 DeepAR的最大优势：整合相关的时间序列　137
7.3 整合额外的数据集到Kiara的能耗模型　137
7.4 准备构建模型　138
7.4.1 下载我们准备的笔记本　138
7.4.2 在SageMaker上设置文件夹　139
7.4.3 将笔记本上传到SageMaker　139
7.4.4 从S3存储桶下载数据集　139
7.4.5 在S3上创建文件夹以保存你的数据　139
7.4.6 将数据集上传到你的AWS存储桶　139
7.5 构建模型　140
7.5.1 第一部分：设置笔记本　140
7.5.2 第二部分：导入数据集　141
7.5.3 第三部分：将数据转换为正确的格式　143
7.5.4 第四部分：创建训练集和测试集　145
7.5.5 第五部分：配置模型并设置服务器以构建模型　147
7.5.6 第六部分：进行预测并绘制结果　151
7.6 删除端点并停止你的笔记本实例　154
7.6.1 删除端点　154
7.6.2 停止笔记本实例　154
7.7 检查以确保端点已被删除　154
7.8 小结　155
第三部分 将机器学习应用到生产环境中
第8章 通过Web提供预测服务　158
8.1 为什么通过Web提供决策和预测服务这么难　158
8.2 本章的步骤概述　159
8.3 SageMaker端点　159
8.4 设置SageMaker端点　160
8.4.1 上传笔记本　161
8.4.2 上传数据　163
8.4.3 运行笔记本并创建端点　165
8.5 设置无服务器API端点　166
8.5.1 在AWS账户上设置AWS证书　167
8.5.2 在本地计算机上设置AWS证书　168
8.5.3 配置证书　169
8.6 创建Web端点　170
8.6.1 安装Chalice　171
8.6.2 创建Hello World API　172
8.6.3 添加为SageMaker端点提供服务的代码　173
8.6.4 配置权限　175
8.6.5 更新requirements.txt文件　176
8.6.6 部署Chalice　176
8.7 提供决策服务　176
8.8 小结　177
第9章 案例研究　179
9.1 案例研究1：WorkPac　180
9.1.1 项目设计　181
9.1.2 第一阶段：准备并测试模型　181
9.1.3 第二阶段：实施POC　183
9.1.4 第三阶段：将流程嵌入公司的运营中　183
9.1.5 接下来的工作　183
9.1.6 吸取的教训　183
9.2 案例研究2：Faethm　184
9.2.1 AI核心　184
9.2.2 使用机器学习优化Faethm公司的流程　184
9.2.3 第一阶段：获取数据　185
9.2.4 第二阶段：识别特征　186
9.2.5 第三阶段：验证结果　186
9.2.6 第四阶段：应用到生产环境中　186
9.3 结论　187
9.3.1 观点1：建立信任　187
9.3.2 观点2：正确获取数据　187
9.3.3 观点3：设计操作模式以充分利用机器学习能力　187
9.3.4 观点4：在各个方面都使用了机器学习后，你的公司看起来怎么样　187
9.4 小结　188
附录A 注册AWS　189
附录B 设置并使用S3以存储文件　195
附录C 设置并使用AWS SageMaker来构建机器学习系统　204
附录D 停止全部服务　208
附录E 安装Python　211
・ ・ ・ ・ ・ ・ (收起)第 1章 MATLAB机器学习初体验 1
1.1　机器学习基础　1
1.2　机器学习算法的分类　4
1.2.1　监督学习　4
1.2.2　非监督学习　5
1.2.3　强化学习　5
1.3　选择正确的算法　6
1.4　构建机器学习模型的流程　7
1.5　MATLAB中的机器学习支持简介　8
1.5.1　操作系统、硬件平台要求　10
1.5.2　MATLAB安装要求　11
1.6　统计机器学习工具箱　11
1.6.1　数据类型　13
1.6.2　统计机器学习工具箱功能简介　13
1.7　神经网络工具箱　18
1.8　MATLAB中的统计学和线性代数　19
1.9　总结　21
第　2章 使用MATLAB导入数据和组织数据　22
2.1　熟悉MATLAB桌面　22
2.2　将数据导入MATLAB　27
2.2.1　导入向导　27
2.2.2　通过程序语句导入数据　29
2.3　从MATLAB导出数据　36
2.4　处理媒体文件　37
2.4.1　处理图像数据　37
2.4.2　音频的导入/导出　39
2.5　数据组织　39
2.5.1　元胞数组　40
2.5.2　结构体数组　42
2.5.3　table类型　44
2.5.4　分类数组　46
2.6　总结　47
第3章　从数据到知识挖掘　49
3.1　区分变量类别　50
3.1.1　定量变量　50
3.1.2　定性变量　50
3.2　数据准备　51
3.2.1　初步查看数据　51
3.2.2　找到缺失值　53
3.2.3　改变数据类型　54
3.2.4　替换缺失值　54
3.2.5　移除缺失值　55
3.2.6　为表格排序　56
3.2.7　找到数据中的异常值　56
3.2.8　将多个数据源合并成一个数据源　57
3.3　探索性统计指标―数值测量　59
3.3.1　位置测量　59
3.3.2　分散度的测量　61
3.3.3　分布形状的测量　64
3.4　探索性可视化　66
3.4.1　图形数据统计分析对话框　67
3.4.2　柱状图　70
3.4.3　箱形图　75
3.4.4　散点图　77
3.5　总结　78
第4章　找到变量之间的关系―回归方法　80
4.1　寻找线性关系　80
4.1.1　最小二乘回归　81
4.1.2　基本拟合接口　86
4.2　如何创建一个线性回归模型　88
4.2.1　通过稳健回归消除异常值的影响　93
4.2.2　多元线性回归　96
4.3　多项式回归　101
4.4　回归学习器App　103
4.5　总结　107
第5章　模式识别之分类算法　108
5.1　决策树分类　108
5.2　概率分类模型―朴素贝叶斯分类　115
5.2.1　概率论基础　116
5.2.2　使用朴素贝叶斯进行分类　119
5.2.3　MATLAB中的贝叶斯方法　120
5.3　判别分析分类　123
5.4　k邻近算法　128
5.5　MATLAB分类学习器App　132
5.6　总结　136
第6章　无监督学习　137
6.1　聚类分析简介　137
6.1.1　相似度与离散度指标　138
6.1.2　聚类方法类型简介　139
6.2　层次聚类算法　141
6.2.1　层次聚类中的相似度指标　141
6.2.2　定义层次聚类中的簇　143
6.2.3　如何理解层次聚类图　145
6.2.4　验证聚类结果　147
6.3　k均值聚类―基于均值聚类　148
6.3.1　k均值算法　148
6.3.2　函数kmeans()　149
6.3.3　silhouette图―可视化聚类结果　152
6.4　k中心点聚类―基于样本中心聚类　153
6.4.1　什么是中心点　154
6.4.2　函数kmedoids()　154
6.4.3　评估聚类结果　156
6.5　高斯混合模型聚类　156
6.5.1　高斯分布　156
6.5.2　MATLAB中的GMM支持　157
6.5.3　使用后验概率分布进行聚类　159
6.6　总结　160
第7章　人工神经网络――模拟人脑的思考方式　162
7.1　神经网络简介　162
7.2　神经网络基础构成　165
7.2.1　隐藏层数量　170
7.2.2　每层的节点数量　170
7.2.3　神经网络训练方法　170
7.3　神经网络工具箱　171
7.4　工具箱的用户界面　175
7.5　使用神经网络进行数据拟合　176
7.5.1　如何使用拟合App（nftool）　178
7.5.2　脚本分析　186
7.6　总结　188
第8章　降维――改进机器学习模型的性能　190
8.1　特征选择　190
8.1.1　分步回归　191
8.1.2　MATLAB中的分步回归　192
8.2　特征提取　199
8.3　总结　210
第9章　机器学习实战　211
9.1　用于预测混凝土质量的数据拟合　211
9.2　使用神经网络诊断甲状腺疾病　222
9.3　使用模糊聚类对学生进行分簇　226
9.4　总结　231
・ ・ ・ ・ ・ ・ (收起)译者序
前 言
致 谢
第1章 引言1
1.1 传统机器学习范式1
1.2 案例3
1.3 终身学习简史7
1.4 终身学习的定义9
1.5 知识类型和关键挑战14
1.6 评估方法和大数据的角色17
1.7 本书大纲18
第2章 相关学习范式20
2.1 迁移学习20
2.1.1 结构对应学习21
2.1.2 朴素贝叶斯迁移分类器22
2.1.3 迁移学习中的深度学习23
2.1.4 迁移学习与终身学习的区别24
2.2 多任务学习25
2.2.1 多任务学习中的任务相关性25
2.2.2 GO-MTL：使用潜在基础任务的多任务学习26
2.2.3 多任务学习中的深度学习28
2.2.4 多任务学习与终身学习的区别30
2.3 在线学习30
2.4 强化学习31
2.5 元学习32
2.6 小结34
第3章 终身监督学习35
3.1 定义和概述36
3.2 基于记忆的终身学习37
3.2.1 两个基于记忆的学习方法37
3.2.2 终身学习的新表达37
3.3 终身神经网络39
3.3.1 MTL网络39
3.3.2 终身EBNN40
3.4 ELLA：高效终身学习算法41
3.4.1 问题设定41
3.4.2 目标函数42
3.4.3 解决一个低效问题43
3.4.4 解决第二个低效问题45
3.4.5 主动的任务选择46
3.5 终身朴素贝叶斯分类47
3.5.1 朴素贝叶斯文本分类47
3.5.2 LSC的基本思想49
3.5.3 LSC技术50
3.5.4 讨论52
3.6 基于元学习的领域词嵌入52
3.7 小结和评估数据集54
第4章 持续学习与灾难性遗忘56
4.1 灾难性遗忘56
4.2 神经网络中的持续学习58
4.3 无遗忘学习61
4.4 渐进式神经网络62
4.5 弹性权重合并63
4.6 iCaRL：增量分类器与表示学习65
4.6.1 增量训练66
4.6.2 更新特征表示67
4.6.3 为新类构建范例集68
4.6.4 在iCaRL中完成分类68
4.7 专家网关69
4.7.1 自动编码网关69
4.7.2 测量训练的任务相关性70
4.7.3 为测试选择相关的专家71
4.7.4 基于编码器的终身学习71
4.8 生成式重放的持续学习72
4.8.1 生成式对抗网络72
4.8.2 生成式重放73
评估灾难性遗忘74
4.10 小结和评估数据集75
第5章 开放式学习79
5.1 问题定义和应用80
5.2 基于中心的相似空间学习81
5.2.1 逐步更新CBS学习模型82
5.2.2 测试CBS学习模型84
5.2.3 用于未知类检测的CBS学习84
5.3 DOC：深度开放式分类87
5.3.1 前馈层和一对其余层87
5.3.2 降低开放空间89
5.3.3 DOC用于图像分类90
5.3.4 发现未知类90
5.4 小结和评估数据集91
第6章 终身主题建模93
6.1 终身主题建模的主要思想93
6.2 LTM：终身主题模型97
6.2.1 LTM模型97
6.2.2 主题知识挖掘99
6.2.3 融合过去的知识100
6.2.4 Gibbs采样器的条件分布102
6.3 AMC：少量数据的终身主题模型102
6.3.1 AMC整体算法103
6.3.2 挖掘must-link知识104
6.3.3 挖掘cannot-link知识107
6.3.4 扩展的Pólya瓮模型108
6.3.5 Gibbs采样器的采样分布110
6.4 小结和评估数据集112
第7章 终身信息提取114
7.1 NELL：停止语言学习器114
7.1.1 NELL结构117
7.1.2 NELL中的提取器与学习118
7.1.3 NELL中的耦合约束120
7.2 终身评价目标提取121
7.2.1 基于的终身学习122
7.2.2 AER算法123
7.2.3 知识学习124
7.2.4 使用过去知识125
7.3 在工作中学习126
7.3.1 条件随机场127
7.3.2 一般依赖特征128
7.3.3 L-CRF算法130
7.4 Lifelong-RL：终身松弛标记法131
7.4.1 松弛标记法132
7.4.2 终身松弛标记法133
7.5 小结和评估数据集133
第8章 聊天机器人的持续知识学习135
8.1 LiLi：终身交互学习与推理136
8.2 LiLi的基本思想139
8.3 LiLi的组件141
8.4 运行示例142
8.5 小结和评估数据集142
第9章 终身强化学习144
9.1 基于多环境的终身强化学习146
9.2 层次贝叶斯终身强化学习147
9.2.1 动机147
9.2.2 层次贝叶斯方法148
9.2.3 MTRL算法149
9.2.4 更新层次模型参数150
9.2.5 对MDP进行采样151
9.3 PG-ELLA：终身策略梯度强化学习152
9.3.1 策略梯度强化学习152
9.3.2 策略梯度终身学习设置154
9.3.3 目标函数和优化154
9.3.4 终身学习的安全策略搜索156
9.3.5 跨领域终身强化学习156
9.4 小结和评估数据集157
第10章 结论及未来方向159
参考文献164
・ ・ ・ ・ ・ ・ (收起)第1章　Python机器学习实践入门 1
1.1　机器学习常用概念 1
1.2　数据的准备、处理和可视化
―NumPy、pandas和matplotlib教程 6
1.2.1　NumPy的用法 6
1.2.2　理解pandas模块 23
1.2.3　matplotlib教程 32
1.3　本书使用的科学计算库 35
1.4　机器学习的应用场景 36
1.5　小结 36
第2章　无监督机器学习 37
2.1　聚类算法 37
2.1.1　分布方法 38
2.1.2　质心点方法 40
2.1.3　密度方法 41
2.1.4　层次方法 44
2.2　降维 52
2.3　奇异值分解（SVD） 57
2.4　小结 58
第3章　有监督机器学习 59
3.1　模型错误评估 59
3.2　广义线性模型 60
3.2.1　广义线性模型的概率
解释 63
3.2.2　k近邻 63
3.3　朴素贝叶斯 64
3.3.1　多项式朴素贝叶斯 65
3.3.2　高斯朴素贝叶斯 66
3.4　决策树 67
3.5　支持向量机 70
3.6　有监督学习方法的对比 75
3.6.1　回归问题 75
3.6.2　分类问题 80
3.7　隐马尔可夫模型 84
3.8　小结 93
第4章　Web挖掘技术 94
4.1　Web结构挖掘 95
4.1.1　Web爬虫 95
4.1.2　索引器 95
4.1.3　排序―PageRank
算法 96
4.2　Web内容挖掘 97
句法解析 97
4.3　自然语言处理 98
4.4　信息的后处理 108
4.4.1　潜在狄利克雷分配 108
4.4.2　观点挖掘（情感
分析） 113
4.5　小结 117
第5章　推荐系统 118
5.1　效用矩阵 118
5.2　相似度度量方法 120
5.3　协同过滤方法 120
5.3.1　基于记忆的协同
过滤 121
5.3.2　基于模型的协同
过滤 126
5.4　CBF方法 130
5.4.1　商品特征平均得分
方法 131
5.4.2　正则化线性回归
方法 132
5.5　用关联规则学习，构建推荐
系统 133
5.6　对数似然比推荐方法 135
5.7　混合推荐系统 137
5.8　推荐系统评估 139
5.8.1　均方根误差（RMSE）
评估 140
5.8.2　分类效果的度量方法 143
5.9　小结 144
第6章　开始Django之旅 145
6.1　HTTP―GET和POST方法的
基础 145
6.1.1　Django的安装和
服务器的搭建 146
6.1.2　配置 147
6.2　编写应用―Django
最重要的功能 150
6.2.1　model 150
6.2.2　HTML网页背后的
URL和view 151
6.2.3　URL声明和view 154
6.3　管理后台 157
6.3.1　shell接口 158
6.3.2　命令 159
6.3.3　RESTful应用编程
接口（API） 160
6.4　小结 162
第7章　电影推荐系统Web应用 163
7.1　让应用跑起来 163
7.2　model 165
7.3　命令 166
7.4　实现用户的注册、登录和
登出功能 172
7.5　信息检索系统（电影查询） 175
7.6　打分系统 178
7.7　推荐系统 180
7.8　管理界面和API 182
7.9　小结 184
第8章　影评情感分析应用 185
8.1　影评情感分析应用用法
简介 185
8.2　搜索引擎的选取和应用的
代码 187
8.3　Scrapy的配置和情感分析
应用代码 189
8.3.1　Scrapy的设置 190
8.3.2　Scraper 190
8.3.3　Pipeline 193
8.3.4　爬虫 194
8.4　Django model 196
8.5　整合Django和Scrapy 197
8.5.1　命令（情感分析模型和
删除查询结果） 198
8.5.2　情感分析模型加载器 198
8.5.3　删除已执行过的查询 201
8.5.4　影评情感分析器―
Django view和HTML
代码 202
8.6　PageRank：Django view和
算法实现 206
8.7　管理后台和API 210
8.8　小结 212
・ ・ ・ ・ ・ ・ (收起)第1章　监督学习　　1
1.1　简介　　1
1.2　数据预处理技术　　2
1.2.1　准备工作　　2
1.2.2　详细步骤　　2
1.3　标记编码方法　　4
1.4　创建线性回归器　　6
1.4.1　准备工作　　6
1.4.2　详细步骤　　7
1.5　计算回归准确性　　9
1.5.1　准备工作　　9
1.5.2　详细步骤　　10
1.6　保存模型数据　　10
1.7　创建岭回归器　　11
1.7.1　准备工作　　11
1.7.2　详细步骤　　12
1.8　创建多项式回归器　　13
1.8.1　准备工作　　13
1.8.2　详细步骤　　14
1.9　估算房屋价格　　15
1.9.1　准备工作　　15
1.9.2　详细步骤　　16
1.10　计算特征的相对重要性　　17
1.11　评估共享单车的需求分布　　19
1.11.1　准备工作　　19
1.11.2　详细步骤　　19
1.11.3　更多内容　　21
第2章　创建分类器　　24
2.1　简介　　24
2.2　建立简单分类器　　25
2.2.1　详细步骤　　25
2.2.2　更多内容　　27
2.3　建立逻辑回归分类器　　27
2.4　建立朴素贝叶斯分类器　　31
2.5　将数据集分割成训练集和测试集　　32
2.6　用交叉验证检验模型准确性　　33
2.6.1　准备工作　　34
2.6.2　详细步骤　　34
2.7　混淆矩阵可视化　　35
2.8　提取性能报告　　37
2.9　根据汽车特征评估质量　　38
2.9.1　准备工作　　38
2.9.2　详细步骤　　38
2.10　生成验证曲线　　40
2.11　生成学习曲线　　43
2.12　估算收入阶层　　45
第3章　预测建模　　48
3.1　简介　　48
3.2　用SVM建立线性分类器　　49
3.2.1　准备工作　　49
3.2.2　详细步骤　　50
3.3　用SVM建立非线性分类器　　53
3.4　解决类型数量不平衡问题　　55
3.5　提取置信度　　58
3.6　寻找最优超参数　　60
3.7　建立事件预测器　　62
3.7.1　准备工作　　62
3.7.2　详细步骤　　62
3.8　估算交通流量　　64
3.8.1　准备工作　　64
3.8.2　详细步骤　　64
第4章　无监督学习――聚类　　67
4.1　简介　　67
4.2　用k-means算法聚类数据　　67
4.3　用矢量量化压缩图片　　70
4.4　建立均值漂移聚类模型　　74
4.5　用凝聚层次聚类进行数据分组　　76
4.6　评价聚类算法的聚类效果　　79
4.7　用DBSCAN算法自动估算集群数量　　82
4.8　探索股票数据的模式　　86
4.9　建立客户细分模型　　88
第5章　构建推荐引擎　　91
5.1　简介　　91
5.2　为数据处理构建函数组合　　92
5.3　构建机器学习流水线　　93
5.3.1　详细步骤　　93
5.3.2　工作原理　　95
5.4　寻找最近邻　　95
5.5　构建一个KNN分类器　　98
5.5.1　详细步骤　　98
5.5.2　工作原理　　102
5.6　构建一个KNN回归器　　102
5.6.1　详细步骤　　102
5.6.2　工作原理　　104
5.7　计算欧氏距离分数　　105
5.8　计算皮尔逊相关系数　　106
5.9　寻找数据集中的相似用户　　108
5.10　生成电影推荐　　109
第6章　分析文本数据　　112
6.1　简介　　112
6.2　用标记解析的方法预处理数据　　113
6.3　提取文本数据的词干　　114
6.3.1　详细步骤　　114
6.3.2　工作原理　　115
6.4　用词形还原的方法还原文本的基本形式　　116
6.5　用分块的方法划分文本　　117
6.6　创建词袋模型　　118
6.6.1　详细步骤　　118
6.6.2　工作原理　　120
6.7　创建文本分类器　　121
6.7.1　详细步骤　　121
6.7.2　工作原理　　123
6.8　识别性别　　124
6.9　分析句子的情感　　125
6.9.1　详细步骤　　126
6.9.2　工作原理　　128
6.10　用主题建模识别文本的模式　　128
6.10.1　详细步骤　　128
6.10.2　工作原理　　131
第7章　语音识别　　132
7.1　简介　　132
7.2　读取和绘制音频数据　　132
7.3　将音频信号转换为频域　　134
7.4　自定义参数生成音频信号　　136
7.5　合成音乐　　138
7.6　提取频域特征　　140
7.7　创建隐马尔科夫模型　　142
7.8　创建一个语音识别器　　143
第8章　解剖时间序列和时序数据　　147
8.1　简介　　147
8.2　将数据转换为时间序列格式　　148
8.3　切分时间序列数据　　150
8.4　操作时间序列数据　　152
8.5　从时间序列数据中提取统计数字　　154
8.6　针对序列数据创建隐马尔科夫模型　　157
8.6.1　准备工作　　158
8.6.2　详细步骤　　158
8.7　针对序列文本数据创建条件随机场　　161
8.7.1　准备工作　　161
8.7.2　详细步骤　　161
8.8　用隐马尔科夫模型分析股票市场数据　　164
第9章　图像内容分析　　166
9.1　简介　　166
9.2　用OpenCV-Pyhon操作图像　　167
9.3　检测边　　170
9.4　直方图均衡化　　174
9.5　检测棱角　　176
9.6　检测SIFT特征点　　178
9.7　创建Star特征检测器　　180
9.8　利用视觉码本和向量量化创建特征　　182
9.9　用极端随机森林训练图像分类器　　185
9.10　创建一个对象识别器　　187
第10章　人脸识别　　189
10.1　简介　　189
10.2　从网络摄像头采集和处理视频信息　　189
10.3　用Haar级联创建一个人脸识别器　　191
10.4　创建一个眼睛和鼻子检测器　　193
10.5　做主成分分析　　196
10.6　做核主成分分析　　197
10.7　做盲源分离　　201
10.8　用局部二值模式直方图创建一个人脸识别器　　205
第11章　深度神经网络　　210
11.1　简介　　210
11.2　创建一个感知器　　211
11.3　创建一个单层神经网络　　213
11.4　创建一个深度神经网络　　216
11.5　创建一个向量量化器　　219
11.6　为序列数据分析创建一个递归神经网络　　221
11.7　在光学字符识别数据库中将字符可视化　　225
11.8　用神经网络创建一个光学字符识别器　　226
第12章　可视化数据　　230
12.1　简介　　230
12.2　画3D散点图　　230
12.3　画气泡图　　232
12.4　画动态气泡图　　233
12.5　画饼图　　235
12.6　画日期格式的时间序列数据　　237
12.7　画直方图　　239
12.8　可视化热力图　　241
12.9　动态信号的可视化模拟　　242
・ ・ ・ ・ ・ ・ (收起)目录
第1章　机器学习基础　　1
1.1　机器学习概要　　2
什么是机器学习　　2
机器学习的种类　　3
机器学习的应用　　8
1.2　机器学习的步骤　　9
数据的重要性　　9
有监督学习（分类）的例子　　11
无监督学习（聚类）的例子　　16
可视化　　18
图形的种类和画法：使用Matplotlib显示图形的方法　　22
使用pandas理解和处理数据　　30
本章小结　　36
第2章　有监督学习　　37
2.1　算法1：线性回归　　38
概述　　38
算法说明　　39
详细说明　　41
2.2　算法2：正则化　　45
概述　　45
算法说明　　48
详细说明　　50
2.3　算法3：逻辑回归　　52
概述　　52
算法说明　　53
详细说明　　55
2.4　算法4：支持向量机　　58
概述　　58
算法说明　　59
详细说明　　60
2.5　算法5：支持向量机（核方法）　　63
概述　　63
算法说明　　64
详细说明　　65
2.6　算法6：朴素贝叶斯　　68
概述　　68
算法说明　　70
详细说明　　74
2.7　算法7：随机森林　　76
概述　　76
算法说明　　77
详细说明　　80
2.8　算法8：神经网络　　81
概述　　81
算法说明　　83
详细说明　　86
2.9　算法9：KNN　　88
概述　　88
算法说明　　89
详细说明　　90
第3章　无监督学习　　93
3.1　算法10：PCA　　94
概述　　94
算法说明　　95
详细说明　　98
3.2　算法11：LSA　　99
概述　　99
算法说明　　100
详细说明　　104
3.3　算法12：NMF　　105
概述　　105
算法说明　　106
详细说明　　108
3.4　算法13：LDA　　111
概述　　111
算法说明　　112
详细说明　　114
3.5　算法14：k-means算法　　117
概述　　117
算法说明　　117
详细说明　　119
3.6　算法15：混合高斯分布　　122
概述　　122
算法说明　　123
详细说明　　126
3.7　算法16：LLE　　127
概述　　127
算法说明　　128
详细说明　　131
3.8　算法17：t-SNE　　133
概述　　133
算法说明　　134
详细说明　　136
第4章　评估方法和各种数据的处理　　139
4.1　评估方法　　140
有监督学习的评估　　140
分类问题的评估方法　　140
回归问题的评估方法　　148
均方误差和决定系数指标的不同　　152
与其他算法进行比较　　152
超参数的设置　　154
模型的过拟合　　155
防止过拟合的方法　　155
将数据分为训练数据和验证数据　　156
交叉验证　　158
搜索超参数　　160
4.2　文本数据的转换处理　　163
基于单词出现次数的转换　　163
基于tf-idf的转换　　164
应用于机器学习模型　　165
4.3　图像数据的转换处理　　167
直接将像素信息作为数值使用　　167
将转换后的向量数据作为输入来应用机器学习模型　　168
第5章　环境搭建 171
5.1　Python 3的安装　　172
Windows　　172
macOS　　172
Linux　　173
使用Anaconda在Windows上安装　　174
5.2　虚拟环境　　175
通过官方安装程序安装Python的情况　　175
通过Anaconda安装Python的情况　　177
5.3　第三方包的安装　　178
什么是第三方包　　178
安装第三方包的方法　　178
参考文献　　180
・ ・ ・ ・ ・ ・ (收起)第1章　学习前的准备　　1
1.1　关于机器学习 2
1.1.1　学习机器学习的窍门 4
1.1.2　机器学习中问题的分类 5
1.1.3　本书的结构 6
1.2　安装Python 7
1.3　Jupyter Notebook 11
1.3.1　Jupyter Notebook的用法 11
1.3.2　输入Markdown格式文本 14
1.3.3　更改文件名 16
1.4　安装Keras和TensorFlow 17
第2章　Python基础知识　　19
2.1　四则运算 20
2.1.1　四则运算的用法 20
2.1.2　幂运算 20
2.2　变量 21
2.2.1　利用变量进行计算 21
2.2.2　变量的命名 21
2.3　类型 22
2.3.1　类型的种类 22
2.3.2　检查类型 22
2.3.3　字符串 23
2.4　print 语句 24
2.4.1　print语句的用法 24
2.4.2　同时显示数值和字符串的方法1 24
2.4.3　同时显示数值和字符串的方法2 25
2.5　list（数组变量） 26
2.5.1　list的用法 26
2.5.2　二维数组 27
2.5.3　创建连续的整数数组 28
2.6　tuple（数组） 29
2.6.1　tuple的用法 29
2.6.2　读取元素 29
2.6.3　长度为1的tuple 30
2.7　if 语句 31
2.7.1　if语句的用法 31
2.7.2　比较运算符 32
2.8　for 语句 33
2.8.1　for语句的用法 33
2.8.2　enumerate的用法 33
2.9　向量 34
2.9.1　NumPy的用法 34
2.9.2　定义向量 35
2.9.3　读取元素 36
2.9.4　替换元素 36
2.9.5　创建连续整数的向量 36
2.9.6　ndarray的注意事项 37
2.10　矩阵 38
2.10.1　定义矩阵 38
2.10.2　矩阵的大小 38
2.10.3　读取元素 39
2.10.4　替换元素 39
2.10.5　生成元素为0和1的ndarray 39
2.10.6　生成元素随机的矩阵 40
2.10.7　改变矩阵的大小 41
2.11　矩阵的四则运算 41
2.11.1　矩阵的四则运算 41
2.11.2　标量×矩阵 42
2.11.3　算术函数 42
2.11.4　计算矩阵乘积 43
2.12　切片 43
2.13　替换满足条件的数据 45
2.14　help 46
2.15　函数 47
2.15.1　函数的用法 47
2.15.2　参数与返回值 47
2.16　保存文件 49
2.16.1　保存一个ndarray类型变量 49
2.16.2　保存多个ndarray类型变量 49
第3章　数据可视化　　51
3.1　绘制二维图形 52
3.1.1　绘制随机图形 52
3.1.2　代码清单的格式 53
3.1.3　绘制三次函数f (x) = (x - 2) x (x + 2) 53
3.1.4　确定绘制范围 54
3.1.5　绘制图形 55
3.1.6　装饰图形 55
3.1.7　并列显示多张图形 58
3.2　绘制三维图形 59
3.2.1　包含两个变量的函数 59
3.2.2　用颜色表示数值：pcolor 60
3.2.3　绘制三维图形：surface 62
3.2.4　绘制等高线：contour 64
第4章　机器学习中的数学　　67
4.1　向量 68
4.1.1　什么是向量 68
4.1.2　用Python定义向量 69
4.1.3　列向量的表示方法 69
4.1.4　转置的表示方法 70
4.1.5　加法和减法 71
4.1.6　标量积 73
4.1.7　内积 74
4.1.8　向量的模 75
4.2　求和符号 76
4.2.1　带求和符号的数学式的变形 77
4.2.2　通过内积求和 79
4.3　累乘符号 79
4.4　导数 80
4.4.1　多项式的导数 80
4.4.2　带导数符号的数学式的变形 82
4.4.3　复合函数的导数 83
4.4.4　复合函数的导数：链式法则 84
4.5　偏导数 85
4.5.1　什么是偏导数 85
4.5.2　偏导数的图形 87
4.5.3　绘制梯度的图形 89
4.5.4　多变量的复合函数的偏导数 91
4.5.5　交换求和与求导的顺序 93
4.6　矩阵 95
4.6.1　什么是矩阵 95
4.6.2　矩阵的加法和减法 97
4.6.3　标量积 99
4.6.4　矩阵的乘积 100
4.6.5　单位矩阵 103
4.6.6　逆矩阵 105
4.6.7　转置 107
4.6.8　矩阵和联立方程式 109
4.6.9　矩阵和映射 111
4.7　指数函数和对数函数 113
4.7.1　指数 113
4.7.2　对数 115
4.7.3　指数函数的导数 118
4.7.4　对数函数的导数 120
4.7.5　Sigmoid函数 121
4.7.6　Softmax函数 123
4.7.7　Softmax函数和Sigmoid函数 127
4.7.8　高斯函数 128
4.7.9　二维高斯函数 129
第5章　有监督学习：回归　　135
5.1　一维输入的直线模型 136
5.1.1　直线模型 138
5.1.2　平方误差函数 139
5.1.3　求参数（梯度法） 142
5.1.4　直线模型参数的解析解 148
5.2　二维输入的平面模型 152
5.2.1　数据的表示方法 154
5.2.2　平面模型 155
5.2.3　平面模型参数的解析解 157
5.3　D维线性回归模型 159
5.3.1　D维线性回归模型 160
5.3.2　参数的解析解 160
5.3.3　扩展到不通过原点的平面 164
5.4　线性基底函数模型 165
5.5　过拟合问题 171
5.6　新模型的生成 181
5.7　模型的选择 185
5.8　小结 186
第6章　有监督学习：分类　　189
6.1　一维输入的二元分类 190
6.1.1　问题设置 190
6.1.2　使用概率表示类别分类 194
6.1.3　最大似然估计 196
6.1.4　逻辑回归模型 199
6.1.5　交叉熵误差 201
6.1.6　学习法则的推导 205
6.1.7　通过梯度法求解 209
6.2　二维输入的二元分类 210
6.2.1　问题设置 210
6.2.2　逻辑回归模型 214
6.3　二维输入的三元分类 219
6.3.1　三元分类逻辑回归模型 219
6.3.2　交叉熵误差 222
6.3.3　通过梯度法求解 223
第7章　神经网络与深度学习　　227
7.1　神经元模型 229
7.1.1　神经细胞 229
7.1.2　神经元模型 230
7.2　神经网络模型 234
7.2.1　二层前馈神经网络 234
7.2.2　二层前馈神经网络的实现 237
7.2.3　数值导数法 242
7.2.4　通过数值导数法应用梯度法 246
7.2.5　误差反向传播法 251
7.2.6　求.E / .vkj 252
7.2.7　求.E / .wji 256
7.2.8　误差反向传播法的实现 262
7.2.9　学习后的神经元的特性 268
7.3　使用Keras实现神经网络模型 270
7.3.1　二层前馈神经网络 271
7.3.2　Keras的使用流程 273
第8章　神经网络与深度学习的应用（手写数字识别）　　277
8.1　MINST数据集 278
8.2　二层前馈神经网络模型 279
8.3　ReLU激活函数 286
8.4　空间过滤器 291
8.5　卷积神经网络 295
8.6　池化 300
8.7　Dropout 301
8.8　融合了各种特性的MNIST识别网络模型 302
第9章　无监督学习　　307
9.1　二维输入数据 308
9.2　K-means算法 310
9.2.1　K-means算法的概要 310
9.2.2　步骤0：准备变量与初始化 311
9.2.3　步骤1：更新R 313
9.2.4　步骤2：更新μ 315
9.2.5　失真度量 318
9.3　混合高斯模型 320
9.3.1　基于概率的聚类 320
9.3.2　混合高斯模型 323
9.3.3　EM算法的概要 328
9.3.4　步骤0：准备变量与初始化 329
9.3.5　步骤1（步骤E）：更新γ 330
9.3.6　步骤2（步骤M）：更新π、μ和Σ 332
9.3.7　似然 336
第10章　本书小结　　339
后记　　349
・ ・ ・ ・ ・ ・ (收起)译者序
原书前言
致谢
第1章 文本机器学习导论1
1.1导论1
1.1.1本章内容组织结构2
1.2文本学习有何特别之处3
1.3文本分析模型4
1.3.1文本预处理和相似度计算4
1.3.2降维与矩阵分解6
1.3.3文本聚类6
1.3.4文本分类与回归建模8
1.3.5结合文本与异构数据的联合分析10
1.3.6信息检索与网页搜索11
1.3.7序列语言建模与嵌入11
1.3.8文本摘要11
1.3.9信息提取11
1.3.10意见挖掘与情感分析12
1.3.11文本分割与事件检测12
1.4本章小结12
1.5参考资料13
1.5.1软件资源13
1.6习题13
第2章 文本预处理与相似度计算15
2.1导论15
2.1.1本章内容组织结构16
2.2原始文本提取与词条化16
2.2.1文本提取中与网页相关的问题18
2.3从词条中提取词项19
2.3.1停用词移除19
2.3.2连字符19
2.3.3大小写转换20
2.3.4基于用法的合并20
2.3.5词干提取21
2.4向量空间表示与归一化21
2.5文本中的相似度计算23
2.5.1idf归一化和词干提取是否总是有用25
2.6本章小结26
2.7参考资料26
2.7.1软件资源26
2.8习题27
第3章 矩阵分解与主题建模28
3.1导论28
3.1.1本章内容组织结构30
3.1.2将二分解归一化为标准的三分解30
3.2奇异值分解（SVD)31
3.2.1SVD的例子33
3.2.2实现SVD的幂迭代法35
3.2.3SVD/LSA的应用35
3.2.4SVD/LSA的优缺点36
3.3非负矩阵分解36
3.3.1非负矩阵分解的可解释性38
3.3.2非负矩阵分解的例子39
3.3.3融入新文档40
3.3.4非负矩阵分解的优缺点41
3.4概率潜在语义分析（PLSA）41
3.4.1与非负矩阵分解的联系44
3.4.2与SVD的比较44
3.4.3PLSA的例子45
3.4.4PLSA的优缺点45
3.5隐含狄利克雷分布（LDA）概览46
3.5.1简化的LDA模型46
3.5.2平滑的LDA模型49
3.6非线性变换和特征工程50
3.6.1选择一个相似度函数52
3.6.2Nystrom估计58
3.6.3相似度矩阵的部分可用性60
3.7本章小结61
3.8参考资料62
3.8.1软件资源62
3.9习题63
第4章 文本聚类65
4.1导论65
4.1.1本章内容组织结构66
4.2特征选择与特征工程66
4.2.1特征选择67
4.2.2特征工程68
4.3主题建模和矩阵分解70
4.3.1混合隶属度模型与重叠簇70
4.3.2非重叠簇与双聚类：矩阵分解的角度70
4.4面向聚类的生成混合模型74
4.4.1伯努利模型75
4.4.2多项式模型76
4.4.3与混合隶属度主题模型的比较77
4.4.4与朴素贝叶斯分类模型的联系77
4.5k均值算法78
4.5.1收敛与初始化80
4.5.2计算复杂度80
4.5.3与概率模型的联系81
4.6层次聚类算法81
4.6.1高效实现与计算复杂度83
4.6.2与k均值的自然联姻84
4.7聚类集成85
4.7.1选择集成分量86
4.7.2混合来自不同分量的结果86
4.8将文本当作序列来进行聚类87
4.8.1面向聚类的核方法87
4.8.2数据相关的核方法：谱聚类90
4.9聚类到有监督学习的转换91
4.9.1实际问题92
4.10聚类评估93
4.10.1内部有效性度量的缺陷93
4.10.2外部有效性度量93
4.11本章小结97
4.12参考资料97
4.12.1软件资源98
4.13习题98
第5章 文本分类：基本模型100
5.1导论100
5.1.1标记的类型与回归建模101
5.1.2训练与测试102
5.1.3归纳、直推和演绎学习器102
5.1.4基本模型103
5.1.5分类器中与文本相关的挑战103
5.2特征选择与特征工程104
5.2.1基尼系数104
5.2.2条件熵105
5.2.3逐点互信息105
5.2.4紧密相关的度量方式106
5.2.5χ2-统计量106
5.2.6嵌入式特征选择模型108
5.2.7特征工程技巧108
5.3朴素贝叶斯模型109
5.3.1伯努利模型109
5.3.2多项式模型111
5.3.3实际观察113
5.3.4利用朴素贝叶斯对输出进行排序113
5.3.5朴素贝叶斯的例子113
5.3.6半监督朴素贝叶斯116
5.4最近邻分类器118
5.4.11-最近邻分类器的属性119
5.4.2Rocchio与最近质心分类121
5.4.3加权最近邻122
5.4.4自适应最近邻：一系列有效的方法124
5.5决策树与随机森林126
5.5.1构造决策树的基本步骤126
5.5.2分裂一个节点127
5.5.3多变量分裂128
5.5.4决策树在文本分类中的问题129
5.5.5随机森林129
5.5.6把随机森林看作自适应最近邻方法130
5.6基于规则的分类器131
5.6.1顺序覆盖算法131
5.6.2从决策树中生成规则133
5.6.3关联分类器134
5.6.4预测135
5.7本章小结135
5.8参考资料135
5.8.1软件资源137
5.9习题137
第6章 面向文本的线性分类与回归140
6.1导论140
6.1.1线性模型的几何解释141
6.1.2我们需要偏置变量吗142
6.1.3使用正则化的线性模型的一般定义143
6.1.4将二值预测推广到多类144
6.1.5面向文本的线性模型的特点145
6.2最小二乘回归与分类145
6.2.1使用L2正则化的最小二乘回归145
6.2.2LASSO:使用L1正则化的最小二乘回归148
6.2.3Fisher线性判别与最小二乘分类器150
6.3支持向量机(SVM)156
6.3.1正则优化解释156
6.3.2最大间隔解释157
6.3.3Pegasos：在原始空间中求解SVM 159
6.3.4对偶SVM优化形式160
6.3.5对偶SVM的学习算法162
6.3.6对偶SVM的自适应最近邻解释163
6.4对数几率回归165
6.4.1正则优化解释165
6.4.2对数几率回归的训练算法166
6.4.3对数几率回归的概率解释167
6.4.4多元对数几率回归与其他推广168
6.4.5关于对数几率回归性能的评述169
6.5线性模型的非线性推广170
6.5.1基于显式变换的核SVM171
6.5.2为什么传统的核函数能够提升线性可分性172
6.5.3不同核函数的优缺点174
6.5.4核技巧175
6.5.5核技巧的系统性应用176
6.6本章小结179
6.7参考资料180
6.7.1软件资源181
6.8习题181
第7章 分类器的性能与评估184
7.1导论184
7.1.1本章内容组织结构184
7.2偏置-方差权衡185
7.2.1一个形式化的观点186
7.2.2偏置和方差的迹象189
7.3偏置-方差权衡在性能方面可能的影响189
7.3.1训练数据规模的影响189
7.3.2数据维度的影响191
7.3.3文本中模型选择可能的影响191
7.4利用集成方法系统性地提升性能192
7.4.1bagging与子采样192
7.4.2boosting193
7.5分类器评估195
7.5.1分割为训练部分和测试部分196
7.5.2绝对准确率度量197
7.5.3面向分类和信息检索的排序度量199
7.6本章小结204
7.7参考资料205
7.7.1boosting与对数几率回归的联系205
7.7.2分类器评估205
7.7.3软件资源206
7.7.4用于评估的数据集206
7.8习题206
第8章 结合异构数据的联合文本挖掘208
8.1导论208
8.1.1本章内容组织结构210
8.2共享矩阵分解的技巧210
8.2.1分解图210
8.2.2应用：结合文本和网页链接进行共享分解211
8.2.3应用：结合文本与无向社交网络214
8.2.4应用：结合文本的图像迁移学习215
8.2.5应用：结合评分和文本的推荐系统217
8.2.6应用：跨语言文本挖掘218
8.3分解机219
8.4联合概率建模技术223
8.4.1面向聚类的联合概率模型223
8.4.2朴素贝叶斯分类器224
8.5到图挖掘技术的转换224
8.6本章小结226
8.7参考资料227
8.7.1软件资源227
8.8习题228
第9章 信息检索与搜索引擎229
9.1导论229
9.1.1本章内容组织结构230
9.2索引和查询处理230
9.2.1词典数据结构231
9.2.2倒排索引233
9.2.3线性时间的索引构建234
9.2.4查询处理236
9.2.5效率优化244
9.3信息检索模型的评分248
9.3.1基于tf-idf的向量空间模型248
9.3.2二值独立模型249
9.3.3使用词项频率的BM25模型251
9.3.4信息检索中的统计语言模型252
9.4网络爬虫与资源发现254
9.4.1一个基本的爬虫算法255
9.4.2带偏好的爬虫256
9.4.3多线程257
9.4.4避开蜘蛛陷阱258
9.4.5用于近似重复检测的Shingling方法258
9.5搜索引擎中的查询处理259
9.5.1分布式索引构建259
9.5.2动态索引更新260
9.5.3查询处理260
9.5.4信誉度的重要性261
9.6基于链接的排序算法262
9.6.1PageRank262
9.6.2HITS267
9.7本章小结269
9.8参考资料269
9.8.1软件资源270
9.9习题270
第10章 文本序列建模与深度学习272
10.1导论272
10.1.1本章内容组织结构274
10.2统计语言模型274
10.2.1skip-gram模型277
10.2.2与嵌入的关系278
10.3核方法279
10.4单词-上下文矩阵分解模型 280
10.4.1使用计数的矩阵分解280
10.4.2GloVe嵌入282
10.4.3PPMI矩阵分解283
10.4.4位移PPMI矩阵分解283
10.4.5融入句法和其他特征283
10.5单词距离的图形化表示284
10.6神经语言模型285
10.6.1神经网络简介285
10.6.2基于word2vec的神经嵌入295
10.6.3word2vec(SGNS)是对数几率矩阵分解302
10.6.4除了单词以外：基于doc2vec的段落嵌入304
10.7循环神经网络(RNN)305
10.7.1实际问题307
10.7.2RNN的语言建模示例308
10.7.3图像描述应用310
10.7.4序列到序列学习与机器翻译311
10.7.5句子级分类应用314
10.7.6使用语言特征的词条级分类315
10.7.7多层循环网络316
10.8本章小结319
10.9参考资料319
10.9.1软件资源320
10.10习题321
第11章 文本摘要323
11.1导论323
11.1.1提取式摘要与抽象式摘要324
11.1.2提取式摘要中的关键步骤324
11.1.3提取式摘要中的分割阶段324
11.1.4本章内容组织结构325
11.2提取式摘要的主题词方法325
11.2.1词项概率325
11.2.2归一化频率权重326
11.2.3主题签名327
11.2.4句子选择方法329
11.3提取式摘要的潜在方法329
11.3.1潜在语义分析330
11.3.2词汇链331
11.3.3基于图的方法332
11.3.4质心摘要333
11.4面向提取式摘要的机器学习334
11.4.1特征提取334
11.4.2使用哪种分类器335
11.5多文档摘要335
11.5.1基于质心的摘要335
11.5.2基于图的方法336
11.6抽象式摘要337
11.6.1句子压缩337
11.6.2信息融合338
11.6.3信息排列338
11.7本章小结338
11.8参考资料339
11.8.1软件资源339
11.9习题340
第12章 信息提取341
12.1导论341
12.1.1历史演变343
12.1.2自然语言处理的角色343
12.1.3本章内容组织结构345
12.2命名实体识别345
12.2.1基于规则的方法346
12.2.2转化为词条级分类任务349
12.2.3隐马尔可夫模型350
12.2.4最大熵马尔可夫模型354
12.2.5条件随机场355
12.3关系提取357
12.3.1转换为分类问题357
12.3.2利用显式的特征工程进行关系预测358
12.3.3利用隐式的特征工程进行关系预测：核方法361
12.4本章小结365
12.5参考资料365
12.5.1弱监督学习方法366
12.5.2无监督与开放式信息提取 366
12.5.3软件资源367
12.6习题367
第13章 意见挖掘与情感分析368
13.1导论368
13.1.1意见词典370
13.1.2把意见挖掘看作槽填充和信息提取任务371
13.1.3本章内容组织结构372
13.2文档级情感分析372
13.2.1面向分类的无监督方法374
13.3短语级与句子级情感分类375
13.3.1句子级与短语级分析的应用376
13.3.2主观性分类到最小割问题的归约376
13.3.3句子级与短语级极性分析中的上下文377
13.4把基于方面的意见挖掘看作信息提取任务377
13.4.1Hu和Liu的无监督方法378
13.4.2OPINE：一种无监督方法379
13.4.3把有监督意见提取看作词条级分类任务380
13.5虚假意见381
13.5.1面向虚假评论检测的有监督方法382
13.5.2面向虚假评论制造者检测的无监督方法384
13.6意见摘要384
13.6.1评分总结384
13.6.2情感总结385
13.6.3基于短语与句子的情感总结385
13.6.4提取式与抽象式总结385
13.7本章小结385
13.8参考资料385
13.8.1软件资源387
13.9习题387
第14章 文本分割与事件检测388
14.1导论388
14.1.1与话题检测和追踪的关系388
14.1.2本章内容组织结构389
14.2文本分割389
14.2.1TextTiling390
14.2.2C99方法390
14.2.3基于现成的分类器的有监督的分割392
14.2.4基于马尔可夫模型的有监督的分割393
14.3文本流挖掘395
14.3.1流式文本聚类395
14.3.2面向首次报道检测的应用 396
14.4事件检测397
14.4.1无监督的事件检测397
14.4.2把有监督的事件检测看作有监督的分割任务399
14.4.3把事件检测看作一个信息提取问题399
14.5本章小结402
14.6参考资料402
14.6.1软件资源402
14.7习题403
参考文献404
・ ・ ・ ・ ・ ・ (收起)译者序
前言
第1章探索数据分析1
1.1Scala入门2
1.2去除分类字段的重复值2
1.3数值字段概述4
1.4基本抽样、分层抽样和一致抽样5
1.5使用Scala和Spark的Note―book工作8
1.6相关性的基础12
1.7总结14
第2章数据管道和建模15
2.1影响图16
2.2序贯试验和风险处理17
2.3探索与利用问题21
2.4不知之不知23
2.5数据驱动系统的基本组件23
2.5.1数据收集24
2.5.2数据转换层25
2.5.3数据分析与机器学习26
2.5.4UI组件26
2.5.5动作引擎28
2.5.6关联引擎28
2.5.7监控28
2.6优化和交互28
2.7总结29
第3章使用Spark和MLlib30
3.1安装Spark31
3.2理解Spark的架构32
3.2.1任务调度32
3.2.2Spark的组件35
3.2.3MQTT、ZeroMQ、Flume和Kafka36
3.2.4HDFS、Cassandra、S3和Tachyon37
3.2.5Mesos、YARN和Standa―lone38
3.3应用38
3.3.1单词计数38
3.3.2基于流的单词计数41
3.3.3SparkSQL和数据框45
3.4机器学习库46
3.4.1SparkR47
3.4.2图算法：Graphx和Graph―Frames48
3.5Spark的性能调整48
3.6运行Hadoop的HDFS49
3.7总结54
第4章监督学习和无监督学习55
4.1记录和监督学习55
4.1.1Iirs数据集56
4.1.2类标签点57
4.1.3SVMWithSGD58
4.1.4logistic回归60
4.1.5决策树62
4.1.6bagging和boosting：集成学习方法66
4.2无监督学习66
4.3数据维度71
4.4总结73
第5章回归和分类74
5.1回归是什么74
5.2连续空间和度量75
5.3线性回归77
5.4logistic回归81
5.5正则化83
5.6多元回归84
5.7异方差84
5.8回归树85
5.9分类的度量87
5.10多分类问题87
5.11感知机87
5.12泛化误差和过拟合90
5.13总结90
第6章使用非结构化数据91
6.1嵌套数据92
6.2其他序列化格式100
6.3Hive和Impala102
6.4会话化104
6.5使用特质109
6.6使用模式匹配110
6.7非结构化数据的其他用途113
6.8概率结构113
6.9投影113
6.10总结113
第7章使用图算法115
7.1图简介115
7.2SBT116
7.3Scala的图项目119
7.3.1增加节点和边121
7.3.2图约束123
7.3.3JSON124
7.4GraphX126
7.4.1谁收到电子邮件130
7.4.2连通分量131
7.4.3三角形计数132
7.4.4强连通分量132
7.4.5PageRank133
7.4.6SVD++134
7.5总结138
第8章Scala与R和Python的集成139
8.1R的集成140
8.1.1R和SparkR的相关配置140
8.1.2数据框144
8.1.3线性模型150
8.1.4广义线性模型152
8.1.5在SparkR中读取JSON文件156
8.1.6在SparkR中写入Parquet文件157
8.1.7从R调用Scala158
8.2Python的集成161
8.2.1安装Python161
8.2.2PySpark162
8.2.3从Java／Scala调用Python163
8.3总结167
第9章Scala中的NLP169
9.1文本分析流程170
9.2Spark的MLlib库177
9.2.1TF―IDF177
9.2.2LDA178
9.3分词、标注和分块185
9.4POS标记186
9.5使用word2vec寻找词关系189
9.6总结192
第10章高级模型监控193
10.1系统监控194
10.2进程监控195
10.3模型监控201
10.3.1随时间变化的性能202
10.3.2模型停用标准202
10.3.3A／B测试202
10.4总结202
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 概 述
1.1 什么是机器学习――从一个小故事开始 / 002
1.2 机器学习的一些应用场景――蝙蝠公司的业务单元 / 003
1.3 机器学习应该如何入门――世上无难事 / 005
1.4 有监督学习与无监督学习 / 007
1.5 机器学习中的分类与回归 / 008
1.6 模型的泛化、过拟合与欠拟合 / 008
1.7 小结 / 009
第2章 基于Python语言的环境配置
2.1 Python的下载和安装 / 012
2.2 Jupyter Notebook的安装与使用方法 / 013
2.2.1 使用pip进行Jupyter Notebook的下载和安装 / 013
2.2.2 运行Jupyter Notebook / 014
2.2.3 Jupyter Notebook的使用方法 / 015
2.3 一些必需库的安装及功能简介 / 017
2.3.1 Numpy――基础科学计算库 / 017
2.3.2 Scipy――强大的科学计算工具集 / 018
2.3.3 pandas――数据分析的利器 / 019
2.3.4 matplotlib――画出优美的图形 / 020
深入浅出Python 机器学习
VIII
2.4 scikit-learn――非常流行的Python机器学习库 / 021
2.5 小结 / 022
第3章 K最近邻算法――近朱者赤，近墨者黑
3.1 K最近邻算法的原理 / 024
3.2 K最近邻算法的用法 / 025
3.2.1 K最近邻算法在分类任务中的应用 / 025
3.2.2 K最近邻算法处理多元分类任务 / 029
3.2.3 K最近邻算法用于回归分析 / 031
3.3 K最近邻算法项目实战――酒的分类 / 034
3.3.1 对数据集进行分析 / 034
3.3.2 生成训练数据集和测试数据集 / 036
3.3.3 使用K最近邻算法进行建模 / 038
3.3.4 使用模型对新样本的分类进行预测 / 039
3.4 小结 / 041
第4章 广义线性模型――“耿直”的算法模型
4.1 线性模型的基本概念 / 044
4.1.1 线性模型的一般公式 / 044
4.1.2 线性模型的图形表示 / 045
4.1.3 线性模型的特点 / 049
4.2 最基本的线性模型――线性回归 / 050
4.2.1 线性回归的基本原理 / 050
4.2.2 线性回归的性能表现 / 051
4.3 使用L2正则化的线性模型――岭回归 / 053
4.3.1 岭回归的原理 / 053
4.3.2 岭回归的参数调节 / 054
4.4 使用L1正则化的线性模型――套索回归 / 058
4.4.1 套索回归的原理 / 058
4.4.2 套索回归的参数调节 / 059
4.4.3 套索回归与岭回归的对比 / 060
目
录
IX
4.5 小结 / 062
第5章 朴素贝叶斯――打雷啦，收衣服啊
5.1 朴素贝叶斯基本概念 / 064
5.1.1 贝叶斯定理 / 064
5.1.2 朴素贝叶斯的简单应用 / 064
5.2 朴素贝叶斯算法的不同方法 / 068
5.2.1 贝努利朴素贝叶斯 / 068
5.2.2 高斯朴素贝叶斯 / 071
5.2.3 多项式朴素贝叶斯 / 072
5.3 朴素贝叶斯实战――判断肿瘤是良性还是恶性 / 075
5.3.1 对数据集进行分析 / 076
5.3.2 使用高斯朴素贝叶斯进行建模 / 077
5.3.3 高斯朴素贝叶斯的学习曲线 / 078
5.4 小结 / 080
第6章 决策树与随机森林――会玩读心术的算法
6.1 决策树 / 082
6.1.1 决策树基本原理 / 082
6.1.2 决策树的构建 / 082
6.1.3 决策树的优势和不足 / 088
6.2 随机森林 / 088
6.2.1 随机森林的基本概念 / 089
6.2.2 随机森林的构建 / 089
6.2.3 随机森林的优势和不足 / 092
6.3 随机森林实例――要不要和相亲对象进一步发展 / 093
6.3.1 数据集的准备 / 093
6.3.2 用get_dummies处理数据 / 094
6.3.3 用决策树建模并做出预测 / 096
6.4 小结 / 098
第7章 支持向量机SVM――专治线性不可分
7.1 支持向量机SVM基本概念 / 100
7.1.1 支持向量机SVM的原理 / 100
7.1.2 支持向量机SVM的核函数 / 102
7.2 SVM的核函数与参数选择 / 104
7.2.1 不同核函数的SVM对比 / 104
7.2.2 支持向量机的gamma参数调节 / 106
7.2.3 SVM算法的优势与不足 / 108
7.3 SVM实例――波士顿房价回归分析 / 108
7.3.1 初步了解数据集 / 109
7.3.2 使用SVR进行建模 / 110
7.4 小结 / 114
第8章 神经网络――曾入“冷宫”，如今得宠
8.1 神经网络的前世今生 / 116
8.1.1 神经网络的起源 / 116
8.1.2 第一个感知器学习法则 / 116
8.1.3 神经网络之父――杰弗瑞・欣顿 / 117
8.2 神经网络的原理及使用 / 118
8.2.1 神经网络的原理 / 118
8.2.2 神经网络中的非线性矫正 / 119
8.2.3 神经网络的参数设置 / 121
8.3 神经网络实例――手写识别 / 127
8.3.1 使用MNIST数据集 / 128
8.3.2 训练MLP神经网络 / 129
8.3.3 使用模型进行数字识别 / 130
8.4 小结 / 131
第9章 数据预处理、降维、特征提取及聚类――快
刀斩乱麻
9.1 数据预处理 / 134
9.1.1 使用StandardScaler进行数据预处理 / 134
9.1.2 使用MinMaxScaler进行数据预处理 / 135
9.1.3 使用RobustScaler进行数据预处理 / 136
9.1.4 使用Normalizer进行数据预处理 / 137
9.1.5 通过数据预处理提高模型准确率 / 138
9.2 数据降维 / 140
9.2.1 PCA主成分分析原理 / 140
9.2.2 对数据降维以便于进行可视化 / 142
9.2.3 原始特征与PCA主成分之间的关系 / 143
9.3 特征提取 / 144
9.3.1 PCA主成分分析法用于特征提取 / 145
9.3.2 非负矩阵分解用于特征提取 / 148
9.4 聚类算法 / 149
9.4.1 K均值聚类算法 / 150
9.4.2 凝聚聚类算法 / 153
9.4.3 DBSCAN算法 / 154
9.5 小结 / 157
第10章 数据表达与特征工程――锦上再添花
10.1 数据表达 / 160
10.1.1 使用哑变量转化类型特征 / 160
10.1.2 对数据进行装箱处理 / 162
10.2 数据“升维” / 166
10.2.1 向数据集添加交互式特征 / 166
10.2.2 向数据集添加多项式特征 / 170
10.3 自动特征选择 / 173
10.3.1 使用单一变量法进行特征选择 / 173
10.3.2 基于模型的特征选择 / 178
10.3.3 迭代式特征选择 / 180
10.4 小结 / 182
第11章 模型评估与优化――只有更好，没有最好
11.1 使用交叉验证进行模型评估 / 184
11.1.1 scikit-learn中的交叉验证法 / 184
11.1.2 随机拆分和“挨个儿试试” / 186
11.1.3 为什么要使用交叉验证法 / 188
11.2 使用网格搜索优化模型参数 / 188
11.2.1 简单网格搜索 / 189
11.2.2 与交叉验证结合的网格搜索 / 191
11.3 分类模型的可信度评估 / 193
11.3.1 分类模型中的预测准确率 / 194
11.3.2 分类模型中的决定系数 / 197
11.4 小结 / 198
第12章 建立算法的管道模型――团结就是力量
12.1 管道模型的概念及用法 / 202
12.1.1 管道模型的基本概念 / 202
12.1.2 使用管道模型进行网格搜索 / 206
12.2 使用管道模型对股票涨幅进行回归分析 / 209
12.2.1 数据集准备 / 209
12.2.2 建立包含预处理和MLP模型的管道模型 / 213
12.2.3 向管道模型添加特征选择步骤 / 214
12.3 使用管道模型进行模型选择和参数调优 / 216
12.3.1 使用管道模型进行模型选择 / 216
12.3.2 使用管道模型寻找更优参数 / 217
12.4 小结 / 220
第13章 文本数据处理――亲，见字如“数”
13.1 文本数据的特征提取、中文分词及词袋模型 / 222
13.1.1 使用CountVectorizer对文本进行特征提取 / 222
13.1.2 使用分词工具对中文文本进行分词 / 223
13.1.3 使用词袋模型将文本数据转为数组 / 224
13.2 对文本数据进一步进行优化处理 / 226
13.2.1 使用n-Gram改善词袋模型 / 226
13.2.2 使用tf-idf模型对文本数据进行处理 / 228
13.2.3 删除文本中的停用词 / 234
13.3 小结 / 236
第14章 从数据获取到话题提取――从“研究员”
到“段子手”
14.1 简单页面的爬取 / 238
14.1.1 准备Requests库和User Agent / 238
14.1.2 确定一个目标网站并分析其结构 / 240
14.1.3 进行爬取并保存为本地文件 / 241
14.2 稍微复杂一点的爬取 / 244
14.2.1 确定目标页面并进行分析 / 245
14.2.2 Python中的正则表达式 / 247
14.2.3 使用BeautifulSoup进行HTML解析 / 251
14.2.4 对目标页面进行爬取并保存到本地 / 256
14.3 对文本数据进行话题提取 / 258
14.3.1 寻找目标网站并分析结构 / 259
14.3.2 编写爬虫进行内容爬取 / 261
14.3.3 使用潜在狄利克雷分布进行话题提取 / 263
14.4 小结 / 265
第15章 人才需求现状与未来学习方向――你是不
是下一个“大牛”
15.1 人才需求现状 / 268
15.1.1 全球AI从业者达190万，人才需求3年翻8倍 / 268
15.1.2 AI人才需求集中于一线城市，七成从业者月薪过万 / 269
15.1.3 人才困境仍难缓解，政策支援亟不可待 / 269
15.2 未来学习方向 / 270
15.2.1 用于大数据分析的计算引擎 / 270
15.2.2 深度学习开源框架 / 271
15.2.3 使用概率模型进行推理 / 272
15.3 技能磨炼与实际应用 / 272
15.3.1 Kaggle算法大赛平台和OpenML平台 / 272
15.3.2 在工业级场景中的应用 / 273
15.3.3 对算法模型进行A/B测试 / 273
15.4 小结 / 274
参考文献 / 275
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序一
序二
序三
前言
第1章　通向智能安全的旅程 1
1.1　人工智能、机器学习与深度学习 1
1.2　人工智能的发展 2
1.3　国内外网络安全形势 3
1.4　人工智能在安全领域的应用 5
1.5　算法和数据的辩证关系 9
1.6　本章小结 9
参考资源 10
第2章　打造机器学习工具箱 11
2.1　Python在机器学习领域的优势 11
2.1.1　NumPy 11
2.1.2　SciPy 15
2.1.3　NLTK 16
2.1.4　Scikit-Learn 17
2.2　TensorFlow简介与环境搭建 18
2.3　本章小结 19
参考资源 20
第3章　机器学习概述 21
3.1　机器学习基本概念 21
3.2　数据集 22
3.2.1　KDD 99数据 22
3.2.2　HTTP DATASET CSIC 2010 26
3.2.3　SEA数据集 26
3.2.4　ADFA-LD数据集 27
3.2.5　Alexa域名数据 29
3.2.6　Scikit-Learn数据集 29
3.2.7　MNIST数据集 30
3.2.8　Movie Review Data 31
3.2.9　SpamBase数据集 32
3.2.10　Enron数据集 33
3.3　特征提取 35
3.3.1　数字型特征提取 35
3.3.2　文本型特征提取 36
3.3.3　数据读取 37
3.4　效果验证 38
3.5　本章小结 40
参考资源 40
第4章　Web安全基础 41
4.1　XSS攻击概述 41
4.1.1　XSS的分类 43
4.1.2　XSS特殊攻击方式 48
4.1.3　XSS平台简介 50
4.1.4　近年典型XSS攻击事件分析 51
4.2　SQL注入概述 53
4.2.1　常见SQL注入攻击 54
4.2.2　常见SQL注入攻击载荷 55
4.2.3　SQL常见工具 56
4.2.4　近年典型SQL注入事件分析 60
4.3　WebShell概述 63
4.3.1　WebShell功能 64
4.3.2　常见WebShell 64
4.4　僵尸网络概述 67
4.4.1　僵尸网络的危害 68
4.4.2　近年典型僵尸网络攻击事件分析 69
4.5　本章小结 72
参考资源 72
第5章　K近邻算法 74
5.1　K近邻算法概述 74
5.2　示例：hello world！K近邻 75
5.3　示例：使用K近邻算法检测异常操作（一） 76
5.4　示例：使用K近邻算法检测异常操作（二） 80
5.5　示例：使用K近邻算法检测Rootkit 81
5.6　示例：使用K近邻算法检测WebShell 83
5.7　本章小结 85
参考资源 86
第6章　决策树与随机森林算法 87
6.1　决策树算法概述 87
6.2　示例：hello world！决策树 88
6.3　示例：使用决策树算法检测POP3暴力破解 89
6.4　示例：使用决策树算法检测FTP暴力破解 91
6.5　随机森林算法概述 93
6.6　示例：hello world！随机森林 93
6.7　示例：使用随机森林算法检测FTP暴力破解 95
6.8　本章小结 96
参考资源 96
第7章　朴素贝叶斯算法 97
7.1　朴素贝叶斯算法概述 97
7.2　示例：hello world！朴素贝叶斯 98
7.3　示例：检测异常操作 99
7.4　示例：检测WebShell（一） 100
7.5　示例：检测WebShell（二） 102
7.6　示例：检测DGA域名 103
7.7　示例：检测针对Apache的DDoS攻击 104
7.8　示例：识别验证码 107
7.9　本章小结 108
参考资源 108
第8章　逻辑回归算法 109
8.1　逻辑回归算法概述 109
8.2　示例：hello world！逻辑回归 110
8.3　示例：使用逻辑回归算法检测Java溢出攻击 111
8.4　示例：识别验证码 113
8.5　本章小结 114
参考资源 114
第9章　支持向量机算法 115
9.1　支持向量机算法概述 115
9.2　示例：hello world！支持向量机 118
9.3　示例：使用支持向量机算法识别XSS 120
9.4　示例：使用支持向量机算法区分僵尸网络DGA家族 124
9.4.1　数据搜集和数据清洗 124
9.4.2　特征化 125
9.4.3　模型验证 129
9.5　本章小结 130
参考资源 130
第10章　K-Means与DBSCAN算法 131
10.1　K-Means算法概述 131
10.2　示例：hello world！K-Means 132
10.3　示例：使用K-Means算法检测DGA域名 133
10.4　DBSCAN算法概述 135
10.5　示例：hello world！DBSCAN 135
10.6　本章小结 137
参考资源 137
第11章　Apriori与FP-growth算法 138
11.1　Apriori算法概述 138
11.2　示例：hello world！Apriori 140
11.3　示例：使用Apriori算法挖掘XSS相关参数 141
11.4　FP-growth算法概述 143
11.5　示例：hello world！FP-growth 144
11.6　示例：使用FP-growth算法挖掘疑似僵尸主机 145
11.7　本章小结 146
参考资源 146
第12章　隐式马尔可夫算法 147
12.1　隐式马尔可夫算法概述 147
12.2　hello world! 隐式马尔可夫 148
12.3　示例：使用隐式马尔可夫算法识别XSS攻击（一） 150
12.4　示例：使用隐式马尔可夫算法识别XSS攻击（二） 153
12.5　示例：使用隐式马尔可夫算法识别DGA域名 159
12.6　本章小结 162
参考资源 162
第13章　图算法与知识图谱 163
13.1　图算法概述 163
13.2　示例：hello world！有向图 164
13.3　示例：使用有向图识别WebShell 169
13.4　示例：使用有向图识别僵尸网络 173
13.5　知识图谱概述 176
13.6　示例：知识图谱在风控领域的应用 177
13.6.1　检测疑似账号被盗 178
13.6.2　检测疑似撞库攻击 179
13.6.3　检测疑似刷单 181
13.7　示例：知识图谱在威胁情报领域的应用 183
13.7.1　挖掘后门文件潜在联系 184
13.7.2　挖掘域名潜在联系 185
13.8　本章小结 187
参考资源 187
第14章　神经网络算法 188
14.1　神经网络算法概述 188
14.2　示例：hello world！神经网络 190
14.3　示例：使用神经网络算法识别验证码 190
14.4　示例：使用神经网络算法检测Java溢出攻击 191
14.5　本章小结 193
参考资源 194
第15章　多层感知机与DNN算法 195
15.1　神经网络与深度学习 195
15.2　TensorFlow编程模型 196
15.2.1　操作 197
15.2.2　张量 197
15.2.3　变量 198
15.2.4　会话 198
15.3　TensorFlow的运行模式 198
15.4　示例：在TensorFlow下识别验证码（一） 199
15.5　示例：在TensorFlow下识别验证码（二） 202
15.6　示例：在TensorFlow下识别验证码（三） 205
15.7　示例：在TensorFlow下识别垃圾邮件（一） 207
15.8　示例：在TensorFlow下识别垃圾邮件（二） 209
15.9　本章小结 210
参考资源 210
第16章　循环神经网络算法 212
16.1　循环神经网络算法概述 212
16.2　示例：识别验证码 213
16.3　示例：识别恶意评论 216
16.4　示例：生成城市名称 220
16.5　示例：识别WebShell 222
16.6　示例：生成常用密码 225
16.7　示例：识别异常操作 227
16.8　本章小结 230
参考资源 230
第17章　卷积神经网络算法 231
17.1　卷积神经网络算法概述 231
17.2　示例：hello world！卷积神经网络 234
17.3　示例：识别恶意评论 235
17.4　示例：识别垃圾邮件 237
17.5　本章小结 240
参考资源 242
・ ・ ・ ・ ・ ・ (收起)第一部分 初始
1 初识机器学习 2
1.1 学习机器学习的误区 2
1.2 什么是机器学习 3
1.3 Python 中的机器学习 3
1.4 学习机器学习的原则 5
1.5 学习机器学习的技巧 5
1.6 这本书不涵盖以下内容 6
1.7 代码说明 6
1.8 总结 6
2 Python 机器学习的生态圈 7
2.1 Python 7
2.2 SciPy 9
2.3 scikit-learn 9
2.4 环境安装 10
2.5 总结 12
3 第一个机器学习项目 13
3.1 机器学习中的 Hello World 项目 13
3.2 导入数据 14
3.3 概述数据 15
3.4 数据可视化 18
3.5 评估算法 20
3.6 实施预测 23
3.7 总结 24
4 Python 和 SciPy 速成 25
4.1 Python 速成 25
4.2 NumPy 速成 34
4.3 Matplotlib 速成 36
4.4 Pandas 速成 39
4.5 总结 41
第二部分 数据理解
5 数据导入 44
5.1 CSV 文件 44
5.2 Pima Indians 数据集 45
5.3 采用标准 Python 类库导入数据 46
5.4 采用 NumPy 导入数据 46
5.5 采用 Pandas 导入数据 47
5.6 总结 47
6 数据理解 48
6.1 简单地查看数据 48
6.2 数据的维度 49
6.3 数据属性和类型 50
6.4 描述性统计 50
6.5 数据分组分布（适用于分类算法） 51
6.6 数据属性的相关性 52
6.7 数据的分布分析 53
6.8 总结 54
7 数据可视化 55
7.1 单一图表 55
7.2 多重图表 58
7.3 总结 61
第三部分 数据准备
8 数据预处理 64
8.1 为什么需要数据预处理 64
8.2 格式化数据 65
8.3 调整数据尺度 65
8.4 正态化数据 67
8.5 标准化数据 68
8.6 二值数据 69
8.7 总结 70
9 数据特征选定 71
9.1 特征选定 72
9.2 单变量特征选定 72
9.3 递归特征消除 73
9.4 主要成分分析 75
9.5 特征重要性 76
9.6 总结 76
第四部分 选择模型
10 评估算法 78
10.1 评估算法的方法 78
10.2 分离训练数据集和评估数据集 79
10.3 K 折交叉验证分离 80
10.4 弃一交叉验证分离 81
10.5 重复随机分离评估数据集与训练数据集 82
10.6 总结 83
11 算法评估矩阵 85
11.1 算法评估矩阵 85
11.2 分类算法矩阵 86
11.3 回归算法矩阵 93
11.4 总结 96
12 审查分类算法 97
12.1 算法审查 97
12.2 算法概述 98
12.3 线性算法 98
12.4 非线性算法 101
12.5 总结 105
13 审查回归算法 106
13.1 算法概述 106
13.2 线性算法 107
13.3 非线性算法 111
13.4 总结 113
14 算法比较 115
14.1 选择最佳的机器学习算法 115
14.2 机器学习算法的比较 116
14.3 总结 118
15 自动流程 119
15.1 机器学习的自动流程 119
15.2 数据准备和生成模型的 Pipeline 120
15.3 特征选择和生成模型的 Pipeline 121
15.4 总结 122
第五部分 优化模型
16 集成算法 124
16.1 集成的方法 124
16.2 装袋算法 125
16.3 提升算法 129
16.4 投票算法 131
16.5 总结 132
17 算法调参 133
17.1 机器学习算法调参 133
17.2 网格搜索优化参数 134
17.3 随机搜索优化参数 135
17.4 总结 136
第六部分 结果部署
18 持久化加载模型 138
18.1 通过 pickle 序列化和反序列化机器学习的模型 138
18.2 通过 joblib 序列化和反序列化机器学习的模型 140
18.3 生成模型的技巧 141
18.4 总结 141
第七部分 项目实践
19 预测模型项目模板 144
19.1 在项目中实践机器学习 145
19.2 机器学习项目的 Python 模板 145
19.3 各步骤的详细说明 146
19.4 使用模板的小技巧 148
19.5 总结 149
20 回归项目实例 150
20.1 定义问题 150
20.2 导入数据 151
20.3 理解数据 152
20.4 数据可视化 155
20.5 分离评估数据集 159
20.6 评估算法 160
20.7 调参改善算法 164
20.8 集成算法 165
20.9 集成算法调参 167
20.10 确定最终模型 168
20.11 总结 169
21 二分类实例 170
21.1 问题定义 170
21.2 导入数据 171
21.3 分析数据 172
21.4 分离评估数据集 180
21.5 评估算法 180
21.6 算法调参 184
21.7 集成算法 187
21.8 确定最终模型 190
21.9 总结 190
22 文本分类实例 192
22.1 问题定义 192
22.2 导入数据 193
22.3 文本特征提取 195
22.4 评估算法 196
22.5 算法调参 198
22.6 集成算法 200
22.7 集成算法调参 201
22.8 确定最终模型 202
22.9 总结 203
・ ・ ・ ・ ・ ・ (收起)第 1章 走进机器学习1
1.1 机器学习概述1
1.2 机器学习过程2
第 2章 了解Python20
2.1 为什么选择Python20
2.2 下载和安装Python22
2.2.1 在Windows中安装Python22
2.2.2 Anaconda24
2.3 首个Python程序26
2.4 Python基础27
2.5 数据结构与循环36
第3章 特征工程42
3.1 什么是特征42
3.2 为什么执行特征工程43
3.3 特征提取43
3.4 特征选择43
3.5 特征工程方法――通用准则44
3.5.1 处理数值特征44
3.5.2 处理分类特征45
3.5.3 处理基于时间的特征47
3.5.4 处理文本特征47
3.5.5 缺失数据48
3.5.6 降维48
3.6 用Python进行特征工程49
3.6.1 Pandas基本操作49
3.6.2 常见任务57
第4章 数据可视化62
4.1 折线图63
4.2 条形图66
4.3 饼图67
4.4 直方图68
4.5 散点图69
4.6 箱线图70
4.7 采用面向对象的方式绘图71
4.8 Seaborn73
4.8.1 分布图74
4.8.2 双变量分布75
4.8.3 二元分布的核密度估计75
4.8.4 成对双变量分布76
4.8.5 分类散点图76
4.8.6 小提琴图77
4.8.7 点图78
第5章 回归79
5.1 简单回归80
5.2 多元回归92
5.3 模型评价94
5.3.1 训练误差95
5.3.2 泛化误差96
5.3.3 测试误差97
5.3.4 不可约误差98
5.3.5 偏差―方差权衡99
第6章 更多回归105
6.1 概述105
6.2 岭回归112
6.3 套索回归118
6.3.1 全子集算法118
6.3.2 用于特征选择的贪心算法119
6.3.3 特征选择的正则化119
6.4 非参数回归122
6.4.1 K-最近邻回归124
6.4.2 核回归127
第7章 分类128
7.1 线性分类器129
7.2 逻辑回归133
7.3 决策树147
7.3.1 关于树的术语148
7.3.2 决策树学习149
7.3.3 决策边界151
7.4 随机森林158
7.5 朴素贝叶斯164
第8章 无监督学习169
8.1 聚类170
8.2 K-均值聚类170
8.2.1 随机分配聚类质心的问题175
8.2.2 查找K的值175
8.3 分层聚类182
8.3.1 距离矩阵184
8.3.2 连接185
第9章 文本分析189
9.1 使用Python进行基本文本处理189
9.1.1 字符串比较191
9.1.2 字符串转换191
9.1.3 字符串操作192
9.2 正则表达式193
9.3 自然语言处理195
9.3.1 词干提取196
9.3.2 词形还原197
9.3.3 分词197
9.4 文本分类200
9.5 主题建模206
第 10章 神经网络与深度学习209
10.1 矢量化210
10.2 神经网络218
10.2.1 梯度下降220
10.2.2 激活函数221
10.2.3 参数初始化224
10.2.4 优化方法227
10.2.5 损失函数227
10.3 深度学习229
10.4 深度学习架构230
10.4.1 深度信念网络231
10.4.2 卷积神经网络231
10.4.3 循环神经网络231
10.4.4 长短期记忆网络231
10.4.5 深度堆栈网络232
10.5 深度学习框架232
第 11章 推荐系统237
11.1 基于流行度的推荐引擎237
11.2 基于内容的推荐引擎240
11.3 基于分类的推荐引擎243
11.4 协同过滤245
第 12章 时间序列分析249
12.1 处理日期和时间249
12.2 窗口函数254
12.3 相关性258
12.4 时间序列预测261
・ ・ ・ ・ ・ ・ (收起)前　言
第1部分　实时机器学习方法论
第1章　实时机器学习综述 2
1.1　什么是机器学习 2
1.2　机器学习发展的前世今生 3
1.2.1　历史上机器学习无法调和的难题 3
1.2.2　现代机器学习的新融合 4
1.3　机器学习领域分类 5
1.4　实时是个“万灵丹” 6
1.5　实时机器学习的分类 7
1.5.1　硬实时机器学习 7
1.5.2　软实时机器学习 7
1.5.3　批实时机器学习 8
1.6　实时应用对机器学习的要求 8
1.7　案例：Netflix在机器学习竞赛中学到的经验 9
1.7.1　Netflix 用户信息被逆向工程 9
1.7.2　Netflix 最终胜出者模型无法在生产环境中使用 9
1.8　实时机器学习模型的生存期 10
第2章　实时监督式机器学习 12
2.1　什么是监督式机器学习 12
2.1.1　“江湖门派”对预测模型的
不同看法 13
2.1.2　工业界的学术门派 14
2.1.3　实时机器学习实战的思路 15
2.2　怎样衡量监督式机器学习模型 16
2.2.1　统计量的优秀 16
2.2.2　应用业绩的优秀 20
2.3　实时线性分类器介绍 20
2.3.1　广义线性模型的定义 20
2.3.2　训练线性模型 21
2.3.3　冷启动问题 22
第3章　数据分析工具 Pandas 23
3.1　颠覆 R 的 Pandas 23
3.2　Pandas 的安装 24
3.3　利用 Pandas 分析实时股票报价数据 24
3.3.1　外部数据导入 25
3.3.2　数据分析基本操作 25
3.3.3　可视化操作 26
3.3.4　秒级收盘价变化率初探 28
3.4　数据分析的三个要点 30
3.4.1　不断验证假设 30
3.4.2　全面可视化，全面监控化 30
第4章　机器学习工具 Scikit-learn 31
4.1　如何站在风口上？向Scikit-learn 学习 31
4.1.1　传统的线下统计软件 R 31
4.1.2　底层软件黑盒子 Weka 32
4.1.3　跨界产品 Scikit-learn 33
4.1.4　Scikit-learn的优势 33
4.2　Scikit-learn 的安装 34
4.3　Scikit-learn 的主要模块 35
4.3.1　监督式、非监督式机器学习 35
4.3.2　建模函数fit和predict 36
4.3.3　数据预处理 38
4.3.4　自动化建模预测 Pipeline 39
4.4　利用 Scikit-learn 进行股票价格波动预测 40
4.4.1　数据导入和预处理 41
4.4.2　编写专有时间序列数据预处理模块 41
4.4.3　利用 Pipeline 进行建模 43
4.4.4　评价建模效果 43
4.4.5　引入成交量和高维交叉项进行建模 44
4.4.6　本书没有告诉你的 45
第2部分　实时机器学习架构
第5章　实时机器学习架构设计 48
5.1　设计实时机器学习架构的
四个要点 48
5.2　Lambda 架构和主要成员 49
5.2.1　实时响应层 49
5.2.2　快速处理层 50
5.2.3　批处理层 50
5.3　常用的实时机器学习架构 50
5.3.1　瀑布流架构 50
5.3.2　并行响应架构 51
5.3.3　实时更新模型混合架构 52
5.4　小结 53
第6章　集群部署工具 Docker 55
6.1　Docker 的前世今生 55
6.2　容器虚拟机的基本组成部分 56
6.3　Docker 引擎命令行工具 57
6.3.1　Docker 引擎的安装 57
6.3.2　Docker 引擎命令行的基本操作 58
6.4　通过 Dockerfile 配置容器虚拟机 61
6.4.1　利用 Dockerfile 配置基本容器虚拟机 62
6.4.2　利用 Dockerfile 进行虚拟机和宿主机之间的文件传输 62
6.5　服务器集群配置工具Docker Compose 64
6.5.1　Docker Compose 的安装 64
6.5.2　Docker Compose 的基本操作 64
6.5.3　利用 Docker Compose 创建网页计数器集群 65
6.6　远端服务器配置工具Docker Machine 68
6.6.1　Docker Machine 的安装 68
6.6.2　安装 Oracle VirtualBox 69
6.6.3　创建和管理 VirtualBox中的虚拟机 69
6.6.4　在 Docker Machine 和 VirtualBox的环境中运行集群 70
6.6.5　利用 Docker Machine 在 Digital Ocean 上配置运行集群 71
6.7　其他有潜力的 Docker 工具 73
第7章　实时消息队列和RabbitMQ 74
7.1　实时消息队列 74
7.2　AMQP 和 RabbitMQ 简介 76
7.3　RabbitMQ的主要构成部分 76
7.4　常用交换中心模式 78
7.4.1　直连结构 78
7.4.2　扇形结构 78
7.4.3　话题结构 79
7.4.4　报头结构 79
7.5　消息传导设计模式 79
7.5.1　任务队列 80
7.5.2　Pub/Sub 发布/监听 80
7.5.3　远程命令 81
7.6　利用 Docker 快速部署RabbitMQ 82
7.7　利用 RabbitMQ 开发队列服务 85
7.7.1　准备案例材料 86
7.7.2　实时报价存储服务 86
7.7.3　实时走势预测服务 89
7.7.4　整合运行实验 93
7.7.5　总结和改进 95
第8章　实战数据库综述 98
8.1　SQL 与 NoSQL，主流数据库分类 98
8.1.1　关系型数据库 99
8.1.2　非关系型数据库 NoSQL 99
8.2　数据库的性能 100
8.2.1　耐分割 100
8.2.2　 一致性 101
8.2.3　可用性 101
8.2.4　CAP 定理 101
8.3　SQL和NoSQL对比 102
8.3.1　数据存储、读取方式 102
8.3.2　数据库的扩展方式 103
8.3.3　性能比较 103
8.4　数据库的发展趋势 103
8.4.1　不同数据库之间自动化同步更为方便 103
8.4.2　云数据库的兴起 104
8.4.3　底层和应用层多层化 104
8.5　MySQL 简介 105
8.6　Cassandra简介 105
8.6.1　Cassandra交互方式简介 105
8.6.2　利用Docker安装Cassandra 106
8.6.3　使用Cassandra存储数据 106
第9章　实时数据监控 ELK 集群 107
9.1　Elasticsearch、LogStash和Kibana 的前世今生 107
9.1.1　Elasticsearch 的平凡起家 108
9.1.2　LogStash 卑微的起源 108
9.1.3　Kibana 惊艳登场 109
9.1.4　ELK 协同作战 109
9.2　Elasticsearch 基本架构 109
9.2.1　文档 110
9.2.2　索引和文档类型 111
9.2.3　分片和冗余 112
9.2.4　Elasticsearch 和数据库进行比较 113
9.3　Elasticsearch 快速入门 113
9.3.1　用 Docker 运行 Elasticsearch 容器虚拟机 113
9.3.2　创建存储文档、文档类型和索引 114
9.3.3　搜索文档 117
9.3.4　对偶搜索 120
9.4　Kibana 快速入门 124
9.4.1　利用 Docker 搭建ELK 集群 125
9.4.2　配置索引格式 127
9.4.3　交互式搜索 128
9.4.4　可视化操作 129
9.4.5　实时检测面板 132
第10章　机器学习系统设计模式 134
10.1　 设计模式的前世今生 134
10.1.1　单机设计模式逐渐式微 134
10.1.2　微服务取代设计模式的示例 135
10.1.3　微服务设计模式的兴起 137
10.2　读：高速键值模式 137
10.2.1　问题场景 137
10.2.2　解决方案 138
10.2.3　其他使用场景 139
10.3　读：缓存高速查询模式 139
10.3.1　问题场景 139
10.3.2　解决方案 139
10.3.3　适用场景 141
10.4　更新：异步数据库更新模式 141
10.4.1　问题场景 141
10.4.2　解决方案 141
10.4.3　使用场景案例 142
10.5　更新：请求重定向模式 144
10.5.1　问题场景 144
10.5.2　解决方案 144
10.5.3　更新流程 145
10.5.4　使用场景案例 146
10.6　处理：硬实时并行模式 146
10.6.1　问题场景 146
10.6.2　解决方案 147
10.6.3　使用场景案例 147
10.7　处理：分布式任务队列模式 148
10.7.1　问题场景 148
10.7.2　解决方案 149
10.7.3　Storm 作为分布式任务队列 150
10.7.4　适用场景 151
10.7.5　结构的演进 152
10.8　处理：批实时处理模式 152
10.8.1　问题场景 152
10.8.2　解决方案 152
10.8.3　适用场景 153
第3部分　未来展望
第11章　Serverless 架构 156
11.1　Serverless 架构的前世今生 156
11.2　Serverless 架构对实时
机器学习的影响 157
第12章　深度学习的风口 159
12.1　深度学习的前世今生 159
12.2　深度学习的难点 161
12.3　如何选择深度学习工具 161
12.3.1　与现有编程平台、技能整合的难易程度 162
12.3.2　此平台除做深度学习之外，还能做什么 163
12.3.3　深度学习平台的成熟程度 164
12.4　未来发展方向 165
・ ・ ・ ・ ・ ・ (收起)I IMAGE FORMATION 1
1 Geometric Camera Models 3
1.1 Image Formation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.1 Pinhole Perspective . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.2 Weak Perspective . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.1.3 Cameras with Lenses . . . . . . . . . . . . . . . . . . . . . . . 8
1.1.4 The Human Eye . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2 Intrinsic and Extrinsic Parameters . . . . . . . . . . . . . . . . . . . 14
1.2.1 Rigid Transformations and Homogeneous Coordinates . . . . 14
1.2.2 Intrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . . 16
1.2.3 Extrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . . 18
1.2.4 Perspective Projection Matrices . . . . . . . . . . . . . . . . . 19
1.2.5 Weak-Perspective Projection Matrices . . . . . . . . . . . . . 20
1.3 Geometric Camera Calibration . . . . . . . . . . . . . . . . . . . . . 22
1.3.1 ALinear Approach to Camera Calibration . . . . . . . . . . . 23
1.3.2 ANonlinear Approach to Camera Calibration . . . . . . . . . 27
1.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2 Light and Shading 32
2.1 Modelling Pixel Brightness . . . . . . . . . . . . . . . . . . . . . . . 32
2.1.1 Reflection at Surfaces . . . . . . . . . . . . . . . . . . . . . . 33
2.1.2 Sources and Their Effects . . . . . . . . . . . . . . . . . . . . 34
2.1.3 The Lambertian+Specular Model . . . . . . . . . . . . . . . . 36
2.1.4 Area Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.2 Inference from Shading . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.2.1 Radiometric Calibration and High Dynamic Range Images . . 38
2.2.2 The Shape of Specularities . . . . . . . . . . . . . . . . . . . 40
2.2.3 Inferring Lightness and Illumination . . . . . . . . . . . . . . 43
2.2.4 Photometric Stereo: Shape from Multiple Shaded Images . . 46
2.3 Modelling Interreflection . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.3.1 The Illumination at a Patch Due to an Area Source . . . . . 52
2.3.2 Radiosity and Exitance . . . . . . . . . . . . . . . . . . . . . 54
2.3.3 An Interreflection Model . . . . . . . . . . . . . . . . . . . . . 55
2.3.4 Qualitative Properties of Interreflections . . . . . . . . . . . . 56
2.4 Shape from One Shaded Image . . . . . . . . . . . . . . . . . . . . . 59
2.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3 Color 68
3.1 Human Color Perception . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.1.1 Color Matching . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.1.2 Color Receptors . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.2 The Physics of Color . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.2.1 The Color of Light Sources . . . . . . . . . . . . . . . . . . . 73
3.2.2 The Color of Surfaces . . . . . . . . . . . . . . . . . . . . . . 76
3.3 Representing Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.3.1 Linear Color Spaces . . . . . . . . . . . . . . . . . . . . . . . 77
3.3.2 Non-linear Color Spaces . . . . . . . . . . . . . . . . . . . . . 83
3.4 AModel of Image Color . . . . . . . . . . . . . . . . . . . . . . . . . 86
3.4.1 The Diffuse Term . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.4.2 The Specular Term . . . . . . . . . . . . . . . . . . . . . . . . 90
3.5 Inference from Color . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.5.1 Finding Specularities Using Color . . . . . . . . . . . . . . . 90
3.5.2 Shadow Removal Using Color . . . . . . . . . . . . . . . . . . 92
3.5.3 Color Constancy: Surface Color from Image Color . . . . . . 95
3.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
II EARLY VISION: JUST ONE IMAGE 105
4 Linear Filters 107
4.1 Linear Filters and Convolution . . . . . . . . . . . . . . . . . . . . . 107
4.1.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2 Shift Invariant Linear Systems . . . . . . . . . . . . . . . . . . . . . 112
4.2.1 Discrete Convolution . . . . . . . . . . . . . . . . . . . . . . . 113
4.2.2 Continuous Convolution . . . . . . . . . . . . . . . . . . . . . 115
4.2.3 Edge Effects in Discrete Convolutions . . . . . . . . . . . . . 118
4.3 Spatial Frequency and Fourier Transforms . . . . . . . . . . . . . . . 118
4.3.1 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . 119
4.4 Sampling and Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.4.1 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4.2 Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.4.3 Smoothing and Resampling . . . . . . . . . . . . . . . . . . . 126
4.5 Filters as Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.5.1 Convolution as a Dot Product . . . . . . . . . . . . . . . . . 131
4.5.2 Changing Basis . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.6 Technique: Normalized Correlation and Finding Patterns . . . . . . 132
4.6.1 Controlling the Television by Finding Hands by Normalized
Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.7 Technique: Scale and Image Pyramids . . . . . . . . . . . . . . . . . 134
4.7.1 The Gaussian Pyramid . . . . . . . . . . . . . . . . . . . . . 135
4.7.2 Applications of Scaled Representations . . . . . . . . . . . . . 136
4.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5 Local Image Features 141
5.1 Computing the Image Gradient . . . . . . . . . . . . . . . . . . . . . 141
5.1.1 Derivative of Gaussian Filters . . . . . . . . . . . . . . . . . . 142
5.2 Representing the Image Gradient . . . . . . . . . . . . . . . . . . . . 144
5.2.1 Gradient-Based Edge Detectors . . . . . . . . . . . . . . . . . 145
5.2.2 Orientations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.3 Finding Corners and Building Neighborhoods . . . . . . . . . . . . . 148
5.3.1 Finding Corners . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.3.2 Using Scale and Orientation to Build a Neighborhood . . . . 151
5.4 Describing Neighborhoods with SIFT and HOG Features . . . . . . 155
5.4.1 SIFT Features . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.4.2 HOG Features . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.5 Computing Local Features in Practice . . . . . . . . . . . . . . . . . 160
5.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6 Texture 164
6.1 Local Texture Representations Using Filters . . . . . . . . . . . . . . 166
6.1.1 Spots and Bars . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.1.2 From Filter Outputs to Texture Representation . . . . . . . . 168
6.1.3 Local Texture Representations in Practice . . . . . . . . . . . 170
6.2 Pooled Texture Representations by Discovering Textons . . . . . . . 171
6.2.1 Vector Quantization and Textons . . . . . . . . . . . . . . . . 172
6.2.2 K-means Clustering for Vector Quantization . . . . . . . . . . 172
6.3 Synthesizing Textures and Filling Holes in Images . . . . . . . . . . 176
6.3.1 Synthesis by Sampling Local Models . . . . . . . . . . . . . . 176
6.3.2 Filling in Holes in Images . . . . . . . . . . . . . . . . . . . . 179
6.4 Image Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.4.1 Non-local Means . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.4.2 Block Matching 3D (BM3D) . . . . . . . . . . . . . . . . . . 183
6.4.3 Learned Sparse Coding . . . . . . . . . . . . . . . . . . . . . 184
6.4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.5 Shape from Texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.5.1 Shape from Texture for Planes . . . . . . . . . . . . . . . . . 187
6.5.2 Shape from Texture for Curved Surfaces . . . . . . . . . . . . 190
6.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
III EARLY VISION: MULTIPLE IMAGES 195
7 Stereopsis 197
7.1 Binocular Camera Geometry and the Epipolar Constraint . . . . . . 198
7.1.1 Epipolar Geometry . . . . . . . . . . . . . . . . . . . . . . . . 198
7.1.2 The Essential Matrix . . . . . . . . . . . . . . . . . . . . . . . 200
7.1.3 The Fundamental Matrix . . . . . . . . . . . . . . . . . . . . 201
7.2 Binocular Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 201
7.2.1 Image Rectification . . . . . . . . . . . . . . . . . . . . . . . . 202
7.3 Human Stereopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.4 Local Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 205
7.4.1 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
7.4.2 Multi-Scale Edge Matching . . . . . . . . . . . . . . . . . . . 207
7.5 Global Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 210
7.5.1 Ordering Constraints and Dynamic Programming . . . . . . . 210
7.5.2 Smoothness and Graphs . . . . . . . . . . . . . . . . . . . . . 211
7.6 Using More Cameras . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
7.7 Application: Robot Navigation . . . . . . . . . . . . . . . . . . . . . 215
7.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
8 Structure from Motion 221
8.1 Internally Calibrated Perspective Cameras . . . . . . . . . . . . . . . 221
8.1.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 223
8.1.2 Euclidean Structure and Motion from Two Images . . . . . . 224
8.1.3 Euclidean Structure and Motion from Multiple Images . . . . 228
8.2 Uncalibrated Weak-Perspective Cameras . . . . . . . . . . . . . . . . 230
8.2.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 231
8.2.2 Affine Structure and Motion from Two Images . . . . . . . . 233
8.2.3 Affine Structure and Motion from Multiple Images . . . . . . 237
8.2.4 From Affine to Euclidean Shape . . . . . . . . . . . . . . . . 238
8.3 Uncalibrated Perspective Cameras . . . . . . . . . . . . . . . . . . . 240
8.3.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 241
8.3.2 Projective Structure and Motion from Two Images . . . . . . 242
8.3.3 Projective Structure and Motion from Multiple Images . . . . 244
8.3.4 From Projective to Euclidean Shape . . . . . . . . . . . . . . 246
8.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
IV MID-LEVEL VISION 253
9 Segmentation by Clustering 255
9.1 Human Vision: Grouping and Gestalt . . . . . . . . . . . . . . . . . 256
9.2 Important Applications . . . . . . . . . . . . . . . . . . . . . . . . . 261
9.2.1 Background Subtraction . . . . . . . . . . . . . . . . . . . . . 261
9.2.2 Shot Boundary Detection . . . . . . . . . . . . . . . . . . . . 264
9.2.3 Interactive Segmentation . . . . . . . . . . . . . . . . . . . . 265
9.2.4 Forming Image Regions . . . . . . . . . . . . . . . . . . . . . 266
9.3 Image Segmentation by Clustering Pixels . . . . . . . . . . . . . . . 268
9.3.1 Basic Clustering Methods . . . . . . . . . . . . . . . . . . . . 269
9.3.2 The Watershed Algorithm . . . . . . . . . . . . . . . . . . . . 271
9.3.3 Segmentation Using K-means . . . . . . . . . . . . . . . . . . 272
9.3.4 Mean Shift: Finding Local Modes in Data . . . . . . . . . . . 273
9.3.5 Clustering and Segmentation with Mean Shift . . . . . . . . . 275
9.4 Segmentation, Clustering, and Graphs . . . . . . . . . . . . . . . . . 277
9.4.1 Terminology and Facts for Graphs . . . . . . . . . . . . . . . 277
9.4.2 Agglomerative Clustering with a Graph . . . . . . . . . . . . 279
9.4.3 Divisive Clustering with a Graph . . . . . . . . . . . . . . . . 281
9.4.4 Normalized Cuts . . . . . . . . . . . . . . . . . . . . . . . . . 284
9.5 Image Segmentation in Practice . . . . . . . . . . . . . . . . . . . . . 285
9.5.1 Evaluating Segmenters . . . . . . . . . . . . . . . . . . . . . . 286
9.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
10 Grouping and Model Fitting 290
10.1 The Hough Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 290
10.1.1 Fitting Lines with the Hough Transform . . . . . . . . . . . . 290
10.1.2 Using the Hough Transform . . . . . . . . . . . . . . . . . . . 292
10.2 Fitting Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . 293
10.2.1 Fitting a Single Line . . . . . . . . . . . . . . . . . . . . . . . 294
10.2.2 Fitting Planes . . . . . . . . . . . . . . . . . . . . . . . . . . 295
10.2.3 Fitting Multiple Lines . . . . . . . . . . . . . . . . . . . . . . 296
10.3 Fitting Curved Structures . . . . . . . . . . . . . . . . . . . . . . . . 297
10.4 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
10.4.1 M-Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
10.4.2 RANSAC: Searching for Good Points . . . . . . . . . . . . . 302
10.5 Fitting Using Probabilistic Models . . . . . . . . . . . . . . . . . . . 306
10.5.1 Missing Data Problems . . . . . . . . . . . . . . . . . . . . . 307
10.5.2 Mixture Models and Hidden Variables . . . . . . . . . . . . . 309
10.5.3 The EM Algorithm for Mixture Models . . . . . . . . . . . . 310
10.5.4 Difficulties with the EM Algorithm . . . . . . . . . . . . . . . 312
10.6 Motion Segmentation by Parameter Estimation . . . . . . . . . . . . 313
10.6.1 Optical Flow and Motion . . . . . . . . . . . . . . . . . . . . 315
10.6.2 Flow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
10.6.3 Motion Segmentation with Layers . . . . . . . . . . . . . . . 317
10.7 Model Selection: Which Model Is the Best Fit? . . . . . . . . . . . . 319
10.7.1 Model Selection Using Cross-Validation . . . . . . . . . . . . 322
10.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
11 Tracking 326
11.1 Simple Tracking Strategies . . . . . . . . . . . . . . . . . . . . . . . . 327
11.1.1 Tracking by Detection . . . . . . . . . . . . . . . . . . . . . . 327
11.1.2 Tracking Translations by Matching . . . . . . . . . . . . . . . 330
11.1.3 Using Affine Transformations to Confirm a Match . . . . . . 332
11.2 Tracking Using Matching . . . . . . . . . . . . . . . . . . . . . . . . 334
11.2.1 Matching Summary Representations . . . . . . . . . . . . . . 335
11.2.2 Tracking Using Flow . . . . . . . . . . . . . . . . . . . . . . . 337
11.3 Tracking Linear Dynamical Models with Kalman Filters . . . . . . . 339
11.3.1 Linear Measurements and Linear Dynamics . . . . . . . . . . 340
11.3.2 The Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . 344
11.3.3 Forward-backward Smoothing . . . . . . . . . . . . . . . . . . 345
11.4 Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
11.4.1 Linking Kalman Filters with Detection Methods . . . . . . . 349
11.4.2 Key Methods of Data Association . . . . . . . . . . . . . . . 350
11.5 Particle Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
11.5.1 Sampled Representations of Probability Distributions . . . . 351
11.5.2 The Simplest Particle Filter . . . . . . . . . . . . . . . . . . . 355
11.5.3 The Tracking Algorithm . . . . . . . . . . . . . . . . . . . . . 356
11.5.4 A Workable Particle Filter . . . . . . . . . . . . . . . . . . . . 358
11.5.5 Practical Issues in Particle Filters . . . . . . . . . . . . . . . 360
11.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
V HIGH-LEVEL VISION 365
12 Registration 367
12.1 Registering Rigid Objects . . . . . . . . . . . . . . . . . . . . . . . . 368
12.1.1 Iterated Closest Points . . . . . . . . . . . . . . . . . . . . . . 368
12.1.2 Searching for Transformations via Correspondences . . . . . . 369
12.1.3 Application: Building Image Mosaics . . . . . . . . . . . . . . 370
12.2 Model-based Vision: Registering Rigid Objects with Projection . . . 375
12.2.1 Verification: Comparing Transformed and Rendered Source
to Target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
12.3 Registering Deformable Objects . . . . . . . . . . . . . . . . . . . . . 378
12.3.1 Deforming Texture with Active Appearance Models . . . . . 378
12.3.2 Active Appearance Models in Practice . . . . . . . . . . . . . 381
12.3.3 Application: Registration in Medical Imaging Systems . . . . 383
12.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
13 Smooth Surfaces and Their Outlines 391
13.1 Elements of Differential Geometry . . . . . . . . . . . . . . . . . . . 393
13.1.1 Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
13.1.2 Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
13.2 Contour Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
13.2.1 The Occluding Contour and the Image Contour . . . . . . . . 402
13.2.2 The Cusps and Inflections of the Image Contour . . . . . . . 403
13.2.3 Koenderink’s Theorem . . . . . . . . . . . . . . . . . . . . . . 404
13.3 Visual Events: More Differential Geometry . . . . . . . . . . . . . . 407
13.3.1 The Geometry of the Gauss Map . . . . . . . . . . . . . . . . 407
13.3.2 Asymptotic Curves . . . . . . . . . . . . . . . . . . . . . . . . 409
13.3.3 The Asymptotic Spherical Map . . . . . . . . . . . . . . . . . 410
13.3.4 Local Visual Events . . . . . . . . . . . . . . . . . . . . . . . 412
13.3.5 The Bitangent Ray Manifold . . . . . . . . . . . . . . . . . . 413
13.3.6 Multilocal Visual Events . . . . . . . . . . . . . . . . . . . . . 414
13.3.7 The Aspect Graph . . . . . . . . . . . . . . . . . . . . . . . . 416
13.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
14 Range Data 422
14.1 Active Range Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . 422
14.2 Range Data Segmentation . . . . . . . . . . . . . . . . . . . . . . . . 424
14.2.1 Elements of Analytical Differential Geometry . . . . . . . . . 424
14.2.2 Finding Step and Roof Edges in Range Images . . . . . . . . 426
14.2.3 Segmenting Range Images into Planar Regions . . . . . . . . 431
14.3 Range Image Registration and Model Acquisition . . . . . . . . . . . 432
14.3.1 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
14.3.2 Registering Range Images . . . . . . . . . . . . . . . . . . . . 434
14.3.3 Fusing Multiple Range Images . . . . . . . . . . . . . . . . . 436
14.4 Object Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
14.4.1 Matching Using Interpretation Trees . . . . . . . . . . . . . . 438
14.4.2 Matching Free-Form Surfaces Using Spin Images . . . . . . . 441
14.5 Kinect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
14.5.1 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
14.5.2 Technique: Decision Trees and Random Forests . . . . . . . . 448
14.5.3 Labeling Pixels . . . . . . . . . . . . . . . . . . . . . . . . . . 450
14.5.4 Computing Joint Positions . . . . . . . . . . . . . . . . . . . 453
14.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
15 Learning to Classify 457
15.1 Classification, Error, and Loss . . . . . . . . . . . . . . . . . . . . . . 457
15.1.1 Using Loss to Determine Decisions . . . . . . . . . . . . . . . 457
15.1.2 Training Error, Test Error, and Overfitting . . . . . . . . . . 459
15.1.3 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 460
15.1.4 Error Rate and Cross-Validation . . . . . . . . . . . . . . . . 463
15.1.5 Receiver Operating Curves . . . . . . . . . . . . . . . . . . . 465
15.2 Major Classification Strategies . . . . . . . . . . . . . . . . . . . . . 467
15.2.1 Example: Mahalanobis Distance . . . . . . . . . . . . . . . . 467
15.2.2 Example: Class-Conditional Histograms and Naive Bayes . . 468
15.2.3 Example: Classification Using Nearest Neighbors . . . . . . . 469
15.2.4 Example: The Linear Support Vector Machine . . . . . . . . 470
15.2.5 Example: Kernel Machines . . . . . . . . . . . . . . . . . . . 473
15.2.6 Example: Boosting and Adaboost . . . . . . . . . . . . . . . 475
15.3 Practical Methods for Building Classifiers . . . . . . . . . . . . . . . 475
15.3.1 Manipulating Training Data to Improve Performance . . . . . 477
15.3.2 Building Multi-Class Classifiers Out of Binary Classifiers . . 479
15.3.3 Solving for SVMS and Kernel Machines . . . . . . . . . . . . 480
15.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
16 Classifying Images 482
16.1 Building Good Image Features . . . . . . . . . . . . . . . . . . . . . 482
16.1.1 Example Applications . . . . . . . . . . . . . . . . . . . . . . 482
16.1.2 Encoding Layout with GIST Features . . . . . . . . . . . . . 485
16.1.3 Summarizing Images with Visual Words . . . . . . . . . . . . 487
16.1.4 The Spatial Pyramid Kernel . . . . . . . . . . . . . . . . . . . 489
16.1.5 Dimension Reduction with Principal Components . . . . . . . 493
16.1.6 Dimension Reduction with Canonical Variates . . . . . . . . 494
16.1.7 Example Application: Identifying Explicit Images . . . . . . 498
16.1.8 Example Application: Classifying Materials . . . . . . . . . . 502
16.1.9 Example Application: Classifying Scenes . . . . . . . . . . . . 502
16.2 Classifying Images of Single Objects . . . . . . . . . . . . . . . . . . 504
16.2.1 Image Classification Strategies . . . . . . . . . . . . . . . . . 505
16.2.2 Evaluating Image Classification Systems . . . . . . . . . . . . 505
16.2.3 Fixed Sets of Classes . . . . . . . . . . . . . . . . . . . . . . . 508
16.2.4 Large Numbers of Classes . . . . . . . . . . . . . . . . . . . . 509
16.2.5 Flowers, Leaves, and Birds: Some Specialized Problems . . . 511
16.3 Image Classification in Practice . . . . . . . . . . . . . . . . . . . . . 512
16.3.1 Codes for Image Features . . . . . . . . . . . . . . . . . . . . 513
16.3.2 Image Classification Datasets . . . . . . . . . . . . . . . . . . 513
16.3.3 Dataset Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
16.3.4 Crowdsourcing Dataset Collection . . . . . . . . . . . . . . . 515
16.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
17 Detecting Objects in Images 519
17.1 The Sliding Window Method . . . . . . . . . . . . . . . . . . . . . . 519
17.1.1 Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 520
17.1.2 Detecting Humans . . . . . . . . . . . . . . . . . . . . . . . . 525
17.1.3 Detecting Boundaries . . . . . . . . . . . . . . . . . . . . . . 527
17.2 Detecting Deformable Objects . . . . . . . . . . . . . . . . . . . . . . 530
17.3 The State of the Art of Object Detection . . . . . . . . . . . . . . . 535
17.3.1 Datasets and Resources . . . . . . . . . . . . . . . . . . . . . 538
17.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
18 Topics in Object Recognition 540
18.1 What Should Object Recognition Do? . . . . . . . . . . . . . . . . . 540
18.1.1 What Should an Object Recognition System Do? . . . . . . . 540
18.1.2 Current Strategies for Object Recognition . . . . . . . . . . . 542
18.1.3 What Is Categorization? . . . . . . . . . . . . . . . . . . . . . 542
18.1.4 Selection: What Should Be Described? . . . . . . . . . . . . . 544
18.2 Feature Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
18.2.1 Improving Current Image Features . . . . . . . . . . . . . . . 544
18.2.2 Other Kinds of Image Feature . . . . . . . . . . . . . . . . . . 546
18.3 Geometric Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
18.4 Semantic Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
18.4.1 Attributes and the Unfamiliar . . . . . . . . . . . . . . . . . . 550
18.4.2 Parts, Poselets and Consistency . . . . . . . . . . . . . . . . . 551
18.4.3 Chunks of Meaning . . . . . . . . . . . . . . . . . . . . . . . . 554
VI APPLICATIONS AND TOPICS 557
19 Image-Based Modeling and Rendering 559
19.1 Visual Hulls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
19.1.1 Main Elements of the Visual Hull Model . . . . . . . . . . . . 561
19.1.2 Tracing Intersection Curves . . . . . . . . . . . . . . . . . . . 563
19.1.3 Clipping Intersection Curves . . . . . . . . . . . . . . . . . . 566
19.1.4 Triangulating Cone Strips . . . . . . . . . . . . . . . . . . . . 567
19.1.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
19.1.6 Going Further: Carved Visual Hulls . . . . . . . . . . . . . . 572
19.2 Patch-Based Multi-View Stereopsis . . . . . . . . . . . . . . . . . . . 573
19.2.1 Main Elements of the PMVS Model . . . . . . . . . . . . . . 575
19.2.2 Initial Feature Matching . . . . . . . . . . . . . . . . . . . . . 578
19.2.3 Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
19.2.4 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
19.2.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
19.3 The Light Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
19.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
20 Looking at People 590
20.1 HMM’s, Dynamic Programming, and Tree-Structured Models . . . . 590
20.1.1 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . 590
20.1.2 Inference for an HMM . . . . . . . . . . . . . . . . . . . . . . 592
20.1.3 Fitting an HMM with EM . . . . . . . . . . . . . . . . . . . . 597
20.1.4 Tree-Structured Energy Models . . . . . . . . . . . . . . . . . 600
20.2 Parsing People in Images . . . . . . . . . . . . . . . . . . . . . . . . 602
20.2.1 Parsing with Pictorial Structure Models . . . . . . . . . . . . 602
20.2.2 Estimating the Appearance of Clothing . . . . . . . . . . . . 604
20.3 Tracking People . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
20.3.1 Why Human Tracking Is Hard . . . . . . . . . . . . . . . . . 606
20.3.2 Kinematic Tracking by Appearance . . . . . . . . . . . . . . . 608
20.3.3 Kinematic Human Tracking Using Templates . . . . . . . . . 609
20.4 3D from 2D: Lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
20.4.1 Reconstruction in an Orthographic View . . . . . . . . . . . . 611
20.4.2 Exploiting Appearance for Unambiguous Reconstructions . . 613
20.4.3 Exploiting Motion for Unambiguous Reconstructions . . . . . 615
20.5 Activity Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
20.5.1 Background: Human Motion Data . . . . . . . . . . . . . . . 617
20.5.2 Body Configuration and Activity Recognition . . . . . . . . . 621
20.5.3 Recognizing Human Activities with Appearance Features . . 622
20.5.4 Recognizing Human Activities with Compositional Models . . 624
20.6 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
20.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
21 Image Search and Retrieval 627
21.1 The Application Context . . . . . . . . . . . . . . . . . . . . . . . . . 627
21.1.1 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
21.1.2 User Needs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
21.1.3 Types of Image Query . . . . . . . . . . . . . . . . . . . . . . 630
21.1.4 What Users Do with Image Collections . . . . . . . . . . . . 631
21.2 Basic Technologies from Information Retrieval . . . . . . . . . . . . . 632
21.2.1 Word Counts . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
21.2.2 Smoothing Word Counts . . . . . . . . . . . . . . . . . . . . . 633
21.2.3 Approximate Nearest Neighbors and Hashing . . . . . . . . . 634
21.2.4 Ranking Documents . . . . . . . . . . . . . . . . . . . . . . . 638
21.3 Images as Documents . . . . . . . . . . . . . . . . . . . . . . . . . . 639
21.3.1 Matching Without Quantization . . . . . . . . . . . . . . . . 640
21.3.2 Ranking Image Search Results . . . . . . . . . . . . . . . . . 641
21.3.3 Browsing and Layout . . . . . . . . . . . . . . . . . . . . . . 643
21.3.4 Laying Out Images for Browsing . . . . . . . . . . . . . . . . 644
21.4 Predicting Annotations for Pictures . . . . . . . . . . . . . . . . . . 645
21.4.1 Annotations from Nearby Words . . . . . . . . . . . . . . . . 646
21.4.2 Annotations from the Whole Image . . . . . . . . . . . . . . 646
21.4.3 Predicting Correlated Words with Classifiers . . . . . . . . . 648
21.4.4 Names and Faces . . . . . . . . . . . . . . . . . . . . . . . . 649
21.4.5 Generating Tags with Segments . . . . . . . . . . . . . . . . . 651
21.5 The State of the Art of Word Prediction . . . . . . . . . . . . . . . . 654
21.5.1 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.2 Comparing Methods . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.3 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 656
21.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659
VII BACKGROUND MATERIAL 661
22 Optimization Techniques 663
22.1 Linear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . . . 663
22.1.1 Normal Equations and the Pseudoinverse . . . . . . . . . . . 664
22.1.2 Homogeneous Systems and Eigenvalue Problems . . . . . . . 665
22.1.3 Generalized Eigenvalues Problems . . . . . . . . . . . . . . . 666
22.1.4 An Example: Fitting a Line to Points in a Plane . . . . . . . 666
22.1.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . 667
22.2 Nonlinear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . 669
22.2.1 Newton’s Method: Square Systems of Nonlinear Equations. . 670
22.2.2 Newton’s Method for Overconstrained Systems . . . . . . . . 670
22.2.3 The Gauss―Newton and Levenberg―Marquardt Algorithms . 671
22.3 Sparse Coding and Dictionary Learning . . . . . . . . . . . . . . . . 672
22.3.1 Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . 672
22.3.2 Dictionary Learning . . . . . . . . . . . . . . . . . . . . . . . 673
22.3.3 Supervised Dictionary Learning . . . . . . . . . . . . . . . . . 675
22.4 Min-Cut/Max-Flow Problems and Combinatorial Optimization . . . 675
22.4.1 Min-Cut Problems . . . . . . . . . . . . . . . . . . . . . . . . 676
22.4.2 Quadratic Pseudo-Boolean Functions . . . . . . . . . . . . . . 677
22.4.3 Generalization to Integer Variables . . . . . . . . . . . . . . . 679
22.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
Bibliography 684
Index 737
List of Algorithms 760
・ ・ ・ ・ ・ ・ (收起)序言
前言
第1篇　基础知识
第1章　引言 2
1.1　人工智能的新焦点――深度学习 2
1.1.1　人工智能――神话传说到影视漫画 2
1.1.2　人工智能的诞生 3
1.1.3　神经科学的研究 4
1.1.4　人工神经网络的兴起 5
1.1.5　神经网络的第一次寒冬 6
1.1.6　神经网络的第一次复兴 8
1.1.7　神经网络的第二次寒冬 9
1.1.8　2006年――深度学习的起点 10
1.1.9　生活中的深度学习 11
1.1.10　常见深度学习框架简介 12
1.2　给计算机一双眼睛――计算机视觉 14
1.2.1　计算机视觉简史 14
1.2.2　2012年――计算机视觉的新起点 16
1.2.3　计算机视觉的应用 17
1.2.4　常见计算机视觉工具包 19
1.3　基于深度学习的计算机视觉 19
1.3.1　从ImageNet竞赛到AlphaGo战胜李世石――计算机视觉超越人类 19
1.3.2　GPU和并行技术――深度学习和计算视觉发展的加速器 21
1.3.3　基于卷积神经网络的计算机视觉应用 22
第2章　深度学习和计算机视觉中的基础数学知识 27
2.1　线性变换和非线性变换 27
2.1.1　线性变换的定义 27
2.1.2　高中教科书中的小例子 28
2.1.3　点积和投影 28
2.1.4　矩阵乘法的几何意义（1） 30
2.1.5　本征向量和本征值 34
2.1.6　矩阵乘法的几何意义（2） 37
2.1.7　奇异值分解 38
2.1.8　线性可分性和维度 39
2.1.9　非线性变换 42
2.2　概率论及相关基础知识 43
2.2.1　条件概率和独立 43
2.2.2　期望值、方差和协方差 44
2.2.3　熵 45
2.2.4　最大似然估计（Maximum Likelihood Estimation，MLE） 47
2.2.5　KL散度（KullbackCLeibler divergence） 49
2.2.6　KL散度和MLE的联系 49
2.3　维度的诅咒 50
2.3.1　采样和维度 50
2.3.2　高维空间中的体积 51
2.3.3　高维空间中的距离 53
2.3.4　中心极限定理和高维样本距离分布的近似 54
2.3.5　数据实际的维度 56
2.3.6　局部泛化 58
2.3.7　函数对实际维度的影响 59
2.3.8　PCA――什么是主成分 60
2.3.9　PCA――通过本征向量和本征值求主成分 60
2.3.10　PCA――通过主成分分析降维 61
2.3.11　PCA――归一化和相关性系数 63
2.3.12　PCA――什么样的数据适合PCA 64
2.3.13　其他降维手段 65
2.4　卷积 66
2.4.1　点积和卷积 66
2.4.2　一维卷积 67
2.4.3　卷积和互相关 68
2.4.4　二维卷积和图像响应 69
2.4.5　卷积的计算 70
2.5　数学优化基础 71
2.5.1　最小值和梯度下降 72
2.5.2　冲量（Momentum） 73
2.5.3　牛顿法 75
2.5.4　学习率和自适应步长 77
2.5.5　学习率衰减（Learning Rate Decay） 78
2.5.6　AdaGrad：每个变量有自己的节奏 78
2.5.7　AdaDelta的进一步改进 79
2.5.8　其他自适应算法 80
2.5.9　损失函数 81
2.5.10　分类问题和负对数似然 82
2.5.11　逻辑回归 83
2.5.12　Softmax：将输出转换为概率 84
2.5.13　链式求导法则 84
第3章　神经网络和机器学习基础 87
3.1　感知机 87
3.1.1　基本概念 87
3.1.2　感知机和线性二分类 87
3.1.3　激活函数 88
3.2　神经网络基础 89
3.2.1　从感知机到神经网络 89
3.2.2　最简单的神经网络二分类例子 90
3.2.3　隐层神经元数量的作用 93
3.2.4　更加复杂的样本和更复杂的神经网络 94
3.3　后向传播算法 95
3.3.1　求神经网络参数的梯度 95
3.3.2　计算图（Computational Graph） 95
3.3.3　利用后向传播算法计算一个神经网络参数的梯度 97
3.3.4　梯度消失 99
3.3.5　修正线性单元（ReLU） 100
3.3.6　梯度爆炸 101
3.3.7　梯度检查（gradient check） 102
3.3.8　从信息传播的角度看后向传播算法 103
3.4　随机梯度下降和批量梯度下降 104
3.4.1　全量数据（full-batch）梯度下降 104
3.4.2　随机梯度下降（SGD）和小批量数据（mini-batch） 104
3.4.3　数据均衡和数据增加（data augmentation） 106
3.5　数据、训练策略和规范化 108
3.5.1　欠拟合和过拟合 108
3.5.2　训练误差和测试误差 109
3.5.3　奥卡姆剃刀没有免费午餐 111
3.5.4　数据集划分和提前停止 112
3.5.5　病态问题和约束 113
3.5.6　L2规范化（L2 Regularization） 113
3.5.7　L1规范化（L1 Regularization） 114
3.5.8　集成（Ensemble）和随机失活（Dropout） 115
3.6　监督学习、非监督学习、半监督学习和强化学习 117
3.6.1　监督学习、非监督学习和半监督学习 117
3.6.2　强化学习（reinforcement learning） 118
第4章　深度卷积神经网络 120
4.1　卷积神经网络 120
4.1.1　基本概念 120
4.1.2　卷积层和特征响应图 121
4.1.3　参数共享 123
4.1.4　稀疏连接 124
4.1.5　多通道卷积 125
4.1.6　激活函数 125
4.1.7　池化、不变性和感受野 126
4.1.8　分布式表征（Distributed Representation） 128
4.1.9　分布式表征和局部泛化 130
4.1.10　分层表达 131
4.1.11　卷积神经网络结构 131
4.2　LeNet――第一个卷积神经网络 132
4.3　新起点――AlexNet 133
4.3.1　网络结构 133
4.3.2　局部响应归一化（Local Response Normalization，LRN） 136
4.4　更深的网络――GoogLeNet 136
4.4.1　1×1卷积和Network In Network 136
4.4.2　Inception结构 138
4.4.3　网络结构 138
4.4.4　批规一化（Batch Normalization，BN） 140
4.5　更深的网络――ResNet 142
4.5.1　困难的深层网络训练：退化问题 142
4.5.2　残差单元 142
4.5.3　深度残差网络 144
4.5.4　从集成的角度看待ResNet 144
4.5.5　结构更复杂的网络 146
第2篇　实例精讲
第5章　Python基础 148
5.1　Python简介 148
5.1.1　Python简史 148
5.1.2　安装和使用Python 149
5.2　Python基本语法 150
5.2.1　基本数据类型和运算 150
5.2.2　容器 153
5.2.3　分支和循环 156
5.2.4　函数、生成器和类 159
5.2.5　map、reduce和filter 162
5.2.6　列表生成（list comprehension） 163
5.2.7　字符串 163
5.2.8　文件操作和pickle 164
5.2.9　异常 165
5.2.10　多进程（multiprocessing） 165
5.2.11　os模块 166
5.3　Python的科学计算包――NumPy 167
5.3.1　基本类型（array） 167
5.3.2　线性代数模块（linalg） 172
5.3.3　随机模块（random） 173
5.4　Python的可视化包――matplotlib 175
5.4.1　2D图表 175
5.4.2　3D图表 178
5.4.3　图像显示 180
第6章　OpenCV基础 182
6.1　OpenCV简介 182
6.1.1　OpenCV的结构 182
6.1.2　安装和使用OpenCV 183
6.2　Python-OpenCV基础 184
6.2.1　图像的表示 184
6.2.2　基本图像处理 185
6.2.3　图像的仿射变换 188
6.2.4　基本绘图 190
6.2.5　视频功能 192
6.3　用OpenCV实现数据增加小工具 193
6.3.1　随机裁剪 194
6.3.2　随机旋转 194
6.3.3　随机颜色和明暗 196
6.3.4　多进程调用加速处理 196
6.3.5　代码：图片数据增加小工具 196
6.4　用OpenCV实现物体标注小工具 203
6.4.1　窗口循环 203
6.4.2　鼠标和键盘事件 205
6.4.3　代码：物体检测标注的小工具 206
第7章　Hello World! 212
7.1　用MXNet实现一个神经网络 212
7.1.1　基础工具、NVIDIA驱动和CUDA安装 212
7.1.2　安装MXNet 213
7.1.3　MXNet基本使用 214
7.1.4　用MXNet实现一个两层神经网络 215
7.2　用Caffe实现一个神经网络 219
7.2.1　安装Caffe 219
7.2.2　Caffe的基本概念 220
7.2.3　用Caffe实现一个两层神经网络 221
第8章　最简单的图片分类――手写数字识别 227
8.1　准备数据――MNIST 227
8.1.1　下载MNIST 227
8.1.2　生成MNIST的图片 227
8.2　基于Caffe的实现 228
8.2.1　制作LMDB数据 229
8.2.2　训练LeNet-5 230
8.2.3　测试和评估 235
8.2.4　识别手写数字 239
8.2.5　增加平移和旋转扰动 240
8.3　基于MXNet的实现 242
8.3.1　制作Image Recordio数据 242
8.3.2　用Module模块训练LeNet-5 243
8.3.3　测试和评估 245
8.3.4　识别手写数字 247
第9章　利用Caffe做回归 249
9.1　回归的原理 249
9.1.1　预测值和标签值的欧式距离 249
9.1.2　EuclideanLoss层 250
9.2　预测随机噪声的频率 250
9.2.1　生成样本：随机噪声 250
9.2.2　制作多标签HDF5数据 252
9.2.3　网络结构和Solver定义 253
9.2.4　训练网络 259
9.2.5　批量装载图片并利用GPU预测 260
9.2.6　卷积核可视化 262
第10章　迁移学习和模型微调 264
10.1　吃货必备――通过Python采集美食图片 264
10.1.1　通过关键词和图片搜索引擎下载图片 264
10.1.2　数据预处理――去除无效和不相关图片 267
10.1.3　数据预处理――去除重复图片 267
10.1.4　生成训练数据 269
10.2　美食分类模型 271
10.2.1　迁移学习 271
10.2.2　模型微调法（Finetune） 272
10.2.3　混淆矩阵（Confusion Matrix） 276
10.2.4　P-R曲线和ROC曲线 278
10.2.5　全局平均池化和激活响应图 284
第11章　目标检测 288
11.1　目标检测算法简介 288
11.1.1　滑窗法 288
11.1.2　PASCAL VOC、mAP和IOU简介 289
11.1.3　Selective Search和R-CNN简介 290
11.1.4　SPP、ROI Pooling和Fast R-CNN简介 291
11.1.5　RPN和Faster R-CNN简介 293
11.1.6　YOLO和SSD简介 294
11.2　基于PASCAL VOC数据集训练SSD模型 296
11.2.1　MXNet的SSD实现 296
11.2.2　下载PASCAL VOC数据集 297
11.2.3　训练SSD模型 298
11.2.4　测试和评估模型效果 299
11.2.5　物体检测结果可视化 299
11.2.6　制作自己的标注数据 302
第12章　度量学习 304
12.1　距离和度量学习 304
12.1.1　欧氏距离和马氏距离 304
12.1.2　欧式距离和余弦距离 305
12.1.3　非线性度量学习和Siamese网络 306
12.1.4　Contrastive Loss：对比损失函数 307
12.2　用MNIST训练Siamese网络 307
12.2.1　数据准备 307
12.2.2　参数共享训练 309
12.2.3　结果和可视化 314
12.2.4　用τ-SNE可视化高维特征 316
第13章　图像风格迁移 317
13.1　风格迁移算法简介 317
13.1.1　通过梯度下降法进行图像重建 317
13.1.2　图像风格重建和Gram矩阵 318
13.1.3　图像风格迁移 320
13.2　MXNet中的图像风格迁移例子 320
13.2.1　MXNet的风格迁移实现 321
13.2.2　对图片进行风格迁移 326
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 概述 1
1.1 什么是计算机视觉？ 2
1.2 简史 8
1.3 本书概述 16
1.4 课程大纲样例 21
1.5 标记法说明 22
1.6 扩展阅读 22
第2章 图像形成 25
2.1 几何基元和变换 26
2.1.1 几何基元 26
2.1.2 2D变换 29
2.1.3 3D变换 32
2.1.4 3D旋转 33
2.1.5 3D到2D投影 37
2.1.6 镜头畸变 46
2.2 光度测定学的图像形成 47
2.2.1 照明 48
2.2.2 反射和阴影 49
2.2.3 光学 54
2.3 数字摄像机 57
2.3.1 采样与混叠 60
2.3.2 色彩 63
2.3.3 压缩 71
2.4 补充阅读 72
2.5 习题 73
第3章 图像处理 77
3.1 点算子 78
3.1.1 像素变换 79
3.1.2 彩色变换 81
3.1.3 合成与抠图 81
3.1.4 直方图均衡化 83
3.1.5 应用：色调调整 86
3.2 线性滤波 86
3.2.1 可分离的滤波 89
3.2.2 线性滤波示例 90
3.2.3 带通和导向滤波器 91
3.3 更多的邻域算子 95
3.3.1 非线性滤波 95
3.3.2 形态学 99
3.3.3 距离变换 100
3.3.4 连通量 101
3.4 傅里叶变换 102
3.4.1 傅里叶变换对 105
3.4.2 二维傅里叶变换 107
3.4.3 维纳滤波 108
3.4.4 应用：锐化，模糊
和去噪 111
3.5 金字塔与小波 111
3.5.1 插值 112
3.5.2 降采样 114
3.5.3 多分辨率表达 116
3.5.4 小波 119
3.5.5 应用：图像融合 123
3.6 几何变换 125
3.6.1 参数化变换 125
3.6.2 基于网格的卷绕 131
3.6.3 应用：基于特征的变形 133
3.7 全局优化 133
3.7.1 正则化 134
3.7.2 马尔科夫随机场 138
3.7.3 应用：图像的恢复 147
3.8 补充阅读 147
3.9 习题 149
第4章 特征检测与匹配 157
4.1 点和块 159
4.1.1 特征检测器 160
4.1.2 特征描述子 169
4.1.3 特征匹配 172
4.1.4 特征跟踪 179
4.1.5 应用：表演驱动的动画 181
4.2 边缘 182
4.2.1 边缘检测 182
4.2.2 边缘连接 187
4.2.3 应用：边缘编辑和增强 189
4.3 线条 190
4.3.1 逐次近似 191
4.3.2 Hough变换 191
4.3.3 消失点 194
4.3.4 应用：矩形检测 196
4.4 扩展阅读 197
4.5 习题 198
第5章 分割 205
5.1 活动轮廓 206
5.1.1 蛇行 207
5.1.2 动态蛇行和
CONDENSATION 211
5.1.3 剪刀 214
5.1.4 水平集 215
5.1.5 应用：轮廓跟踪和
转描机 217
5.2 分裂与归并 218
5.2.1 分水岭 218
5.2.2 区域分裂(区分式聚类) 219
5.2.3 区域归并(凝聚式聚类) 219
5.2.4 基于图的分割 219
5.2.5 概率聚集 220
5.3 均值移位和模态发现 221
5.3.1 k-均值和高斯混合 222
5.3.2 均值移位 224
5.4 规范图割 227
5.5 图割和基于能量的方法 230
5.6 补充阅读 234
5.7 习题 235
第6章 基于特征的配准 237
6.1 基于2D和3D特征的配准 238
6.1.1 使用最小二乘的
2D配准 238
6.1.2 应用：全景图 240
6.1.3 迭代算法 241
6.1.4 鲁棒最小二乘
和RANSAC 243
6.1.5 3D配准 245
6.2 姿态估计 246
6.2.1 线性算法 246
6.2.2 迭代算法 248
6.2.3 应用：增强现实 249
6.3 几何内参数标定 250
6.3.1 标定模式 250
6.3.2 消失点 252
6.3.3 应用：单视图测量学 253
6.3.4 旋转运动 254
6.3.5 径向畸变 256
6.4 补充阅读 257
6.5 习题 258
第7章 由运动到结构 263
7.1 三角测量 264
7.2 二视图由运动到结构 266
7.2.1 投影(未标定的)重建 270
7.2.2 自标定 271
7.2.3 应用：视图变形 273
7.3 因子分解 274
7.3.1 透视与投影因子分解 276
7.3.2 应用：稀疏3D模型
提取 277
7.4 光束平差法 278
7.4.1 挖掘稀疏性 280
7.4.2 应用：匹配运动和增强
现实 282
7.4.3 不确定性和二义性 283
7.4.4 应用：由因特网照片
重建 284
7.5 限定结构和运动 287
7.5.1 基于线条的方法 287
7.5.2 基于平面的方法 288
7.6 补充阅读 289
7.7 习题 290
第8章 稠密运动估计 293
8.1 平移配准 294
8.1.1 分层运动估计 297
8.1.2 基于傅里叶的配准 298
8.1.3 逐次求精 300
8.2 参数化运动 305
8.2.1 应用：视频稳定化 308
8.2.2 学到的运动模型 308
8.3 基于样条的运动 309
8.4 光流 312
8.4.1 多帧运动估计 315
8.4.2 应用：视频去噪 316
8.4.3 应用：去隔行扫描 316
8.5 层次运动 317
8.5.1 应用：帧插值 319
8.5.2 透明层和反射 320
8.6 补充阅读 321
8.7 习题 322
第9章 图像拼接 327
9.1 运动模型 329
9.1.1 平面透视运动 329
9.1.2 应用：白板和文档扫描 330
9.1.3 旋转全景图 331
9.1.4 缝隙消除 333
9.1.5 应用：视频摘要和压缩 334
9.1.6 圆柱面和球面坐标 335
9.2 全局配准 338
9.2.1 光束平差法 338
9.2.2 视差消除 341
9.2.3 认出全景图 343
9.2.4 直接配准和基于特征的
?配准 345
9.3 合成 346
9.3.1 合成表面的选择 346
9.3.2 像素选择和加权
(去虚影) 348
9.3.3 应用：照片蒙太奇 352
9.3.4 融合 353
9.4 补充阅读 355
9.5 习题 356
第10章 计算摄影学 359
10.1 光度学标定 361
10.1.1 辐射度响应函数 362
10.1.2 噪声水平估计 363
10.1.3 虚影 364
10.1.4 光学模糊(空间响应)
估计 365
10.2 高动态范围成像 368
10.2.1 色调映射 374
10.2.2 应用：闪影术 380
10.3 超分辨率和模糊去除 381
10.3.1 彩色图像去马赛克 385
10.3.2 应用：彩色化 387
10.4 图像抠图和合成 388
10.4.1 蓝屏抠图 389
10.4.2 自然图像抠图 391
10.4.3 基于优化的抠图 394
10.4.4 烟、阴影和闪抠图 396
10.4.5 视频抠图 397
10.5 纹理分析与合成 398
10.5.1 应用：空洞填充
与修图 400
10.5.2 应用：非真实感绘制 401
10.6 补充阅读 403
10.7 习题 404
第11章 立体视觉对应 409
11.1 极线几何学 412
11.1.1 矫正 412
11.1.2 平面扫描 414
11.2 稀疏对应 416
11.3 稠密对应 418
11.4 局部方法 420
11.4.1 亚像素估计
与不确定性 422
11.4.2 应用：基于立体视觉的
头部跟踪 423
11.5 全局优化 424
11.5.1 动态规划 425
11.5.2 基于分割的方法 427
11.5.3 应用：z-键控与背景
替换 428
11.6 多视图立体视觉 429
11.6.1 体积与3D表面重建 432
11.6.2 由轮廓到形状 436
11.7 补充阅读 438
11.8 习题 439
第12章 3D重建 443
12.1 由X到形状 444
12.1.1 由阴影到形状与光度
测量立体视觉 445
12.1.2 由纹理到形状 447
12.1.3 由聚焦到形状 448
12.2 主动距离获取 449
12.2.1 距离数据归并 451
12.2.2 应用：数字遗产 453
12.3 表面表达 454
12.3.1 表面插值 454
12.3.2 表面简化 455
12.3.3 几何图像 456
12.4 基于点的表达 456
12.5 体积表达 457
12.6 基于模型的重建 459
12.6.1 建筑结构 459
12.6.2 头部和人脸 461
12.6.3 应用：脸部动画 463
12.6.4 完整人体建模与跟踪 465
12.7 恢复纹理映射与反照率 469
12.7.1 估计BRDF 470
12.7.2 应用：3D摄影学 471
12.8 补充阅读 472
12.9 习题 473
第13章 基于图像的绘制 477
13.1 视图插值 478
13.1.1 视图相关的纹理映射 480
13.1.2 应用：照片游览 481
13.2 层次深度图像 482
13.3 光场与发光图 484
13.3.1 非结构化发光图 487
13.3.2 表面光场 488
13.3.3 应用：同心拼图 489
13.4 环境影像形板 490
13.4.1 更高维光场 491
13.4.2 从建模到绘制 492
13.5 基于视频的绘制 493
13.5.1 基于视频的动画 493
13.5.2 视频纹理 494
13.5.3 应用：图片动画 497
13.5.4 3D视频 497
13.5.5 应用：基于视频的
游览 499
13.6 补充阅读 501
13.7 习题 503
第14章 识别 507
14.1 物体检测 509
14.1.1 人脸检测 509
14.1.2 行人检测 515
14.2 人脸识别 518
14.2.1 特征脸 518
14.2.2 活动表观与3D形状
模型 525
14.2.3 应用：个人照片收藏 528
14.3 实例识别 529
14.3.1 几何配准 530
14.3.2 大型数据库 531
14.3.3 应用：位置识别 535
14.4 类别识别 537
14.4.1 词袋 539
14.4.2 基于部件的模型 542
14.4.3 基于分割的识别 545
14.4.4 应用：智能照片编辑 548
14.5 上下文与场景理解 550
14.5.1 学习与大型图像收集 552
14.5.2 应用：图像搜索 554
14.6 识别数据库和测试集 555
14.7 补充阅读 559
14.8 习题 562
第15章 结语 567
附录A 线性代数与数值方法 569
A.1 矩阵分解 570
A.1.1 奇异值分解 570
A.1.2 特征值分解 571
A.1.3 QR因子分解 573
A.1.4 乔里斯基分解 574
A.2 线性最小二乘 575
A.3 非线性最小二乘 578
A.4 直接稀疏矩阵方法 579
A.5 迭代方法 580
A.5.1 共轭梯度 581
A.5.2 预处理 582
A.5.3 多重网格 583
附录B 贝叶斯建模与推断 585
B.1 估计理论 586
B.2 最大似然估计与最小二乘 589
B.3 鲁棒统计学 590
B.4 先验模型与贝叶斯推断 591
B.5 马尔科夫随机场 592
B.5.1 梯度下降与模拟退火 594
B.5.2 动态规划 595
B.5.3 置信传播 596
B.5.4 图割 598
B.5.5 线性规划 601
B.6 不确定性估计(误差分析) 602
附录C 补充材料 604
C.1 数据集 605
C.2 软件 607
C.3 幻灯片与讲座 615
C.4 参考文献 615
词汇表 617
・ ・ ・ ・ ・ ・ (收起)《python计算机视觉编程》
推荐序 xi
前言 xiii
第1章　基本的图像操作和处理 1
1.1　pil：python图像处理类库 1
1.1.1　转换图像格式 2
1.1.2　创建缩略图 3
1.1.3　复制和粘贴图像区域 3
1.1.4　调整尺寸和旋转 3
1.2　matplotlib 4
1.2.1　绘制图像、点和线 4
1.2.2　图像轮廓和直方图 6
1.2.3　交互式标注 7
1.3　numpy 8
1.3.1　图像数组表示 8
1.3.2　灰度变换 9
1.3.3　图像缩放 11
1.3.4　直方图均衡化 11
1.3.5　图像平均 13
1.3.6　图像的主成分分析（pca） 14
1.3.7　使用pickle模块 16
1.4　scipy 17
1.4.1　图像模糊 18
1.4.2　图像导数 19
1.4.3　形态学：对象计数 22
1.4.4　一些有用的scipy模块 23
1.5　高级示例：图像去噪 24
练习 28
代码示例约定 29
第2章　局部图像描述子 31
2.1　harris角点检测器 31
2.2　sift（尺度不变特征变换） 39
2.2.1　兴趣点 39
2.2.2　描述子 39
2.2.3　检测兴趣点 40
2.2.4　匹配描述子 43
2.3　匹配地理标记图像 47
2.3.1　从panoramio下载地理标记图像 47
2.3.2　使用局部描述子匹配 50
2.3.3　可视化连接的图像 52
练习 54
第3章　图像到图像的映射 57
3.1　单应性变换 57
3.1.1　直接线性变换算法 59
3.1.2　仿射变换 60
3.2　图像扭曲 61
3.2.1　图像中的图像 63
3.2.2　分段仿射扭曲 67
3.2.3　图像配准 70
3.3　创建全景图 76
3.3.1　ransac 77
3.3.2　稳健的单应性矩阵估计 78
3.3.3　拼接图像 81
练习 84
第4章　照相机模型与增强现实 85
4.1　针孔照相机模型 85
4.1.1　照相机矩阵 86
4.1.2　三维点的投影 87
4.1.3　照相机矩阵的分解 89
4.1.4　计算照相机中心 90
4.2　照相机标定 91
4.3　以平面和标记物进行姿态估计 93
4.4　增强现实 97
4.4.1　pygame和pyopengl 97
4.4.2　从照相机矩阵到opengl格式 98
4.4.3　在图像中放置虚拟物体 100
4.4.4　综合集成 102
4.4.5　载入模型 104
练习 106
第5章　多视图几何 107
5.1　外极几何 107
5.1.1　一个简单的数据集 109
5.1.2　用matplotlib绘制三维数据 111
5.1.3　计算f：八点法 112
5.1.4　外极点和外极线 113
5.2　照相机和三维结构的计算 116
5.2.1　三角剖分 116
5.2.2　由三维点计算照相机矩阵 118
5.2.3　由基础矩阵计算照相机矩阵 120
5.3　多视图重建 122
5.3.1　稳健估计基础矩阵 123
5.3.2　三维重建示例 125
5.3.3　多视图的扩展示例 129
5.4　立体图像 130
练习 135
第6章　图像聚类 137
6.1　k-means聚类 137
6.1.1　scipy聚类包 138
6.1.2　图像聚类 139
6.1.3　在主成分上可视化图像 140
6.1.4　像素聚类 142
6.2　层次聚类 144
6.3　谱聚类 152
练习 157
第7章　图像搜索 159
7.1　基于内容的图像检索 159
7.2　视觉单词 160
7.3　图像索引 164
7.3.1　建立数据库 164
7.3.2　添加图像 165
7.4　在数据库中搜索图像 167
7.4.1　利用索引获取候选图像 168
7.4.2　用一幅图像进行查询 169
7.4.3　确定对比基准并绘制结果 171
7.5　使用几何特性对结果排序 172
7.6　建立演示程序及web应用 176
7.6.1　用cherrypy创建web应用 176
7.6.2　图像搜索演示程序 176
练习 179
第8章　图像内容分类 181
8.1　k邻近分类法（knn） 181
8.1.1　一个简单的二维示例 182
8.1.2　用稠密sift作为图像特征 185
8.1.3　图像分类：手势识别 187
8.2　贝叶斯分类器 190
8.3　支持向量机 195
8.3.1　使用libsvm 196
8.3.2　再论手势识别 198
8.4　光学字符识别 199
8.4.1　训练分类器 200
8.4.2　选取特征 200
8.4.3　多类支持向量机 201
8.4.4　提取单元格并识别字符 202
8.4.5　图像校正 205
练习 206
第9章　图像分割 209
9.1　图割（graph cut） 209
9.1.1　从图像创建图 211
9.1.2　用户交互式分割 216
9.2　利用聚类进行分割 218
9.3　变分法 224
练习 226
第10章　opencv 227
10.1　opencv的python接口 227
10.2　opencv基础知识 228
10.2.1　读取和写入图像 228
10.2.2　颜色空间 228
10.2.3　显示图像及结果 229
10.3　处理视频 232
10.3.1　视频输入 232
10.3.2　将视频读取到numpy数组中 234
10.4　跟踪 234
10.4.1　光流 235
10.4.2　lucas-kanade算法 237
10.5　更多示例 243
10.5.1　图像修复 243
10.5.2　利用分水岭变换进行分割 244
10.5.3　利用霍夫变换检测直线 245
练习 246
附录a　安装软件包 247
a.1　numpy和scipy 247
a.1.1　windows 247
a.1.2　mac os x 247
a.1.3　linux 248
a.2　matplotlib 248
a.3　pil 248
a.4　libsvm 249
a.5　opencv 249
a.5.1　windows 和 unix 249
a.5.2　mac os x 249
a.5.3　linux 250
a.6　vlfeat 250
a.7　pygame 250
a.8　pyopengl 250
a.9　pydot 251
a.10　python-graph 251
a.11　simplejson 252
a.12　pysqlite 252
a.13　cherrypy 252
附录b　图像集 253
b.1　flickr 253
b.2　panoramio 254
b.3　牛津大学视觉几何组 255
b.4　肯塔基大学识别基准图像 255
b.5　其他 256
b.5.1　prague texture segmentation datagenerator与基准 256
b.5.2　微软研究院grab cut数据集 256
b.5.3　caltech 101 256
b.5.4　静态手势数据库 256
b.5.5　middlebury stereo数据集 256
附录c　图片来源 257
c.1　来自flickr的图像 257
c.2　其他图像 258
c.3　插图 258
参考文献 259
索引 263
・ ・ ・ ・ ・ ・ (收起)第1章　图像编程入门　　1
1.1　简介　　1
1.2　安装OpenCV库　　1
1.2.1　准备工作　　1
1.2.2　如何实现　　2
1.2.3　实现原理　　4
1.2.4　扩展阅读　　5
1.2.5　参阅　　6
1.3　装载、显示和存储图像　　6
1.3.1　准备工作　　6
1.3.2　如何实现　　6
1.3.3　实现原理　　8
1.3.4　扩展阅读　　9
1.3.5　参阅　　11
1.4　深入了解cv::Mat　　11
1.4.1　如何实现　　11
1.4.2　实现原理　　13
1.4.3　扩展阅读　　16
1.4.4　参阅　　17
1.5　定义感兴趣区域　　17
1.5.1　准备工作　　17
1.5.2　如何实现　　17
1.5.3　实现原理　　18
1.5.4　扩展阅读　　18
1.5.5　参阅　　19
第2章　操作像素　　20
2.1　简介　　20
2.2　访问像素值　　21
2.2.1　准备工作　　21
2.2.2　如何实现　　21
2.2.3　实现原理　　23
2.2.4　扩展阅读　　24
2.2.5　参阅　　24
2.3　用指针扫描图像　　24
2.3.1　准备工作　　25
2.3.2　如何实现　　25
2.3.3　实现原理　　26
2.3.4　扩展阅读　　27
2.3.5　参阅　　31
2.4　用迭代器扫描图像　　31
2.4.1　准备工作　　31
2.4.2　如何实现　　31
2.4.3　实现原理　　32
2.4.4　扩展阅读　　33
2.4.5　参阅　　33
2.5　编写高效的图像扫描循环　　33
2.5.1　如何实现　　34
2.5.2　实现原理　　34
2.5.3　扩展阅读　　36
2.5.4　参阅　　36
2.6　扫描图像并访问相邻像素　　36
2.6.1　准备工作　　36
2.6.2　如何实现　　36
2.6.3　实现原理　　38
2.6.4　扩展阅读　　38
2.6.5　参阅　　39
2.7　实现简单的图像运算　　39
2.7.1　准备工作　　39
2.7.2　如何实现　　40
2.7.3　实现原理　　40
2.7.4　扩展阅读　　41
2.8　图像重映射　　42
2.8.1　如何实现　　42
2.8.2　实现原理　　43
2.8.3　参阅　　44
第3章　处理图像的颜色　　45
3.1　简介　　45
3.2　用策略设计模式比较颜色　　45
3.2.1　如何实现　　46
3.2.2　实现原理　　47
3.2.3　扩展阅读　　50
3.2.4　参阅　　53
3.3　用GrabCut算法分割图像　　53
3.3.1　如何实现　　54
3.3.2　实现原理　　56
3.3.3　参阅　　56
3.4　转换颜色表示法　　56
3.4.1　如何实现　　57
3.4.2　实现原理　　58
3.4.3　参阅　　59
3.5　用色调、饱和度和亮度表示颜色　　59
3.5.1　如何实现　　59
3.5.2　实现原理　　61
3.5.3　拓展阅读　　64
3.5.4　参阅　　66
第4章　用直方图统计像素　　67
4.1　简介　　67
4.2　计算图像直方图　　67
4.2.1　准备工作　　68
4.2.2　如何实现　　68
4.2.3　实现原理　　72
4.2.4　扩展阅读　　72
4.2.5　参阅　　74
4.3　利用查找表修改图像外观　　74
4.3.1　如何实现　　74
4.3.2　实现原理　　75
4.3.3　扩展阅读　　76
4.3.4　参阅　　78
4.4　直方图均衡化　　78
4.4.1　如何实现　　78
4.4.2　实现原理　　79
4.5　反向投影直方图检测特定图像内容　　79
4.5.1　如何实现　　80
4.5.2　实现原理　　81
4.5.3　扩展阅读　　82
4.5.4　参阅　　84
4.6　用均值平移算法查找目标　　85
4.6.1　如何实现　　85
4.6.2　实现原理　　87
4.6.3　参阅　　88
4.7　比较直方图搜索相似图像　　88
4.7.1　如何实现　　88
4.7.2　实现原理　　90
4.7.3　参阅　　90
4.8　用积分图像统计像素　　91
4.8.1　如何实现　　91
4.8.2　实现原理　　92
4.8.3　扩展阅读　　93
4.8.4　参阅　　99
第5章　用形态学运算变换图像　　100
5.1　简介　　100
5.2　用形态学滤波器腐蚀和膨胀图像　　100
5.2.1　准备工作　　101
5.2.2　如何实现　　101
5.2.3　实现原理　　102
5.2.4　扩展阅读　　103
5.2.5　参阅　　104
5.3　用形态学滤波器开启和闭合图像　　104
5.3.1　如何实现　　104
5.3.2　实现原理　　105
5.3.3　参阅　　106
5.4　在灰度图像中应用形态学运算　　106
5.4.1　如何实现　　106
5.4.2　实现原理　　107
5.4.3　参阅　　108
5.5　用分水岭算法实现图像分割　　108
5.5.1　如何实现　　109
5.5.2　实现原理　　111
5.5.3　扩展阅读　　112
5.5.4　参阅　　114
5.6　用MSER算法提取特征区域　　114
5.6.1　如何实现　　114
5.6.2　实现原理　　116
5.6.3　参阅　　118
第6章　图像滤波　　119
6.1　简介　　119
6.2　低通滤波器　　120
6.2.1　如何实现　　120
6.2.2　实现原理　　121
6.2.3　参阅　　123
6.3　用滤波器进行缩减像素采样　　124
6.3.1　如何实现　　124
6.3.2　实现原理　　125
6.3.3　扩展阅读　　126
6.3.4　参阅　　127
6.4　中值滤波器　　128
6.4.1　如何实现　　128
6.4.2　实现原理　　129
6.5　用定向滤波器检测边缘　　129
6.5.1　如何实现　　130
6.5.2　实现原理　　132
6.5.3　扩展阅读　　135
6.5.4　参阅　　136
6.6　计算拉普拉斯算子　　136
6.6.1　如何实现　　137
6.6.2　实现原理　　138
6.6.3　扩展阅读　　141
6.6.4　参阅　　142
第7章　提取直线、轮廓和区域　　143
7.1　简介　　143
7.2　用Canny算子检测图像轮廓　　143
7.2.1　如何实现　　143
7.2.2　实现原理　　145
7.2.3　参阅　　146
7.3　用霍夫变换检测直线　　146
7.3.1　准备工作　　146
7.3.2　如何实现　　147
7.3.3　实现原理　　151
7.3.4　扩展阅读　　153
7.3.5　参阅　　155
7.4　点集的直线拟合　　155
7.4.1　如何实现　　155
7.4.2　实现原理　　157
7.4.3　扩展阅读　　158
7.5　提取连续区域　　158
7.5.1　如何实现　　159
7.5.2　实现原理　　160
7.5.3　扩展阅读　　161
7.6　计算区域的形状描述子　　161
7.6.1　如何实现　　162
7.6.2　实现原理　　163
7.6.3　扩展阅读　　164
第8章　检测兴趣点　　166
8.1　简介　　166
8.2　检测图像中的角点　　166
8.2.1　如何实现　　167
8.2.2　实现原理　　171
8.2.3　扩展阅读　　172
8.2.4　参阅　　174
8.3　快速检测特征　　174
8.3.1　如何实现　　174
8.3.2　实现原理　　175
8.3.3　扩展阅读　　176
8.3.4　参阅　　178
8.4　尺度不变特征的检测　　178
8.4.1　如何实现　　179
8.4.2　实现原理　　180
8.4.3　扩展阅读　　181
8.4.4　参阅　　183
8.5　多尺度FAST特征的检测　　183
8.5.1　如何实现　　183
8.5.2　实现原理　　184
8.5.3　扩展阅读　　185
8.5.4　参阅　　186
第9章　描述和匹配兴趣点　　187
9.1　简介　　187
9.2　局部模板匹配　　187
9.2.1　如何实现　　188
9.2.2　实现原理　　190
9.2.3　扩展阅读　　191
9.2.4　参阅　　192
9.3　描述并匹配局部强度值模式　　192
9.3.1　如何实现　　193
9.3.2　实现原理　　195
9.3.3　扩展阅读　　196
9.3.4　参阅　　199
9.4　用二值描述子匹配关键点　　199
9.4.1　如何实现　　199
9.4.2　实现原理　　200
9.4.3　扩展阅读　　201
9.4.4　参阅　　202
第10章　估算图像之间的投影关系　　203
10.1　简介　　203
10.2　计算图像对的基础矩阵　　205
10.2.1　准备工作　　205
10.2.2　如何实现　　206
10.2.3　实现原理　　208
10.2.4　参阅　　209
10.3　用RANSAC（随机抽样一致性）算法匹配图像　　209
10.3.1　如何实现　　209
10.3.2　实现原理　　212
10.3.3　扩展阅读　　213
10.4　计算两幅图像之间的单应矩阵　　214
10.4.1　准备工作　　214
10.4.2　如何实现　　215
10.4.3　实现原理　　217
10.4.4　扩展阅读　　218
10.4.5　参阅　　219
10.5　检测图像中的平面目标　　219
10.5.1　如何实现　　219
10.5.2　实现原理　　221
10.5.3　参阅　　224
第11章　三维重建　　225
11.1　简介　　225
11.2　相机标定　　226
11.2.1　如何实现　　227
11.2.2　实现原理　　230
11.2.3　扩展阅读　　232
11.2.4　参阅　　233
11.3　相机姿态还原　　233
11.3.1　如何实现　　233
11.3.2　实现原理　　235
11.3.3　扩展阅读　　236
11.3.4　参阅　　238
11.4　用标定相机实现三维重建　　238
11.4.1　如何实现　　238
11.4.2　实现原理　　241
11.4.3　扩展阅读　　243
11.4.4　参阅　　244
11.5　计算立体图像的深度　　244
11.5.1　准备工作　　244
11.5.2　如何实现　　245
11.5.3　实现原理　　247
11.5.4　参阅　　247
第12章　处理视频序列　　248
12.1　简介　　248
12.2　读取视频序列　　248
12.2.1　如何实现　　248
12.2.2　实现原理　　250
12.2.3　扩展阅读　　251
12.2.4　参阅　　251
12.3　处理视频帧　　251
12.3.1　如何实现　　251
12.3.2　实现原理　　252
12.3.3　扩展阅读　　256
12.3.4　参阅　　258
12.4　写入视频帧　　258
12.4.1　如何实现　　259
12.4.2　实现原理　　259
12.4.3　扩展阅读　　262
12.4.4　参阅　　263
12.5　提取视频中的前景物体　　263
12.5.1　如何实现　　264
12.5.2　实现原理　　266
12.5.3　扩展阅读　　266
12.5.4　参阅　　268
第13章　跟踪运动目标　　269
13.1　简介　　269
13.2　跟踪视频中的特征点　　269
13.2.1　如何实现　　269
13.2.2　实现原理　　274
13.2.3　参阅　　274
13.3　估算光流　　275
13.3.1　准备工作　　275
13.3.2　如何实现　　276
13.3.3　实现原理　　278
13.3.4　参阅　　279
13.4　跟踪视频中的物体　　279
13.4.1　如何实现　　279
13.4.2　实现原理　　282
13.4.3　参阅　　284
第14章　实用案例　　285
14.1　简介　　285
14.2　人脸识别　　286
14.2.1　如何实现　　286
14.2.2　实现原理　　288
14.2.3　参阅　　290
14.3　人脸定位　　291
14.3.1　准备工作　　291
14.3.2　如何实现　　292
14.3.3　实现原理　　295
14.3.4　扩展阅读　　297
14.3.5　参阅　　298
14.4　行人检测　　298
14.4.1　准备工作　　298
14.4.2　如何实现　　299
14.4.3　实现原理　　302
14.4.4　扩展阅读　　304
14.4.5　参阅　　308
・ ・ ・ ・ ・ ・ (收起)第一章 绪论
1・1 生物视觉通路简介
1・2 Marr的计算视觉理论框架
1・3 本书各章内容简介
1・4 计算机视觉的现状与阅读本书需注意的问题
思考题
参考文献
第二章 边缘检测
2・1 边缘检测与微分滤波器
2・2 边缘检测与正则化方法
2・3 多尺度滤波器与过零点定理
2・4 最优边缘检测滤波器
2・5 边缘检测快速算法
2・6 图像低层次处理的其他问题
思考题
参考文献
第三章 射影几何与几何元素表达
3・1 仿射变换与射影变换的几何表达
3・2 仿射坐标系与射影坐标系
3・3 仿射变换与射影变换的代数表达
3・4 不变量
3・5 由对应点求射影变换
3・6 点
3・7 指向和方向
3・8 平面直线及点线对偶关系
3・9 空间平面及点面对偶关系
3・10 空间直线
3・11 二次曲线与二次曲面
思考题
参考文献
第四章 摄像机定标
4・1 线性模型摄像机定标
4・2 非线性模型摄像机定标
4・3 立体视觉摄像机定标
4・4 机器人手眼定标
4・5 摄像机自定标技术
思考题
参考文献
第五章 立体视觉
5・1 立体视觉与三维重建
5・2 极线约束
5・3 对应基元匹配
5・4 射影几何意义下的三维重建
思考题
参考文献
第六章 运动与不确定性表达
6・1 欧氏平面上的刚体运动
6・2 欧氏空间中的刚体运动
6・3 不确定性的描述
6・4 不确定性的运算
6・5 不确定性运算的几个例子
6・6 三维直线段的不确定性
6・7 不确定性的显示
思考题
参考文献
第七章 基于光流场的运动分析
7・1 光流场和运动场
7・2 光流的约束方程
7・3 微分技术
7・4 其他方法
7・5 基于光流场的定性运动解释
思考题
参考文献
第八章 长序列运动图像特征跟踪
8・1 引论
8・2 参数估计理论初步
8・3 特征运动模型
8・4 特征跟踪的阐述
8・5 匹配
8・6 实际应用中需要考虑的问题
思考题
参考文献
第九章 基于二维特征对应的运动分析
9・1 极线方程和本质矩阵
9・2 基于点匹配的运动计算
9・3 图像是一个空间平面的投影时的运动计算
9・4 基于直线匹配的运动计算
9・5 基本矩阵的估计
思考题
参考文献
第十章 基于三维特征对应的运动分析
10・1 由三维点匹配估计运动
10・2 不需显式匹配的方法
10・3 从三维直线匹配估计运动
10・4 从平面匹配估计运动
10・5 二维-三维的物体定位
思考题
参考文献
第十一章 由图像灰度恢复三维物体形状
11・1 辐射度学与光度学
11・2 光照模型
11・3 由多幅图像恢复三维物体形状
11・4 由单幅图像恢复三维物体形状
思考题
参考文献
第十二章 建模与识别
12・1 CAD系统中的三维模型表达
12・2 曲线与曲面的表达
12・3 三维世界的多层次模型
12・4 由二维图像建模
12・5 识别的一般原则――问题与策略
12・6 特征关系图匹配
12・7 “假设检验”识别方法
思考题
参考文献
第十三章 距离图像获取与处理
13・1 距离传感器
13・2 数据预处理
13・3 深度图分割
思考题
参考文献
第十四章 计算机视觉系统体系结构讨论与展望
14・1 计算机视觉系统的基本体系结构
14・2 视觉系统体系结构讨论
14・3 主动视觉
14・4 计算机视觉的应用展望
参考文献
附录A 实验数据及参考结构
A・1 图像的格式
A・2 摄像机定标
A・3 立体视觉
A・4 基于光流场的运动分析
A・5 长序列运动图像特征跟踪
A・6 基于二维特征对应的运动分析
A・7 基于三维特征对应的运动分析
・ ・ ・ ・ ・ ・ (收起)译者序
前言
致老师
第一部分　导论
第1章　计算机视觉的定义及其历史2
1.1　简介2
1.2　定义2
1.3　局部全局问题3
1.4　生物视觉4
1.4.1　生物动因4
1.4.2　视觉感知6
参考文献7
第2章　编写图像处理程序8
2.1　简介8
2.2　图像处理的基本程序结构8
2.3　良好的编程风格9
2.4　计算机视觉的重点9
2.5　图像分析软件工具包10
2.6　makefile10
2.7　作业11
参考文献11
第3章　数学原理回顾12
3.1　简介12
3.2　线性代数简要回顾12
3.2.1　向量12
3.2.2　向量空间14
3.2.3　零空间15
3.2.4　函数空间16
3.2.5　线性变换17
3.2.6　导数和导数算子19
3.2.7　特征值和特征向量20
3.2.8　特征分解21
3.2.9　奇异值分解21
3.3　函数最小化简要回顾23
3.3.1　梯度下降23
3.3.2　局部最小值和全局最小值26
3.3.3　模拟退火27
3.4　概率论简要回顾28
3.5　作业30
参考文献31
第4章　图像：表示和创建32
4.1　简介32
4.2　图像表示32
4.2.1　标志性表示（图像）32
4.2.2　函数表示(方程)34
4.2.3　线性表示(向量)34
4.2.4　概率表示（随机场）35
4.2.5　图形表示（图）35
4.2.6　邻接悖论和六边形像素36
4.3　作为曲面的图像38
4.3.1　梯度38
4.3.2　等值线38
4.3.3　脊39
4.4　作业39
参考文献40
第二部分　预处理
第5章　卷积核算子42
5.1　简介42
5.2　线性算子42
5.3　图像的向量表示44
5.4　导数估计45
5.4.1　使用核估计导数46
5.4.2　通过函数拟合来估计导数46
5.4.3　图像基向量49
5.4.4　核作为采样可微分函数50
5.4.5　其他高阶导数53
5.4.6　尺度简介54
5.5　边缘检测55
5.6　尺度空间58
5.6.1　金字塔58
5.6.2　没有重采样的尺度空间59
5.7　示例61
5.8　数字梯度检测器的性能63
5.8.1　方向导数63
5.8.2　方向估计67
5.8.3　讨论70
5.9　总结71
5.10　作业71
参考文献76
第6章　去噪78
6.1　简介78
6.2　图像平滑78
6.2.1　一维情况79
6.2.2　二维情况79
6.3　使用双边滤波器实现保边平滑82
6.4　使用扩散方程实现保边平滑84
6.4.1　一维空间的扩散方程84
6.4.2　PDE模拟85
6.4.3　二维空间的扩散方程85
6.4.4　可变电导扩散86
6.5　使用优化实现保边平滑87
6.5.1　噪声消除的目标函数87
6.5.2　寻找一个先验项90
6.5.3　MAP算法实现和均场退火92
6.5.4　病态问题和正则化94
6.6　等效算法95
6.7　总结97
6.8　作业97
参考文献99
第7章　数学形态学101
7.1　简介101
7.2　二值形态学101
7.2.1　膨胀101
7.2.2　腐蚀106
7.2.3　膨胀和腐蚀的性质107
7.2.4　开运算和闭运算108
7.2.5　开运算和闭运算的性质109
7.3　灰度形态学109
7.3.1　使用平面结构元素的灰度图像110
7.3.2　使用灰度结构元素的灰度图像113
7.3.3　使用集合运算的灰度形态学114
7.4　距离变换114
7.4.1　使用迭代最近邻计算DT115
7.4.2　使用二值形态运算计算DT115
7.4.3　使用掩码计算DT115
7.4.4　使用维诺图计算DT117
7.5　边缘链接的应用117
7.6　总结120
7.7　作业121
参考文献122
第三部分　图像理解
第8章　分割124
8.1　简介124
8.2　阈值：仅基于亮度的分割125
8.2.1　阈值的局部性质125
8.2.2　通过直方图分析选择阈值126
8.2.3　用高斯和拟合直方图129
8.2.4　高斯混合模型与期望最大化130
8.3　聚类：基于颜色相似度的分割132
8.3.1　k-均值聚类133
8.3.2　均值移位聚类135
8.4　连接组件：使用区域增长的空间分割136
8.4.1　递归方法136
8.4.2　迭代方法138
8.4.3　示例应用139
8.5　使用主动轮廓进行分割140
8.5.1　snake：离散和连续140
8.5.2　水平集：包含边或者不包含边144
8.6　分水岭：基于亮度曲面的分割151
8.7　图割：基于图论的分割156
8.7.1　目标函数157
8.7.2　求解目标函数158
8.8　使用MFA进行分割159
8.9　评估分割的质量160
8.10　总结161
8.11　作业162
参考文献163
第9章　参数变换167
9.1　简介167
9.2　霍夫变换168
9.2.1　垂线问题169
9.2.2　如何找到交点――累加器数组169
9.2.3　使用梯度降低计算复杂度170
9.3　寻找圆171
9.3.1　由任意三个非共线像素表示的圆的位置推导171
9.3.2　当原点未知但半径已知时找圆172
9.3.3　利用梯度信息减少找圆的计算172
9.4　寻找椭圆172
9.5　广义霍夫变换174
9.6　寻找峰值175
9.7　寻找三维形状――高斯图176
9.8　寻找对应体――立体视觉中的参数一致性177
9.9　总结179
9.10　作业179
参考文
・ ・ ・ ・ ・ ・ (收起)第1章　图像编程入门　　1
1.1　简介　　1
1.2　安装OpenCV库　　1
1.2.1　准备工作　　1
1.2.2　安装　　2
1.2.3　实现原理　　3
1.2.4　扩展阅读　　4
1.2.5　参阅　　6
1.3　装载、显示和存储图像　　6
1.3.1　准备工作　　6
1.3.2　如何实现　　6
1.3.3　实现原理　　8
1.3.4　扩展阅读　　9
1.3.5　参阅　　12
1.4　深入了解cv::Mat　　12
1.4.1　如何实现　　12
1.4.2　实现原理　　14
1.4.3　扩展阅读　　16
1.4.4　参阅　　17
1.5　定义兴趣区域　　18
1.5.1　准备工作　　18
1.5.2　如何实现　　18
1.5.3　实现原理　　19
1.5.4　扩展阅读　　19
1.5.5　参阅　　20
第2章　操作像素　　21
2.1　简介　　21
2.2　访问像素值　　22
2.2.1　准备工作　　22
2.2.2　如何实现　　22
2.2.3　实现原理　　24
2.2.4　扩展阅读　　24
2.2.5　参阅　　25
2.3　用指针扫描图像　　25
2.3.1　准备工作　　25
2.3.2　如何实现　　26
2.3.3　实现原理　　27
2.3.4　扩展阅读　　28
2.3.5　参阅　　31
2.4　用迭代器扫描图像　　31
2.4.1　准备工作　　32
2.4.2　如何实现　　32
2.4.3　实现原理　　32
2.4.4　扩展阅读　　33
2.4.5　参阅　　34
2.5　编写高效的图像扫描循环　　34
2.5.1　如何实现　　34
2.5.2　实现原理　　34
2.5.3　扩展阅读　　36
2.5.4　参阅　　36
2.6　扫描图像并访问相邻像素　　36
2.6.1　准备工作　　36
2.6.2　如何实现　　37
2.6.3　实现原理　　38
2.6.4　扩展阅读　　39
2.6.5　参阅　　39
2.7　实现简单的图像运算　　40
2.7.1　准备工作　　40
2.7.2　如何实现　　40
2.7.3　实现原理　　41
2.7.4　扩展阅读　　41
2.8　图像重映射　　42
2.8.1　如何实现　　43
2.8.2　实现原理　　43
2.8.3　参阅　　44
第3章　用类处理彩色图像　　45
3.1　简介　　45
3.2　在算法设计中使用策略模式　　45
3.2.1　准备工作　　46
3.2.2　如何实现　　46
3.2.3　实现原理　　47
3.2.4　扩展阅读　　50
3.2.5　参阅　　52
3.3　用控制器设计模式实现功能模块间通信　　52
3.3.1　准备工作　　53
3.3.2　如何实现　　53
3.3.3　实现原理　　55
3.3.4　扩展阅读　　56
3.4　转换颜色表示法　　57
3.4.1　准备工作　　57
3.4.2　如何实现　　57
3.4.3　实现原理　　58
3.4.4　参阅　　59
3.5　用色调、饱和度、亮度表示颜色.59
3.5.1　如何实现　　60
3.5.2　实现原理　　61
3.5.3　扩展阅读　　63
第4章　用直方图统计像素　　66
4.1　简介　　66
4.2　计算图像直方图　　66
4.2.1　准备工作　　67
4.2.2　如何实现　　67
4.2.3　实现原理　　71
4.2.4　扩展阅读　　71
4.2.5　参阅　　73
4.3　利用查找表修改图像外观　　73
4.3.1　如何实现　　74
4.3.2　实现原理　　74
4.3.3　扩展阅读　　75
4.3.4　参阅　　77
4.4　直方图均衡化　　78
4.4.1　如何实现　　78
4.4.2　实现原理　　79
4.5　反向投影直方图检测特定图像内容　　 79
4.5.1　如何实现　　79
4.5.2　实现原理　　81
4.5.3　扩展阅读　　81
4.5.4　参阅　　84
4.6　均值平移算法查找目标　　84
4.6.1　如何实现　　85
4.6.2　实现原理　　87
4.6.3　参阅　　88
4.7　比较直方图搜索相似图像　　88
4.7.1　如何实现　　88
4.7.2　实现原理　　90
4.7.3　参阅　　90
4.8　用积分图像统计像素　　91
4.8.1　如何实现　　91
4.8.2　实现原理　　92
4.8.3　扩展阅读　　93
4.8.4　参阅　　99
第5章　用形态学运算变换图像　　100
5.1　简介　　100
5.2　形态学滤波器腐蚀和膨胀图像　　100
5.2.1　准备工作　　101
5.2.2　如何实现　　101
5.2.3　实现原理　　102
5.2.4　扩展阅读　　104
5.2.5　参阅　　104
5.3　用形态学滤波器开启和闭合图像　　 104
5.3.1　如何实现　　104
5.3.2　实现原理　　105
5.3.3　参阅　　106
5.4　用形态学滤波器检测边缘和角点　　 106
5.4.1　准备工作　　106
5.4.2　如何实现　　107
5.4.3　实现原理　　109
5.4.4　参阅　　110
5.5　用分水岭算法实现图像分割　　110
5.5.1　如何实现　　111
5.5.2　实现原理　　114
5.5.3　扩展阅读　　115
5.5.4　参阅　　116
5.6　用MSER算法提取特征区域　　116
5.6.1　如何实现　　117
5.6.2　实现原理　　118
5.6.3　参阅　　121
5.7　用GrabCut算法提取前景物体　　121
5.7.1　如何实现　　121
5.7.2　实现原理　　123
5.7.3　参阅　　124
第6章　图像滤波　　125
6.1　简介　　125
6.2　低通滤波器　　126
6.2.1　如何实现　　126
6.2.2　实现原理　　127
6.2.3　扩展阅读　　129
6.2.4　参阅　　132
6.3　中值滤波器　　133
6.3.1　如何实现　　133
6.3.2　实现原理　　134
6.4　用定向滤波器检测边缘　　134
6.4.1　如何实现　　135
6.4.2　实现原理　　137
6.4.3　扩展阅读　　139
6.4.4　参阅　　141
6.5　计算拉普拉斯算子　　141
6.5.1　如何实现　　141
6.5.2　实现原理　　143
6.5.3　扩展阅读　　145
6.5.4　参阅　　146
第7章　提取直线、轮廓和区域　　147
7.1　简介　　147
7.2　用Canny算子检测图像轮廓　　147
7.2.1　如何实现　　147
7.2.2　实现原理　　148
7.2.3　参阅　　150
7.3　用霍夫变换检测直线　　150
7.3.1　准备工作　　150
7.3.2　如何实现　　150
7.3.3　实现原理　　154
7.3.4　扩展阅读　　157
7.3.5　参阅　　158
7.4　点集的直线拟合　　158
7.4.1　如何实现　　159
7.4.2　实现原理　　161
7.4.3　扩展阅读　　161
7.5　提取区域的轮廓　　161
7.5.1　如何实现　　162
7.5.2　实现原理　　163
7.5.3　扩展阅读　　164
7.6　计算区域的形状描述子　　164
7.6.1　如何实现　　165
7.6.2　实现原理　　166
7.6.3　扩展阅读　　167
第8章　检测兴趣点　　169
8.1　简介　　169
8.2　检测图像中的角点　　169
8.2.1　如何实现　　170
8.2.2　实现原理　　174
8.2.3　扩展阅读　　176
8.2.4　参阅　　177
8.3　快速检测特征　　178
8.3.1　如何实现　　178
8.3.2　实现原理　　179
8.3.3　扩展阅读　　180
8.3.4　参阅　　182
8.4　尺度不变特征的检测　　182
8.4.1　如何实现　　183
8.4.2　实现原理　　184
8.4.3　扩展阅读　　185
8.4.4　参阅　　186
8.5　多尺度FAST 特征的检测　　187
8.5.1　如何实现　　187
8.5.2　实现原理　　188
8.5.3　扩展阅读　　188
8.5.4　参阅　　190
第9章　描述和匹配兴趣点　　191
9.1　简介　　191
9.2　局部模板匹配　　191
9.2.1　如何实现　　192
9.2.2　实现原理　　194
9.2.3　扩展阅读　　195
9.2.4　参阅　　196
9.3　描述局部强度值模式　　196
9.3.1　如何实现　　197
9.3.2　实现原理　　198
9.3.3　扩展阅读　　200
9.3.4　参阅　　203
9.4　用二值特征描述关键点　　203
9.4.1　如何实现　　203
9.4.2　实现原理　　204
9.4.3　扩展阅读　　205
9.4.4　参阅　　206
第10章　估算图像之间的投影关系　　207
10.1　简介　　207
10.2　相机校准　　209
10.2.1　如何实现　　209
10.2.2　实现原理　　213
10.2.3　扩展阅读　　216
10.2.4　参阅　　216
10.3　计算图像对的基础矩阵　　216
10.3.1　准备工作　　217
10.3.2　如何实现　　218
10.3.3　实现原理　　219
10.3.4　参阅　　220
10.4　用RANSAC（随机抽样一致性）算法匹配图像　　220
10.4.1　如何实现　　221
10.4.2　实现原理　　224
10.4.3　扩展阅读　　225
10.5　计算两幅图像之间的单应矩阵.226
10.5.1　准备工作　　226
10.5.2　如何实现　　227
10.5.3　实现原理　　229
10.5.4　扩展阅读　　230
10.5.5　参阅　　232
第11章　处理视频序列　　233
11.1　简介　　233
11.2　读取视频序列　　233
11.2.1　如何实现　　233
11.2.2　实现原理　　235
11.2.3　扩展阅读　　236
11.2.4　参阅　　236
11.3　处理视频帧　　236
11.3.1　如何实现　　236
11.3.2　实现原理　　237
11.3.3　扩展阅读　　241
11.3.4　参阅　　244
11.4　写入视频帧　　244
11.4.1　如何实现　　244
11.4.2　实现原理　　245
11.4.3　扩展阅读　　247
11.4.4　参阅　　249
11.5　跟踪视频中的特征点　　249
11.5.1　如何实现　　249
11.5.2　实现原理　　253
11.5.3　参阅　　254
11.6　提取视频中的前景物体　　254
11.6.1　如何实现　　255
11.6.2　实现原理　　257
11.6.3　扩展阅读　　257
11.6.4　参阅　　259
・ ・ ・ ・ ・ ・ (收起)第1章 浅谈人工智能、神经网络和计算机视觉 1
1.1 人工还是智能 1
1.2 人工智能的三起两落 2
1.2.1 两起两落 2
1.2.2 卷土重来 3
1.3 神经网络简史 5
1.3.1 生物神经网络和人工神经网络 5
1.3.2 M-P模型 6
1.3.3 感知机的诞生 9
1.3.4 你好，深度学习 10
1.4 计算机视觉 11
1.5 深度学习+ 12
1.5.1 图片分类 12
1.5.2 图像的目标识别和语义分割 13
1.5.3 自动驾驶 13
1.5.4 图像风格迁移 14
第2章 相关的数学知识 15
2.1 矩阵运算入门 15
2.1.1 标量、向量、矩阵和张量 15
2.1.2 矩阵的转置 17
2.1.3 矩阵的基本运算 18
2.2 导数求解 22
2.2.1 一阶导数的几何意义 23
2.2.2 初等函数的求导公式 24
2.2.3 初等函数的和、差、积、商求导 26
2.2.4 复合函数的链式法则 27
第3章 深度神经网络基础 29
3.1 监督学习和无监督学习 29
3.1.1 监督学习 30
3.1.2 无监督学习 32
3.1.3 小结 33
3.2 欠拟合和过拟合 34
3.2.1 欠拟合 34
3.2.2 过拟合 35
3.3 后向传播 36
3.4 损失和优化 38
3.4.1 损失函数 38
3.4.2 优化函数 39
3.5 激活函数 42
3.5.1 Sigmoid 44
3.5.2 tanh 45
3.5.3 ReLU 46
3.6 本地深度学习工作站 47
3.6.1 GPU和CPU 47
3.6.2 配置建议 49
第4章 卷积神经网络 51
4.1 卷积神经网络基础 51
4.1.1 卷积层 51
4.1.2 池化层 54
4.1.3 全连接层 56
4.2 LeNet模型 57
4.3 AlexNet模型 59
4.4 VGGNet模型 61
4.5 GoogleNet 65
4.6 ResNet 69
第5章 Python基础 72
5.1 Python简介 72
5.2 Jupyter Notebook 73
5.2.1 Anaconda的安装与使用 73
5.2.2 环境管理 76
5.2.3 环境包管理 77
5.2.4 Jupyter Notebook的安装 79
5.2.5 Jupyter Notebook的使用 80
5.2.6 Jupyter Notebook常用的快捷键 86
5.3 Python入门 88
5.3.1 Python的基本语法 88
5.3.2 Python变量 92
5.3.3 常用的数据类型 94
5.3.4 Python运算 99
5.3.5 Python条件判断语句 107
5.3.6 Python循环语句 109
5.3.7 Python中的函数 113
5.3.8 Python中的类 116
5.4 Python中的NumPy 119
5.4.1 NumPy的安装 119
5.4.2 多维数组 119
5.4.3 多维数组的基本操作 125
5.5 Python中的Matplotlib 133
5.5.1 Matplotlib的安装 133
5.5.2 创建图 133
第6章 PyTorch基础 142
6.1 PyTorch中的Tensor 142
6.1.1 Tensor的数据类型 143
6.1.2 Tensor的运算 146
6.1.3 搭建一个简易神经网络 153
6.2 自动梯度 156
6.2.1 torch.autograd和Variable 156
6.2.2 自定义传播函数 159
6.3 模型搭建和参数优化 162
6.3.1 PyTorch之torch.nn 162
6.3.2 PyTorch之torch.optim 167
6.4 实战手写数字识别 169
6.4.1 torch和torchvision 170
6.4.2 PyTorch之torch.transforms 171
6.4.3 数据预览和数据装载 173
6.4.4 模型搭建和参数优化 174
第7章 迁移学习 180
7.1 迁移学习入门 180
7.2 数据集处理 181
7.2.1 验证数据集和测试数据集 182
7.2.2 数据预览 182
7.3 模型搭建和参数优化 185
7.3.1 自定义VGGNet 185
7.3.2 迁移VGG16 196
7.3.3 迁移ResNet50 203
7.4 小结 219
第8章 图像风格迁移实战 220
8.1 风格迁移入门 220
8.2 PyTorch图像风格迁移实战 222
8.2.1 图像的内容损失 222
8.2.2 图像的风格损失 223
8.2.3 模型搭建和参数优化 224
8.2.4 训练新定义的卷积神经网络 226
8.3 小结 232
第9章 多模型融合 233
9.1 多模型融合入门 233
9.1.1 结果多数表决 234
9.1.2 结果直接平均 236
9.1.3 结果加权平均 237
9.2 PyTorch之多模型融合实战 239
9.3 小结 246
第10章 循环神经网络 247
10.1 循环神经网络入门 247
10.2 PyTorch之循环神经网络实战 249
10.3 小结 257
第11章 自动编码器 258
11.1 自动编码器入门 258
11.2 PyTorch之自动编码实战 259
11.2.1 通过线性变换实现自动编码器模型 260
11.2.2 通过卷积变换实现自动编码器模型 267
11.3 小结 273
・ ・ ・ ・ ・ ・ (收起)第1章 视觉系统实践――图像显示、输入/输出和库函数调用 1
1.1 OpenCV 1
1.2 基本的OpenCV代码 2
1.2.1 IplImage数据结构 3
1.2.2 读写图像 5
1.2.3 图像显示 6
1.2.4 示例 6
1.3 图像捕捉 9
1.4 和AIPCV库的接口 11
1.5 网站文件 15
1.6 参考文献 15
第2章 边缘检测技术 17
2.1 边缘检测的目的 17
2.2 传统的方法和理论 19
2.2.1 边缘的模型 20
2.2.2 噪声 21
2.2.3 导数算子 24
2.2.4 基于模板的边缘检测 29
2.3 边缘模型：Marr-Hildreth边缘检测器 31
2.4 Canny Edge边缘检测器 34
2.5 Shen-Castan(ISEF)边缘检测器 39
2.6 两种最优边缘检测器的比较 41
2.7 彩色边缘 44
2.8 Marr-Hildreth边缘检测器的源代码 46
2.9 Canny边缘检测器的源代码 50
2.10 Shen-Castan边缘检测器的源代码 58
2.11 网站文件 67
2.12 参考文献 69
第3章 数码形态学 73
3.1 形态学定义 73
3.2 连通性 73
3.3 数码形态学的基本元素――二值操作 75
3.3.1 二值膨胀 75
3.3.2 实现二值膨胀 79
3.3.3 二值腐蚀 82
3.3.4 二值腐蚀的实现 86
3.3.5 开启和闭合 88
3.3.6 MAX――用于形态学的高级程序设计语言 93
3.3.7 “命中/不命中”变换 97
3.3.8 识别区域边缘 99
3.3.9 条件膨胀 100
3.3.10 区域计数 102
3.4 灰阶形态学 103
3.4.1 开启操作和闭合操作 105
3.4.2 平滑操作 108
3.4.3 梯度 109
3.4.4 纹理的分割 110
3.4.5 对象的大小分布 111
3.5 彩色形态学 112
3.6 网站文件 113
3.7 参考文献 115
第4章 灰阶分割 117
4.1 灰阶分割的基础 117
4.1.1 使用边缘像素 119
4.1.2 迭代选择法 119
4.1.3 灰阶直方图法 120
4.1.4 使用熵 121
4.1.5 模糊集合 124
4.1.6 最小误差阈值法 126
4.1.7 单阈值选择的示例结果 127
4.2 使用区域阈值 129
4.2.1 Chow-Kaneko算法 130
4.2.2 通过边缘对光照进行
建模 133
4.2.3 实现和结果 135
4.2.4 对比 136
4.3 松弛法 137
4.4 移动平均法 142
4.5 基于聚类的阈值 145
4.6 多重阈值 146
4.7 网站文件 147
4.8 参考文献 148
第5章 纹理和色彩 151
5.1 纹理和分割 151
5.2 灰阶图像中纹理的简单分析 152
5.3 灰阶共生矩阵 155
5.3.1 最大概率 157
5.3.2 矩 157
5.3.3 对比度 157
5.3.4 同质性 157
5.3.5 熵 158
5.3.6 GLCM描述符的测试结果 158
5.3.7 纹理操作符的加速 159
5.4 边缘和纹理 161
5.5 能量和纹理 162
5.6 表面和纹理 164
5.6.1 向量散射算法 164
5.6.2 表面曲度算法 166
5.7 分形维度 168
5.8 彩色分割 171
5.9 彩色纹理 174
5.10 网站文件 174
5.11 参考文献 175
第6章 图像细化 179
6.1 骨架概述 179
6.2 中轴变换 180
6.3 迭代式形态学方法 181
6.4 等高线的使用 188
6.5 把对象看做多边形 192
6.6 基于力的图像细化 194
6.6.1 定义 195
6.6.2 力场的使用 195
6.6.3 子像素骨架 198
6.7 Zhang-Suen/Stentiford/Holt组合算法的源代码 200
6.8 网站文件 210
6.9 参考文献 211
第7章 图像还原 215
7.1 图像降质――真实世界 215
7.2 频域 217
7.2.1 傅里叶变换 217
7.2.2 快速傅里叶变换 219
7.2.3 逆傅里叶变换 222
7.2.4 二维傅里叶变换 223
7.2.5 OpenCV中的傅里叶变换 224
7.2.6 创建人工模糊 226
7.3 逆滤波器 231
7.4 Wiener滤波器 232
7.5 结构化噪声 233
7.6 运动模糊――一种特殊情况 236
7.7 同态滤波器――过滤照度 237
7.7.1 通用频率过滤器 238
7.7.2 分离光照产生的效果 240
7.8 网站文件 241
7.9 参考文献 242
第8章 分类 245
8.1 对象、模式和统计数据 245
8.1.1 特征和区域 247
8.1.2 训练和测试 251
8.1.3 类别内和类别外的差异 253
8.2 最小距离分类器 256
8.2.1 距离度量 257
8.2.2 特征之间的距离 259
8.3 交叉验证 260
8.4 支持向量机 262
8.5 多重分类器――整合分类器 264
8.5.1 合并多种方法 264
8.5.2 整合类型1的响应 265
8.5.3 评估 266
8.5.4 响应类型之间的转换 267
8.5.5 整合类型2的响应 267
8.5.6 整合类型3的响应 269
8.6 bagging和boosting 269
8.6.1 bagging 269
8.6.2 boosting 269
8.7 网站文件 271
8.8 参考文献 271
第9章 符号识别 273
9.1 问题描述 273
9.2 对简单的完美图像进行
OCR 274
9.3 在扫描的图像上进行OCR――图像分割 277
9.3.1 噪声 277
9.3.2 分离独立的字形 279
9.3.3 匹配模板 282
9.3.4 统计识别 284
9.4 传真图像的OCR――针对印刷字符 287
9.4.1 朝向――倾斜检测 287
9.4.2 使用边缘 291
9.5 手写字符 294
9.5.1 字符轮廓的属性 295
9.5.2 凸缺 297
9.5.3 向量模板 301
9.5.4 神经网络 305
9.6 使用多重分类器 312
9.6.1 合并多种方法 312
9.6.2 多重分类器的结果 314
9.7 印刷乐谱识别――案例研究 315
9.7.1 五线谱线 315
9.7.2 分割 317
9.7.3 音乐符号识别 319
9.8 神经网络识别系统的源代码 320
9.9 网站文件 327
9.10 参考文献 328
第10章 基于内容的搜索――通过示例搜索图像 333
10.1 搜索图像 333
10.2 维护图像集合 334
10.3 通过示例搜索的特征 336
10.3.1 彩色图像的特征 336
10.3.2 灰阶图像特征 343
10.4 考虑空间因素 345
10.4.1 整体区域 346
10.4.2 矩形区域 346
10.4.3 角度区域 346
10.4.4 环状区域 347
10.4.5 混合区域 348
10.4.6 空间采样的测试 348
10.5 其他要考虑的因素 350
10.5.1 纹理 351
10.5.2 对象、等高线和边缘 351
10.5.3 数据集 351
10.6 网站文件 352
10.7 参考文献 353
第11章 将高性能计算用于视觉处理和图像处理 357
11.1 多处理器计算的范式 358
11.1.1 共享内存 358
11.1.2 消息传递 359
11.2 执行时间 359
11.2.1 使用clock()函数 359
11.2.2 使用QueryPerformance-Counter函数 361
11.3 消息传递接口系统 363
11.3.1 安装MPI 363
11.3.2 使用MPI 364
11.3.3 进程间通信 364
11.3.4 运行MPI程序 366
11.3.5 真实的图像计算 367
11.3.6 使用计算机网络――集群计算 370
11.4 共享内存系统――使用PC的图形处理器 372
11.4.1 GLSL 373
11.4.2 OpenGL基础 373
11.4.3 OpenGL中的纹理实践 375
11.4.4 着色器编程基础 378
11.4.5 读入并转换图像 381
11.4.6 向着色程序传递参数 382
11.4.7 整合以上内容 384
11.4.8 通过GPU加速 385
11.4.9 开发和测试着色器代码 385
11.5 寻找所需的软件 386
11.6 网站文件 387
11.7 参考文献 387
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习基础 1
1.1 神经网络 1
1.1.1 感知机 1
1.1.2 神经网络原理 2
1.2 卷积神经网络 3
1.2.1 CNN基本操作 3
1.2.2 CNN原理 6
1.3 循环神经网络 7
1.3.1 RNN 7
1.3.2 LSTM与GRU 8
1.4 经典网络 9
1.4.1 AlexNet 9
1.4.2 VGG 10
1.4.3 GoogLeNet 11
1.4.4 ResNet 12
1.4.5 MobileNet 13
1.5 进阶必备：如何学习深度学习并“落地”求职 16
1.5.1 深度学习如何快速入门 16
1.5.2 深度学习行业求职技巧 17
第2章 计算机视觉基础 18
2.1 目标检测Two-stage算法 18
2.1.1 R-CNN算法 18
2.1.2 Fast R-CNN算法 20
2.1.3 Faster R-CNN算法 21
2.2 目标检测One-stage算法 23
2.2.1 YOLO系列算法 23
2.2.2 SSD算法 29
2.3 图像分割算法 31
2.3.1 FCN算法 31
2.3.2 U-Net算法 33
2.3.3 DeepLab系列算法 34
2.3.4 Mask R-CNN算法 37
2.4 进阶必备：计算机视觉方向知多少 38
第3章 基础图像处理 40
3.1 线性滤波 40
3.1.1 案例1：使用方框滤波 41
3.1.2 案例2：使用均值滤波 46
3.1.3 案例3：使用高斯滤波 48
3.2 非线性滤波 50
3.2.1 案例4：使用中值滤波例 50
3.2.2 案例5：使用双边滤波 52
3.3　OpenCV形态学运算 54
3.3.1 案例6：进行膨胀操作 55
3.3.2 案例7：进行腐蚀操作 57
3.3.3 案例8：使用形态学运算 58
3.4　案例9：使用漫水填充 63
3.5　图像金字塔 67
3.5.1 案例10：使用高斯金字塔 67
3.5.2 案例11：使用拉普拉斯金字塔 70
3.6　阈值化 73
3.6.1 案例12：使用基本阈值 74
3.6.2 案例13：使用自适应阈值 78
3.7　进阶必备：选择一款合适的图像处理工具 80
3.7.1 OpenCV 80
3.7.2 Matlab 81
第4章 图像变换 83
4.1　边缘检测 83
4.1.1 案例14：Sobel算法 83
4.1.2 案例15：Scharr算法 87
4.1.3 案例16：Laplacian算法 90
4.1.4 案例17：Canny算法 91
4.2　案例18：绘制轮廓 94
4.3　霍夫变换 97
4.3.1 案例19：霍夫线变换 97
4.3.2 案例20：霍夫圆变换 101
4.4　案例21：重映射 103
4.5　案例22：仿射变换 106
4.6　案例23：透视变换 109
4.7　直方图 111
4.7.1 案例24：直方图的计算与绘制 111
4.7.2 案例25：直方图均衡化 113
4.8　进阶必备：图像变换应用之文本图像矫正 114
4.8.1 图像变换知识总结 114
4.8.2 案例26：文本图像矫正 115
第5章 角点检测 117
5.1 案例27：Harris角点检测 117
5.2　案例28：Shi-Tomasi角点检测 119
5.3　案例29：亚像素级角点检测 122
5.4　进阶必备：角点检测知识总结 125
第6章 特征点检测与匹配 127
6.1　特征点检测 127
6.1.1 opencv-contrib环境安装 127
6.1.2 案例30：SIFT特征点检测 130
6.1.3 案例31：SURF特征点检测 137
6.2　特征匹配 139
6.2.1 案例32：BruteForce匹配 139
6.2.2 案例33：FLANN匹配 146
6.3　案例34：ORB特征提取 148
6.4　进阶必备：利用特征点拼接图像 151
6.4.1 特征点检测算法汇总 151
6.4.2 案例35：基于特征点检测与匹配的图像拼接 151
第7章 手写数字识别 155
7.1　Keras的应用 155
7.1.1 Keras模型 155
7.1.2 Keras层 156
7.1.3 模型编译 157
7.1.4 模型训练 158
7.2　LeNet算法 159
7.3　案例36：使用Keras实现手写数字识别 160
7.3.1 模型训练 160
7.3.2 手写数字识别模型推理 164
7.4　进阶必备：算法模型开发流程 167
7.4.1 数据准备 167
7.4.2 网络搭建 169
7.4.3 模型训练 170
第8章 CIFAR-10图像分类 171
8.1　图像分类数据集 171
8.1.1 CIFAR-10数据集和CIFAR-100数据集 171
8.1.2 ImageNet数据集 172
8.1.3 PASCAL VOC数据集 173
8.2　案例37：CIFAR-10图像分类 173
8.2.1 模型训练过程 174
8.2.2 模型推理 179
8.3　进阶必备：COCO数据集与使用HOGTSVM方法实现图像分类 180
8.3.1 COCO数据集 180
8.3.2 案例38：使用HOG+SVM方法实现图像分类 180
第9章 验证码识别 184
9.1　TensorFlow应用 184
9.1.1 案例39：TensorFlow的基本使用 184
9.1.2 TensorFlow的常用模块 186
9.2　案例40：验证码识别 188
9.2.1 生成验证码图片 188
9.2.2 基于TensorFlow的验证码识别 189
9.3　进阶必备：算法模型开发技巧 194
9.3.1 数据预处理技巧 194
9.3.2 网络搭建技巧 195
9.3.3 模型训练技巧 196
第10章 文本检测实战 197
10.1　文本检测算法 197
10.1.1 CTPN算法 198
10.1.2 EAST算法 200
10.2　案例41：基于EAST算法的文本检测 202
10.2.1 数据预处理 202
10.2.2 网络搭建 205
10.2.3 模型训练 212
10.2.4 文本检测验证 217
10.3　进阶必备：在不同场景下文本检测的应对方式 218
10.3.1 复杂场景文本检测 219
10.3.2 案例42：使用形态学运算实现简单场景文本检测 220
10.3.3 案例43：使用MSER+NMS实现简单场景文本检测 223
第11章 文本识别实战 226
11.1　文本识别算法 226
11.1.1 CRNN算法 226
11.1.2 Attention OCR算法 229
11.2　案例44：基于C-RNN算法的文本识别 231
11.2.1 数据预处理 231
11.2.2 网络搭建 232
11.2.3 模型训练 236
11.2.4 文本识别验证 237
11.3　进阶必备：单字OCR 238
11.3.1 OCR探究 238
11.3.2 案例45：文本图片字符切割 238
第12章 TensorFlow Lite 244
12.1　TensorFlow Lite介绍 244
12.1.1 TensorFlow Lite基础 245
12.1.2 TensorFlow Lite源码分析 246
12.2　模型转换 248
12.2.1 FlatBuffers文件格式 248
12.2.2 案例46：其他格式转换为.tflite模型 250
12.3　模型量化 252
12.3.1 案例47：量化感知训练 252
12.3.2 案例48：训练后量化 255
12.4　进阶必备：模型转换与模型部署优化答疑 257
12.4.1 模型转换问题 257
12.4.2 模型部署优化 258
第13章 基于TensorFlow Lite的AI功能部署实战 260
13.1　部署流程 260
13.2　案例49：移动端部署 261
13.2.1 搭建开发环境 262
13.2.2 编译运行项目 262
13.2.3 调用过程解析 264
13.3　PC端部署 266
13.3.1 案例50：Windows端部署 266
13.3.2 案例51：Linux端部署 278
13.3.3 案例52：ARM平台部署 282
13.3.4 案例53：MIPS平台部署 285
13.4　进阶必备：推理框架拓展与OpenCV编译部署 286
13.4.1 其他深度学习推理框架 286
13.4.2 OpenCV编译 286
・ ・ ・ ・ ・ ・ (收起)第 1章 图像的获取和表示 1
1.1 图像传感器技术 1
1.1.1 传感器材料 2
1.1.2 传感器光电二极管元件 3
1.1.3 传感器配置：马赛克、Faveon和BSI 3
1.1.4 动态范围和噪声 5
1.1.5 传感器处理 5
1.1.6 去马赛克 6
1.1.7 坏像素的校正 6
1.1.8 颜色和照明校正 6
1.1.9 几何校正 7
1.2 摄像机和计算成像 7
1.2.1 计算成像概述 7
1.2.2 单像素的摄像头计算 8
1.2.3 二维可计算摄像机 9
1.2.4 三维深度的摄像机系统 10
1.3 三维深度处理 21
1.3.1 方法概述 21
1.3.2 深度感知和处理中存在的问题 22
1.3.3 单目深度处理 27
1.4 三维表示：体元、深度图、网格和点云 31
1.5 总结 32
第 2章 图像预处理 33
2.1 图像处理概述 33
2.2 图像预处理要解决的问题 34
2.2.1 计算机视觉的流程和图像预处理 34
2.2.2 图像校正 36
2.2.3 图像增强 36
2.2.4 为特征提取准备图像 37
2.3 图像处理方法分类 41
2.3.1 点运算 42
2.3.2 直线运算 42
2.3.3 区域运算 42
2.3.4 算法 42
2.3.5 数据转换 43
2.4 色度学 43
2.4.1 色彩管理系统概述 44
2.4.2 光源、白点、黑点和中性轴 44
2.4.3 设备色彩模型 45
2.4.4 颜色空间与色彩感知 45
2.4.5 色域映射与渲染目的 46
2.4.6 色彩增强的实际考虑 47
2.4.7 色彩的准确度与精度 48
2.5 空间滤波 48
2.5.1 卷积滤波与检测 48
2.5.2 核滤波与形状选择 50
2.5.3 点滤波 51
2.5.4 噪声与伪像滤波 52
2.5.5 积分图与盒式滤波器 53
2.6 边缘检测器 54
2.6.1 核集合: Sobel, Scharr, Prewitt, Roberts, Kirsch, Robinson和Frei-Chen 54
2.6.2 Canny检测器 55
2.7 变换滤波、Fourier变换及其他 56
2.7.1 Fourier变换 56
2.7.2 其他变换 58
2.8 形态学与分割 59
2.8.1 二值形态学 59
2.8.2 灰度和彩色形态学 61
2.8.3 形态学优化和改进 61
2.8.4 欧氏距离映射 61
2.8.5 超像素分割 62
2.8.6 深度图分割 63
2.8.7 色彩分割 64
2.9 阈值化 64
2.9.1 全局阈值化 65
2.9.2 局部阈值化 68
2.10 总结 69
第3章 全局特征和区域特征 70
3.1 视觉特征的历史概述 70
3.1.1 核心思想：全局、区域和局部 71
3.1.2 纹理分析 73
3.1.3 统计方法 76
3.2 纹理区域度量 77
3.2.1 边缘度量 77
3.2.2 互相关和自相关 79
3.2.3 Fourier频谱、小波和基签名 79
3.2.4 共生矩阵和Haralick特征 80
3.2.5 Laws纹理度量 89
3.2.6 LBP局部二值模式 90
3.2.7 动态纹理 91
3.3 统计区域度量 91
3.3.1 图像矩特征 92
3.3.2 点度量特征 92
3.3.3 全局直方图 94
3.3.4 局部区域直方图 94
3.3.5 散点图和3D直方图 95
3.3.6 多分辨率和多尺度直方图 97
3.3.7 径向直方图 98
3.3.8 轮廓或边缘直方图 99
3.4 基空间度量 99
3.4.1 Fourier描述 101
3.4.2 Walsh-Hadamard变换 102
3.4.3 HAAR变换 103
3.4.4 斜变换 103
3.4.5 Zernike多项式 103
3.4.6 导向滤波器 104
3.4.7 Karhunen-Loeve变换与Hotelling变换 104
3.4.8 小波变换和Gabor滤波器 105
3.4.9 Hough变换与Radon变换 106
3.5 总结 108
第4章 局部特征设计、分类和学习 109
4.1 局部特征 109
4.1.1 检测器、兴趣点、关键点、锚点、标注 110
4.1.2 描述子、特征描述、特征提取 110
4.1.3 稀疏局部模式方法 111
4.2 局部特征属性 111
4.2.1 选择特征描述子和兴趣点 111
4.2.2 特征描述子和特征匹配 112
4.2.3 好特征的标准 112
4.2.4 可重复性，相对于困难的查找算容易 113
4.2.5 判别性与非判别性 114
4.2.6 相对和绝 对位置 114
4.2.7 匹配代价和一致性 114
4.3 距离函数 115
4.3.1 关于距离函数的早期研究成果 115
4.3.2 欧氏或笛卡儿距离度量 116
4.3.3 网格距离度量 118
4.3.4 基于统计学的差异性度量 119
4.3.5 二值或布尔距离度量 120
4.4 描述子的表示 121
4.4.1 坐标空间和复数空间 121
4.4.2 笛卡儿坐标 121
4.4.3 极坐标和对数极坐标 121
4.4.4 径向坐标 122
4.4.5 球面坐标 122
4.4.6 Gauge坐标 122
4.4.7 多元空间和多模数据 122
4.4.8 特征金字塔 123
4.5 描述子的密度 123
4.5.1 丢弃兴趣点和描述子 124
4.5.2 稠密与稀疏特征描述 124
4.6 描述子形状拓扑 125
4.6.1 关联性模板 125
4.6.2 块和形状 125
4.6.3 对象多边形 127
4.7 局部二值描述与点对模式 128
4.7.1 FREAK视网膜模式 129
4.7.2 Brisk 模式 130
4.7.3 ORB和BRIEF模式 131
4.8 描述子判别性 131
4.8.1 谱的判别性 132
4.8.2 区域、形状和模式的判别性 133
4.8.3 几何判别因素 133
4.8.4 通过特征可视化来评价判别性 134
4.8.5 精度与可跟踪 136
4.8.6 精度优化、子区域重叠、Gaussian权重和池化 138
4.8.7 亚像素精度 138
4.9 搜索策略与优化 139
4.9.1 密集搜索 139
4.9.2 网格搜索 139
4.9.3 多尺度金字塔搜索 140
4.9.4 尺度空间和图像金字塔 140
4.9.5 特征金字塔 142
4.9.6 稀疏预测搜索与跟踪 142
4.9.7 跟踪区域限制搜寻 143
4.9.8 分割限制搜索 143
4.9.9 深度或Z限制搜索 143
4.10 计算机视觉、模型和结构 144
4.10.1 特征空间 144
4.10.2 对象模型 145
4.10.3 约束 146
4.10.4 选择检测器和特征 146
4.10.5 训练概述 147
4.10.6 特征和对象的分类 148
4.10.7 特征学习、稀疏编码和卷积网络 154
4.11 总结 158
第5章 特征描述属性的分类学 159
5.1 特征描述子系列 160
5.2 计算机视觉分类学方面的早期研究成果 161
5.3 鲁棒性和精度 161
5.4 通用的鲁棒性分类学 162
5.4.1 光照 163
5.4.2 颜色准则 163
5.4.3 不完全性 164
5.4.4 分辨率和精度 164
5.4.5 几何失真 165
5.4.6 效率变量、费用和效益 165
5.4.7 判别性和唯 一性 165
5.5 通用的视觉度量分类学 166
5.5.1 特征描述子族 168
5.5.2 频谱维度 168
5.5.3 频谱类型 168
5.5.4 兴趣点 171
5.5.5 存储格式 171
5.5.6 数据类型 172
5.5.7 描述子内存 172
5.5.8 特征形状 173
5.5.9 特征模式 173
5.5.10 特征密度 174
5.5.11 特征搜索方法 174
5.5.12 模式对采样 175
5.5.13 模式区域大小 176
5.5.14 距离函数 176
5.6 特征度量评估 177
5.6.1 效率变量、成本和效益 177
5.6.2 图像重建的效率度量 178
5.6.3 特征度量评估举例 178
5.7 总结 180
第6章 兴趣点检测与特征描述子研究 181
6.1 兴趣点调整 181
6.2 兴趣点概念 182
6.3 兴趣点方法概述 184
6.3.1 Laplacian 和Gaussian -Laplacian 185
6.3.2 Moravac角点检测器 185
6.3.3 Harris方法、Harris-Stephens、Shi-Tomasi以及Hessian类型的检测器 186
6.3.4 Hessian矩阵检测器和Hessian-Laplace 186
6.3.5 Gaussian差 187
6.3.6 显著性区域 187
6.3.7 SUSAN、Trajkovic 以及 Hedly 187
6.3.8 Fast、Faster以及 AGHAST 188
6.3.9 局部曲率方法 189
6.3.10 形态兴趣区域 189
6.4 特征描述子介绍 190
6.4.1 局部二值描述子 190
6.4.2 Census 197
6.4.3 BRIEF 198
6.4.4 ORB 199
6.4.5 BRISK 200
6.4.6 FREAK 201
6.5 谱描述子 202
6.5.1 SIFT 202
6.5.2 SIFT-PCA 206
6.5.3 SIFT-GLOH 207
6.5.4 改进的SIF-SIFER 207
6.5.5 SIFT CS-LBP改造 208
6.5.6 RootSIFT改造 208
6.5.7 CenSurE和STAR 209
6.5.8 相关模板 210
6.5.9 HAAR特征 212
6.5.10 使用类HAAR特征的Viola Jones算法 213
6.5.11 SURF 214
6.5.12 其他SURF算法 215
6.5.13 梯度直方图及变种 216
6.5.14 PHOG和相关方法 217
6.5.15 Daisy和O-Daisy 218
6.5.16 CARD 219
6.5.17 具有鲁棒性的快速特征匹配 221
6.5.18 RIFF和CHOG 222
6.5.19 链码直方图 223
6.5.20 D-NETS 224
6.5.21 局部梯度模式 225
6.5.22 局部相位量化 225
6.6 基空间描述子 226
6.6.1 傅里叶描述子 227
6.6.2 用其他基函数来构建描述子 228
6.6.3 稀疏编码方法 228
6.7 多边形形状描述 229
6.7.1 MSER方法 229
6.7.2 针对斑点和多边形的物体形状度量 230
6.7.3 形状上下文 233
6.8 3D、4D、体积以及多模态描述子 234
6.8.1 3D HOG 235
6.8.2 HON 4D 235
6.8.3 3D SIFT 236
6.9 总结 237
第7章 基准数据、内容、度量和分析 238
7.1 什么是基准数据？ 238
7.2 先前关于标注数据方面的研究：艺术与科学 240
7.2.1 质量性能的一般度量 240
7.2.2 算法性能的衡量 241
7.2.3 Rosin关于角点方面的研究工作 242
7.3 构造基准数据的关键问题 243
7.3.1 内容：采用、修改或创建 243
7.3.2 可用的基准数据介绍 243
7.3.3 使用数据拟合算法 244
7.3.4 场景构成和标记 245
7.4 定义目标和预期 247
7.4.1 Mikolajczyk和Schmid的方法学 247
7.4.2 开放式评价系统 248
7.4.3 极 端情况和限制 248
7.4.4 兴趣点和特征 248
7.5 基准数据的鲁棒性准则 249
7.5.1 举例说明鲁棒性准则 249
7.5.2 将鲁棒性准则用于实际应用 250
7.6 度量与基准数据的配对 252
7.6.1 兴趣点、特征和基准数据的配对和优化 252
7.6.2 一般的视觉分类学的例子 253
7.7 合成的特征字母表 254
7.7.1 合成数据集的目标 254
7.7.2 合成兴趣点字母表 256
7.7.3 将合成字母表叠加到真实图像上 258
7.8 总结 260
第8章 可视流程及优化 261
8.1 阶段、操作和资源 261
8.2 计算资源预算 263
8.2.1 计算单元、ALU和加速器 265
8.2.2 能耗的使用 266
8.2.3 内存的利用 266
8.2.4 I/O性能 269
8.3 计算机视觉流程的实例 270
8.3.1 汽车识别 270
8.3.2 人脸检测、情感识别以及年龄识别 277
8.3.3 图像分类 285
8.3.4 增强现实 289
8.4 可选的加速方案 294
8.4.1 内存优化 294
8.4.2 粗粒度并行 296
8.4.3 细粒度数据并行 297
8.4.4 高 级指令集和加速器 300
8.5 计算机视觉算法的优化与调整 301
8.5.1 编译器优化与手工优化 301
8.5.2 特征描述子改造、检测器和距离函数 302
8.5.3 Boxlets与卷积加速 303
8.5.4 数据类型优化，整型与浮点型 303
8.6 优化资源 304
8.7 总结 304
附录A 合成特征分析 306
A.1 目标的背景与期望 307
A.2 测试方法和结果 309
A.3 合成字母基准图像概述 311
A.4 测试1：合成兴趣点字母检测 313
A.5 测试2：合成角点字母检测 323
A.6 测试3：叠加到真实图像上的合成字母检测 333
A.7 测试4：字母的旋转不变性 333
A.8 结果分析和不可重复性异常 336
附录B 基准数据集概述 339
附录C 成像和计算机视觉资源 347
C.1 商业产品 347
C.2 开放源码 348
C.3 组织、机构和标准 350
C.4 在线资源 351
附录D 扩展SDM准则 353
译后记 370
参考文献 372
・ ・ ・ ・ ・ ・ (收起)第 1章 机器学习与sklearn 1
1．1 sklearn 环境配置 2
1．1．1 环境要求 2
1．1．2 安装方法 2
1．1．3 修改pip 源 3
1．1．4 安装Jupyter Notebook 4
1．2 数据集 5
1．2．1 自带的小型数据集 6
1．2．2 在线下载的数据集 8
1．2．3 计算机生成的数据集 8
1．3 分类 9
1．3．1 加载数据与模型 10
1．3．2 建立分类模型 11
1．3．3 模型的训练及预测 12
1．3．4 模型评价 12
1．4 回归 14
1．4．1 线性回归 15
1．4．2 回归模型评价 16
1．5 聚类 17
1．5．1 K-means 17
1．5．2 DBSCAN 17
1．5．3 聚类实例 18
1．6 降维 19
1．6．1 PCA 降维 19
1．6．2 LDA 降维 22
1．7 模型验证 23
1．8 模型持久化 27
1．8．1 joblib 27
1．8．2 pickle 28
1．9 小结 28
第 2章 传统图像处理方法 29
2．1 图像分类 29
2．1．1 HOG 的原理 29
2．1．2 工具介绍 30
2．1．3 CIFAR-10 分类 31
2．1．4 手写字符分类 33
2．2 目标检测 36
2．3 图像分割 40
2．4 图像搜索 41
2．5 小结 43
第3章 深度学习与PyTorch 44
3．1 框架介绍 44
3．2 环境配置 46
3．3 运算基本单元 48
3．3．1 Tensor 数据类型 48
3．3．2 Tensor 与ndarray 49
3．3．3 CPU 与GPU 运算 49
3．3．4 PyTorch 实现K-means 51
3．4 自动求导 55
3．5 数据加载 57
3．5．1 Dataset 58
3．5．2 DataLoader 59
3．6 神经网络工具包 60
3．6．1 Module 模块 61
3．6．2 线性层 62
3．6．3 卷积层 62
3．6．4 池化层 64
3．6．5 BatchNorm 层 65
3．6．6 激活层 65
3．6．7 神经网络各层输出的可视化 72
3．6．8 循环神经网络 76
3．6．9 Sequential 和ModuleList 78
3．6．10 损失函数 79
3．7 模型优化器optim 82
3．7．1 optim 用法 82
3．7．2 优化器的选择 82
3．7．3 学习率的选择 86
3．8 参数初始化init 94
3．9 模型持久化 96
3．10 JIT 编译器 98
3．11 模型迁移ONNX 99
3．12 数据可视化TensorBoard 101
3．13 机器视觉工具包torchvision 103
3．13．1 数据 103
3．13．2 模型 104
3．13．3 图像处理 106
3．14 小结 110
第4章 卷积神经网络中的分类与回归 111
4．1 卷积神经网络中的分类问题 112
4．1．1 CIFAR-10 图像分类 112
4．1．2 卷积神经网络的发展 117
4．1．3 分类网络的实现 121
4．1．4 模型训练 127
4．1．5 模型展示 132
4．1．6 多标签分类 134
4．2 卷积神经网络中的回归问题 142
4．2．1 生成数据集 142
4．2．2 模型训练 145
4．2．3 模型展示 146
4．3 小结 148
第5章 目标检测 149
5．1 深度学习物体检测算法 149
5．1．1 两段式检测 150
5．1．2 一段式检测 153
5．2 数据集构建 155
5．2．1 选择目标物体图片 155
5．2．2 背景图片下载 156
5．2．3 图片合成 156
5．3 数据加载 162
5．4 数据标记与损失函数构建 166
5．4．1 数据标记 167
5．4．2 损失函数 167
5．5 模型搭建与训练 172
5．6 模型预测 175
5．7 小结 180
第6章 图像分割 181
6．1 数据加载 184
6．2 模型搭建 189
6．3 模型训练 191
6．4 模型展示 194
6．5 智能弹幕 195
6．6 像素级回归问题：超分辨率重建 196
6．6．1 超分辨率重建算法的发展 197
6．6．2 数据加载 198
6．6．3 模型搭建与训练 202
6．6．4 模型展示 205
6．7 小结 206
第7章 图像搜索 207
7．1 分类网络的特征 208
7．2 深度学习人脸识别技术 208
7．2．1 FaceNet 209
7．2．2 CosFace 和ArcFace 209
7．3 数据处理 210
7．3．1 数据下载 210
7．3．2 数据检查 212
7．3．3 数据提取 213
7．4 模型训练 214
7．4．1 普通分类模型 214
7．4．2 CosFace 218
7．5 图像搜索 219
7．5．1 图像比对 219
7．5．2 KD-Tree 搜索 221
7．6 小结 224
第8章 图像压缩 225
8．1 AutoEncoder 226
8．1．1 AutoEncoder 的原理 226
8．1．2 AutoEncoder 模型搭建 226
8．1．3 数据加载 229
8．1．4 模型训练 230
8．1．5 结果展示 232
8．2 GAN 234
8．2．1 GAN 原理 234
8．2．2 GAN 训练流程 235
8．2．3 GAN 随机生成人脸图片 235
8．2．4 GAN 与AutoEncoder 的结合 242
8．2．5 图像修复 247
8．3 小结 250
第9章 不定长文本识别 251
9．1 循环神经网络概述 251
9．2 时间序列预测 252
9．2．1 创建模型 253
9．2．2 生成数据 253
9．2．3 模型训练 255
9．2．4 模型预测 256
9．3 CRNN 模型 257
9．3．1 CRNN 算法简介 257
9．3．2 CTCLoss 函数 258
9．3．3 模型结构 259
9．3．4 数据预处理 261
9．3．5 模型训练 264
9．3．6 模型预测 266
9．4 小结 267
第 10章 神经网络压缩与部署 268
10．1 剪枝 268
10．1．1 模型设计 269
10．1．2 训练基础模型 271
10．1．3 模型稀疏化 273
10．1．4 压缩模型通道 276
10．2 量化 283
10．3 混合精度训练 287
10．4 深度学习模型的服务端部署 289
10．4．1 创建接口 289
10．4．2 访问接口 291
10．5 小结 292
・ ・ ・ ・ ・ ・ (收起)前言
作者简介
审校者简介
第1章　OpenCV入门1
1.1　了解人类视觉系统1
1.2　人类如何理解图像内容3
1.3　你能用OpenCV做什么4
1.3.1　内置数据结构和输入/输出4
1.3.2　图像处理操作5
1.3.3　GUI5
1.3.4　视频分析6
1.3.5　3D重建6
1.3.6　特征提取7
1.3.7　对象检测7
1.3.8　机器学习8
1.3.9　计算摄影8
1.3.10　形状分析9
1.3.11　光流算法9
1.3.12　人脸和对象识别9
1.3.13　表面匹配10
1.3.14　文本检测和识别10
1.3.15　深度学习10
1.4　安装OpenCV10
1.4.1　Windows11
1.4.2　Mac OS X11
1.4.3　Linux13
1.5　总结14
第2章　OpenCV基础知识导论15
2.1　技术要求15
2.2　基本CMake配置文件16
2.3　创建一个库16
2.4　管理依赖项17
2.5　让脚本更复杂18
2.6　图像和矩阵20
2.7　读/写图像22
2.8　读取视频和摄像头25
2.9　其他基本对象类型27
2.9.1　Vec对象类型27
2.9.2　Scalar对象类型28
2.9.3　Point对象类型28
2.9.4　Size对象类型29
2.9.5　Rect对象类型29
2.9.6　RotatedRect对象类型29
2.10　基本矩阵运算30
2.11　基本数据存储32
2.12　总结34
第3章　学习图形用户界面35
3.1　技术要求35
3.2　OpenCV用户界面介绍36
3.3　OpenCV的基本图形用户界面36
3.4　Qt图形用户界面44
3.5　OpenGL支持50
3.6　总结54
第4章　深入研究直方图和滤波器55
4.1　技术要求56
4.2　生成CMake脚本文件56
4.3　创建图形用户界面57
4.4　绘制直方图59
4.5　图像颜色均衡62
4.6　Lomography效果64
4.7　卡通效果68
4.8　总结72
第5章　自动光学检查、对象分割和检测73
5.1　技术要求73
5.2　隔离场景中的对象74
5.3　为AOI创建应用程序76
5.4　预处理输入图像78
5.4.1　噪声消除78
5.4.2　用光模式移除背景进行分割79
5.4.3　阈值84
5.5　分割输入图像85
5.5.1　连通组件算法85
5.5.2　findContours算法90
5.6　总结92
第6章　学习对象分类94
6.1　技术要求94
6.2　机器学习概念介绍95
6.3　计算机视觉和机器学习工作流程98
6.4　自动对象检查分类示例100
6.4.1　特征提取102
6.4.2　训练SVM模型105
6.4.3　输入图像预测109
6.5　总结111
第7章　检测面部部位与覆盖面具112
7.1　技术要求112
7.2　了解Haar级联112
7.3　什么是积分图像114
7.4　在实时视频中覆盖面具115
7.5　戴上太阳镜118
7.6　跟踪鼻子、嘴巴和耳朵121
7.7　总结122
第8章　视频监控、背景建模和形态学操作123
8.1　技术要求123
8.2　理解背景减除124
8.3　直接的背景减除124
8.4　帧差分128
8.5　高斯混合方法131
8.6　形态学图像处理133
8.7　使形状变细134
8.8　使形状变粗135
8.9　其他形态运算符136
8.9.1　形态开口136
8.9.2　形态闭合137
8.9.3　绘制边界138
8.9.4　礼帽变换139
8.9.5　黑帽变换140
8.10　总结140
第9章　学习对象跟踪141
9.1　技术要求141
9.2　跟踪特定颜色的对象141
9.3　构建交互式对象跟踪器143
9.4　用Harris角点检测器检测点148
9.5　用于跟踪的好特征151
9.6　基于特征的跟踪153
9.6.1　Lucas-Kanade方法153
9.6.2　Farneback算法157
9.7　总结161
第10章　开发用于文本识别的分割算法162
10.1　技术要求162
10.2　光学字符识别介绍162
10.3　预处理阶段164
10.3.1　对图像进行阈值处理164
10.3.2　文本分割165
10.4　在你的操作系统上安装Tesseract OCR172
10.4.1　在Windows上安装Tesseract172
10.4.2　在Mac上安装Tesseract173
10.5　使用Tesseract OCR库173
10.6　总结177
第11章　用Tesseract进行文本识别178
11.1　技术要求178
11.2　文本API的工作原理179
11.2.1　场景检测问题179
11.2.2　极值区域180
11.2.3　极值区域过滤181
11.3　使用文本API182
11.3.1　文本检测182
11.3.2　文本提取187
11.3.3　文本识别189
11.4　总结193
第12章　使用OpenCV进行深度学习194
12.1　技术要求194
12.2　深度学习简介195
12.2.1　什么是神经网络，我们如何从数据中学习195
12.2.2　卷积神经网络197
12.3　OpenCV中的深度学习198
12.4　YOLO用于实时对象检测199
12.4.1　YOLO v3深度学习模型架构200
12.4.2　YOLO数据集、词汇表和模型200
12.4.3　将YOLO导入OpenCV201
12.5　用SSD进行人脸检测204
12.5.1　SSD模型架构204
12.5.2　将SSD人脸检测导入OpenCV204
12.6　总结208
・ ・ ・ ・ ・ ・ (收起)译者序
前言
作者简介
审校者简介
译者简介
第1章　安装OpenCV 1
1.1　选择和使用合适的安装工具 2
1.1.1　在Windows上安装 2
1.1.2　在OS X系统中安装 6
1.1.3　在Ubuntu及其衍生版本中安装 11
1.1.4　在其他类Unix系统中安装 12
1.2　安装Contrib模块 13
1.3　运行示例 13
1.4　查找文档、帮助及更新 14
1.5　总结 15
第2章　处理文件、摄像头和图形用户界面 16
2.1　基本I/O脚本 16
2.1.1　读/写图像文件 16
2.1.2　图像与原始字节之间的转换 19
2.1.3　使用numpy.array访问图像数据 20
2.1.4　视频文件的读/写 22
2.1.5　捕获摄像头的帧 23
2.1.6　在窗口显示图像 24
2.1.7　在窗口显示摄像头帧 25
2.2　Cameo项目（人脸跟踪和图像处理） 26
2.3　Cameo―面向对象的设计 27
2.3.1　使用managers. CaptureManager提取视频流 27
2.3.2　使用managers.WindowManager抽象窗口和键盘 32
2.3.3　cameo.Cameo的强大实现 33
2.4　总结 34
第3章　使用OpenCV 3处理图像 36
3.1　不同色彩空间的转换 36
3.2　傅里叶变换 37
3.2.1　高通滤波器 37
3.2.2　低通滤波器 39
3.3　创建模块 39
3.4　边缘检测 40
3.5　用定制内核做卷积 41
3.6　修改应用 43
3.7　Canny边缘检测 44
3.8　轮廓检测 45
3.9　边界框、最小矩形区域和最小闭圆的轮廓 46
3.10　凸轮廓与Douglas-Peucker算法 48
3.11　直线和圆检测 50
3.11.1　直线检测 50
3.11.2　圆检测 51
3.12　检测其他形状 52
3.13　总结 52
第4章　深度估计与分割 53
4.1　创建模块 53
4.2　捕获深度摄像头的帧 54
4.3　从视差图得到掩模 56
4.4　对复制操作执行掩模 57
4.5　使用普通摄像头进行深度估计 59
4.6　使用分水岭和GrabCut算法进行物体分割 63
4.6.1　用GrabCut进行前景检测的例子 64
4.6.2　使用分水岭算法进行图像分割 66
4.7　总结 69
第5章　人脸检测和识别 70
5.1　Haar级联的概念 70
5.2　获取Haar级联数据 71
5.3　使用OpenCV进行人脸检测 72
5.3.1　静态图像中的人脸检测 72
5.3.2　视频中的人脸检测 74
5.3.3　人脸识别 76
5.4　总结 82
第6章　图像检索以及基于图像描述符的搜索 83
6.1　特征检测算法 83
6.1.1　特征定义 84
6.1.2　使用DoG和SIFT进行特征提取与描述 86
6.1.3　使用快速Hessian算法和SURF来提取和检测特征 89
6.1.4　基于ORB的特征检测和特征匹配 91
6.1.5　ORB特征匹配 93
6.1.6　K-最近邻匹配 95
6.1.7　FLANN匹配 96
6.1.8　FLANN的单应性匹配 99
6.1.9　基于文身取证的应用程序示例 102
6.2　总结 105
第7章　目标检测与识别 106
7.1　目标检测与识别技术 106
7.1.1　HOG描述符 107
7.1.2　检测人 112
7.1.3　创建和训练目标检测器 113
7.2　汽车检测 116
7.2.1　代码的功能 118
7.2.2　SVM和滑动窗口 122
7.3　总结 134
第8章　目标跟踪 135
8.1　检测移动的目标 135
8.2　背景分割器：KNN、MOG2和GMG 138
8.2.1　均值漂移和CAMShift 142
8.2.2　彩色直方图 144
8.2.3　返回代码 146
8.3　CAMShift 147
8.4　卡尔曼滤波器 149
8.4.1　预测和更新 149
8.4.2　范例 150
8.4.3　一个基于行人跟踪的例子 153
8.4.4　Pedestrian类 154
8.4.5　主程序 157
8.5　总结 159
第9章　基于OpenCV的神经网络简介 160
9.1　人工神经网络 160
9.2　人工神经网络的结构 161
9.2.1　网络层级示例 162
9.2.2　学习算法 163
9.3　OpenCV中的ANN 164
9.3.1　基于ANN的动物分类 166
9.3.2　训练周期 169
9.4　用人工神经网络进行手写数字识别 170
9.4.1　MNIST―手写数字数据库 170
9.4.2　定制训练数据 170
9.4.3　初始参数 171
9.4.4　迭代次数 171
9.4.5　其他参数 171
9.4.6　迷你库 172
9.4.7　主文件 175
9.5　可能的改进和潜在的应用 180
9.5.1　改进 180
9.5.2　应用 181
9.6　总结 181
・ ・ ・ ・ ・ ・ (收起)第 1 部分 基于 OpenCV 的传统视觉应用
第 1 章 图像生成 /2
1.1 图像显示 /3
1.1.1 使用 OpenCV 显示图像 /3
1.1.2 使用 Matplotlib 显示图像 /3
1.1.3 案例实现――使用OpenCV 显示图像 /3
1.1.4 案例实现――使用Matplotlib 显示图像 /5
1.2 图像读取 /6
1.2.1 使用 OpenCV 读取图像 /6
1.2.2 使用 Matplotlib 读取图像 /7
1.2.3 案例实现――使用OpenCV 读取图像 /7
1.2.4 案例实现――使用Matplotlib 读取图像 /9
1.3 图像保存 /10
1.3.1 使用 OpenCV 保存图像 /10
1.3.2 使用 Matplotlib 保存图像/11
1.3.3 案例实现――使用OpenCV 保存图像 /11
1.3.4 案例实现――使用Matplotlib 保存图像 /14
本章总结 /16
作业与练习 /16
第 2 章 OpenCV 图像处理（1） /17
2.1 图像模糊 /17
2.1.1 均值滤波 /17
2.1.2 中值滤波 /18
2.1.3 高斯滤波 /18
2.1.4 案例实现 /18
2.2 图像锐化 /21
2.2.1 图像锐化简介 /21
2.2.2 案例实现 /21
本章总结 /24
作业与练习 /24
第 3 章 OpenCV 图像处理（2） /26
3.1 OpenCV 绘图 /26
3.1.1 使用 OpenCV 绘制各种图形 /26
3.1.2 案例实现 /27
3.2 图像的几何变换 /31
3.2.1 几何变换操作 /31
3.2.2 案例实现 /32
本章总结 /38
作业与练习 /38
第 4 章 图像特征检测 /40
4.1 边缘编辑和增强 /41
4.1.1 Canny 边缘检测简介 /41
4.1.2 案例实现 /42
4.2 图像轮廓检测 /44
4.2.1 轮廓查找步骤 /45
4.2.2 查找轮廓函数 /45
4.2.3 绘制轮廓函数 /45
4.2.4 案例实现 /46
4.3 图像角点和线条检测 /48
4.3.1 角点的定义 /48
4.3.2 Harris 角点简介 /48
4.3.3 Harris 角点检测函数 /49
4.3.4 案例实现 /49
本章总结 /51
作业与练习 /52
第 5 章 图像特征匹配 /53
5.1 ORB 关键点检测与匹配 /53
5.1.1 FAST 算法 /54
5.1.2 BRIEF 算法 /55
5.1.3 特征匹配 /56
5.1.4 代码流程 /56
5.2 案例实现 /57
本章总结 /59
作业与练习 /59
第 6 章 图像对齐与拼接 /60
6.1 全景图像拼接 /60
6.1.1 全景图像的拼接原理 /61
6.1.2 算法步骤 /61
6.1.3 Ransac 算法介绍 /62
6.1.4 全景图像剪裁 /63
6.2 案例实现 /64
本章总结 /67
作业与练习 /67
第 7 章 相机运动估计 /68
7.1 双目相机运动估计 /68
7.1.1 相机测距流程 /68
7.1.2 双目相机成像模型 /69
7.1.3 极限约束 /70
7.1.4 双目测距的优势 /70
7.1.5 双目测距的难点 /70
7.2 案例实现 /72
本章总结 /82
作业与练习 /83
第 2 部分 基于机器学习和深度学习的视觉应用
第 8 章 基于 SVM 模型的手写数字识别/85
8.1 手写数字识别 /85
8.1.1 手写数字图像 /85
8.1.2 图像处理 /86
8.2 案例实现 /87
本章总结 /95
作业与练习 /95
第 9 章 基于 HOG+SVM 的行人检测 /96
9.1 行人检测 /96
9.1.1 HOG+SVM /96
9.1.2 检测流程 /97
9.1.3 滑动窗口 /98
9.1.4 非极大值抑制 /100
9.2 案例实现 /101
本章总结 /109
作业与练习 /109
第 10 章 数据标注 /110
10.1 目标检测数据标注 /110
10.1.1 数据收集与数据标注 /111
10.1.2 数据标注的通用规则 /112
10.1.3 案例实现 /113
10.2 视频目标跟踪数据标注 /118
10.2.1 视频与图像数据标注的差异 /118
10.2.2 案例实现 /119
本章总结 /127
作业与练习 /127
第 11 章 水果识别 /128
11.1 LeNet-5 模型的训练与评估 /128
11.1.1 卷积层 /129
11.1.2 池化层 /130
11.1.3 ReLU 层 /131
11.1.4 LeNet-5 模型 /131
11.1.5 Keras /132
11.1.6 案例实现 /133
11.2 LeNet-5 模型的应用 /139
11.2.1 使用 OpenCV 操作摄像头 /139
11.2.2 OpenCV 的绘图功能 /140
11.2.3 OpenCV 绘图函数的常见参数 /140
11.2.4 Keras 模型的保存和加载 /140
11.2.5 案例实现 /142
本章总结 /145
作业与练习 /145
第 12 章 病虫害识别 /147
12.1 植物叶子病虫害识别 /147
12.1.1 PlantVillage 数据集 /148
12.1.2 性能评估 /148
12.1.3 感受野 /149
12.2 案例实现 /149
本章总结 /161
作业与练习 /162
第 13 章 相似图像搜索 /163
13.1 以图搜图 /163
13.1.1 VGG 模型 /164
13.1.2 H5 模型文件 /165
13.1.3 案例实现 /166
13.2 人脸识别 /173
13.2.1 人脸检测 /173
13.2.2 分析面部特征 /174
13.2.3 人脸识别特征提取 /175
13.2.4 人脸相似性比较 /176
13.2.5 案例实现 /176
本章总结 /184
作业与练习 /185
第 14 章 多目标检测 /186
14.1 人脸口罩佩戴检测 /186
14.1.1 目标检测 /187
14.1.2 YOLO 模型 /188
14.1.3 YOLOv3 模型 /190
14.1.4 YOLOv3-Tiny 模型 /191
14.2 案例实现 /192
本章总结 /198
作业与练习 /198
第 15 章 可采摘作物检测 /199
15.1 番茄成熟度检测 /199
15.1.1 数据集 /200
15.1.2 RCNN 模型 /201
15.1.3 SPP-Net 模型 /202
15.1.4 Fast-RCNN 模型 /202
15.1.5 Faster-RCNN 模型 /202
15.1.6 Mask-RCNN 模型 /203
15.2 案例实现 /204
本章总结 /213
作业与练习 /213
第 16 章 智能照片编辑 /214
16.1 图像自动着色 /214
16.1.1 GAN 模型的基本结构与原理 /215
16.1.2 构建 GAN 模型 /216
16.2 案例实现 /218
本章总结 /225
作业与练习 /225
第 17 章 超分辨率 /227
17.1 图像超分辨率 /227
17.1.1 SRGAN 模型的结构 /228
17.1.2 SRGAN 模型的损失函数 /229
17.1.3 SRGAN 模型的评价指标 /230
17.2 案例实现 /230
本章总结 /237
作业与练习 /238
第 18 章 医学图像分割 /239
18.1 眼底血管图像分割 /239
18.1.1 图像分割 /240
18.1.2 语义分割 /241
18.1.3 全卷积神经网络 /243
18.1.4 反卷积 /244
18.1.5 U-Net 模型 /244
18.2 案例实现 /245
本章总结 /253
作业与练习 /253
第 19 章 医学图像配准 /255
19.1 头颈部 CT 图像配准 /255
19.1.1 图像配准方法 /256
19.1.2 VoxelMorph 配准框架 /257
19.1.3 TensorFlow-pix2pix /259
19.2 案例实现 /260
本章总结 /265
作业与练习 /265
第 20 章 视频内容分析 /267
20.1 人体动作识别 /267
20.1.1 视频动作识别模型 /268
20.1.2 UCF-101 数据集 /269
20.2 案例实现 /270
本章总结 /275
作业与练习 /275
第 21 章 图像语义理解 /277
21.1 视觉问答 /277
21.1.1 编码器-解码器模型 /279
21.1.2 光束搜索 /281
21.2 案例实现 /282
本章总结 /288
作业与练习 /288
第 3 部分 基于深度学习的新兴视觉应用
第 22 章 三维空间重建 /291
22.1 3D-R2N2 算法 /291
22.1.1 算法简介 /292
22.1.2 算法的优势 /292
22.1.3 算法的结构 /292
22.2 案例实现 /294
本章总结 /298
作业与练习 /298
第 23 章 视频稳定 /300
23.1 人脸视频稳定 /300
23.1.1 MobileNet 模型 /301
23.1.2 SSD 模型 /302
23.1.3 MobileNet-SSD 模型 /303
23.1.4 模型评估 /303
23.1.5 实时影响 /303
23.2 案例实现 /304
本章总结 /317
作业与练习 /317
第 24 章 目标检测与跟踪 /319
24.1 车辆检测与跟踪 /319
24.1.1 UA-DETRAC 数据集 /320
24.1.2 目标跟踪 /322
24.1.3 DeepSORT 目标跟踪 /323
24.2 案例实现 /324
本章总结 /337
作业与练习 /337
第 25 章 风格迁移 /339
25.1 图像与视频风格迁移 /339
25.1.1 理解图像内容和图像风格 /340
25.1.2 图像重建 /341
25.1.3 风格重建 /342
25.2 案例实现 /343
本章总结 /354
作业与练习 /355
附录 A 企业级综合教学项目介绍 /356
1.1 智慧停车场管理系统 /356
1.1.1 项目概述 /356
1.1.2 技能目标 /358
1.2 智慧景区管理系统 /358
1.2.1 项目概述 /358
1.2.2 技能目标 /359
1.3 智能考勤打卡系统 /360
1.3.1 项目概述 /360
1.3.2 技能目标 /361
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
作者简介
第一部分预备知识
第1章数据
1.1可视化
1.2离散化
1.2.1采样
1.2.2量化
1.3表示
1.4噪声
1.5本章小结
参考文献
习题
第2章技术
2.1插值
2.1.1线性插值
2.1.2双线性插值
2.2几何相交
2.3本章小结
参考文献
习题
第二部分基于图像的视觉计算
第3章卷积
3.1线性系统
3.1.1线性系统的响应
3.1.2卷积的性质
3.2线性滤波器
3.2.1全通、低通、带通和高通滤波器
3.2.2设计新滤波器
3.2.3二维滤波器的可分性
3.2.4相关和模式匹配
3.3实现细节
3.4本章小结
参考文献
习题
第4章谱分析
4.1离散傅里叶变换
4.2极坐标
4.2.1性质
4.2.2信号分析示例
4.3频域的周期性
4.4混叠
4.5推广到二维插值
4.5.1周期性的影响
4.5.2陷波器
4.5.3混叠效应示例
4.6对偶性
4.7本章小结
参考文献
习题
第5章特征检测
5.1边缘检测
5.1.1边缘子检测器
5.1.2多分辨率边缘检测
5.1.3边缘子聚合
5.2特征检测
5.3其他非线性滤波器
5.4本章小结
参考文献
习题
第三部分基于几何的视觉计算
第6章几何变换
6.1齐次坐标
6.2线性变换
6.3欧氏和仿射变换
6.3.1平移
6.3.2旋转
6.3.3缩放
6.3.4剪切
6.3.5一些现象
6.4变换的串联
6.4.1相对于中心点的缩放
6.4.2相对于任意轴的旋转
6.5坐标系
6.6串联的性质
6.7透视变换
6.8自由度
6.9非线性变换
6.10本章小结
参考文献
习题
第7章针孔相机
7.1针孔相机模型
7.1.1相机标定
7.1.2三维深度估计
7.1.3单应性
7.2实际相机的一些考虑
7.3本章小结
参考文献
习题
第8章对极几何
8.1背景
8.2多视几何中的匹配
8.3基础矩阵
8.3.1性质
8.3.2基础矩阵的估计
8.3.3仿前置双眼的相机设置
8.4本质矩阵
8.5整流
8.6应用对极几何
8.6.1根据视差恢复深度
8.6.2根据光流恢复深度
8.7本章小结
参考文献
习题
第四部分基于辐射度的视觉计算
第9章光照
9.1辐射度学
9.1.1双向反射分布函数
9.1.2光传播方程
9.2光度学与色彩
9.2.1CIE XYZ色彩空间
9.2.2CIE XYZ空间的认知结构
9.2.3认知一致色彩空间
9.3本章小结
参考文献
习题
第10章色彩还原
10.1加性色彩混合的建模
10.1.1设备的色域
10.1.2色调映射算子
10.1.3强度分辨率
10.1.4显示器示例
10.2色彩管理
10.2.1色域变换
10.2.2色域匹配
10.3减性色彩混合的建模
10.4局限性
10.4.1高动态范围成像
10.4.2多光谱成像
10.5本章小结
参考文献
习题
第11章光度处理
11.1直方图处理
11.2图像融合
11.2.1图像混合
11.2.2图像割
11.3光度立体视觉
11.3.1阴影处理
11.3.2光照方向计算
11.3.3色彩处理
11.4本章小结
参考文献
习题
第五部分视觉内容合成
第12章多样化域
12.1建模
12.2处理
12.3渲染
12.4应用
12.5本章小结
参考文献
第13章交互式图形流程
13.1顶点的几何变换
13.1.1模型变换
13.1.2视图变换
13.1.3透视投影变换
13.1.4遮挡处理
13.1.5窗口坐标变换
13.1.6最终变换
13.2裁剪和属性的顶点插值
13.3光栅化和属性的像素插值
13.4本章小结
参考文献
习题
第14章真实感与性能
14.1光照
14.2着色
14.3阴影
14.4纹理贴图
14.4.1纹理至对象空间映射
14.4.2对象至屏幕空间映射
14.4.3分级细化贴图
14.5凹凸贴图
14.6环境贴图
14.7透明度
14.8累积缓存
14.9背面剔除
14.10可见性剔除
14.10.1包围体
14.10.2空间细分
14.10.3其他用途
14.11本章小结
参考文献
习题
第15章图形编程
15.1图形处理单元的发展
15.2图形API和程序库的发展
15.3现代GPU和CUDA
15.3.1GPU架构
15.3.2CUDA编程模型
15.3.3CUDA存储模型
15.4本章小结
参考文献
・ ・ ・ ・ ・ ・ (收起)第 1章 图像的获取和表示1
1．1 图像传感器技术 1
1．1．1 传感器材料 2
1．1．2 传感器光电二极管元件 3
1．1．3 传感器配置：马赛克、Foveon和BSI 3
1．1．4 动态范围、噪声和超分辨率 4
1．1．5 传感器处理 5
1．1．6 去马赛克 5
1．1．7 坏像素校正 5
1．1．8 色彩和光照校正 6
1．1．9 几何校正 6
1．2 照相机和计算成像 6
1．2．1 计算成像概述 7
1．2．2 单像素可计算相机 7
1．2．3 二维可计算照相机 8
1．2．4 三维深度的照相机系统 9
1．3 三维深度处理 18
1．3．1 方法概述 18
1．3．2 深度感知和处理中存在的问题 18
1．3．3 单目深度处理 23
1．4 三维表示：体元、深度图、网格和点云 26
1．5 总结 27
1．6 习题 27
第 2章 图像预处理 29
2．1 图像处理概述 29
2．2 图像预处理要解决的问题 29
2．2．1 计算机视觉的流程和图像预处理 30
2．2．2 图像校正 31
2．2．3 图像增强 31
2．2．4 为特征提取准备图像 32
2．3 图像处理方法分类 36
2．3．1 点运算 36
2．3．2 直线运算 36
2．3．3 区域运算 37
2．3．4 算法 37
2．3．5 数据转换 37
2．4 色彩学 37
2．4．1 色彩管理系统概述 38
2．4．2 光源、白点、黑点和中性轴 38
2．4．3 设备颜色模型 39
2．4．4 色彩空间与色彩感知 39
2．4．5 色域映射与渲染的目标 40
2．4．6 色彩增强的实际考虑 41
2．4．7 色彩的准确度与精度 41
2．5 空间滤波 41
2．5．1 卷积滤波与检测 41
2．5．2 核滤波与形状选择 43
2．5．3 点滤波 44
2．5．4 噪声与伪像滤波 45
2．5．5 积分图与方框滤波器 46
2．6 边缘检测器 46
2．6．1 核集合 47
2．6．2 Canny检测器 48
2．7 变换滤波、Fourier变换及其他 48
2．7．1 Fourier变换 48
2．7．2 其他变换 50
2．8 形态学与分割 51
2．8．1 二值形态学 51
2．8．2 灰度和彩色形态学 52
2．8．3 形态学优化和改进 53
2．8．4 欧氏距离映射 53
2．8．5 超像素分割 53
2．8．6 深度图分割 54
2．8．7 色彩分割 55
2．9 阈值化 55
2．9．1 全局阈值化 56
2．9．2 局部阈值化 59
2．10 总结 60
2．11 习题 60
第3章 全局特征和区域特征 63
3．1 视觉特征的历史概述 63
3．1．1 全局度量、区域度量和局部度量的核心思想 64
3．1．2 纹理分析 65
3．1．3 统计方法 68
3．2 纹理区域度量 68
3．2．1 边缘度量 69
3．2．2 互相关性和自相关性 70
3．2．3 Fourier谱、小波和基签名 71
3．2．4 共生矩阵、Haralick特征 71
3．2．5 Laws纹理度量 78
3．2．6 LBP局部二值模式 79
3．2．7 动态纹理 80
3．3 统计区域度量 81
3．3．1 图像矩特征 81
3．3．2 点度量特征 81
3．3．3 全局直方图 83
3．3．4 局部区域直方图 83
3．3．5 散点图、3D直方图 84
3．3．6 多分辨率、多尺度直方图 85
3．3．7 径向直方图 87
3．3．8 轮廓或边缘直方图 87
3．4 基空间度量 88
3．4．1 Fourier描述 90
3．4．2 Walsh-Hadamard变换 90
3．4．3 HAAR变换 91
3．4．4 斜变换 91
3．4．5 Zernike多项式 91
3．4．6 导向滤波器 92
3．4．7 Karhunen-Loeve变换与Hotelling变换 93
3．4．8 小波变换和Gabor滤波器 93
3．4．9 Hough变换与Radon变换 95
3．5 总结 96
3．6 习题 96
第4章 局部特征设计 97
4．1 局部特征 97
4．1．1 检测器、兴趣点、关键点、锚点和特征点 98
4．1．2 描述子、特征描述和特征提取 98
4．1．3 稀疏局部模式方法 98
4．2 局部特征属性 99
4．2．1 选择特征描述子和兴趣点 99
4．2．2 特征描述子和特征匹配 99
4．2．3 好特征的标准 99
4．2．4 可重复性，困难和容易的查找 101
4．2．5 判别性与非判别性 101
4．2．6 相对位置和绝对位置 101
4．2．7 匹配代价和一致性 101
4．3 距离函数 102
4．3．1 距离函数的早期工作 102
4．3．2 欧氏或笛卡儿距离度量 103
4．3．3 网格距离度量 104
4．3．4 基于统计学的差异性度量 105
4．3．5 二值或布尔距离度量 106
4．4 描述子的表示 107
4．4．1 坐标空间和复合空间 107
4．4．2 笛卡儿坐标 107
4．4．3 极坐标和对数极坐标 107
4．4．4 径向坐标 107
4．4．5 球面坐标 108
4．4．6 Gauge坐标 108
4．4．7 多元空间和多模数据 108
4．4．8 特征金字塔 109
4．5 描述子的密度 109
4．5．1 丢弃兴趣点和描述子 109
4．5．2 稠密与稀疏特征描述 110
4．6 描述子形状 110
4．6．1 关联性模板 111
4．6．2 块和形状 111
4．6．3 对象多边形 113
4．7 局部二值描述子与点对模式 113
4．7．1 FREAK视网膜模式 114
4．7．2 BRISK模式 115
4．7．3 ORB和BRIEF模式 116
4．8 描述子的判别性 116
4．8．1 谱的判别性 117
4．8．2 区域、形状和模式的判别性 118
4．8．3 几何判别因素 118
4．8．4 通过特征可视化来评价判别性 119
4．8．5 精度与可跟踪性 121
4．8．6 精度优化、子区域重叠、Gaussian加权和池化 122
4．8．7 亚像素精度 123
4．9 搜索策略与优化 123
4．9．1 密集搜索 124
4．9．2 网格搜索 124
4．9．3 多尺度金字塔搜索 124
4．9．4 尺度空间和图像金字塔 125
4．9．5 特征金字塔 126
4．9．6 稀疏预测搜索与跟踪 127
4．9．7 跟踪区域限制搜寻 127
4．9．8 分割限制搜索 127
4．9．9 深度或Z限制搜索 127
4．10 计算机视觉、模型和结构 128
4．10．1 特征空间 128
4．10．2 对象模型 129
4．10．3 约束 130
4．10．4 选择检测器和特征 131
4．10．5 训练概述 131
4．10．6 特征和对象的分类 132
4．10．7 特征学习、稀疏编码和卷积网络 136
4．11 总结 139
4．12 习题 139
第5章 特征描述属性的分类 141
5．1 一般的鲁棒性分类 143
5．2 一般的视觉度量分类 146
5．3 特征度量评估 155
5．3．1 SIFT的示例 156
5．3．2 LBP的示例 156
5．3．3 形状因子的示例 157
5．4 总结 158
5．5 习题 158
第6章 兴趣点检测与特征描述子 159
6．1 兴趣点调整 159
6．2 兴趣点的概念 160
6．3 兴趣点方法概述 162
6．3．1 Laplacian和LoG 163
6．3．2 Moravac角点检测器 163
6．3．3 Harris方法、Harris-Stephens、Shi-Tomasi和Hessian类型的检测器 163
6．3．4 Hessian矩阵检测器和Hessian-Laplace 164
6．3．5 Gaussian差 164
6．3．6 显著性区域 164
6．3．7 SUSAN、Trajkovic-Hedly 165
6．3．8 FAST 165
6．3．9 局部曲率方法 166
6．3．10 形态兴趣区域 167
6．4 特征描述简介 167
6．4．1 局部二值描述子 168
6．4．2 Census 173
6．4．3 改进的Census变换 174
6．4．4 BRIEF 174
6．4．5 ORB 175
6．4．6 BRISK 176
6．4．7 FREAK 176
6．5 谱描述子 177
6．5．1 SIFT 177
6．5．2 SIFT-PCA 181
6．5．3 SIFT-GLOH 181
6．5．4 SIFT-SIFER 182
6．5．5 SIFT CS-LBP 182
6．5．6 ROOTSIFT 183
6．5．7 CenSurE和STAR 183
6．5．8 相关模板 185
6．5．9 HAAR特征 186
6．5．10 使用类HAAR特征的Viola和Jones算法 187
6．5．11 SURF 187
6．5．12 改进的SURF算法 189
6．5．13 梯度直方图（HOG）及改进方法 189
6．5．14 PHOG和相关方法 190
6．5．15 Daisy和O-Daisy 191
6．5．16 CARD 193
6．5．17 具有鲁棒性的快速特征匹配 194
6．5．18 RIFF和CHOG 195
6．5．19 链码直方图 196
6．5．20 D-NETS 196
6．5．21 局部梯度模式 197
6．5．22 局部相位量化 198
6．6 基空间描述子 198
6．6．1 Fourier描述子 199
6．6．2 用其他基函数来构建描述子 200
6．6．3 稀疏编码方法 200
6．7 多边形形状描述 200
6．7．1 MSER方法 201
6．7．2 针对斑点和多边形的目标形状度量 202
6．7．3 形状上下文 204
6．8 3D和4D描述子 205
6．8．1 3D HOG 206
6．8．2 HON 4D 206
6．8．3 3D SIFT 207
6．9 总结 208
6．10 习题 208
第7章 基准数据、内容、度量和分析 210
7．1 基准数据 210
7．2 先前关于基准数据方面的工作：艺术与科学 212
7．2．1 质量的一般度量 212
7．2．2 算法性能的度量 212
7．2．3 Rosin关于角点方面的工作 213
7．3 构造基准数据的关键问题 214
7．3．1 内容：采用、修改或创建 214
7．3．2 可用的基准数据集 215
7．3．3 拟合基准数据的算法 215
7．3．4 场景构成和标注 216
7．4 定义目标和预期 218
7．4．1 Mikolajczyk和Schmid的方法 218
7．4．2 开放式评价系统 219
7．4．3 极端情况和限制 219
7．4．4 兴趣点和特征 219
7．5 基准数据的鲁棒性准则 220
7．5．1 举例说明鲁棒性标准 220
7．5．2 将鲁棒性标准用于实际应用 221
7．6 度量与基准数据配对 222
7．6．1 兴趣点、特征和基准数据的配对和优化 222
7．6．2 一般的视觉分类例子 223
7．7 合成的特征字母表 224
7．7．1 合成数据集的目标 224
7．7．2 合成兴趣点字母表 226
7．7．3 将合成字母表叠加到真实图像上 228
7．8 总结 229
7．9 习题 230
第8章 可视流程及优化 231
8．1 阶段、操作和资源 231
8．2 计算资源预算 233
8．2．1 计算单元、ALU和加速器 234
8．2．2 能耗的使用 235
8．2．3 内存的利用 235
8．2．4 I O性能 238
8．3 计算机视觉流程的实例 238
8．3．1 汽车识别 239
8．3．2 人脸检测、情感识别和年龄识别 244
8．3．3 图像分类 250
8．3．4 增强现实 254
8．4 可选的加速方案 258
8．4．1 内存优化 258
8．4．2 粗粒度并行 260
8．4．3 细粒度数据并行 261
8．4．4 高级指令集和加速器 263
8．5 视觉算法的优化与调整 263
8．5．1 编译器优化与手工优化 264
8．5．2 特征描述子改进、检测器和距离函数 265
8．5．3 Boxlets与卷积加速 265
8．5．4 数据类型优化（整数与浮点） 265
8．6 优化资源 266
8．7 总结 266
第9章 特征学习的架构分类和神经科学背景 267
9．1 计算机视觉中的神经科学思想 268
9．2 特征生成与特征学习 269
9．3 计算机视觉中所使用的神经科学术语 269
9．4 特征学习的分类 274
9．4．1 卷积特征权重学习 275
9．4．2 局部特征描述子学习 275
9．4．3 基本特征的组合和字典学习 275
9．4．4 特征学习方法总结 276
9．5 计算机视觉中的机器学习模型 276
9．5．1 专家系统 277
9．5．2 统计和数学分析方法 278
9．5．3 受神经科学启发的方法 278
9．5．4 深度学习 278
9．6 机器学习和特征学习的历史 280
9．6．1 历史回顾：20世纪40年代至21世纪初 280
9．6．2 人工神经网络（ANN）分类 284
9．7 特征学习概述 285
9．7．1 通过学习得到的各类描述子 285
9．7．2 层次特征学习 285
9．7．3 要学习多少特征 286
9．7．4 深度神经网络的优势 286
9．7．5 特征编码的有效性 286
9．7．6 手工设计的特征与深度学习 287
9．7．7 特征学习的不变性和鲁棒性 288
9．7．8 最好的特征和学习架构 288
9．7．9 大数据、分析和计算机视觉的统一 289
9．7．10 关键技术的推动因素 291
9．8 神经科学的概念 292
9．8．1 生物学及其整体结构 293
9．8．2 难以找到统一的学习理论 294
9．8．3 人类视觉系统的架构 295
9．9 特征学习的结构分类 299
9．9．1 架构拓扑 301
9．9．2 架构组件和层 302
9．10 总结 313
9．11 习题 313
第 10章 特征学习和深度学习架构概述 315
10．1 架构概述 315
10．1．1 FNN架构简介 316
10．1．2 RNN的结构简介 372
10．1．3 BFN的结构简介 395
10．2 集成方法 427
10．3 深度神经网络的未来 429
10．3．1 增加最大深度―深度残差学习 429
10．3．2 使用更简单的MLP来近似复杂模型（模型压缩） 430
10．3．3 分类器的分解与重组 431
10．4 总结 432
10．5 习题 432
附录A 合成特征分析 435
附录B 基准数据集概述 464
附录C 成像和计算机视觉资源 470
附录D 扩展SDM准则 474
附录E 视觉基因组模型（VGM） 487
参考文献 508
译后记 541
・ ・ ・ ・ ・ ・ (收起)目　录
第1章　人工智能的发展概况　1
1．1　人工智能的诞生与初兴　2
1．2　人工智能的复兴与计算机视觉的初露端倪　4
1．3　数据被重视，人工智能崛起　4
第2　章 数据标注行业的国内现状与未来展望　7
2．1　国内数据标注行业的现状　8
2．2　数据标注工程师简介　9
2．3　数据标注行业的发展前景　12
第3　章 人工智能治理　15
3．1　人工智能的可持续发展　16
3．2　数据是AI治理的第一道防火墙　17
3．3　数据服务产业是AI治理落地的试验田　17
3．3．1　数据来源的合法合规问题　18
3．3．2　技术的安全性　19
3．3．3　问责机制　20
3．4　旷视，AI发展与治理双轮驱动　20
第4章　数据标注服务产品及旷视Data++数据标注平台　23
4．1　数据标注服务产品　24
4．2　数据服务标注平台流程　26
4．2．1　创建项目　26
4．2．2　数据上传　27
4．2．3　项目发布　27
4．2．4　项目交付　28
4．3　旷视Data++数据标注平台　28
4．3．1　用户注册　29
4．3．2　标注操作流程　31
第5章　通用标注工具　35
5．1　行人属性筛选　36
5．1．1　行人属性筛选定义　36
5．1．2　行人属性筛选工具介绍　37
5．1．3　行人属性筛选分类　39
5．1．4　标注注意事项　43
5．1．5　标注难点　46
5．1．6　实际中的应用　46
5．1．7　思考与讨论　48
5．1．8　行人属性筛选工具现状及展望　48
5．1．9　小结　50
5．2　属性标注　50
5．2．1　属性标注工具介绍　50
5．2．2　标注内容　52
5．2．3　标注方法　53
5．2．4　标注难点　54
5．2．5　生活中的应用　56
5．2．6　属性标注在Objects365中的应用　57
5．2．7　小结　59
5．3　框+属性　59
5．3．1　“框+属性”工具介绍　60
5．3．2　标注方法　61
5．3．3　标注难点　64
5．3．4　生活中的应用　66
5．3．5　小实验　67
5．3．6　小结　67
5．4　多边形+属性　68
5．4．1　多边形+属性工具介绍　68
5．4．2　标注标准　70
5．4．3　标注难点　74
5．4．4　“多边形+属性”工具在生活中的应用　75
5．4．5　小结　76
第6章　检测标注工具　79
6．1　人脸8点　80
6．1．1　人脸关键点检测定义　80
6．1．2　人脸8点工具介绍　81
6．1．3　标注方法　83
6．1．4　标注难点　84
6．1．5　生活中的应用　90
6．1．6　小实验　92
6．1．7　人脸8点工具现状及展望　92
6．1．8　小结　93
6．2　人体骨骼点　94
6．2．1　人体骨骼点14点定义　94
6．2．2　人体骨骼点工具介绍　95
6．2．3　标注方法　97
6．2．4　标注难点　99
6．2．5　生活中的应用　101
6．2．6　人体骨骼点工具现状及未来展望　104
6．2．7　小结　104
6．3　手部关键点　105
6．3．1　手部关键点21点定义　105
6．3．2　手部关键点标注工具介绍　107
6．3．3　标注方法　108
6．3．4　标注难点　110
6．3．5　手部关键点标注提升方法　114
6．3．6　生活中的应用　115
6．3．7　手部关键点工具现状及展望　117
6．3．8　小结　118
第7章　识别标注工具　121
7．1　一人所属照片清洗　122
7．1．1　一人所属照片清洗工具介绍　122
7．1．2　标注方法　124
7．1．3　标注难点　129
7．1．4　一人所属照片清洗工具在生活中的应用　131
7．1．5　照片清洗工具现状　133
7．1．6　小实验　134
7．1．7　小结　134
7．2　行人重识别　135
7．2．1　行人重识别合并标注工具介绍　135
7．2．2　标注方法　142
7．2．3　标注难点　147
7．2．4　生活中的应用　151
7．2．5　行人重识别技术现状与发展　152
7．2．6　小结　152
第8章　其他标注工具　155
8．1　视频人脸8点　156
8．1．1　视频人脸8点工具介绍　157
8．1．2　标注方法　159
8．1．3　生活中的应用　161
8．1．4　视频人脸8点工具的现状与发展　164
8．1．5　小结　165
8．2　人脸3D朝向　165
8．2．1　人脸3D朝向工具　165
8．2．2　人脸3D朝向工具介绍　166
8．2．3　标注方法　167
8．2．4　标注难点　170
8．2．5　生活中的应用　170
8．2．6　人脸3D朝向工具现状与展望　171
8．2．7　小结　172
8．3　精细分割　173
8．3．1　人像抠图工具介绍　173
8．3．2　标注方法　176
8．3．3　标注难点　185
8．3．4　生活中的应用　186
8．3．5　精细分割标注工具的现状与发展　187
8．3．6　小结　188
声明　189
・ ・ ・ ・ ・ ・ (收起)第1部分 基础知识导读篇
第1章 数字图像基础 2
1.1 图像表示基础 2
1.1.1 艺术与生活 2
1.1.2 数字图像 3
1.1.3 二值图像的处理 5
1.1.4 像素值的范围 5
1.1.5 图像索引 7
1.2 彩色图像的表示 8
1.3 应用基础 9
1.3.1 量化 10
1.3.2 特征 10
1.3.3 距离 11
1.3.4 图像识别 13
1.3.5 信息隐藏 15
1.4 智能图像处理基础 16
1.5 抽象 18
第2章 Python基础 21
2.1 如何开始 21
2.2 基础语法 22
2.2.1 变量的概念 22
2.2.2 变量的使用 22
2.3 数据类型 24
2.3.1 基础类型 25
2.3.2 列表 25
2.3.3 元组 28
2.3.4 字典 29
2.4 选择结构 31
2.5 循环结构 35
2.6 函数 39
2.6.1 什么是函数 39
2.6.2 内置函数 41
2.6.3 自定义函数 42
2.7 模块 44
2.7.1 标准模块 44
2.7.2 第三方模块 45
2.7.3 自定义模块 46
第3章 OpenCV基础 47
3.1 基础 47
3.1.1 安装OpenCV 47
3.1.2 读取图像 49
3.1.3 显示图像 50
3.1.4 保存图像 51
3.2 图像处理 52
3.2.1 像素处理 52
3.2.2 通道处理 57
3.2.3 调整图像大小 60
3.3 感兴趣区域 62
3.4 掩模 63
3.4.1 掩模基础及构造 64
3.4.2 乘法运算 65
3.4.3 逻辑运算 66
3.4.4 掩模作为函数参数 68
3.5 色彩处理 69
3.5.1 色彩空间基础 69
3.5.2 色彩空间转换 71
3.5.3 获取皮肤范围 72
3.6 滤波处理 73
3.6.1 均值滤波 75
3.6.2 高斯滤波 78
3.6.3 中值滤波 82
3.7 形态学 84
3.7.1 腐蚀 85
3.7.2 膨胀 88
3.7.3 通用形态学函数 91
第2部分 基础案例篇
第4章 图像加密与解密 94
4.1 加密与解密原理 94
4.2 图像整体加密与解密 96
4.3 脸部打码及解码 98
4.3.1 掩模方式实现 98
4.3.2 ROI方式实现 101
第5章 数字水印 105
5.1 位平面 106
5.2 数字水印原理 114
5.3 实现方法 115
5.4 具体实现 119
5.5 可视化水印 121
5.5.1 ROI 121
5.5.2 加法运算 123
5.6 扩展学习 125
5.6.1 算术运算实现数字水印 125
5.6.2 艺术字 128
第6章 物体计数 131
6.1 理论基础 131
6.1.1 如何计算图像的中心点 131
6.1.2 获取图像的中心点 133
6.1.3 按照面积筛选前景对象 135
6.2 核心程序 138
6.2.1 核函数 138
6.2.2 zip函数 140
6.2.3 阈值处理函数threshold 140
6.3 程序设计 141
6.4 实现程序 142
第7章 缺陷检测 144
7.1 理论基础 144
7.1.1 开运算 144
7.1.2 距离变换函数distanceTransform 146
7.1.3 最小包围圆形 148
7.1.4 筛选标准 149
7.2 程序设计 150
7.3 实现程序 151
第8章 手势识别 153
8.1 理论基础 154
8.1.1 获取凸包 154
8.1.2 凸缺陷 156
8.1.3 凸缺陷占凸包面积比 159
8.2 识别过程 161
8.2.1 识别流程 162
8.2.2 实现程序 165
8.3 扩展学习：石头、剪刀、布的识别 167
8.3.1 形状匹配 167
8.3.2 实现程序 170
第9章 答题卡识别 173
9.1 单道题目的识别 173
9.1.1 基本流程及原理 173
9.1.2 实现程序 178
9.2 整张答题卡识别原理 180
9.2.1 图像预处理 180
9.2.2 答题卡处理 181
9.2.3 筛选出所有选项 189
9.2.4 将选项按照题目分组 190
9.2.5 处理每一道题目的选项 195
9.2.6 显示结果 195
9.3 整张答题卡识别程序 195
第10章 隐身术 201
10.1 图像的隐身术 201
10.1.1 基本原理与实现 201
10.1.2 实现程序 213
10.1.3 问题及优化方向 214
10.2 视频隐身术 215
第11章 以图搜图 217
11.1 原理与实现 218
11.1.1 算法原理 218
11.1.2 感知哈希值计算方法 220
11.1.3 感知哈希值计算函数 224
11.1.4 计算距离 224
11.1.5 计算图像库内所有图像的哈希值 225
11.1.6 结果显示 226
11.2 实现程序 228
11.3 扩展学习 230
第12章 手写数字识别 231
12.1 基本原理 232
12.2 实现细节 233
12.3 实现程序 235
12.4 扩展阅读 236
第13章 车牌识别 238
13.1 基本原理 238
13.1.1 提取车牌 238
13.1.2 分割车牌 240
13.1.3 识别车牌 242
13.2 实现程序 246
13.3 下一步学习 249
第14章 指纹识别 250
14.1 指纹识别基本原理 251
14.2 指纹识别算法概述 251
14.2.1 描述关键点特征 251
14.2.2 特征提取 252
14.2.3 MCC匹配方法 254
14.2.4 参考资料 258
14.3 尺度不变特征变换 258
14.3.1 尺度空间变换 260
14.3.2 关键点定位 266
14.3.3 通过方向描述关键点 267
14.3.4 显示关键点 271
14.4 基于SIFT的指纹识别 273
14.4.1 距离计算 273
14.4.2 特征匹配 274
14.4.3 算法及实现程序 277
第3部分 机器学习篇
第15章 机器学习导读 282
15.1 机器学习是什么 283
15.2 机器学习基础概念 284
15.2.1 机器学习的类型 284
15.2.2 泛化能力 289
15.2.3 数据集的划分 290
15.2.4 模型的拟合 291
15.2.5 性能度量 292
15.2.6 偏差与方差 293
15.3 OpenCV中的机器学习模块 294
15.3.1 人工神经网络 295
15.3.2 决策树 296
15.3.3 EM模块 300
15.3.4 K近邻模块 300
15.3.5 logistic回归 303
15.3.6 贝叶斯分类器 305
15.3.7 支持向量机 308
15.3.8 随机梯度下降 SVM 分类器 310
15.4 OpenCV机器学习模块的使用 312
15.4.1 使用KNN模块分类 312
15.4.2 使用SVM模块分类 314
第16章 KNN实现字符识别 317
16.1 手写数字识别 317
16.2 英文字母识别 319
第17章 求解数独图像 322
17.1 基本过程 322
17.2 定位数独图像内的单元格 323
17.3 构造KNN模型 327
17.4 识别数独图像内的数字 330
17.5 求解数独 332
17.6 绘制数独求解结果 334
17.7 实现程序 335
17.8 扩展学习 338
第18章 SVM数字识别 339
18.1 基本流程 339
18.2 倾斜校正 340
18.3 HOG特征提取 343
18.4 数据处理 348
18.5 构造及使用SVM分类器 351
18.6 实现程序 352
18.7 参考学习 354
第19章 行人检测 355
19.1 方向梯度直方图特征 355
19.2 基础实现 358
19.2.1 基本流程 359
19.2.2 实现程序 359
19.3 函数detectMultiScale参数及优化 360
19.3.1 参数winStride 360
19.3.2 参数padding 362
19.3.3 参数scale 364
19.3.4 参数useMeanshiftGrouping 366
19.4 完整程序 369
19.5 参考学习 370
第20章 K均值聚类实现艺术画 371
20.1 理论基础 371
20.1.1 案例 371
20.1.2 K均值聚类的基本步骤 373
20.2 K均值聚类模块 374
20.3 艺术画 377
第4部分 深度学习篇
第21章 深度学习导读 384
21.1 从感知机到人工神经网络 384
21.1.1 感知机 384
21.1.2 激活函数 385
21.1.3 人工神经网络 387
21.1.4 完成分类 388
21.2 人工神经网络如何学习 389
21.3 深度学习是什么 390
21.3.1 深度的含义 390
21.3.2 表示学习 391
21.3.3 端到端 392
21.3.4 深度学习可视化 393
21.4 激活函数的分类 394
21.4.1 sigmoid函数 394
21.4.2 tanh函数 395
21.4.3 ReLU函数 395
21.4.4 Leaky ReLU函数 396
21.4.5 ELU函数 396
21.5 损失函数 397
21.5.1 为什么要用损失值 397
21.5.2 损失值如何起作用 398
21.5.3 均方误差 399
21.5.4 交叉熵误差 400
21.6 学习的技能与方法 401
21.6.1 全连接 401
21.6.2 随机失活 402
21.6.3 One-hot编码 403
21.6.4 学习率 403
21.6.5 正则化 404
21.6.6 mini-batch方法 405
21.6.7 超参数 406
21.7 深度学习游乐场 406
第22章 卷积神经网络基础 407
22.1 卷积基础 407
22.2 卷积原理 409
22.2.1 数值卷积 409
22.2.2 图像卷积 410
22.2.3 如何获取卷积核 411
22.3 填充和步长 412
22.4 池化操作 413
22.5 感受野 414
22.6 预处理与初始化 416
22.6.1 扩充数据集 416
22.6.2 标准化与归一化 417
22.6.3 网络参数初始化 418
22.7 CNN 418
22.7.1 LeNet 418
22.7.2 AlexNet 419
22.7.3 VGG网络 420
22.7.4 NiN 420
22.7.5 GooLeNet 421
22.7.6 残差网络 423
第23章 DNN模块 426
23.1 工作流程 427
23.2 模型导入 428
23.3 图像预处理 429
23.4 推理相关函数 438
第24章 深度学习应用实践 440
24.1 图像分类 441
24.1.1 图像分类模型 441
24.1.2 实现程序 442
24.2 目标检测 443
24.2.1 YOLO 444
24.2.2 SSD 447
24.3 图像分割 450
24.3.1 语义分割 450
24.3.2 实例分割 453
24.4 风格迁移 458
24.5 姿势识别 460
24.6 说明 463
第5部分 人脸识别篇
第25章 人脸检测 466
25.1 基本原理 466
25.2 级联分类器的使用 469
25.3 函数介绍 470
25.4 人脸检测实现 471
25.5 表情检测 473
第26章 人脸识别 475
26.1 人脸识别基础 475
26.1.1 人脸识别基本流程 475
26.1.2 OpenCV人脸识别基础 476
26.2 LBPH人脸识别 478
26.2.1 基本原理 478
26.2.2 函数介绍 482
26.2.3 案例介绍 482
26.3 EigenFaces人脸识别 484
26.3.1 基本原理 484
26.3.2 函数介绍 485
26.3.3 案例介绍 485
26.4 FisherFaces人脸识别 487
26.4.1 基本原理 487
26.4.2 函数介绍 489
26.4.3 案例介绍 489
26.5 人脸数据库 491
第27章 dlib库 493
27.1 定位人脸 493
27.2 绘制关键点 494
27.3 勾勒五官轮廓 497
27.4 人脸对齐 500
27.5 调用CNN实现人脸检测 502
第28章 人脸识别应用案例 504
28.1 表情识别 504
28.2 驾驶员疲劳检测 507
28.3 易容术 511
28.3.1 仿射 511
28.3.2 算法流程 512
28.3.3 实现程序 514
28.4 年龄和性别识别 517
・ ・ ・ ・ ・ ・ (收起)第1章 基于直方图优化的图像去雾技术 1
1.1 案例背景 1
1.2 理论基础 1
1.2.1 空域图像增强 1
1.2.2 直方图均衡化 2
1.3 程序实现 3
1.3.1 设计GUI界面 4
1.3.2 全局直方图处理 4
1.3.3 局部直方图处理 6
1.3.4 Retinex增强处理 8
1.4 延伸阅读 12
第2章 基于形态学的权重自适应图像去噪 13
2.1 案例背景 13
2.2 理论基础 14
2.2.1 图像去噪的方法 14
2.2.2 数学形态学的原理 15
2.2.3 权重自适应的多结构形态学去噪 15
2.3 程序实现 16
2.4 延伸阅读 22
第3章 基于多尺度形态学提取眼前节组织 24
3.1 案例背景 24
3.2 理论基础 25
3.3 程序实现 28
3.3.1 多尺度结构设计 28
3.3.2 多尺度边缘提取 29
3.3.3 多尺度边缘融合 31
3.4 延伸阅读 33
第4章 基于Hough变化的答题卡识别 34
4.1 案例背景 34
4.2 理论基础 34
4.2.1 图像二值化 35
4.2.2 倾斜校正 35
4.2.3 图像分割 38
4.3 程序实现 40
4.3.1 图像灰度化 40
4.3.2 灰度图像二值化 41
4.3.3 图像平滑滤波 41
4.3.4 图像矫正 41
4.3.5 完整性核查 42
4.4 延伸阅读 51
第5章 基于阈值分割的车牌定位识别 53
5.1 案例背景 53
5.2 理论基础 53
5.2.1 车牌图像处理 54
5.2.2 车牌定位原理 58
5.2.3 车牌字符处理 58
5.2.4 车牌字符识别 60
5.3 程序实现 62
5.4 延伸阅读 69
第6章 基于分水岭分割进行肺癌诊断 71
6.1 案例背景 71
6.2 理论基础 71
6.2.1 模拟浸水的过程 72
6.2.2 模拟降水的过程 72
6.2.3 过度分割问题 72
6.2.4 标记分水岭分割算法 72
6.3 程序实现 73
6.4 延伸阅读 77
第7章 基于主成分分析的人脸二维码识别 79
7.1 案例背景 79
7.2 理论基础 79
7.2.1 QR二维码简介 80
7.2.2 QR二维码的编码和译码流程 82
7.2.3 主成分分析方法 84
7.3 程序实现 85
7.3.1 人脸建库 85
7.3.2 人脸识别 87
7.3.3 人脸二维码 87
7.4 延伸阅读 92
第8章 基于知识库的手写体数字识别 94
8.1 案例背景 94
8.2 理论基础 94
8.2.1 算法流程 94
8.2.2 特征提取 95
8.2.3 模式识别 96
8.3 程序实现 97
8.3.1 图像处理 97
8.3.2 特征提取 98
8.3.3 模式识别 101
8.4 延伸阅读 102
8.4.1 识别器选择 102
8.4.2 特征库改善 102
第9章 基于特征匹配的英文印刷字符识别 103
9.1 案例背景 103
9.2 理论基础 104
9.2.1 图像预处理 104
9.2.2 图像识别技术 105
9.3 程序实现 106
9.3.1 界面设计 106
9.3.2 回调识别 111
9.4 延伸阅读 112
第10章 基于不变矩的数字验证码识别 113
10.1 案例背景 113
10.2 理论基础 114
10.3 程序实现 114
10.3.1 设计GUI界面 114
10.3.2 载入验证码图像 115
10.3.3 验证码图像去噪 116
10.3.4 验证码数字定位 118
10.3.5 验证码归一化 120
10.3.6 验证码数字识别 121
10.3.7 手动确认并入库 124
10.3.8 重新生成模板库 125
10.4 延伸阅读 128
第11章 基于小波技术进行图像融合 129
11.1 案例背景 129
11.2 理论基础 130
11.3 程序实现 132
11.3.1 设计GUI界面 132
11.3.2 图像载入 133
11.3.3 小波融合 135
11.4 延伸阅读 137
第12章 基于块匹配的全景图像拼接 138
12.1 案例背景 138
12.2 理论基础 138
12.2.1 图像匹配 139
12.2.2 图像融合 141
12.3 程序实现 142
12.3.1 设计GUI界面 142
12.3.2 载入图片 143
12.3.3 图像匹配 144
12.3.4 图像拼接 148
12.4 延伸阅读 153
第13章 基于霍夫曼图像编码的图像压缩和重建 155
13.1 案例背景 155
13.2 理论基础 155
13.2.1 霍夫曼编码的步骤 156
13.2.2 霍夫曼编码的特点 157
13.3 程序实现 158
13.3.1 设计GUI界面 158
13.3.2 压缩和重建 159
13.3.3 效果对比 164
13.4 延伸阅读 167
第14章 基于主成分分析的图像压缩和重建 168
14.1 案例背景 168
14.2 理论基础 168
14.2.1 主成分降维分析原理 168
14.2.2 由得分矩阵重建样本 169
14.2.3 主成分分析数据压缩比 170
14.2.4 基于主成分分析的图像压缩 170
14.3 程序实现 171
14.3.1 主成分分析的源代码 171
14.3.2 图像数组和样本矩阵之间的转换 172
14.3.3 基于主成分分析的图像压缩 173
14.4 延伸阅读 176
第15章 基于小波的图像压缩技术 177
15.1 案例背景 177
15.2 理论基础 178
15.3 程序实现 180
15.4 延伸阅读 188
第16章 基于融合特征的以图搜图技术 189
16.1 案例背景 189
16.2 理论基础 189
16.3 程序实现 191
16.3.1 图像预处理 191
16.3.2 计算特征 191
16.3.3 图像检索 194
16.3.4 结果分析 194
16.4 延伸阅读 196
第17章 基于Harris的角点特征检测 198
17.1 案例背景 198
17.2 理论基础 199
17.2.1 Harris的基本原理 199
17.2.2 Harris算法的流程 201
17.2.3 Harris角点的性质 201
17.3 程序实现 202
17.3.1 Harris算法的代码 202
17.3.2 角点检测实例 204
17.4 延伸阅读 205
第18章 基于GUI搭建通用视频处理工具 206
18.1 案例背景 206
18.2 理论基础 206
18.3 程序实现 208
18.3.1 设计GUI界面 208
18.3.2 实现GUI界面 209
18.4 延伸阅读 220
第19章 基于语音识别的信号灯图像
模拟控制技术 221
19.1 案例背景 221
19.2 理论基础 221
19.3 程序实现 223
19.4 延伸阅读 232
第20章 基于帧间差法进行视频目标检测 234
20.1 案例背景 234
20.2 理论基础 234
20.2.1 帧间差分法 235
20.2.2 背景差分法 236
20.2.3 光流法 236
20.3 程序实现 237
20.4 延伸阅读 24
第21章 路面裂缝检测系统设计 247
21.1 案例背景 247
21.2 理论基础 247
21.2.1 图像灰度化 248
21.2.2 图像滤波 250
21.2.3 图像增强 252
21.2.4 图像二值化 253
21.3 程序实现 255
21.4 延伸阅读 267
第22章 基于K-means聚类算法的图像分割 268
22.1 案例背景 268
22.2 理论基础 268
22.2.1 K-means聚类算法的原理 268
22.2.2 K-means聚类算法的要点 269
22.2.3 K-means聚类算法的缺点 270
22.2.4 基于K-means聚类算法进行图像分割 270
22.3 程序实现 271
22.3.1 样本间的距离 271
22.3.2 提取特征向量 272
22.3.3 图像聚类分割 273
22.4 延伸阅读 275
第23章 基于光流场的车流量计数应用 276
23.1 案例背景 276
23.2 理论基础 276
23.2.1 基于光流法检测运动的原理 276
23.2.2 光流场的主要计算方法 277
23.2.3 梯度光流场约束方程 278
23.2.4 Horn-Schunck光流算法 280
23.3 程序实现 281
23.3.1 计算视觉系统工具箱简介 281
23.3.2 基于光流法检测汽车运动 282
23.4 延伸阅读 287
第24章 基于Simulink进行图像和视频处理 289
24.1 案例背景 289
24.2 模块介绍 289
24.2.1 分析和增强模块库（Analysis和Enhancement） 290
24.2.2 转化模块库（Conversions） 291
24.2.3 滤波模块库（Filtering） 292
24.2.4 几何变换模块库（Geometric Transformations） 292
24.2.5 形态学操作模块库（Morphological Operations） 292
24.2.6 输入模块库（Sources） 293
24.2.7 输出模块库（Sinks） 293
24.2.8 统计模块库（Statistics） 294
24.2.9 文本和图形模块库（Text 和 Graphic） 295
24.2.10 变换模块库（Transforms） 295
24.2.11 其他工具模块库（Utilities） 295
24.3 仿真案例 296
24.3.1 搭建组织模型 296
24.3.2 仿真执行模型 298
24.3.3 自动生成报告 299
24.4 延伸阅读 302
第25章 基于小波变换的数字水印技术 304
25.1 案例背景 304
25.2 理论基础 304
25.2.1 数字水印技术的原理 305
25.2.2 典型的数字水印算法 307
25.2.3 数字水印攻击和评价 309
25.2.4 基于小波的水印技术 310
25.3 程序实现 312
25.3.1 准备载体和水印图像 312
25.3.2 小波数字水印的嵌入 313
25.3.3 小波数字水印的提取 317
25.3.4 小波水印的攻击试验 319
25.4 延伸阅读 323
第26章 基于最小误差法的胸片分割技术 325
26.1 案例背景 325
26.2 理论基础 325
26.2.1 图像增强 326
26.2.2 区域选择 326
26.2.3 形态学滤波 327
26.2.4 基于最小误差法进行胸片分割 328
26.3 程序实现 329
26.3.1 设计GUI界面 329
26.3.2 图像预处理 330
26.3.3 基于最小误差法进行图像分割 333
26.3.4 形态学后处理 335
26.4 延伸阅读 338
第27章 基于区域生长的肝脏影像分割系统 339
27.1 案例背景 339
27.2 理论基础 340
27.2.1 阈值分割 340
27.2.2 区域生长 340
27.2.3 基于阈值预分割的区域生长 341
27.3 程序实现 342
27.4 延伸阅读 346
第28章 基于计算机视觉的自动驾驶应用 347
28.1 案例背景 347
28.2 理论基础 348
28.2.1 环境感知 348
28.2.2 行为决策 348
28.2.3 路径规划 349
28.2.4 运动控制 349
28.3 程序实现 349
28.3.1 传感器数据载入 349
28.3.2 追踪器创建 351
28.3.3 碰撞预警 353
28.4 延伸阅读 358
第29章 基于深度学习的汽车目标检测 359
29.1 案例背景 359
29.2 理论基础 360
29.2.1 基本架构 360
29.2.2 卷积层 360
29.2.3 池化层 362
29.3 程序实现 362
29.3.1 加载数据 362
29.3.2 构建CNN 364
29.3.3 训练CNN 365
29.3.4 评估训练效果 367
29.4 延伸阅读 368
第30章 基于深度学习的视觉场景
识别 370
30.1 案例背景 370
30.2 理论基础 371
30.3 程序实现 371
30.3.1 环境配置 372
30.3.2 数据集制作 373
30.3.3 网络训练 375
30.3.4 网络测试 381
30.4 延伸阅读 383
第31章 深度学习综合应用 385
31.1 应用背景 385
31.2 理论基础 387
31.2.1 分类识别 387
31.2.2 目标检测 391
31.3 案例实现1：基于CNN的数字识别 395
31.3.1 自定义CNN 397
31.3.2 AlexNet 399
31.3.3 基于MATLAB进行实验设计 405
31.3.4 基于TensorFlow进行实验设计 413
31.3.5 实验小结 418
31.4 案例实现2：基于CNN的物体识别 418
31.4.1 CIFAR-10数据集 418
31.4.2 VggNet 421
31.4.3 ResNet 422
31.4.4 实验设计 424
31.4.5 实验小结 432
31.5 案例实现3：基于CNN的图像矫正 432
31.5.1 倾斜数据集 432
31.5.2 自定义CNN回归网络 434
31.5.3 AlexNet回归网络 436
31.5.4 实验设计 437
31.5.5 实验小结 445
31.6 案例实现4：基于LSTM的时间序列分析 445
31.6.1 厄尔尼诺南方涛动指数数据 446
31.6.2 样条拟合分析 446
31.6.3 基于MATLAB进行LSTM分析 448
31.6.4 基于Keras进行LSTM分析 451
31.6.5 实验小结 455
31.7 案例实现5：基于深度学习的以图搜图技术 455
31.7.1 人脸的深度特征 455
31.7.2 AlexNet的特征 460
31.7.3 GoogleNet的特征 461
31.7.4 深度特征融合计算 462
31.7.5 实验设计 462
31.7.6 实验小结 46731.8 案例实现6：基于YOLO的交通目标检测应用 467
31.8.1 车辆目标的YOLO检测 468
31.8.2 交通标志的YOLO检测 475
31.9 延伸阅读 481
・ ・ ・ ・ ・ ・ (收起)Summary of Contents
Foreword 23
Preface 25
About the Authors 31
1 Introduction 35
I Words
2 Regular Expressions and Automata 51
3 Words and Transducers 　　 79
4 N-Grams 117
5 Part-of-Speech Tagging 　　 157
6 Hidden Markov and Maximum Entropy Models 207
7 Phonetics 249
8 Speech Synthesis 283
9 Automatic Speech Recognition 319
10 Speech Recognition: Advanced Topics 369
11 Computational Phonology 　　 395
12 Formal Grammars of English 　 419
13 Syntactic Parsing 461
14 Statistical Parsing 493
15 Features and Uni?cation 　　 523
16 Language and Complexity 　　 563
IV Semantics and Pragmatics
17 The Representation ofMeaning　 579
18 Computational Semantics 　　 617
19 Lexical Semantics　 645
20 Computational Lexical Semantics　 671
21 Computational Discourse 　　 715
V Applications
22 Information Extraction 　　 759
23 Question Answering and Summarization 799
24 Dialogue and Conversational Agents 847
25 Machine Translation 　　 895
Bibliography 945
Author Index 995
Subject Index 1007
Contents
Foreword 23
Preface 25
About the Authors 31
1 Introduction 35
1.1 Knowledge in Speech and Language Processing 　 36
1.2 Ambiguity 38
1.3 Models andAlgorithms 39
1.4 Language, Thought, and Understanding 　　 40
1.5 TheState of theArt 42
1.6 SomeBriefHistory 43
1.6.1 Foundational Insights: 1940s and 1950s 　 43
1.6.2 The Two Camps: 1957C1970 　　 44
1.6.3 Four Paradigms: 1970C1983 　　 45
1.6.4 Empiricism and Finite-State Models Redux: 1983C1993 　 46
1.6.5 The Field Comes Together: 1994C1999　 46
1.6.6 The Rise of Machine Learning: 2000C2008 　 46
1.6.7 On Multiple Discoveries 　 47
1.6.8 A Final Brief Note on Psychology 　　 48
1.7 Summary 　 48
Bibliographical and Historical Notes 　 49
I Words
2 Regular Expressions and Automata　 51
2.1 RegularExpressions 　 51
2.1.1 Basic Regular Expression Patterns 　　 52
2.1.2 Disjunction, Grouping, and Precedence　 55
2.1.3 ASimpleExample　 56
2.1.4 A More Complex Example　 57
2.1.5 AdvancedOperators 　 58
2.1.6 Regular Expression Substitution, Memory, and ELIZA 　 59
2.2 Finite-StateAutomata 　 60
2.2.1 Use of an FSA to Recognize Sheeptalk 　 61
2.2.2 Formal Languages　 64
2.2.3 Another Example 　 65
2.2.4 Non-Deterministic FSAs . 66
2.2.5 Use of an NFSA to Accept Strings 　 67
2.2.6 Recognition as Search 69
2.2.7 Relation of Deterministic and Non-Deterministic Automata 　 72
Foreword 　 23
Preface 　 25
About the Authors　 31
1 Introduction 　 35
1.1 Knowledge in Speech and Language Processing　 36
1.2 Ambiguity 　 38
1.3 Models andAlgorithms 　 39
1.4 Language, Thought, and Understanding 　　　40
1.5 TheState of theArt . 42
1.6 SomeBriefHistory . 43
1.6.1 Foundational Insights: 1940s and 1950s 43
1.6.2 The Two Camps: 1957C1970 　　 44
1.6.3 Four Paradigms: 1970C1983 　　 45
1.6.4 Empiricism and Finite-State Models Redux: 1983C1993 46
1.6.5 The Field Comes Together: 1994C1999 46
1.6.6 The Rise of Machine Learning: 2000C2008 46
1.6.7 On Multiple Discoveries 47
1.6.8 A Final Brief Note on Psychology 　　 48
1.7 Summary 　 48
Bibliographical and Historical Notes 49
I Words
2 Regular Expressions and Automata 51
2.1 RegularExpressions 51
2.1.1 Basic Regular Expression Patterns 　　 52
2.1.2 Disjunction, Grouping, and Precedence　 55
2.1.3 ASimpleExample　 56
2.1.4 A More Complex Example 　 57
2.1.5 AdvancedOperators 　 58
2.1.6 Regular Expression Substitution, Memory, and ELIZA　 59
2.2 Finite-StateAutomata　 60
2.2.1 Use of an FSA to Recognize Sheeptalk　 61
2.2.2 Formal Languages　 64
2.2.3 Another Example 　 65
2.2.4 Non-Deterministic FSAs 　 66
2.2.5 Use of an NFSA to Accept Strings 　　 67
2.2.6 Recognition as Search　 69
2.2.7 Relation of Deterministic and Non-Deterministic Automata　 72
2.3 Regular Languages and FSAs　 72
2.4 Summary 　 75
Bibliographical and Historical Notes 76
Exercises 76
3 Words and Transducers 79
3.1 Survey of (Mostly) English Morphology 　 81
3.1.1 In?ectional Morphology 　 82
3.1.2 Derivational Morphology　 84
3.1.3 Cliticization 　 85
3.1.4 Non-Concatenative Morphology 　　 85
3.1.5 Agreement 　 86
3.2 Finite-State Morphological Parsing　 86
3.3 Construction of a Finite-State Lexicon 　　 88
3.4 Finite-StateTransducers 　 91
3.4.1 Sequential Transducers and Determinism 　 93
3.5 FSTs for Morphological Parsing 　 94
3.6 Transducers and Orthographic Rules 　　 96
3.7 The Combination of an FST Lexicon and Rules 　 99
3.8 Lexicon-Free FSTs: The Porter Stemmer 　　 102
3.9 Word and Sentence Tokenization　 102
3.9.1 Segmentation in Chinese　 104
3.10 Detection and Correction of Spelling Errors 　 106
3.11 MinimumEditDistance 　 107
3.12 Human Morphological Processing 　 111
3.13 Summary 　 113
Bibliographical and Historical Notes 　 114
Exercises 115
4 N-Grams 　 117
4.1 WordCounting inCorpora　 119
4.2 Simple (Unsmoothed) N-Grams　 120
4.3 Training andTestSets 　 125
4.3.1 N-Gram Sensitivity to the Training Corpus　 126
4.3.2 Unknown Words: Open Versus Closed Vocabulary Tasks 　 129
4.4 Evaluating N-Grams: Perplexity 　 129
4.5 Smoothing 　 131
4.5.1 LaplaceSmoothing 　 132
4.5.2 Good-Turing Discounting　 135
4.5.3 Some Advanced Issues in Good-Turing Estimation 　 136
4.6 Interpolation 　 138
4.7 Backoff 　 139
4.7.1 Advanced: Details of Computing Katz Backoff α and P 141
4.8 Practical Issues: Toolkits and Data Formats 　　 142
4.9 Advanced Issues in Language Modeling 　　 143
4.9.1 Advanced Smoothing Methods: Kneser-Ney Smoothing 　 143
4.9.2 Class-Based N-Grams　 145
4.9.3 Language Model Adaptation and Web Use　 146
4.9.4 Using Longer-Distance Information: A Brief Summary 　 146
4.10 Advanced: Information Theory Background 　　148
4.10.1 Cross-Entropy for Comparing Models 　　 150
4.11 Advanced: The Entropy of English and Entropy Rate Constancy 152
4.12 Summary 　 153
Bibliographical and Historical Notes 154
Exercises 155
5 Part-of-Speech Tagging 　 157
5.1 (Mostly) English Word Classes　 158
5.2 Tagsets forEnglish 　 164
5.3 Part-of-Speech Tagging 　 167
5.4 Rule-Based Part-of-Speech Tagging 　169
5.5 HMM Part-of-Speech Tagging　 173
5.5.1 Computing the Most Likely Tag Sequence: An Example　 176
5.5.2 Formalizing Hidden Markov Model Taggers　 178
5.5.3 Using the Viterbi Algorithm for HMM Tagging 　 179
5.5.4 Extending the HMM Algorithm to Trigrams 　 183
5.6 Transformation-Based Tagging 　 185
5.6.1 How TBL Rules Are Applied 　　 186
5.6.2 How TBL Rules Are Learned 　　 186
5.7 Evaluation and Error Analysis 　 187
5.7.1 ErrorAnalysis　 190
5.8 Advanced Issues in Part-of-Speech Tagging 　　 191
5.8.1 Practical Issues: Tag Indeterminacy and Tokenization 　 191
5.8.2 Unknown Words . 192
5.8.3 Part-of-Speech Tagging for Other Languages　 194
5.8.4 Tagger Combination 197
5.9 Advanced: The Noisy Channel Model for Spelling 　 197
5.9.1 Contextual Spelling Error Correction 　　 201
5.10 Summary 　 202
Bibliographical and Historical Notes 203
Exercises 205
6 Hidden Markov and Maximum Entropy Models 207
6.1 MarkovChains 　 208
6.2 TheHiddenMarkovModel 　 210
6.3 Likelihood Computation: The Forward Algorithm 　 213
6.4 Decoding: The Viterbi Algorithm　 218
6.5 HMM Training: The Forward-Backward Algorithm 　 220
6.6 Maximum Entropy Models: Background 　 227
6.6.1 LinearRegression 　 228
6.6.2 Logistic Regression 231
6.6.3 Logistic Regression: Classi?cation 　　233
6.6.4 Advanced: Learning in Logistic Regression 　 234
6.7 Maximum Entropy Modeling 　 235
6.7.1 Why We Call It Maximum Entropy 　　 239
6.8 Maximum Entropy Markov Models 241
6.8.1 Decoding and Learning in MEMMs 　　 244
6.9 Summary 　 245
Bibliographical and Historical Notes 246
Exercises 247
II Speech
7 Phonetics 　 249
7.1 Speech Sounds and Phonetic Transcription　 250
7.2 Articulatory Phonetics 　 251
7.2.1 TheVocalOrgans 　 252
7.2.2 Consonants: Place of Articulation 　　254
7.2.3 Consonants: Manner of Articulation 　　 255
7.2.4 Vowels 256
7.2.5 Syllables 257
7.3 Phonological Categories and Pronunciation Variation 259
7.3.1 Phonetic Features . 261
7.3.2 Predicting Phonetic Variation 　　 . 262
7.3.3 Factors In?uencing Phonetic Variation 　　 263
7.4 Acoustic Phonetics and Signals 264
7.4.1 Waves 　 264
7.4.2 Speech Sound Waves 　 265
7.4.3 Frequency and Amplitude; Pitch and Loudness 　 267
7.4.4 Interpretation of Phones from a Waveform　 270
7.4.5 Spectra and the Frequency Domain 　 270
7.4.6 The Source-Filter Model 　　274
7.5 Phonetic Resources 　 275
7.6 Advanced: Articulatory and Gestural Phonology 　 278
7.7 Summary 　 279
Bibliographical and Historical Notes　 280
Exercises 　 281
8 Speech Synthesis 　283
8.1 TextNormalization 　 285
8.1.1 Sentence Tokenization 　 285
8.1.2 Non-Standard Words 　 286
8.1.3 Homograph Disambiguation 　　290
8.2 Phonetic Analysis 　 291
8.2.1 Dictionary Lookup 　 291
8.2.2 Names 　 292
8.2.3 Grapheme-to-Phoneme Conversion 　　 293
8.3 ProsodicAnalysis 　 296
8.3.1 ProsodicStructure　 296
8.3.2 Prosodic Prominence 　 297
8.3.3 Tune 　 299
8.3.4 More Sophisticated Models: ToBI 　 300
8.3.5 Computing Duration from Prosodic Labels 　302
8.3.6 Computing F0 from Prosodic Labels 　　303
8.3.7 Final Result of Text Analysis: Internal Representation　 305
8.4 Diphone Waveform Synthesis 　 306
8.4.1 Steps for Building a Diphone Database 306
8.4.2 Diphone Concatenation and TD-PSOLA for Prosody 　308
8.5 Unit Selection (Waveform) Synthesis　 310
8.6 Evaluation 　 314
Bibliographical and Historical Notes 　 315
Exercises 　 318
9 Automatic Speech Recognition 　　319
9.1 Speech Recognition Architecture 　 321
9.2 The Hidden Markov Model Applied to Speech 　 325
9.3 Feature Extraction: MFCC Vectors　 329
9.3.1 Preemphasis　 330
9.3.2 Windowing 　 330
9.3.3 Discrete Fourier Transform 　 332
9.3.4 Mel Filter Bank and Log 　 333
9.3.5 The Cepstrum: Inverse Discrete Fourier Transform　 334
9.3.6 Deltas andEnergy　 336
9.3.7 Summary:MFCC 　 336
9.4 Acoustic Likelihood Computation　 337
9.4.1 Vector Quantization 　 337
9.4.2 GaussianPDFs 　 340
9.4.3 Probabilities, Log-Probabilities, and Distance Functions　 347
9.5 The Lexicon and Language Model 　 348
9.6 Search andDecoding 　 348
9.7 EmbeddedTraining 　 358
9.8 Evaluation: Word Error Rate 362
9.9 Summary 　 364
Bibliographical and Historical Notes 　 365
Exercises 　 367
10 Speech Recognition: Advanced Topics　 369
10.1 Multipass Decoding: N-Best Lists and Lattices 　　 369
10.2 A? (“Stack”)Decoding　 375
10.3 Context-Dependent Acoustic Models: Triphones 　 379
10.4 DiscriminativeTraining　 383
10.4.1 Maximum Mutual Information Estimation　 384
10.4.2 Acoustic Models Based on Posterior Classi?ers 385
10.5 ModelingVariation 　 386
10.5.1 Environmental Variation and Noise 　　386
10.5.2 Speaker Variation and Speaker Adaptation 　 387
10.5.3 Pronunciation Modeling: Variation Due to Genre 388
10.6 Metadata: Boundaries, Punctuation, and Dis?uencies 　 390
10.7 Speech Recognition by Humans　 392
10.8 Summary 　 393
Bibliographical and Historical Notes 　 393
Exercises 　 394
11 Computational Phonology 　 395
11.1 Finite-State Phonology 　 395
11.2 Advanced Finite-State Phonology 　 399
11.2.1 Harmony 　 399
11.2.2 Templatic Morphology　 400
11.3 Computational Optimality Theory 　 401
11.3.1 Finite-State Transducer Models of Optimality Theory 　 403
11.3.2 Stochastic Models of Optimality Theory　 404
11.4 Syllabi?cation 　 406
11.5 Learning Phonology and Morphology 　 409
11.5.1 Learning Phonological Rules 　 409
11.5.2 Learning Morphology 411
11.5.3 Learning in Optimality Theory 　　414
11.6 Summary 415
Bibliographical and Historical Notes 　 415
Exercises 417
III Syntax
12 Formal Grammars of English 419
12.1 Constituency 420
12.2 Context-FreeGrammars 421
12.2.1 Formal De?nition of Context-Free Grammar 425
12.3 Some Grammar Rules for English 　 426
12.3.1 Sentence-Level Constructions 　　426
12.3.2 Clauses and Sentences 　 428
12.3.3 The Noun Phrase　 428
12.3.4 Agreement 　 432
12.3.5 The Verb Phrase and Subcategorization　 434
12.3.6 Auxiliaries 　 436
12.3.7 Coordination　 437
12.4 Treebanks 438
12.4.1 Example: The Penn Treebank Project 　　 438
12.4.2 Treebanks as Grammars 　 440
12.4.3 Treebank Searching　 442
12.4.4 Heads and Head Finding 　443
12.5 Grammar Equivalence and Normal Form　 446
12.6 Finite-State and Context-Free Grammars 　 447
12.7 DependencyGrammars 448
12.7.1 The Relationship Between Dependencies and Heads 449
12.7.2 Categorial Grammar 451
12.8 Spoken Language Syntax 　 451
12.8.1 Dis?uencies andRepair 　 452
12.8.2 Treebanks for Spoken Language 　　453
12.9 Grammars and Human Processing 　 454
12.10 Summary 455
Bibliographical and Historical Notes 　456
Exercises 　 458
13 Syntactic Parsing 　 461
13.1 Parsing asSearch 　 462
13.1.1 Top-DownParsing 　 463
13.1.2 Bottom-UpParsing　 464
13.1.3 Comparing Top-Down and Bottom-Up Parsing 465
13.2 Ambiguity 466
13.3 Search in the Face of Ambiguity . 468
13.4 Dynamic Programming Parsing Methods 　　 469
13.4.1 CKYParsing 470
13.4.2 The Earley Algorithm 477
13.4.3 ChartParsing 482
13.5 PartialParsing . 484
13.5.1 Finite-State Rule-Based Chunking 　　 486
13.5.2 Machine Learning-Based Approaches to Chunking 486
13.5.3 Chunking-System Evaluations 　　 . 489
13.6 Summary 　490
Bibliographical and Historical Notes 　 491
Exercises 　 492
14 Statistical Parsing 　　493
14.1 Probabilistic Context-Free Grammars 　 494
14.1.1 PCFGs for Disambiguation 　 495
14.1.2 PCFGs for Language Modeling 　 497
14.2 Probabilistic CKY Parsing of PCFGs 　 498
14.3 Ways to Learn PCFG Rule Probabilities 　 501
14.4 ProblemswithPCFGs 　502
14.4.1 Independence Assumptions Miss Structural Dependencies BetweenRules　 502
14.4.2 Lack of Sensitivity to Lexical Dependencies　 503
14.5 Improving PCFGs by Splitting Non-Terminals 　 505
14.6 Probabilistic Lexicalized CFGs 　507
14.6.1 The Collins Parser 　509
14.6.2 Advanced: Further Details of the Collins Parser 　 511
14.7 EvaluatingParsers 　513
14.8 Advanced: Discriminative Reranking 　 515
14.9 Advanced: Parser-Based Language Modeling 　　 516
14.10 HumanParsing 　517
14.11 Summary 　519
Bibliographical and Historical Notes 　 520
Exercises 522
15 Features and Uni?cation　 523
15.1 FeatureStructures 　524
15.2 Uni?cation of Feature Structures 　 526
15.3 Feature Structures in the Grammar 　531
15.3.1 Agreement 　532
15.3.2 HeadFeatures 　534
15.3.3 Subcategorization 　535
15.3.4 Long-Distance Dependencies 　　 540
15.4 Implementation of Uni?cation　 541
15.4.1 Uni?cation Data Structures 　 541
15.4.2 The Uni?cationAlgorithm 　 543
15.5 Parsing with Uni?cation Constraints 　 547
15.5.1 Integration of Uni?cation into an Earley Parser 　548
15.5.2 Uni?cation-Based Parsing 　 553
15.6 Types and Inheritance 　 555
15.6.1 Advanced: Extensions to Typing 　 558
15.6.2 Other Extensions to Uni?cation 　 559
15.7 Summary 　 559
Bibliographical and Historical Notes　 560
Exercises 561
16 Language and Complexity 　 563
16.1 TheChomskyHierarchy 　 564
16.2 Ways to Tell if a Language Isn’t Regular 　　 566
16.2.1 The Pumping Lemma 567
16.2.2 Proofs that Various Natural Languages Are Not Regular　 569
16.3 Is Natural Language Context Free?　 571
16.4 Complexity and Human Processing 　 573
16.5 Summary 576
Bibliographical and Historical Notes 577
Exercises 578
17 The Representation of Meaning 579
17.1 Computational Desiderata for Representations 　 581
17.1.1 Veri?ability 581
17.1.2 Unambiguous Representations 　582
17.1.3 Canonical Form 　 583
17.1.4 Inference and Variables　 584
17.1.5 Expressiveness 　585
17.2 Model-Theoretic Semantics　 586
17.3 First-OrderLogic 　 589
17.3.1 Basic Elements of First-Order Logic 　　 589
17.3.2 Variables and Quanti?ers . 591
17.3.3 LambdaNotation . 593
17.3.4 The Semantics of First-Order Logic 　594
17.3.5 Inference 　 595
17.4 Event and State Representations　 597
17.4.1 RepresentingTime　 600
17.4.2 Aspect 　 603
17.5 DescriptionLogics 　 606
17.6 Embodied and Situated Approaches to Meaning 　 612
17.7 Summary 　 614
Bibliographical and Historical Notes 　 614
Exercises 616
18 Computational Semantics　 617
18.1 Syntax-Driven Semantic Analysis 　 617
18.2 Semantic Augmentations to Syntactic Rules 　 619
18.3 Quanti?er Scope Ambiguity and Underspeci?cation 　 626
18.3.1 Store and Retrieve Approaches 　　 626
18.3.2 Constraint-Based Approaches 　　 629
18.4 Uni?cation-Based Approaches to Semantic Analysis 　 632
18.5 Integration of Semantics into the Earley Parser 　 638
18.6 Idioms and Compositionality 　 639
18.7 Summary 　 641
Bibliographical and Historical Notes　 641
Exercises 　 643
19 Lexical Semantics 　645
19.1 WordSenses 　 646
19.2 Relations Between Senses 　 649
19.2.1 Synonymy and Antonymy 　 649
19.2.2 Hyponymy 　 650
19.2.3 SemanticFields 　 651
19.3 WordNet: A Database of Lexical Relations 　　 651
19.4 EventParticipants　 653
19.4.1 ThematicRoles 　 654
19.4.2 Diathesis Alternations 　656
19.4.3 Problems with Thematic Roles 　　 657
19.4.4 The Proposition Bank　 658
19.4.5 FrameNet 　 659
19.4.6 Selectional Restrictions 　 661
19.5 Primitive Decomposition 　 663
19.6 Advanced: Metaphor 665
19.7 Summary 　 666
Bibliographical and Historical Notes 　 667
Exercises 　 668
20 Computational Lexical Semantics 　 671
20.1 Word Sense Disambiguation: Overview 　　 672
20.2 Supervised Word Sense Disambiguation 　　 673
20.2.1 Feature Extraction for Supervised Learning　 674
20.2.2 Naive Bayes and Decision List Classi?ers 　 675
20.3 WSD Evaluation, Baselines, and Ceilings 　 678
20.4 WSD: Dictionary and Thesaurus Methods 　　680
20.4.1 The Lesk Algorithm 　 680
20.4.2 Selectional Restrictions and Selectional Preferences 　 682
20.5 Minimally Supervised WSD: Bootstrapping 　　 684
20.6 Word Similarity: Thesaurus Methods 　　 686
20.7 Word Similarity: Distributional Methods 　　 692
20.7.1 De?ning a Word’s Co-Occurrence Vectors 　 693
20.7.2 Measuring Association with Context 　　695
20.7.3 De?ning Similarity Between Two Vectors　 697
20.7.4 Evaluating Distributional Word Similarity 　 701
20.8 Hyponymy and Other Word Relations 　 701
20.9 SemanticRoleLabeling 　 704
20.10 Advanced: Unsupervised Sense Disambiguation　 708
20.11 Summary 709
Bibliographical and Historical Notes 710
Exercises 713
21 Computational Discourse　 715
21.1 DiscourseSegmentation　 718
21.1.1 Unsupervised Discourse Segmentation 　718
21.1.2 Supervised Discourse Segmentation 　　720
21.1.3 Discourse Segmentation Evaluation 　 722
21.2 TextCoherence　 723
21.2.1 Rhetorical Structure Theory 　 724
21.2.2 Automatic Coherence Assignment 　 726
21.3 ReferenceResolution 　 729
21.4 ReferencePhenomena 　 732
21.4.1 Five Types of Referring Expressions 　　 732
21.4.2 Information Status 　 734
21.5 Features for Pronominal Anaphora Resolution 　　 735
21.5.1 Features for Filtering Potential Referents　 735
21.5.2 Preferences in Pronoun Interpretation 　 736
21.6 Three Algorithms for Anaphora Resolution 　 738
21.6.1 Pronominal Anaphora Baseline: The Hobbs Algorithm 　 738
21.6.2 A Centering Algorithm for Anaphora Resolution 　 740
21.6.3 A Log-Linear Model for Pronominal Anaphora Resolution 　 742
21.6.4 Features for Pronominal Anaphora Resolution 　743
21.7 Coreference Resolution 　 744
21.8 Evaluation of Coreference Resolution 　 746
21.9 Advanced: Inference-Based Coherence Resolution 　 747
21.10 Psycholinguistic Studies of Reference 　 752
21.11 Summary 　753
Bibliographical and Historical Notes 　 754
Exercises　 756
V Applications
22 Information Extraction 　 759
22.1 Named Entity Recognition 　 761
22.1.1 Ambiguity in Named Entity Recognition 　 763
22.1.2 NER as Sequence Labeling 　 763
22.1.3 Evaluation of Named Entity Recognition　 766
22.1.4 Practical NER Architectures 　　 768
22.2 Relation Detection and Classi?cation 　　 768
22.2.1 Supervised Learning Approaches to Relation Analysis 769
22.2.2 Lightly Supervised Approaches to Relation Analysis . 772
22.2.3 Evaluation of Relation Analysis Systems . 776
22.3 Temporal and Event Processing 777
22.3.1 Temporal Expression Recognition 　　 777
22.3.2 Temporal Normalization 　 780
22.3.3 Event Detection and Analysis 　　 783
22.3.4 TimeBank 　784
22.4 Template Filling 　786
22.4.1 Statistical Approaches to Template-Filling 　 786
22.4.2 Finite-State Template-Filling Systems 　　 788
22.5 Advanced: Biomedical Information Extraction 　　 791
22.5.1 Biological Named Entity Recognition 　　 792
22.5.2 Gene Normalization　 793
22.5.3 Biological Roles and Relations 　 794
22.6 Summary 　 796
Bibliographical and Historical Notes　 796
Exercises 　 797
23 Question Answering and Summarization　 799
23.1 InformationRetrieval 　 801
23.1.1 The Vector Space Model 　 802
23.1.2 TermWeighting 　 804
23.1.3 Term Selection and Creation 　 806
23.1.4 Evaluation of Information-Retrieval Systems 806
23.1.5 Homonymy, Polysemy, and Synonymy 　 810
23.1.6 Ways to Improve User Queries 　　810
23.2 Factoid Question Answering　 812
23.2.1 Question Processing 　 813
23.2.2 PassageRetrieval　 815
23.2.3 AnswerProcessing　 817
23.2.4 Evaluation of Factoid Answers 　　 821
23.3 Summarization 　 821
23.4 Single-Document Summarization 　 824
23.4.1 Unsupervised Content Selection 　　 824
23.4.2 Unsupervised Summarization Based on Rhetorical Parsing 　 826
23.4.3 Supervised Content Selection 　　 828
23.4.4 Sentence Simpli?cation 　 829
23.5 Multi-Document Summarization　 830
23.5.1 Content Selection in Multi-Document Summarization　 831
23.5.2 Information Ordering in Multi-Document Summarization 　 832
23.6 Focused Summarization and Question Answering 　 835
23.7 Summarization Evaluation 　 839
23.8 Summary 　 841
Bibliographical and Historical Notes 　 842
Exercises 844
24 Dialogue and Conversational Agents 　847
24.1 Properties of Human Conversations 　849
24.1.1 Turns and Turn-Taking 　849
24.1.2 Language as Action: Speech Acts 　　 851
24.1.3 Language as Joint Action: Grounding 　 852
24.1.4 Conversational Structure 　 854
24.1.5 Conversational Implicature　 855
24.2 Basic Dialogue Systems 　 857
24.2.1 ASR Component　 857
24.2.2 NLU Component 　 858
24.2.3 Generation and TTS Components 　 861
24.2.4 Dialogue Manager 　 863
24.2.5 Dealing with Errors: Con?rmation and Rejection 867
24.3 VoiceXML 868
24.4 Dialogue System Design and Evaluation 　　 872
24.4.1 Designing Dialogue Systems 　　 872
24.4.2 Evaluating Dialogue Systems 　 872
24.5 Information-State and Dialogue Acts 　 874
24.5.1 Using Dialogue Acts 　 876
24.5.2 Interpreting Dialogue Acts　 877
24.5.3 Detecting Correction Acts　 880
24.5.4 Generating Dialogue Acts: Con?rmation and Rejection 　881
24.6 Markov Decision Process Architecture 　　 882
24.7 Advanced: Plan-Based Dialogue Agents 　　 886
24.7.1 Plan-Inferential Interpretation and Production　 887
24.7.2 The Intentional Structure of Dialogue 　 889
24.8 Summary 　891
Bibliographical and Historical Notes 　 892
Exercises 　 894
25 Machine Translation　 895
25.1 Why Machine Translation Is Hard 　 898
25.1.1 Typology 　 898
25.1.2 Other Structural Divergences 　　 900
25.1.3 LexicalDivergences 　 901
25.2 Classical MT and the Vauquois Triangle 903
25.2.1 Direct Translation 　 904
25.2.2 Transfer 　 906
25.2.3 Combined Direct and Transfer Approaches in Classic MT　 908
25.2.4 The Interlingua Idea: Using Meaning 　　 909
25.3 StatisticalMT 　 910
25.4 P(F|E): The Phrase-Based Translation Model　　 913
25.5 Alignment inMT 　 915
25.5.1 IBMModel 1 　 916
25.5.2 HMMAlignment 　　919
25.6 Training Alignment Models　 921
25.6.1 EM for Training Alignment Models 　　922
25.7 Symmetrizing Alignments for Phrase-Based MT　 924
25.8 Decoding for Phrase-Based Statistical MT 　　 926
25.9 MTEvaluation 　 930
25.9.1 Using Human Raters 　 930
25.9.2 Automatic Evaluation: BLEU 　　 931
25.10 Advanced: Syntactic Models for MT 　　 934
25.11 Advanced: IBM Model 3 and Fertility 　　935
25.11.1 Training forModel 3 　939
25.12 Advanced: Log-Linear Models for MT 　　 939
25.13 Summary 　940
Bibliographical and Historical Notes 　 941
Exercises 943
Bibliography 　 945
Author Index　 995
Subject Index 　 1007
・ ・ ・ ・ ・ ・ (收起)推荐序III
推荐语IV
前言V
数学符号IX
第1 章绪论1
1.1 自然语言处理的概念 2
1.2 自然语言处理的难点2
1.2.1 抽象性 2
1.2.2 组合性 2
1.2.3 歧义性 3
1.2.4 进化性3
1.2.5 非规范性3
1.2.6 主观性3
1.2.7 知识性3
1.2.8 难移植性4
1.3 自然语言处理任务体系.4
1.3.1 任务层级4
1.3.2 任务类别5
1.3.3 研究对象与层次6
1.4 自然语言处理技术发展历史7
第2 章自然语言处理基础11
2.1 文本的表示.12
2.1.1 词的独热表示13
2.1.2 词的分布式表示13
2.1.3 词嵌入表示19
2.1.4 文本的词袋表示19
2.2 自然语言处理任务20
2.2.1 语言模型20
2.2.2 自然语言处理基础任务23
2.2.3 自然语言处理应用任务31
2.3 基本问题35
2.3.1 文本分类问题35
2.3.2 结构预测问题36
2.3.3 序列到序列问题38
2.4 评价指标40
2.5 小结43
第3 章基础工具集与常用数据集45
3.1 NLTK 工具集46
3.1.1 常用语料库和词典资源46
3.1.2 常用自然语言处理工具集.49
3.2 LTP 工具集51
3.2.1 中文分词51
3.2.2 其他中文自然语言处理功能.52
3.3 PyTorch 基础52
3.3.1 张量的基本概念53
3.3.2 张量的基本运算54
3.3.3 自动微分57
3.3.4 调整张量形状58
3.3.5 广播机制59
3.3.6 索引与切片60
3.3.7 降维与升维60
3.4 大规模预训练数据61
3.4.1 维基百科数据62
3.4.2 原始数据的获取62
3.4.3 语料处理方法62
3.4.4 Common Crawl 数据66
3.5 更多数据集.66
3.6 小结68
第4 章自然语言处理中的神经网络基础69
4.1 多层感知器模型70
4.1.1 感知器70
4.1.2 线性回归71
4.1.3 Logistic 回归71
4.1.4 Softmax 回归72
4.1.5 多层感知器74
4.1.6 模型实现76
4.2 卷积神经网络78
4.2.1 模型结构78
4.2.2 模型实现80
4.3 循环神经网络83
4.3.1 模型结构83
4.3.2 长短时记忆网络85
4.3.3 模型实现87
4.3.4 基于循环神经网络的序列到序列模型88
4.4 注意力模型.89
4.4.1 注意力机制89
4.4.2 自注意力模型90
4.4.3 Transformer 91
4.4.4 基于Transformer 的序列到序列模型93
4.4.5 Transformer 模型的优缺点94
4.4.6 模型实现94
4.5 神经网络模型的训练96
4.5.1 损失函数96
4.5.2 梯度下降98
4.6 情感分类实战101
4.6.1 词表映射101
4.6.2 词向量层102
4.6.3 融入词向量层的多层感知器103
4.6.4 数据处理106
4.6.5 多层感知器模型的训练与测试108
4.6.6 基于卷积神经网络的情感分类109
4.6.7 基于循环神经网络的情感分类110
4.6.8 基于Transformer 的情感分类111
4.7 词性标注实战113
4.7.1 基于前馈神经网络的词性标注114
4.7.2 基于循环神经网络的词性标注114
4.7.3 基于Transformer 的词性标注116
4.8 小结116
第5 章静态词向量预训练模型119
5.1 神经网络语言模型120
5.1.1 预训练任务120
5.1.2 模型实现124
5.2 Word2vec 词向量130
5.2.1 概述130
5.2.2 负采样133
5.2.3 模型实现134
5.3 GloVe 词向量140
5.3.1 概述140
5.3.2 预训练任务140
5.3.3 参数估计140
5.3.4 模型实现141
5.4 评价与应用.143
5.4.1 词义相关性144
5.4.2 类比性146
5.4.3 应用147
5.5 小结148
第6 章动态词向量预训练模型151
6.1 词向量――从静态到动态152
6.2 基于语言模型的动态词向量预训练153
6.2.1 双向语言模型153
6.2.2 ELMo 词向量155
6.2.3 模型实现156
6.2.4 应用与评价169
6.3 小结171
第7 章预训练语言模型173
7.1 概述174
7.1.1 大数据174
7.1.2 大模型175
7.1.3 大算力175
7.2 GPT 177
7.2.1 无监督预训练178
7.2.2 有监督下游任务精调179
7.2.3 适配不同的下游任务180
7.3 BERT 182
7.3.1 整体结构182
7.3.2 输入表示183
7.3.3 基本预训练任务184
7.3.4 更多预训练任务190
7.3.5 模型对比194
7.4 预训练语言模型的应用194
7.4.1 概述194
7.4.2 单句文本分类195
7.4.3 句对文本分类198
7.4.4 阅读理解201
7.4.5 序列标注206
7.5 深入理解BERT .211
7.5.1 概述211
7.5.2 自注意力可视化分析212
7.5.3 探针实验213
7.6 小结.215
第8 章预训练语言模型进阶217
8.1 模型优化.218
8.1.1 XLNet 218
8.1.2 RoBERTa .223
8.1.3 ALBERT .227
8.1.4 ELECTRA 229
8.1.5 MacBERT 232
8.1.6 模型对比234
8.2 长文本处理.234
8.2.1 概述234
8.2.2 Transformer-XL 235
8.2.3 Reformer .238
8.2.4 Longformer 242
8.2.5 BigBird .243
8.2.6 模型对比244
8.3 模型蒸馏与压缩244
8.3.1 概述244
8.3.2 DistilBERT 246
8.3.3 TinyBERT 248
8.3.4 MobileBERT 250
8.3.5 TextBrewer 252
8.4 生成模型257
8.4.1 BART 257
8.4.2 UniLM 260
8.4.3 T5 .263
8.4.4 GPT-3 264
8.4.5 可控文本生成265
8.5 小结.267
第9 章多模态融合的预训练模型269
9.1 多语言融合.270
9.1.1 多语言BERT .270
9.1.2 跨语言预训练语言模型272
9.1.3 多语言预训练语言模型的应用273
9.2 多媒体融合.274
9.2.1 VideoBERT 274
9.2.2 VL-BERT 275
9.2.3 DALL・E 275
9.2.4 ALIGN 276
9.3 异构知识融合276
9.3.1 融入知识的预训练277
9.3.2 多任务学习282
9.4 更多模态的预训练模型285
9.5 小结.285
参考文献287
术语表297
・ ・ ・ ・ ・ ・ (收起)第1章 新手上路 1
1.1 自然语言与编程语言 2
1.1.1 词汇量 2
1.1.2 结构化 2
1.1.3 歧义性 3
1.1.4 容错性 3
1.1.5 易变性 4
1.1.6 简略性 4
1.2 自然语言处理的层次 4
1.2.1 语音、图像和文本 5
1.2.2 中文分词、词性标注和命名实体识别 5
1.2.3 信息抽取 6
1.2.4 文本分类与文本聚类 6
1.2.5 句法分析 6
1.2.6 语义分析与篇章分析 7
1.2.7 其他高级任务 7
1.3 自然语言处理的流派 8
1.3.1 基于规则的专家系统 8
1.3.2 基于统计的学习方法 9
1.3.3 历史 9
1.3.4 规则与统计 11
1.3.5 传统方法与深度学习 11
1.4 机器学习 12
1.4.1 什么是机器学习 13
1.4.2 模型 13
1.4.3 特征 13
1.4.4 数据集 15
1.4.5 监督学习 16
1.4.6 无监督学习 17
1.4.7 其他类型的机器学习算法 18
1.5 语料库 19
1.5.1 中文分词语料库 19
1.5.2 词性标注语料库 19
1.5.3 命名实体识别语料库 20
1.5.4 句法分析语料库 20
1.5.5 文本分类语料库 20
1.5.6 语料库建设 21
1.6 开源工具 21
1.6.1 主流NLP工具比较 21
1.6.2 Python接口 23
1.6.3 Java接口 28
1.7 总结 31
第2章 词典分词 32
2.1 什么是词 32
2.1.1 词的定义 32
2.1.2 词的性质--齐夫定律 33
2.2 词典 34
2.2.1 HanLP词典 34
2.2.2 词典的加载 34
2.3 切分算法 36
2.3.1 完全切分 36
2.3.2 正向最长匹配 37
2.3.3 逆向最长匹配 39
2.3.4 双向最长匹配 40
2.3.5 速度评测 43
2.4 字典树 46
2.4.1 什么是字典树 46
2.4.2 字典树的节点实现 47
2.4.3 字典树的增删改查实现 48
2.4.4 首字散列其余二分的字典树 50
2.4.5 前缀树的妙用 53
2.5 双数组字典树 55
2.5.1 双数组的定义 55
2.5.2 状态转移 56
2.5.3 查询 56
2.5.4 构造* 57
2.5.5 全切分与最长匹配 60
2.6 AC自动机 60
2.6.1 从字典树到AC自动机 61
2.6.2 goto表 61
2.6.3 output表 62
2.6.4 fail表 63
2.6.5 实现 65
2.7 基于双数组字典树的AC自动机 67
2.7.1 原理 67
2.7.2 实现 67
2.8 HanLP的词典分词实现 71
2.8.1 DoubleArrayTrieSegment 72
2.8.2 AhoCorasickDoubleArrayTrie-Segment 73
2.9 准确率评测 74
2.9.1 准确率 74
2.9.2 混淆矩阵与TP/FN/FP/TN 75
2.9.3 精确率 76
2.9.4 召回率 76
2.9.5 F1值 77
2.9.6 中文分词中的P、R、F1计算 77
2.9.7 实现 78
2.9.8 第二届国际中文分词评测 79
2.9.9 OOVRecallRate与IVRecallRate 81
2.10 字典树的其他应用 83
2.10.1 停用词过滤 83
2.10.2 简繁转换 87
2.10.3 拼音转换 90
2.11 总结 91
第3章 二元语法与中文分词 92
3.1 语言模型 92
3.1.1 什么是语言模型 92
3.1.2 马尔可夫链与二元语法 94
3.1.3 n元语法 95
3.1.4 数据稀疏与平滑策略 96
3.2 中文分词语料库 96
3.2.11998 年《人民日报》语料库PKU 97
3.2.2 微软亚洲研究院语料库MSR 98
3.2.3 繁体中文分词语料库 98
3.2.4 语料库统计 99
3.3 训练 100
3.3.1 加载语料库 101
3.3.2 统计一元语法 101
3.3.3 统计二元语法 103
3.4 预测 104
3.4.1 加载模型 104
3.4.2 构建词网 107
3.4.3 节点间的距离计算 111
3.4.4 词图上的维特比算法 112
3.4.5 与用户词典的集成 115
3.5 评测 118
3.5.1 标准化评测 118
3.5.2 误差分析 118
3.5.3 调整模型 119
3.6 日语分词 122
3.6.1 日语分词语料 122
3.6.2 训练日语分词器 123
3.7 总结 124
第4章 隐马尔可夫模型与序列标注 125
4.1 序列标注问题 125
4.1.1 序列标注与中文分词 126
4.1.2 序列标注与词性标注 127
4.1.3 序列标注与命名实体识别 128
4.2 隐马尔可夫模型 129
4.2.1 从马尔可夫假设到隐马尔可夫模型 129
4.2.2 初始状态概率向量 130
4.2.3 状态转移概率矩阵 131
4.2.4 发射概率矩阵 132
4.2.5 隐马尔可夫模型的三个基本用法 133
4.3 隐马尔可夫模型的样本生成 133
4.3.1 案例--医疗诊断 133
4.3.2 样本生成算法 136
4.4 隐马尔可夫模型的训练 138
4.4.1 转移概率矩阵的估计 138
4.4.2 初始状态概率向量的估计 139
4.4.3 发射概率矩阵的估计 140
4.4.4 验证样本生成与模型训练 141
4.5 隐马尔可夫模型的预测 142
4.5.1 概率计算的前向算法 142
4.5.2 搜索状态序列的维特比算法 143
4.6 隐马尔可夫模型应用于中文分词 147
4.6.1 标注集 148
4.6.2 字符映射 149
4.6.3 语料转换 150
4.6.4 训练 151
4.6.5 预测 152
4.6.6 评测 153
4.6.7 误差分析 154
4.7 二阶隐马尔可夫模型* 154
4.7.1 二阶转移概率张量的估计 155
4.7.2 二阶隐马尔可夫模型中的维特比算法 156
4.7.3 二阶隐马尔可夫模型应用于中文分词 158
4.8 总结 159
第5章 感知机分类与序列标注 160
5.1 分类问题 160
5.1.1 定义 160
5.1.2 应用 161
5.2 线性分类模型与感知机算法 161
5.2.1 特征向量与样本空间 162
5.2.2 决策边界与分离超平面 164
5.2.3 感知机算法 167
5.2.4 损失函数与随机梯度下降* 169
5.2.5 投票感知机和平均感知机 171
5.3 基于感知机的人名性别分类 174
5.3.1 人名性别语料库 174
5.3.2 特征提取 174
5.3.3 训练 175
5.3.4 预测 176
5.3.5 评测 177
5.3.6 模型调优 178
5.4 结构化预测问题 180
5.4.1 定义 180
5.4.2 结构化预测与学习的流程 180
5.5 线性模型的结构化感知机算法 180
5.5.1 结构化感知机算法 180
5.5.2 结构化感知机与序列标注 182
5.5.3 结构化感知机的维特比解码算法 183
5.6 基于结构化感知机的中文分词 186
5.6.1 特征提取 187
5.6.2 多线程训练 189
5.6.3 特征裁剪与模型压缩* 190
5.6.4 创建感知机分词器 192
5.6.5 准确率与性能 194
5.6.6 模型调整与在线学习* 195
5.6.7 中文分词特征工程* 197
5.7 总结 199
第6章 条件随机场与序列标注 200
6.1 机器学习的模型谱系 200
6.1.1 生成式模型与判别式模型 201
6.1.2 有向与无向概率图模型 202
6.2 条件随机场 205
6.2.1 线性链条件随机场 205
6.2.2 条件随机场的训练* 207
6.2.3 对比结构化感知机 210
6.3 条件随机场工具包 212
6.3.1 CRF++的安装 212
6.3.2 CRF++语料格式 213
6.3.3 CRF++特征模板 214
6.3.4 CRF++命令行训练 215
6.3.5 CRF++模型格式* 216
6.3.6 CRF++命令行预测 217
6.3.7 CRF++代码分析* 218
6.4 HanLP中的CRF++API 220
6.4.1 训练分词器 220
6.4.2 标准化评测 220
6.5 总结 221
第7章 词性标注 222
7.1 词性标注概述 222
7.1.1 什么是词性 222
7.1.2 词性的用处 223
7.1.3 词性标注 223
7.1.4 词性标注模型 223
7.2 词性标注语料库与标注集 224
7.2.1 《人民日报》语料库与PKU标注集 225
7.2.2 国家语委语料库与863标注集 231
7.2.3 《诛仙》语料库与CTB标注集 234
7.3 序列标注模型应用于词性标注 236
7.3.1 基于隐马尔可夫模型的词性标注 237
7.3.2 基于感知机的词性标注 238
7.3.3 基于条件随机场的词性标注 240
7.3.4 词性标注评测 241
7.4 自定义词性 242
7.4.1 朴素实现 242
7.4.2 标注语料 243
7.5 总结 244
第8章 命名实体识别 245
8.1 概述 245
8.2 基于规则的命名实体识别 246
8.3 命名实体识别语料库 250
8.4 基于层叠隐马尔可夫模型的角色标注框架 252
8.5 基于序列标注的命名实体识别 260
8.6 自定义领域命名实体识别 266
8.7 总结 268
第9章 信息抽取 270
9.1 新词提取 270
9.2 关键词提取 276
9.3 短语提取 283
9.4 关键句提取 284
9.5 总结 287
第10章 文本聚类 288
10.1 概述 288
10.2 文档的特征提取 291
10.3 k均值算法 293
10.4 重复二分聚类算法 300
10.5 标准化评测 303
10.6 总结 305
第11章 文本分类 306
11.1 文本分类的概念 306
11.2 文本分类语料库 307
11.3 文本分类的特征提取 308
11.4 朴素贝叶斯分类器 312
11.5 支持向量机分类器 317
11.6 标准化评测 320
11.7 情感分析 321
11.8 总结 323
第12章 依存句法分析 324
12.1 短语结构树 324
12.1.3 宾州树库和中文树库 326
12.2 依存句法树 327
12.3 依存句法分析 333
12.4 基于转移的依存句法分析 334
12.5 依存句法分析API 340
12.6 案例：基于依存句法树的意见抽取 342
12.7 总结 344
第13章 深度学习与自然语言处理 345
13.1 传统方法的局限 345
13.2 深度学习与优势 348
13.3 word2vec 353
13.4 基于神经网络的高性能依存句法分析器 360
13.5 自然语言处理进阶 363
自然语言处理学习资料推荐 365
・ ・ ・ ・ ・ ・ (收起)扉页
版权声明
内容提要
译者简介
译者序
序
前言
致谢
关于本书
关于作者
关于封面插画
资源与支持
目录
第一部分 处理文本的机器
第1章 NLP概述
1.1 自然语言与编程语言
1.2 神奇的魔法
1.2.1 会交谈的机器
1.2.2 NLP中的数学
1.3 实际应用
1.4 计算机“眼”中的语言
1.4.1 锁的语言（正则表达式）
1.4.2 正则表达式
1.4.3 一个简单的聊天机器人
1.4.4 另一种方法
1.5 超空间简述
1.6 词序和语法
1.7 聊天机器人的自然语言流水线
1.8 深度处理
1.9 自然语言智商
1.10 小结
第2章 构建自己的词汇表――分词
2.1 挑战（词干还原预览）
2.2 利用分词器构建词汇表
2.2.1 点积
2.2.2 度量词袋之间的重合度
2.2.3 标点符号的处理
2.2.4 将词汇表扩展到 n-gram
2.2.5 词汇表归一化
2.3 情感
2.3.1 VADER：一个基于规则的情感分析器
2.3.2 朴素贝叶斯
2.4 小结
第3章 词中的数学
3.1 词袋
3.2 向量化
向量空间
3.3 齐普夫定律
3.4 主题建模
3.4.1 回到齐普夫定律
3.4.2 相关度排序
3.4.3 工具
3.4.4 其他工具
3.4.5 Okapi BM25
3.4.6 未来展望
3.5 小结
第4章 词频背后的语义
4.1 从词频到主题得分
4.1.1 TF-IDF向量及词形归并
4.1.2 主题向量
4.1.3 思想实验
4.1.4 一个主题评分算法
4.1.5 一个 LDA分类器
4.2 潜在语义分析
思想实验的实际实现
4.3 奇异值分解
4.3.1 左奇异向量 U
4.3.2 奇异值向量 S
4.3.3 右奇异向量 V
4.3.4 SVD矩阵的方向
4.3.5 主题约简
4.4 主成分分析
4.4.1 三维向量上的 PCA
4.4.2 回归 NLP
4.4.3 基于 PCA的短消息语义分析
4.4.4 基于截断的 SVD的短消息语义分析
4.4.5 基于 LSA的垃圾短消息分类的效果
4.5 潜在狄利克雷分布（LDiA）
4.5.1 LDiA思想
4.5.2 基于 LDiA主题模型的短消息语义分析
4.5.3 LDiA+LDA=垃圾消息过滤器
4.5.4 更公平的对比：32 个 LDiA主题
4.6 距离和相似度
4.7 反馈及改进
线性判别分析
4.8 主题向量的威力
4.8.1 语义搜索
4.8.2 改进
4.9 小结
第二部分 深度学习（神经网络）
第5章 神经网络初步（感知机与反向传播）
5.1 神经网络的组成
5.2 小结
第6章 词向量推理（Word2vec）
6.1 语义查询与类比
类比问题
6.2 词向量
6.2.1 面向向量的推理
6.2.2 如何计算 Word2vec 表示
6.2.3 如何使用 gensim.word2vec 模块
6.2.4 生成定制化词向量表示
6.2.5 Word2vec 和 GloVe
6.2.6 fastText
6.2.7 Word2vec 和 LSA
6.2.8 词关系可视化
6.2.9 非自然词
6.2.10 利用 Doc2vec 计算文档相似度
6.3 小结
第7章 卷积神经网络（CNN）
7.1 语义理解
7.2 工具包
7.3 卷积神经网络
7.3.1 构建块
7.3.2 步长
7.3.3 卷积核的组成
7.3.4 填充
7.3.5 学习
7.4 狭窄的窗口
7.4.1 Keras 实现：准备数据
7.4.2 卷积神经网络架构
7.4.3 池化
7.4.4 dropout
7.4.5 输出层
7.4.6 开始学习（训练）
7.4.7 在流水线中使用模型
7.4.8 前景展望
7.5 小结
第8章 循环神经网络（RNN）
8.1 循环网络的记忆功能
8.2 整合各个部分
8.3 自我学习
8.4 超参数
8.5 预测
8.5.1 有状态性
8.5.2 双向 RNN
8.5.3 编码向量
8.6 小结
第9章 改进记忆力：长短期记忆网络（LSTM）
9.1 长短期记忆（LSTM）
9.1.1 随时间反向传播
9.1.2 模型的使用
9.1.3 脏数据
9.1.4 “未知”词条的处理
9.1.5 字符级建模
9.1.6 生成聊天文字
9.1.7 进一步生成文本
9.1.8 文本生成的问题：内容不受控
9.1.9 其他记忆机制
9.1.10 更深的网络
9.2 小结
第10章 序列到序列建模和注意力机制
10.1 编码-解码架构
10.1.1 解码思想
10.1.2 似曾相识？
10.1.3 序列到序列对话
10.1.4 回顾 LSTM
10.2 组装一个序列到序列的流水线
10.2.1 为序列到序列训练准备数据集
10.2.2 Keras 中的序列到序列模型
10.2.3 序列编码器
10.2.4 思想解码器
10.2.5 组装一个序列到序列网络
10.3 训练序列到序列网络
生成输出序列
10.4 使用序列到序列网络构建一个聊天机器人
10.4.1 为训练准备语料库
10.4.2 建立字符字典
10.4.3 生成独热编码训练集
10.4.4 训练序列到序列聊天机器人
10.4.5 组装序列生成模型
10.4.6 预测输出序列
10.4.7 生成回复
10.4.8 与聊天机器人交谈
10.5 增强
10.5.1 使用装桶法降低训练复杂度
10.5.2 注意力机制
10.6 实际应用
10.7 小结
第三部分 进入现实世界（现实中的 NLP 挑战）
第11章 信息提取（命名实体识别与问答系统）
11.1 命名实体与关系
11.1.1 知识库
11.1.2 信息提取
11.2 正则模式
11.2.1 正则表达式
11.2.2 把信息提取当作机器学习里的特征提取任务
11.3 值得提取的信息
11.3.1 提取 GPS位置
11.3.2 提取日期
11.4 提取人物关系（事物关系）
11.4.1 词性标注
11.4.2 实体名称标准化
11.4.3 实体关系标准化和提取
11.4.4 单词模式
11.4.5 文本分割
11.4.6 为什么 split('.!?')函数不管用
11.4.7 使用正则表达式进行断句
11.5 现实世界的信息提取
11.6 小结
第12章 开始聊天（对话引擎）
12.1 语言技能
12.1.1 现代方法
12.1.2 混合方法
12.2 模式匹配方法
12.2.1 基于 AIML 的模式匹配聊天机器人
12.2.2 模式匹配的网络视图
12.3 知识方法
12.4 检索（搜索）方法
12.4.1 上下文挑战
12.4.2 基于示例检索的聊天机器人
12.4.3 基于搜索的聊天机器人
12.5 生成式方法
12.5.1 聊聊 NLPIA
12.5.2 每种方法的利弊
12.6 四轮驱动
Will的成功
12.7 设计过程
12.8 技巧
12.8.1 用带有可预测答案的问题提问
12.8.2 要有趣
12.8.3 当其他所有方法都失败时，搜索
12.8.4 变得受欢迎
12.8.5 成为连接器
12.8.6 变得有情感
12.9 现实世界
12.10 小结
第13章 可扩展性（优化、并行化和批处理）
13.1 太多（数据）未必是好事
13.2 优化NLP算法
13.2.1 索引
13.2.2 高级索引
13.2.3 基于 Annoy 的高级索引
13.2.4 究竟为什么要使用近似索引
13.2.5 索引变通方法：离散化
13.3 常数级内存算法
13.3.1 gensim
13.3.2 图计算
13.4 并行化NLP计算
13.4.1 在 GPU上训练 NLP模型
13.4.2 租与买
13.4.3 GPU租赁选择
13.4.4 张量处理单元 TPU
13.5 减少模型训练期间的内存占用
13.6 使用TensorBoard 了解模型
如何可视化词嵌入
13.7 小结
附录A 本书配套的NLP工具
附录B 有趣的Python和正则表达式
附录C 向量和矩阵（线性代数基础）
附录D 机器学习常见工具与技术
附录E 设置亚马逊云服务（ AWS）上的GPU
附录F 局部敏感哈希
资源
词汇表
・ ・ ・ ・ ・ ・ (收起)第1章导论
1.1语音与语言处理中的知识
1.2歧义
1.3模型和算法
1.4语言、思维和理解
1.5学科现状与近期发展
1.6语音和语言处理简史
1.6.1基础研究：20世纪40年代和20世纪50年代
1.6.2两个阵营：1957年至1970年
1.6.3四个范型：1970年至1983年
1.6.4经验主义和有限状态模型的复苏：1983年至1993年
1.6.5不同领域的合流：1994年至1999年
1.6.6机器学习的兴起：2000年至2008年
1.6.7关于多重发现
1.6.8心理学的简要注记
1.7小结
1.8文献和历史说明
第一部分词汇的计算机处理
第2章正则表达式与自动机
2.1正则表达式
2.1.1基本正则表达式模式
2.1.2析取、组合与优先关系
2.1.3一个简单的例子
2.1.4一个比较复杂的例子
2.1.5高级算符
2.1.6正则表达式中的替换、存储器与ELIZA
2.2有限状态自动机
2.2.1用FSA来识别羊的语言
2.2.2形式语言
2.2.3其他例子
2.2.4非确定FSA
2.2.5使用NFSA接收符号串
2.2.6识别就是搜索
2.2.7确定自动机与非确定自动机的关系
2.3正则语言与FSA
2.4小结
2.5文献和历史说明
第3章词与转录机
3.1英语形态学概观
3.1.1屈折形态学
3.1.2派生形态学
3.1.3附着
3.1.4非毗连形态学
3.1.5一致关系
3.2有限状态形态剖析
3.3有限状态词表的建造
3.4有限状态转录机
3.4.1定序转录机和确定性
3.5用于形态剖析的FST
3.6转录机和正词法规则
3.7把FST词表与规则相结合
3.8与词表无关的FST：Porter词干处理器
3.9单词和句子的词例还原
3.9.1中文的自动切词
3.10拼写错误的检查与更正
3.11最小编辑距离
3.12人是怎样进行形态处理的
3.13小结
3.14文献和历史说明
第4章N元语法
4.1语料库中单词数目的计算
4.2简单的（非平滑的）N元语法
4.3训练集和测试集
4.3.1N元语法及其对训练语料库的敏感性
4.3.2未知词：开放词汇与封闭词汇
4.4N元语法的评测：困惑度
4.5平滑
4.5.1Laplace平滑
4.5.2GoodTuring打折法
4.5.3GoodTuring估计的一些高级专题
4.6插值法
4.7回退法
4.7.1高级专题：计算Katz回退的α和P*
4.8实际问题：工具包和数据格式
4.9语言模型建模中的高级专题
4.9.1高级的平滑方法：KneserNey平滑法
4.9.2基于类别的N元语法
4.9.3语言模型的自适应和网络（Web）应用
4.9.4长距离信息的使用：简要的综述
4.10信息论背景
4.10.1用于比较模型的交叉熵
4.11高级问题：英语的熵和熵率均衡性
4.12小结
4.13文献和历史说明
第5章词类标注
5.1（大多数）英语词的分类
5.2英语的标记集
5.3词类标注
5.4基于规则的词类标注
5.5基于隐马尔可夫模型的词类标注
5.5.1计算最可能的标记序列：一个实例
5.5.2隐马尔可夫标注算法的形式化
5.5.3使用Viterbi算法来进行HMM标注
5.5.4把HMM扩充到三元语法
5.6基于转换的标注
5.6.1怎样应用TBL规则
5.6.2怎样学习TBL规则
5.7评测和错误分析
5.7.1错误分析
5.8词类标注中的高级专题
5.8.1实际问题：标记的不确定性与词例还原
5.8.2未知词
5.8.3其他语言中的词类标注
5.8.4标注算法的结合
5.9高级专题：拼写中的噪声信道模型
5.9.1上下文错拼更正
5.10小结
5.11文献和历史说明
第6章隐马尔可夫模型与最大熵模型
6.1马尔可夫链
6.2隐马尔可夫模型
6.3似然度的计算：向前算法
6.4解码：Viterbi算法
6.5HMM的训练：向前向后算法
6.6最大熵模型：背景
6.6.1线性回归
6.6.2逻辑回归
6.6.3逻辑回归：分类
6.6.4高级专题：逻辑回归的训练
6.7最大熵模型
6.7.1为什么称为最大熵
6.8最大熵马尔可夫模型
6.8.1MEMM的解码和训练
6.9小结
6.10文献和历史说明
第二部分语音的计算机处理
第7章语音学
7.1言语语音与语音标音法
7.2发音语音学
7.2.1发音器官
7.2.2辅音：发音部位
7.2.3辅音：发音方法
7.2.4元音
7.2.5音节
7.3音位范畴与发音变异
7.3.1语音特征
7.3.2语音变异的预测
7.3.3影响语音变异的因素
7.4声学语音学和信号
7.4.1波
7.4.2语音的声波
7.4.3频率与振幅：音高和响度
7.4.4从波形来解释音子
7.4.5声谱和频域
7.4.6声源滤波器模型
7.5语音资源
7.6高级问题：发音音系学与姿态音系学
7.7小结
7.8文献和历史说明
第8章语音合成
8.1文本归一化
8.1.1句子的词例还原
8.1.2非标准词
8.1.3同形异义词的排歧
8.2语音分析
8.2.1查词典
8.2.2名称
8.2.3字位―音位转换
8.3韵律分析
8.3.1韵律的结构
8.3.2韵律的突显度
8.3.3音调
8.3.4更精巧的模型：ToBI
8.3.5从韵律标记计算音延
8.3.6从韵律标记计算F0
8.3.7文本分析的最后结果：内部表示
8.4双音子波形合成
8.4.1建立双音子数据库的步骤
8.4.2双音子毗连和用于韵律的TD―PSOLA
8.5单元选择（波形）合成
8.6评测
8.7文献和历史说明
第9章语音自动识别
9.1语音识别的总体结构
9.2隐马尔可夫模型应用于语音识别
9.3特征抽取：MFCC矢量
9.3.1预加重
9.3.2加窗
9.3.3离散傅里叶变换
9.3.4Mel滤波器组和对数
9.3.5倒谱：逆向傅里叶变换
9.3.6Delta特征与能量
9.3.7总结：MFCC
9.4声学似然度的计算
9.4.1矢量量化
9.4.2高斯概率密度函数
9.4.3概率、对数概率和距离函数
9.5词典和语言模型
9.6搜索与解码
9.7嵌入式训练
9.8评测：词错误率
9.9小结
9.10文献和历史说明
第10章语音识别：高级专题
10.1多遍解码：N最佳表和格
10.2A*解码算法（“栈”解码算法）
10.3依赖于上下文的声学模型：三音子
10.4分辨训练
10.4.1最大互信息估计
10.4.2基于后验分类器的声学模型
10.5语音变异的建模
10.5.1环境语音变异和噪声
10.5.2说话人变异和说话人适应
10.5.3发音建模：由于语类的差别而产生的变异
10.6元数据：边界、标点符号和不流利现象
10.7人的语音识别
10.8小结
10.9文献和历史说明
第11章计算音系学
11.1有限状态音系学
11.2高级有限状态音系学
11.2.1元音和谐
11.2.2模板式形态学
11.3计算优选理论
11.3.1优选理论中的有限状态转录机模型
11.3.2优选理论的随机模型
11.4音节切分
11.5音位规则和形态规则的机器学习
11.5.1音位规则的机器学习
11.5.2形态规则的机器学习
11.5.3优选理论中的机器学习
11.6小结
11.7文献和历史说明
第三部分句法的计算机处理
第12章英语的形式语法
12.1组成性
12.2上下文无关语法
12.2.1上下文无关语法的形式定义
12.3英语的一些语法规则
12.3.1句子一级的结构
12.3.2子句与句子
12.3.3名词短语
12.3.4一致关系
12.3.5动词短语和次范畴化
12.3.6助动词
12.3.7并列关系
12.4树库
12.4.1树库的例子：宾州树库课题
12.4.2作为语法的树库
12.4.3树库搜索
12.4.4中心词与中心词的发现
12.5语法等价与范式
12.6有限状态语法和上下文无关语法
12.7依存语法
12.7.1依存和中心词之间的关系
12.7.2范畴语法
12.8口语的句法
12.8.1不流畅现象与口语修正
12.8.2口语树库
12.9语法和人的语言处理
12.10小结
12.11文献和历史说明
第13章句法剖析
13.1剖析就是搜索
13.1.1自顶向下剖析
13.1.2自底向上剖析
13.1.3自顶向下剖析与自底向上剖析比较
13.2歧义
13.3面对歧义的搜索
13.4动态规划剖析方法
13.4.1CKY剖析
13.4.2Earley算法
13.4.3线图剖析
13.5局部剖析
13.5.1基于规则的有限状态组块分析
13.5.2基于机器学习的组块分析方法
13.5.3组块分析系统的评测
13.6小结
13.7文献和历史说明
第14章统计剖析
14.1概率上下文无关语法
14.1.1PCFG用于排歧
14.1.2PCFG用于语言建模
14.2PCFG的概率CKY剖析
14.3PCFG规则概率的学习途径
14.4PCFG的问题
14.4.1独立性假设忽略了规则之间的结构依存关系
14.4.2缺乏对词汇依存关系的敏感性
14.5使用分离非终极符号的办法来改进PCFG
14.6概率词汇化的CFG
14.6.1Collins剖析器
14.6.2高级问题：Collins剖析器更多的细节
14.7剖析器的评测
14.8高级问题：分辨再排序
14.9高级问题：基于剖析器的语言模型
14.10人的剖析
14.11小结
14.12文献和历史说明
第15章特征与合一
15.1特征结构
15.2特征结构的合一
15.3语法中的特征结构
15.3.1一致关系
15.3.2中心语特征
15.3.3次范畴化
15.3.4长距离依存关系
15.4合一的实现
15.4.1合一的数据结构
15.4.2合一算法
15.5带有合一约束的剖析
15.5.1把合一结合到Earley剖析器中
15.5.2基于合一的剖析
15.6类型与继承
15.6.1高级问题：类型的扩充
15.6.2合一的其他扩充
15.7小结
15.8文献和历史说明
第16章语言和复杂性
16.1Chomsky层级
16.2怎么判断一种语言不是正则的
16.2.1抽吸引理
16.2.2证明各种自然语言不是正则语言
16.3自然语言是上下文无关的吗
16.4计算复杂性和人的语言处理
16.5小结
16.6文献和历史说明
第四部分语义和语用的计算机处理
第17章意义的表示
17.1意义表示的计算要求
17.1.1可验证性
17.1.2无歧义性
17.1.3规范形式
17.1.4推理与变量
17.1.5表达能力
17.2模型论语义学
17.3一阶逻辑
17.3.1一阶逻辑基础
17.3.2变量和量词
17.3.3λ表示法
17.3.4一阶逻辑的语义
17.3.5推理
17.4事件与状态的表示
17.4.1时间表示
17.4.2体
17.5描述逻辑
17.6意义的具体化与情境表示方法
17.7小结
17.8文献和历史说明
第18章计算语义学
18.1句法驱动的语义分析
18.2句法规则的语义扩充
18.3量词辖域歧义及非确定性
18.3.1存储与检索方法
18.3.2基于约束的方法
18.4基于合一的语义分析方法
18.5语义与Earley分析器的集成
18.6成语和组成性
18.7小结
18.8文献和历史说明
第19章词汇语义学
19.1词义
19.2含义间的关系
19.2.1同义关系和反义关系
19.2.2上下位关系
19.2.3语义场
19.3WordNet：词汇关系信息库
19.4事件参与者
19.4.1题旨角色
19.4.2因素交替（DiathesisAlternations）
19.4.3题旨角色的问题
19.4.4命题库
19.4.5FrameNet
19.4.6选择限制
19.5基元分解
19.6高级问题：隐喻
19.7小结
19.8文献和历史说明
第20章计算词汇语义学
20.1词义排歧：综述
20.2有监督词义排歧
20.2.1监督学习的特征抽取
20.2.2朴素贝叶斯分类器和决策表分类器
20.3WSD评价方法、基准线和上限
20.4WSD：字典方法和同义词库方法
20.4.1Lesk算法
20.4.2选择限制和选择优先度
20.5最低限度的监督WSD：自举法
20.6词语相似度：语义字典方法
20.7词语相似度：分布方法
20.7.1定义词语的共现向量
20.7.2度量与上下文的联系
20.7.3定义两个向量之间的相似度
20.7.4评价分布式词语相似度
20.8下位关系和其他词语关系
20.9语义角色标注
20.10高级主题：无监督语义排歧
20.11小结
20.12文献和历史说明
第21章计算话语学
21.1话语分割
21.1.1无监督话语分割
21.1.2有监督话语分割
21.1.3话语分割的评价
21.2文本连贯性
21.2.1修辞结构理论
21.2.2自动连贯指派
21.3指代消解
21.4指代现象
21.4.1指示语的五种类型
21.4.2信息状态
21.5代词指代消解所使用的特征
21.5.1用来过滤潜在指代对象的特征
21.5.2代词解释中的优先关系
21.6指代消解的三种算法
21.6.1代词指代基准系统：Hobbs算法
21.6.2指代消解的中心算法
21.6.3代词指代消解的对数线性模型
21.6.4代词指代消解的特征
21.7共指消解
21.8共指消解的评价
21.9高级问题：基于推理的连贯判定
21.10所指的心理语言学研究
21.11小结
21.12文献和历史说明
第五部分应用
第22章信息抽取
22.1命名实体识别
22.1.1命名实体识别中的歧义
22.1.2基于序列标注的命名实体识别
22.1.3命名实体识别的评价
22.1.4实用NER架构
22.2关系识别和分类
22.2.1用于关系分析的有监督学习方法
22.2.2用于关系分析的弱监督学习方法
22.2.3关系分析系统的评价
22.3时间和事件处理
22.3.1时间表达式的识别
22.3.2时间的归一化
22.3.3事件检测和分析
22.3.4TimeBank
22.4模板填充
22.4.1模板填充的统计方法
22.4.2有限状态机模板填充系统
22.5高级话题：生物医学信息的抽取
22.5.1生物学命名实体识别
22.5.2基因归一化
22.5.3生物学角色和关系
22.6小结
22.7文献和历史说明
第23章问答和摘要
23.1信息检索
23.1.1向量空间模型
23.1.2词语权重计算
23.1.3词语选择和建立
23.1.4信息检索系统的评测
23.1.5同形关系、多义关系和同义关系
23.1.6改进用户查询的方法
23.2事实性问答
23.2.1问题处理
23.2.2段落检索
23.2.3答案处理
23.2.4事实性答案的评价
23.3摘要
23.4单文档摘要
23.4.1无监督的内容选择
23.4.2基于修辞分析的无监督摘要
23.4.3有监督的内容选择
23.4.4句子简化
23.5多文档摘要
23.5.1多文档摘要的内容选择
23.5.2多文档摘要的信息排序
23.6主题摘要和问答
23.7摘要的评价
23.8小结
23.9文献和历史说明
第24章对话与会话智能代理
24.1人类会话的属性
24.1.1话轮和话轮转换
24.1.2语言作为行动：言语行为
24.1.3语言作为共同行动：对话的共同基础
24.1.4会话结构
24.1.5会话隐含
24.2基本的对话系统
24.2.1ASR组件
24.2.2NLU组件
24.2.3生成和TTS组件
24.2.4对话管理器
24.2.5错误处理：确认和拒绝
24.3VoiceXML
24.4对话系统的设计和评价
24.4.1设计对话系统
24.4.2评价对话系统
24.5信息状态和对话行为
24.5.1使用对话行为
24.5.2解释对话行为
24.5.3检测纠正行为
24.5.4生成对话行为：确认和拒绝
24.6马尔可夫决策过程架构
24.7高级问题：基于规划的对话行为
24.7.1规划推理解释和生成
24.7.2对话的意图结构
24.8小结
24.9文献和历史说明
第25章机器翻译
25.1为什么机器翻译如此困难
25.1.1类型学
25.1.2其他的结构差异
25.1.3词汇的差异
25.2经典的机器翻译方法与Vauquois三角形
25.2.1直接翻译
25.2.2转换方法
25.2.3传统机器翻译系统中的直接和转换相融合的方法
25.2.4中间语言的思想：使用意义
25.3统计机器翻译
25.4P（F|E）：基于短语的翻译模型
25.5翻译中的对齐
25.5.1IBM模型1
25.5.2HMM对齐
25.6对齐模型的训练
25.6.1训练对齐模型的EM算法
25.7用于基于短语机器翻译的对称对齐
25.8基于短语统计机器翻译的解码
25.9机器翻译评价
25.9.1使用人工评价者
25.9.2自动评价：BLEU
25.10高级问题：机器翻译的句法模型
25.11高级问题：IBM模型3和繁衍度
25.11.1模型3的训练
25.12高级问题：机器翻译的对数线性模型
25.13小结
25.14文献和历史说明
参考文献
・ ・ ・ ・ ・ ・ (收起)Preface
1.Language Processing and Python
1.1 Computing with Language： Texts and Words
1.2 A Closer Look at Python： Texts as Lists of Words
1.3 Computing with Language： Simple Statistics
1.4 Back to Python： Making Decisions and Taking Control
1.5 Automatic Natural Language Understanding
1.6 Summary
1.7 Further Reading
1.8 Exercises
2.Accessing Text Corpora and Lexical Resources
2.1 Accessing Text Corpora
2.2 Conditional Frequency Distributions
2.3 More Python： Reusing Code
2.4 Lexical Resources
2.5 WordNet
2.6 Summary
2.7 Further Reading
2.8 Exercises
3.Processing Raw Text
3.1 Accessing Text from the Web and from Disk
3.2 Strings： Text Processing at the Lowest Level
3.3 Text Processing with Unicode
3.4 Regular Expressions for Detecting Word Patterns
3.5 Useful Applications of Regular Expressions
3.6 Normalizing Text
3.7 Regular Expressions for Tokenizing Text
3.8 Segmentation
3.9 Formatting： From Lists to Strings
3.10 Summary
3.11 Further Reading
3.12 Exercises
4.Writing Structured Programs
4.1 Back to the Basics
4.2 Sequences
4.3 Questions of Style
4.4 Functions： The Foundation of Structured Programming
4.5 Doing More with Functions
4.6 Program Development
4.7 Algorithm Design
4.8 A Sample of Python Libraries
4.9 Summary
4.10 Further Reading
4.11 Exercises
5.Categorizing andTagging Words
5.1 Using a Tagger
5.2 Tagged Corpora
5.3 Mapping Words to Properties Using Python Dictionaries
5.4 Automatic Tagging
5.5 N-Gram Tagging
5.6 Transformation-Based Tagging
5.7 How to Determine the Category of a Word
5.8 Summary
5.9 Further Reading
5.10 Exercises
6.Learning to Classify Text
6.1 Supervised Classification
6.2 Further Examples of Supervised Classification
6.3 Evaluation
6.4 Decision Trees
6.5 Naive Bayes Classifiers
6.6 Maximum Entropy Classifiers
6.7 Modeling Linguistic Patterns
6.8 Summary
6.9 Further Reading
6.10 Exercises
7.Extracting Information from Text
7.1 Information Extraction
7.2 Chunking
7.3 Developing and Evaluating Chunkers
7.4 Recursion in Linguistic Structure
7.5 Named Entity Recognition
7.6 Relation Extraction
7.7 Summary
7.8 Further Reading
7.9 Exercises
8.Analyzing Sentence Structure
8.1 Some Grammatical Dilemmas
8.2 Whats the Use of Syntax?
8.3 Context-Free Grammar
8.4 Parsing with Context-Free Grammar
8.5 Dependencies and Dependency Grammar
8.6 Grammar Development
8.7 Summary
8.8 Further Reading
8.9 Exercises
9.Building Feature-Based Grammars
9.1 Grammatical Features
9.2 Processing Feature Structures
9.3 Extending a Feature-Based Grammar
9.4 Summary
9.5 Further Reading
9.6 Exercises
10.Analyzing the Meaning of Sentences
10.1 Natural Language Understanding
10.2 Propositional Logic
10.3 First-Order Logic
10.4 The Semantics of English Sentences
10.5 Discourse Semantics
10.6 Summary
10.7 Further Reading
10.8 Exercises
11.Managing Linguistic Data
11.1 Corpus Structure： A Case Study
11.2 The Life Cycle of a Corpus
11.3 Acquiring Data
11.4 Working with XML
11.5 Working with Toolbox Data
11.6 Describing Language Resources Using OLAC Metadata
11.7 Summary
11.8 Further Reading
11.9 Exercises
Afterword： The Language Challenge
Bibliography
NLTK Index
General Index
・ ・ ・ ・ ・ ・ (收起)《python自然语言处理》
第1章 语言处理与python 1
1.1 语言计算：文本和词汇 1
1.2 近观python：将文本当做词链表 10
1.3 计算语言：简单的统计 17
1.4 回到python:决策与控制 24
1.5 自动理解自然语言 29
1.6 小结 35
1.7 深入阅读 36
1.8 练习 37
第2章 获得文本语料和词汇资源 41
2.1 获取文本语料库 41
2.2 条件频率分布 55
2.3 更多关于python：代码重用 60
2.4 词典资源 63
2.5 wordnet 72
2.6 小结 78
2.7 深入阅读 79
2.8 练习 80
第3章 处理原始文本 84
3.1 从网络和硬盘访问文本 84
3.2 字符串：最底层的文本处理 93
3.3 使用unicode进行文字处理 100
3.4 使用正则表达式检测词组搭配 105
3.5 正则表达式的有益应用 109
3.6 规范化文本 115
3.7 用正则表达式为文本分词 118
3.8 分割 121
3.9 格式化：从链表到字符串 126
3.10 小结 132
3.11 深入阅读 133
3.12 练习 134
第4章 编写结构化程序 142
4.1 回到基础 142
4.2 序列 147
4.3 风格的问题 152
4.4 函数：结构化编程的基础 156
4.5 更多关于函数 164
4.6 程序开发 169
4.7 算法设计 175
4.8 python库的样例 183
4.9 小结 188
4.10 深入阅读 189
4.11 练习 189
第5章 分类和标注词汇 195
5.1 使用词性标注器 195
5.2 标注语料库 197
5.3 使用python字典映射词及其属性 206
5.4 自动标注 216
5.5 n-gram标注 221
5.6 基于转换的标注 228
5.7 如何确定一个词的分类 230
5.8 小结 233
5.9 深入阅读 234
5.10 练习 235
第6章 学习分类文本 241
6.1 监督式分类 241
6.2 监督式分类的举例 254
6.3 评估 258
6.4 决策树 263
6.5 朴素贝叶斯分类器 266
6.6 最大熵分类器 271
6.7 为语言模式建模 275
6.8 小结 276
6.9 深入阅读 277
6.10 练习 278
第7章 从文本提取信息 281
7.1 信息提取 281
7.2 分块 284
7.3 开发和评估分块器 291
7.4 语言结构中的递归 299
7.5 命名实体识别 302
7.6 关系抽取 306
7.7 小结 307
7.8 深入阅读 308
7.9 练习 308
第8章 分析句子结构 312
8.1 一些语法困境 312
8.2 文法的用途 316
8.3 上下文无关文法 319
8.4 上下文无关文法分析 323
8.5 依存关系和依存文法 332
8.6 文法开发 336
8.7 小结 343
8.8 深入阅读 344
8.9 练习 344
第9章 建立基于特征的文法 349
9.1 文法特征 349
9.2 处理特征结构 359
9.3 扩展基于特征的文法 367
9.4 小结 379
9.5 深入阅读 380
9.6 练习 381
第10章 分析语句的含义 384
10.1 自然语言理解 384
10.2 命题逻辑 391
10.3 一阶逻辑 395
10.4 英语语句的语义 409
10.5 段落语义层 422
10.6 小结 428
10.7 深入阅读 429
10.8 练习 430
第11章 语言数据管理 434
11.1 语料库结构：案例研究 434
11.2 语料库生命周期 439
11.3 数据采集 443
11.4 使用xml 452
11.5 使用toolbox数据 459
11.6 使用olac元数据描述语言资源 463
11.7 小结 466
11.8 深入阅读 466
11.9 练习 467
后记 470
参考文献 476
・ ・ ・ ・ ・ ・ (收起)译者序
推荐序
作者介绍
关于审校人员
前言
第1章 引言 1
1.1 自然语言处理 1
1.2 基础应用 5
1.3 高级应用 6
1.4 NLP和Python相结合的优势 7
1.5 nltk环境搭建 7
1.6 读者提示 8
1.7 总结 9
第2章 实践理解语料库和数据集 10
2.1 语料库 10
2.2 语料库的作用 11
2.3 语料分析 13
2.4 数据属性的类型 16
2.4.1 分类或定性数据属性 16
2.4.2 数值或定量数据属性 17
2.5 不同文件格式的语料 18
2.6 免费语料库资源 19
2.7 为NLP应用准备数据集 20
2.7.1 挑选数据 20
2.7.2 预处理数据集 20
2.8 网页爬取 21
2.9 总结 23
第3章 理解句子的结构 24
3.1 理解NLP的组成 24
3.1.1 自然语言理解 24
3.1.2 自然语言生成 25
3.1.3 NLU和NLG的区别 25
3.1.4 NLP的分支 26
3.2 上下文无关文法 26
3.3 形态分析 28
3.3.1 形态学 28
3.3.2 词素 28
3.3.3 词干 28
3.3.4 形态分析 28
3.3.5 词 29
3.3.6 词素的分类 29
3.3.7 词干和词根的区别 32
3.4 词法分析 32
3.4.1 词条 33
3.4.2 词性标注 33
3.4.3 导出词条的过程 33
3.4.4 词干提取和词形还原的区别 34
3.4.5 应用 34
3.5 句法分析 34
3.6 语义分析 36
3.6.1 语义分析概念 36
3.6.2 词级别的语义 37
3.6.3 上下位关系和多义词 37
3.6.4 语义分析的应用 38
3.7 消歧 38
3.7.1 词法歧义 38
3.7.2 句法歧义 39
3.7.3 语义歧义 39
3.7.4 语用歧义 39
3.8 篇章整合 40
3.9 语用分析 40
3.10 总结 40
第4章 预处理 42
4.1 处理原始语料库文本 42
4.1.1 获取原始文本 42
4.1.2 小写化转换 44
4.1.3 分句 44
4.1.4 原始文本词干提取 46
4.1.5 原始文本词形还原 46
4.1.6 停用词去除 48
4.2 处理原始语料库句子 50
4.2.1 词条化 50
4.2.2 单词词形还原 51
4.3 基础预处理 52
4.4 实践和个性化预处理 57
4.4.1 由你自己决定 57
4.4.2 预处理流程 57
4.4.3 预处理的类型 57
4.4.4 理解预处理的案例 57
4.5 总结 62
第5章 特征工程和NLP算法 63
5.1 理解特征工程 64
5.1.1 特征工程的定义 64
5.1.2 特征工程的目的 64
5.1.3 一些挑战 65
5.2 NLP中的基础特征 65
5.2.1 句法分析和句法分析器 65
5.2.2 词性标注和词性标注器 81
5.2.3 命名实体识别 85
5.2.4 n元语法 88
5.2.5 词袋 89
5.2.6 语义工具及资源 91
5.3 NLP中的基础统计特征 91
5.3.1 数学基础 92
5.3.2 TF-IDF 96
5.3.3 向量化 99
5.3.4 规范化 100
5.3.5 概率模型 101
5.3.6 索引 103
5.3.7 排序 103
5.4 特征工程的优点 104
5.5 特征工程面临的挑战 104
5.6 总结 104
第6章 高级特征工程和NLP算法 106
6.1 词嵌入 106
6.2 word2vec基础 106
6.2.1 分布语义 107
6.2.2 定义word2vec 108
6.2.3 无监督分布语义模型中的必需品 108
6.3 word2vec模型从黑盒到白盒 109
6.4 基于表示的分布相似度 110
6.5 word2vec模型的组成部分 111
6.5.1 word2vec的输入 111
6.5.2 word2vec的输出 111
6.5.3 word2vec模型的构建模块 111
6.6 word2vec模型的逻辑 113
6.6.1 词汇表构建器 114
6.6.2 上下文环境构建器 114
6.6.3 两层的神经网络 116
6.6.4 算法的主要流程 119
6.7 word2vec模型背后的算法和数学理论 120
6.7.1 word2vec算法中的基本数学理论 120
6.7.2 词汇表构建阶段用到的技术 121
6.7.3 上下文环境构建过程中使用的技术 122
6.8 神经网络算法 123
6.8.1 基本神经元结构 123
6.8.2 训练一个简单的神经元 124
6.8.3 单个神经元的应用 126
6.8.4 多层神经网络 127
6.8.5 反向传播算法 127
6.8.6 word2vec背后的数学理论 128
6.9 生成最终词向量和概率预测结果的技术 130
6.10 word2vec相关的一些事情 131
6.11 word2vec的应用 131
6.11.1 实现一些简单例子 132
6.11.2 word2vec的优势 133
6.11.3 word2vec的挑战 133
6.11.4 在实际应用中使用word2vec 134
6.11.5 何时使用word2vec 135
6.11.6 开发一些有意思的东西 135
6.11.7 练习 138
6.12 word2vec概念的扩展 138
6.12.1 para2vec 139
6.12.2 doc2vec 139
6.12.3 doc2vec的应用 140
6.12.4 GloVe 140
6.12.5 练习 141
6.13 深度学习中向量化的重要性 141
6.14 总结 142
第7章 规则式自然语言处理系统 143
7.1 规则式系统 144
7.2 规则式系统的目的 146
7.2.1 为何需要规则式系统 146
7.2.2 使用规则式系统的应用 147
7.2.3 练习 147
7.2.4 开发规则式系统需要的资源 147
7.3 规则式系统的架构 148
7.3.1 从专家系统的角度来看规则式系统的通用架构 149
7.3.2 NLP应用中的规则式系统的实用架构 150
7.3.3 NLP应用中的规则式系统的定制架构 152
7.3.4 练习 155
7.3.5 Apache UIMA架构 155
7.4 规则式系统的开发周期 156
7.5 规则式系统的应用 156
7.5.1 使用规则式系统的NLP应用 156
7.5.2 使用规则式系统的通用AI应用 157
7.6 使用规则式系统来开发NLP应用 157
7.6.1 编写规则的思维过程 158
7.6.2 基于模板的聊天机器人应用 165
7.7 规则式系统与其他方法的对比 168
7.8 规则式系统的优点 169
7.9 规则式系统的缺点 169
7.10 规则式系统面临的挑战 170
7.11 词义消歧的基础 170
7.12 规则式系统近期发展的趋势 171
7.13 总结 171
第8章 自然语言处理中的机器学习方法 172
8.1 机器学习的基本概念 172
8.2 自然语言处理应用的开发步骤 176
8.2.1 第一次迭代时的开发步骤 177
8.2.2 从第二次到第N次迭代的开发步骤 177
8.3 机器学习算法和其他概念 179
8.3.1 有监督机器学习方法 179
8.3.2 无监督机器学习方法 206
8.3.3 半监督机器学习算法 210
8.3.4 一些重要概念 211
8.3.5 特征选择 215
8.3.6 维度约减 219
8.4 自然语言处理中的混合方法 221
8.5 总结 221
第9章 NLU和NLG问题中的深度学习 223
9.1 人工智能概览 223
9.1.1 人工智能的基础 223
9.1.2 人工智能的阶段 225
9.1.3 人工智能的种类 227
9.1.4 人工智能的目标和应用 227
9.2 NLU和NLG之间的区别 232
9.2.1 自然语言理解 232
9.2.2 自然语言生成 232
9.3 深度学习概览 233
9.4 神经网络基础 234
9.4.1 神经元的第一个计算模型 235
9.4.2 感知机 236
9.4.3 理解人工神经网络中的数学概念 236
9.5 实现神经网络 249
9.5.1 单层反向传播神经网络 249
9.5.2 练习 251
9.6 深度学习和深度神经网络 251
9.6.1 回顾深度学习 251
9.6.2 深度神经网络的基本架构 251
9.6.3 NLP中的深度学习 252
9.6.4 传统NLP和深度学习NLP技术的区别 253
9.7 深度学习技术和NLU 255
9.8 深度学习技术和NLG 262
9.8.1 练习 262
9.8.2 菜谱摘要和标题生成 262
9.9 基于梯度下降的优化 265
9.10 人工智能与人类智能 269
9.11 总结 269
第10章 高级工具 270
10.1 使用Apache Hadoop作为存储框架 270
10.2 使用Apache Spark作为数据处理框架 272
10.3 使用Apache Flink作为数据实时处理框架 274
10.4 Python中的可视化类库 274
10.5 总结 275
第11章 如何提高你的NLP技能 276
11.1 开始新的NLP职业生涯 276
11.2 备忘列表 277
11.3 确定你的领域 277
11.4 通过敏捷的工作来实现成功 278
11.5 NLP和数据科学方面一些有用的博客 278
11.6 使用公开的数据集 278
11.7 数据科学领域需要的数学知识 278
11.8 总结 279
第12章 安装指导 280
12.1 安装Python、pip和NLTK 280
12.2 安装PyCharm开发环境 280
12.3 安装依赖库 280
12.4 框架安装指导 281
12.5 解决你的疑问 281
12.6 总结 281
・ ・ ・ ・ ・ ・ (收起)译者序
作者简介
审校者简介
前言
第1章 NLP简介 1
1.1 什么是NLP 2
1.2 为何使用NLP 3
1.3 NLP的难点 4
1.4 NLP工具汇总 5
1.4.1 Apache OpenNLP 6
1.4.2 Stanford NLP 7
1.4.3 LingPipe 9
1.4.4 GATE 10
1.4.5 UIMA 10
1.5 文本处理概览 10
1.5.1 文本分词 11
1.5.2 文本断句 12
1.5.3 人物识别 14
1.5.4 词性判断 16
1.5.5 文本分类 17
1.5.6 关系提取 18
1.5.7 方法组合 20
1.6 理解NLP模型 20
1.6.1 明确目标 20
1.6.2 选择模型 21
1.6.3 构建、训练模型 21
1.6.4 验证模型 22
1.6.5 使用模型 22
1.7 准备数据 22
1.8 本章小结 24
第2章 文本分词 25
2.1 理解文本分词 25
2.2 什么是分词 26
2.3 一些简单的Java分词器 28
2.3.1 使用Scanner类 29
2.3.2 使用split方法 30
2.3.3 使用BreakIterator类 31
2.3.4 使用StreamTokenizer类 32
2.3.5 使用StringTokenizer类 34
2.3.6 使用Java核心分词法的性能考虑 34
2.4 NLP分词器的API 34
2.4.1 使用OpenNLPTokenizer类分词器 35
2.4.2 使用Stanford分词器 37
2.4.3 训练分词器进行文本分词 41
2.4.4 分词器的比较 44
2.5 理解标准化处理 45
2.5.1 转换为小写字母 45
2.5.2 去除停用词 46
2.5.3 词干化 49
2.5.4 词形还原 51
2.5.5 使用流水线进行标准化处理 54
2.6 本章小结 55
第3章 文本断句 56
3.1 SBD方法 56
3.2 SBD难在何处 57
3.3 理解LingPipe的HeuristicSen-tenceModel类的SBD规则 59
3.4 简单的Java SBD 60
3.4.1 使用正则表达式 60
3.4.2 使用BreakIterator类 62
3.5 使用NLP API 63
3.5.1 使用OpenNLP 64
3.5.2 使用Stanford API 66
3.5.3 使用LingPipe 74
3.6 训练文本断句模型 78
3.6.1 使用训练好的模型 80
3.6.2 使用SentenceDetector-Evaluator类评估模型 81
3.7 本章小结 82
第4章 人物识别 83
4.1 NER难在何处 84
4.2 NER的方法 84
4.2.1 列表和正则表达式 85
4.2.2 统计分类器 85
4.3 使用正则表达式进行NER 86
4.3.1 使用Java的正则表达式来寻找实体 86
4.3.2 使用LingPipe的RegEx-Chunker类 88
4.4 使用NLP API 89
4.4.1 使用OpenNLP进行NER 89
4.4.2 使用Stanford API进行NER 95
4.4.3 使用LingPipe进行NER 96
4.5 训练模型 100
4.6 本章小结 103
第5章 词性判断 104
5.1 词性标注 104
5.1.1 词性标注器的重要性 107
5.1.2 词性标注难在何处 107
5.2 使用NLP API 109
5.2.1 使用OpenNLP词性标注器 110
5.2.2 使用Stanford词性标注器 118
5.2.3 使用LingPipe词性标注器 125
5.2.4 训练OpenNLP词性标注模型 129
5.3 本章小结 131
第6章 文本分类 132
6.1 文本分类问题 132
6.2 情感分析介绍 134
6.3 文本分类技术 135
6.4 使用API进行文本分类 136
6.4.1 OpenNLP的使用 136
6.4.2 Stanford API的使用 140
6.4.3 使用LingPipe进行文本分类 145
6.5 本章小结 152
第7章 关系提取 153
7.1 关系类型 154
7.2 理解解析树 155
7.3 关系提取的应用 156
7.4 关系提取 159
7.5 使用NLP API 159
7.5.1 OpenNLP的使用 159
7.5.2 使用Stanford API 162
7.5.3 判断共指消解的实体 166
7.6 问答系统的关系提取 168
7.6.1 判断单词依赖关系 169
7.6.2 判断问题类型 170
7.6.3 搜索答案 171
7.7 本章小结 173
第8章 方法组合 174
8.1 准备数据 175
8.1.1 使用Boilerpipe从HTML中提取文本 175
8.1.2 使用POI从Word文档中提取文本 177
8.1.3 使用PDFBox从PDF文档中提取文本 181
8.2 流水线 182
8.2.1 使用Stanford流水线 182
8.2.2 在Standford流水线中使用多核处理器 187
8.3 创建一个文本搜索的流水线 188
8.4 本章小结 193
・ ・ ・ ・ ・ ・ (收起)第1 章基础入门 1
1.1 什么是自然语言处理 1
1.1.1 自然语言处理概述 1
1.1.2 自然语言处理的发展历史 3
1.1.3 自然语言处理的工作原理 6
1.1.4 自然语言处理的应用前景 7
1.2 开发工具与环境 7
1.2.1 Sublime Text 和Anaconda 介绍 7
1.2.2 开发环境的安装与配置 8
1.3 实战：第一个小程序的诞生 13
1.3.1 实例介绍 13
1.3.2 源码实现 13
第2 章快速上手Python 15
2.1 初识Python 编程语言 15
2.1.1 Python 概述 15
2.1.2 Python 能做什么 17
2.1.3 Python 的语法和特点 19
2.2 Python 进阶 24
2.2.1 Hello World 24
2.2.2 语句和控制流 24
2.2.3 函数 27
2.2.4 List 列表 29
2.2.5 元组 32
2.2.6 set 集合 33
2.2.7 字典 33
2.2.8 面向对象编程：类 34
2.2.9 标准库 36
2.3 Python 深入――第三方库 36
2.3.1 Web 框架 36
2.3.2 科学计算 37
2.3.3 GUI 37
2.3.4 其他库 37
第3 章线性代数 39
3.1 线性代数介绍 39
3.2 向量 40
3.2.1 向量定义 40
3.2.2 向量表示 42
3.2.3 向量定理 42
3.2.4 向量运算 43
3.3 矩阵 47
3.3.1 矩阵定义 47
3.3.2 矩阵表示 48
3.3.3 矩阵运算 48
3.3.4 线性方程组 51
3.3.5 行列式 51
3.3.6 特征值和特征向量 55
3.4 距离计算 56
3.4.1 余弦距离 56
3.4.2 欧氏距离 57
3.4.3 曼哈顿距离 58
3.4.4 明可夫斯基距离 59
3.4.5 切比雪夫距离 61
3.4.6 杰卡德距离 62
3.4.7 汉明距离 63
3.4.8 标准化欧式距离 64
3.4.9 皮尔逊相关系数 65
第4 章概率论 67
4.1 概率论介绍 67
4.2 事件 68
4.2.1 随机试验 68
4.2.2 随机事件和样本空间 69
4.2.3 事件的计算 70
4.3 概率 71
4.4 概率公理 73
4.5 条件概率和全概率 76
4.5.1 条件概率 76
4.5.2 全概率 77
4.6 贝叶斯定理 78
4.7 信息论 79
4.7.1 信息论的基本概念 79
4.7.2 信息度量 80
第5 章统计学 85
5.1 图形可视化 85
5.1.1 饼图 85
5.1.2 条形图 88
5.1.3 热力图 91
5.1.4 折线图 93
5.1.5 箱线图 96
5.1.6 散点图 99
5.1.7 雷达图 102
5.1.8 仪表盘 104
5.1.9 可视化图表用法 106
5.2 数据度量标准 108
5.2.1 平均值 108
5.2.2 中位数 108
5.2.3 众数 110
5.2.4 期望 111
5.2.5 方差 112
5.2.6 标准差 113
5.2.7 标准分 114
5.3 概率分布 115
5.3.1 几何分布 115
5.3.2 二项分布 116
5.3.3 正态分布 118
5.3.4 泊松分布 121
5.4 统计假设检验 123
5.5 相关和回归 125
5.5.1 相关 125
5.5.2 回归 127
5.5.3 相关和回归的联系 130
第6 章语言学 132
6.1 语音 132
6.1.1 什么是语音 132
6.1.2 语音的三大属性 133
6.1.3 语音单位 134
6.1.4 记音符号 135
6.1.5 共时语流音变 136
6.2 词汇 137
6.2.1 什么是词汇 137
6.2.2 词汇单位 137
6.2.3 词的构造 138
6.2.4 词义及其分类 140
6.2.5 义项与义素 141
6.2.6 语义场 142
6.2.7 词汇的构成 143
6.3 语法 143
6.3.1 什么是语法 143
6.3.2 词类 144
6.3.3 短语 148
6.3.4 单句 150
6.3.5 复句 152
第7 章自然语言处理 155
7.1 自然语言处理的任务和限制 155
7.2 自然语言处理的主要技术范畴 156
7.2.1 语音合成 156
7.2.2 语音识别 156
7.2.3 中文自动分词 157
7.2.4 词性标注 158
7.2.5 句法分析 158
7.2.6 文本分类 159
7.2.7 文本挖掘 160
7.2.8 信息抽取 161
7.2.9 问答系统 161
7.2.10 机器翻译 162
7.2.11 文本情感分析 163
7.2.12 自动摘要 164
7.2.13 文字蕴涵 165
7.3 自然语言处理的难点 165
7.3.1 语言环境复杂 165
7.3.2 文本结构形式多样 166
7.3.3 边界识别限制 166
7.3.4 词义消歧 167
7.3.5 指代消解 168
7.4 自然语言处理展望 169
第8 章语料库 173
8.1 语料库浅谈 173
8.2 语料库深入 174
8.3 自然语言处理工具包：NLTK 176
8.3.1 NLTK 简介 176
8.3.2 安装NLTK 177
8.3.3 使用NLTK 180
8.3.4 在Python NLTK 下使用Stanford NLP 186
8.4 获取语料库 194
8.4.1 国内外著名语料库 195
8.4.2 网络数据获取 197
8.4.3 NLTK 获取语料库 200
8.5 综合案例：走进大秦帝国 208
8.5.1 数据采集和预处理 208
8.5.2 构建本地语料库 208
8.5.3 大秦帝国语料操作 209
第9 章中文自动分词 216
9.1 中文分词简介 216
9.2 中文分词的特点和难点 218
9.3 常见中文分词方法 219
9.4 典型中文分词工具 220
9.4.1 HanLP 中文分词 220
9.4.2 其他中文分词工具 223
9.5 结巴中文分词 224
9.5.1 基于Python 的结巴中文分词 224
9.5.2 结巴分词工具详解 227
9.5.3 结巴分词核心内容 230
9.5.4 结巴分词基本用法 233
第10 章数据预处理 241
10.1 数据清洗 241
10.2 分词处理 242
10.3 特征构造 242
10.4 特征降维与选择 243
10.4.1 特征降维 243
10.4.2 特征选择 243
10.5 简单实例 244
10.6 本章小结 249
第11 章马尔可夫模型 250
11.1 马尔可夫链 250
11.1.1 马尔可夫简介 250
11.1.2 马尔可夫链的基本概念 251
11.2 隐马尔可夫模型 253
11.2.1 形式化描述 253
11.2.2 数学形式描述 255
11.3 向前算法解决HMM 似然度 256
11.3.1 向前算法定义 256
11.3.2 向前算法原理 256
11.3.3 现实应用：预测成都天气的冷热 258
11.4 文本序列标注案例：Viterbi 算法 259
第12 章条件随机场 263
12.1 条件随机场介绍 263
12.2 简单易懂的条件随机场 265
12.2.1 CRF 的形式化表示 265
12.2.2 CRF 的公式化表示 266
12.2.3 深度理解条件随机场 268
第13 章模型评估 269
13.1 从统计角度介绍模型概念 269
13.1.1 算法模型 269
13.1.2 模型评估和模型选择 270
13.1.3 过拟合与欠拟合的模型选择 272
13.2 模型评估与选择 275
13.2.1 模型评估的概念 275
13.2.2 模型评估的评测指标 275
13.2.3 以词性标注为例分析模型评估 276
13.2.4 模型评估的几种方法 278
13.3 ROC 曲线比较学习器模型 279
第14 章命名实体识别 281
14.1 命名实体识别概述 281
14.2 命名实体识别的特点与难点 284
14.3 命名实体识别方法 284
14.4 中文命名实体识别的核心技术 286
14.5 展望 295
第15 章自然语言处理实战 296
15.1 GitHub 数据提取与可视化分析 296
15.1.1 了解GitHub 的API 296
15.1.2 使用NetworkX 作图 299
15.1.3 使用NetworkX 构建兴趣图 301
15.1.4 NetWorkX 部分统计指标 304
15.1.5 构建GitHub 的兴趣图 305
15.1.6 可视化 318
15.2 微博话题爬取与存储分析 320
15.2.1 数据采集 320
15.2.2 数据提取 329
15.2.3 数据存储 332
15.2.4 项目运行与分析 333
附录A Python 与其他语言调用 337
附录B Git 项目上传简易教程 339
参考文献 341
・ ・ ・ ・ ・ ・ (收起)模块1　NLTK基础知识
第　1章 自然语言处理简介　3
1.1　为什么要学习NLP　4
1.2　从Python的基本知识开始　7
1.2.1　列表　7
1.2.2　自助　8
1.2.3　正则表达式　9
1.2.4　词典　11
1.2.5　编写函数　11
1.3　NLTK　13
1.4　试一试　18
1.5　本章小结　18
第　2章 文本的整理和清洗　19
2.1　文本整理　19
2.2　文本清洗　21
2.3　句子拆分器　22
2.4　标记解析　22
2.5　词干提取　24
2.6　词形还原　25
2.7　停用词删除　26
2.8　生僻字删除　27
2.9　拼写校正　27
2.10　试一试　28
2.11　本章小结　28
第3章　词性标注　30
3.1　什么是词性标注　30
3.1.1　斯坦福标注器　33
3.1.2　深入了解标注器　34
3.1.3　序列标注器　35
3.1.4　布里尔标注器　37
3.1.5　基于标注器的机器学习　37
3.2　命名实体识别　38
3.3　试一试　40
3.4　本章小结　41
第4章　对文本的结构进行语法分析　42
4.1　浅层语法分析与深层语法
分析　42
4.2　语法分析的两种方法　43
4.3　为什么需要语法分析　43
4.4　不同类型的语法分析器　45
4.4.1　递归下降的语法分析器　45
4.4.2　移位归约语法分析器　45
4.4.3　图表语法分析器　45
4.4.4　正则表达式语法
分析器　46
4.5　依存分析　47
4.6　组块化　49
4.7　信息抽取　51
4.7.1　命名实体识别　52
4.7.2　关系抽取　52
4.8　本章小结　53
第5章　NLP应用　54
5.1　构建第 一个NLP应用　54
5.2　其他的NLP应用　58
5.2.1　机器翻译　58
5.2.2　统计机器翻译　59
5.2.3　信息检索　59
5.2.4　语音识别　61
5.2.5　文本分类　62
5.2.6　信息提取　63
5.2.7　问答系统　64
5.2.8　对话系统　64
5.2.9　词义消歧　64
5.2.10　主题建模　64
5.2.11　语言检测　65
5.2.12　光学字符识别　65
5.3　本章小结　65
第6章　文本分类　66
6.1　机器学习　67
6.2　文本分类　68
6.3　采样　70
6.3.1　朴素贝叶斯　73
6.3.2　决策树　75
6.3.3　随机梯度下降　76
6.3.4　逻辑回归　77
6.3.5　支持向量机　78
6.4　随机森林算法　79
6.5　文本聚类　79
6.6　文本的主题建模　81
6.7　参考资料　83
6.8　本章小结　83
第7章　网络爬取　85
7.1　网络爬虫　85
7.2　编写第 一个爬虫程序　86
7.3　Scrapy中的数据流　89
7.3.1　Scrapy命令行界面　89
7.3.2　项　94
7.4　站点地图蜘蛛　96
7.5　项管道　97
7.6　外部参考　98
7.7　本章小结　99
第8章　与其他Python库一同
使用NLTK　100
8.1　NumPy　100
8.1.1　ndarray　101
8.1.2　基本操作　102
8.1.3　从数组中提取数据　103
8.1.4　复杂的矩阵运算　103
8.2　SciPy　107
8.2.1　线性代数　108
8.2.2　特征值和特征向量　108
8.2.3　稀疏矩阵　109
8.2.4　优化　110
8.3　Pandas　111
8.3.1　读取数据　112
8.3.2　时序数据　114
8.3.3　列转换　115
8.3.4　噪声数据　116
8.4　Matplotlib　117
8.4.1　subplot　118
8.4.2　添加轴　119
8.4.3　散点图　120
8.4.4　柱状图　120
8.4.5　3D图　121
8.5　外部参考　121
8.6　本章小结　121
第9章　使用Python进行社交媒体
挖掘　122
9.1　数据收集　122
9.2　数据提取　126
9.3　地理可视化　128
9.3.1　影响者检测　129
9.3.2　Facebook　130
9.3.3　影响者的朋友　134
9.4　本章小结　135
第　10章 大规模的文本挖掘　136
10.1　在Hadoop上使用Python的
不同方法　136
10.1.1　Python的流　137
10.1.2　Hive/Pig UDF　137
10.1.3　流包装器　137
10.2　在Hadoop上运行NLTK　138
10.2.1　UDF　138
10.2.2　Python流　140
10.3　在Hadoop上运行
Scikit-learn　141
10.4　PySpark　144
10.5　本章小结　146
模块2　使用Python 3的NLTK 3进行文本处理
第　1章 标记文本和WordNet的基础　149
1.1　引言　149
1.2　将文本标记成句子　150
1.2.1　准备工作　150
1.2.2　工作方式　151
1.2.3　工作原理　151
1.2.4　更多信息　151
1.2.5　请参阅　152
1.3　将句子标记成单词　152
1.3.1　工作方式　152
1.3.2　工作原理　153
1.3.3　更多信息　153
1.3.4　请参阅　154
1.4　使用正则表达式标记语句　154
1.4.1　准备工作　155
1.4.2　工作方式　155
1.4.3　工作原理　155
1.4.4　更多信息　155
1.4.5　请参阅　156
1.5　训练语句标记生成器　156
1.5.1　准备工作　156
1.5.2　工作方式　156
1.5.3　工作原理　157
1.5.4　更多信息　158
1.5.5　请参阅　158
1.6　在已标记的语句中过滤
停用词　158
1.6.1　准备工作　158
1.6.2　工作方式　159
1.6.3　工作原理　159
1.6.4　更多信息　159
1.6.5　请参阅　160
1.7　查找WordNet中单词的
Synset　160
1.7.1　准备工作　160
1.7.2　工作方式　160
1.7.3　工作原理　161
1.7.4　更多信息　161
1.7.5　请参阅　163
1.8　在WordNet中查找词元和
同义词　163
1.8.1　工作方式　163
1.8.2　工作原理　163
1.8.3　更多信息　163
1.8.4　请参阅　165
1.9　计算WordNet和Synset的
相似度　165
1.9.1　工作方式　165
1.9.2　工作原理　165
1.9.3　更多信息　166
1.9.4　请参阅　167
1.10　发现单词搭配　167
1.10.1　准备工作　167
1.10.2　工作方式　167
1.10.3　工作原理　168
1.10.4　更多信息　168
1.10.5　请参阅　169
第　2章 替换和校正单词　170
2.1　引言　170
2.2　词干提取　170
2.2.1　工作方式　171
2.2.2　工作原理　171
2.2.3　更多信息　171
2.2.4　请参阅　173
2.3　使用WordNet进行词形还原　173
2.3.1　准备工作　173
2.3.2　工作方式　173
2.3.3　工作原理　174
2.3.4　更多信息　174
2.3.5　请参阅　175
2.4　基于匹配的正则表达式替换
单词　175
2.4.1　准备工作　175
2.4.2　工作方式　175
2.4.3　工作原理　176
2.4.4　更多信息　177
2.4.5　请参阅　177
2.5　移除重复字符　177
2.5.1　准备工作　177
2.5.2　工作方式　178
2.5.3　工作原理　178
2.5.4　更多信息　179
2.5.5　请参阅　179
2.6　使用Enchant进行拼写校正　180
2.6.1　准备工作　180
2.6.2　工作方式　180
2.6.3　工作原理　181
2.6.4　更多信息　181
2.6.5　请参阅　183
2.7　替换同义词　183
2.7.1　准备工作　183
2.7.2　工作方式　183
2.7.3　工作原理　184
2.7.4　更多信息　184
2.7.5　请参阅　185
2.8　使用反义词替换否定形式　186
2.8.1　工作方式　186
2.8.2　工作原理　187
2.8.3　更多信息　187
2.8.4　请参阅　188
第3章　创建自定义语料库　189
3.1　引言　189
3.2　建立自定义语料库　190
3.2.1　准备工作　190
3.2.2　工作方式　190
3.2.3　工作原理　191
3.2.4　更多信息　192
3.2.5　请参阅　192
3.3　创建词汇表语料库　192
3.3.1　准备工作　192
3.3.2　工作方式　193
3.3.3　工作原理　193
3.3.4　更多信息　194
3.3.5　请参阅　194
3.4　创建已标记词性单词的
语料库　195
3.4.1　准备工作　195
3.4.2　工作方式　195
3.4.3　工作原理　196
3.4.4　更多信息　196
3.4.5　请参阅　199
3.5　创建已组块短语的语料库　199
3.5.1　准备工作　199
3.5.2　工作方式　199
3.5.3　工作原理　201
3.5.4　更多信息　201
3.5.5　请参阅　203
3.6　创建已分类文本的语料库　203
3.6.1　准备工作　204
3.6.2　工作方式　204
3.6.3　工作原理　204
3.6.4　更多信息　205
3.6.5　请参阅　206
3.7　创建已分类组块语料库
读取器　206
3.7.1　准备工作　206
3.7.2　工作方式　207
3.7.3　工作原理　208
3.7.4　更多信息　209
3.7.5　请参阅　213
3.8　懒惰语料库加载　213
3.8.1　工作方式　213
3.8.2　工作原理　214
3.8.3　更多信息　214
3.9　创建自定义语料库视图　215
3.9.1　工作方式　215
3.9.2　工作原理　216
3.9.3　更多信息　217
3.9.4　请参阅　218
3.10　创建基于MongoDB的
语料库读取器　218
3.10.1　准备工作　219
3.10.2　工作方式　219
3.10.3　工作原理　220
3.10.4　更多信息　221
3.10.5　请参阅　221
3.11　在加锁文件的情况下编辑
语料库　221
3.11.1　准备工作　221
3.11.2　工作方式　221
3.11.3　工作原理　222
第4章　词性标注　224
4.1　引言　224
4.2　默认标注　225
4.2.1　准备工作　225
4.2.2　工作方式　225
4.2.3　工作原理　226
4.2.4　更多信息　227
4.2.5　请参阅　228
4.3　训练一元组词性标注器　228
4.3.1　工作方式　228
4.3.2　工作原理　229
4.3.3　更多信息　230
4.3.4　请参阅　231
4.4　回退标注的组合标注器　231
4.4.1　工作方式　231
4.4.2　工作原理　232
4.4.3　更多信息　232
4.4.4　请参阅　233
4.5　训练和组合N元标注器　233
4.5.1　准备工作　233
4.5.2　工作方式　233
4.5.3　工作原理　234
4.5.4　更多信息　235
4.5.5　请参阅　236
4.6　创建似然单词标签的
模型　236
4.6.1　工作方式　236
4.6.2　工作原理　237
4.6.3　更多信息　237
4.6.4　请参阅　238
4.7　使用正则表达式标注　238
4.7.1　准备工作　238
4.7.2　工作方式　238
4.7.3　工作原理　239
4.7.4　更多信息　239
4.7.5　请参阅　239
4.8　词缀标签　239
4.8.1　工作方式　239
4.8.2　工作原理　240
4.8.3　更多信息　240
4.8.4　请参阅　241
4.9　训练布里尔标注器　241
4.9.1　工作方式　241
4.9.2　工作原理　242
4.9.3　更多信息　243
4.9.4　请参阅　244
4.10　训练TnT标注器　244
4.10.1　工作方式　244
4.10.2　工作原理　244
4.10.3　更多信息　245
4.10.4　请参阅　246
4.11　使用WordNet进行
标注　246
4.11.1　准备工作　246
4.11.2　工作方式　247
4.11.3　工作原理　248
4.11.4　请参阅　248
4.12　标注专有名词　248
4.12.1　工作方式　248
4.12.2　工作原理　249
4.12.3　请参阅　249
4.13　基于分类器的标注　249
4.13.1　工作方式　250
4.13.2　工作原理　250
4.13.3　更多信息　251
4.13.4　请参阅　252
4.14　使用NLTK训练器训练
标注器　253
4.14.1　工作方式　253
4.14.2　工作原理　254
4.14.3　更多信息　258
4.14.4　请参阅　260
第5章　提取组块　261
5.1　引言　261
5.2　使用正则表达式组块和
隔断　262
5.2.1　准备工作　262
5.2.2　工作方式　262
5.2.3　工作原理　263
5.2.4　更多信息　265
5.2.5　请参阅　267
5.3　使用正则表达式合并和拆分
组块　267
5.3.1　工作方式　267
5.3.2　工作原理　269
5.3.3　更多信息　270
5.3.4　请参阅　271
5.4　使用正则表达式扩展和删除
组块　271
5.4.1　工作方式　271
5.4.2　工作原理　272
5.4.3　更多信息　273
5.4.4　请参阅　273
5.5　使用正则表达式进行部分
解析　273
5.5.1　工作方式　273
5.5.2　工作原理　274
5.5.3　更多信息　275
5.5.4　请参阅　276
5.6　训练基于标注器的组块器　276
5.6.1　工作方式　276
5.6.2　工作原理　277
5.6.3　更多信息　278
5.6.4　请参阅　279
5.7　基于分类的分块　279
5.7.1　工作方式　279
5.7.2　工作原理　282
5.7.3　更多信息　282
5.7.4　请参阅　283
5.8　提取命名实体　283
5.8.1　工作方式　283
5.8.2　工作原理　284
5.8.3　更多信息　284
5.8.4　请参阅　285
5.9　提取专有名词组块　285
5.9.1　工作方式　286
5.9.2　工作原理　286
5.9.3　更多信息　286
5.10　提取部位组块　287
5.10.1　工作方式　288
5.10.2　工作原理　290
5.10.3　更多信息　290
5.10.4　请参阅　290
5.11　训练命名实体组块器　290
5.11.1　工作方式　290
5.11.2　工作原理　292
5.11.3　更多信息　292
5.11.4　请参阅　293
5.12　使用NLTK训练器训练
组块器　293
5.12.1　工作方式　293
5.12.2　工作原理　294
5.12.3　更多信息　295
5.12.4　请参阅　299
第6章　转换组块与树　300
6.1　引言　300
6.2　过滤句子中无意义的
单词　301
6.2.1　准备工作　301
6.2.2　工作方式　301
6.2.3　工作原理　302
6.2.4　更多信息　302
6.2.5　请参阅　303
6.3　纠正动词形式　303
6.3.1　准备工作　303
6.3.2　工作方式　303
6.3.3　工作原理　305
6.3.4　请参阅　306
6.4　交换动词短语　306
6.4.1　工作方式　306
6.4.2　工作原理　307
6.4.3　更多信息　307
6.4.4　请参阅　307
6.5　交换名词基数　308
6.5.1　工作方式　308
6.5.2　工作原理　309
6.5.3　请参阅　309
6.6　交换不定式短语　309
6.6.1　工作方式　309
6.6.2　工作原理　310
6.6.3　更多信息　310
6.6.4　请参阅　310
6.7　单数化复数名词　310
6.7.1　工作方式　310
6.7.2　工作原理　311
6.7.3　请参阅　311
6.8　链接组块变换　311
6.8.1　工作方式　311
6.8.2　工作原理　312
6.8.3　更多信息　312
6.8.4　请参阅　313
6.9　将组块树转换为文本　313
6.9.1　工作方式　313
6.9.2　工作原理　314
6.9.3　更多信息　314
6.9.4　请参阅　314
6.10　平展深度树　314
6.10.1　准备工作　315
6.10.2　工作方式　315
6.10.3　工作原理　316
6.10.4　更多信息　317
6.10.5　请参阅　318
6.11　创建浅树　318
6.11.1　工作方式　318
6.11.2　工作原理　320
6.11.3　请参阅　320
6.12　转换树标签　320
6.12.1　准备工作　320
6.12.2　工作方式　321
6.12.3　工作原理　322
6.12.4　请参阅　322
第7章　文本分类　323
7.1　引言　323
7.2　词袋特征提取　324
7.2.1　工作方式　324
7.2.2　工作原理　325
7.2.3　更多信息　325
7.2.4　请参阅　327
7.3　训练朴素贝叶斯
分类器　327
7.3.1　准备工作　327
7.3.2　工作方式　328
7.3.3　工作原理　329
7.3.4　更多信息　330
7.3.5　请参阅　333
7.4　训练决策树分类器　334
7.4.1　工作方式　334
7.4.2　工作原理　335
7.4.3　更多信息　335
7.4.4　请参阅　337
7.5　训练最大熵分类器　337
7.5.1　准备工作　337
7.5.2　工作方式　337
7.5.3　工作原理　338
7.5.4　更多信息　339
7.5.5　请参阅　340
7.6　训练scikit-learn
分类器　340
7.6.1　准备工作　341
7.6.2　工作方式　341
7.6.3　工作原理　342
7.6.4　更多信息　343
7.6.5　请参阅　345
7.7　衡量分类器的精准率和
召回率　346
7.7.1　工作方式　346
7.7.2　工作原理　347
7.7.3　更多信息　348
7.7.4　请参阅　349
7.8　计算高信息量单词　349
7.8.1　工作方式　350
7.8.2　工作原理　351
7.8.3　更多信息　352
7.8.4　请参阅　354
7.9　使用投票组合分类器　354
7.9.1　准备工作　355
7.9.2　工作方式　355
7.9.3　工作原理　356
7.9.4　请参阅　356
7.10　使用多个二元分类器
分类　357
7.10.1　准备工作　357
7.10.2　工作方式　357
7.10.3　工作原理　361
7.10.4　更多信息　362
7.10.5　请参阅　363
7.11　使用NLTK训练器训练
分类器　363
7.11.1　工作方式　363
7.11.2　工作原理　364
7.11.3　更多信息　365
7.11.4　请参阅　371
第8章　分布式进程和大型数据集的
处理　372
8.1　引言　372
8.2　使用execnet进行分布式
标注　372
8.2.1　准备工作　373
8.2.2　工作方式　373
8.2.3　工作原理　374
8.2.4　更多内容　375
8.2.5　请参阅　377
8.3　使用execnet进行分布式
组块　377
8.3.1　准备工作　377
8.3.2　工作方式　377
8.3.3　工作原理　378
8.3.4　更多内容　379
8.3.5　请参阅　379
8.4　使用execnet并行处理
列表　379
8.4.1　工作方式　379
8.4.2　工作原理　380
8.4.3　更多内容　381
8.4.4　请参阅　381
8.5　在Redis中存储频率分布　382
8.5.1　准备工作　382
8.5.2　工作方式　382
8.5.3　工作原理　384
8.5.4　更多内容　385
8.5.5　请参阅　386
8.6　在Redis中存储条件频率
分布　386
8.6.1　准备工作　386
8.6.2　工作方式　386
8.6.3　工作原理　387
8.6.4　更多内容　388
8.6.5　请参阅　388
8.7　在Redis中存储有序
字典　388
8.7.1　准备工作　388
8.7.2　工作方式　388
8.7.3　工作原理　390
8.7.4　更多内容　391
8.7.5　请参阅　392
8.8　使用Redis和execnet进行
分布式单词评分　392
8.8.1　准备工作　392
8.8.2　工作方式　392
8.8.3　工作原理　393
8.8.4　更多内容　396
8.8.5　请参阅　396
第9章　解析特定的数据类型　397
9.1　引言　397
9.2　使用dateutil解析日期和
时间　398
9.2.1　准备工作　398
9.2.2　工作方式　398
9.2.3　工作原理　399
9.2.4　更多信息　399
9.2.5　请参阅　399
9.3　时区的查找和转换　400
9.3.1　准备工作　400
9.3.2　工作方式　400
9.3.3　工作原理　402
9.3.4　更多信息　402
9.3.5　请参阅　403
9.4　使用lxml从HTML中提取
URL　403
9.4.1　准备工作　403
9.4.2　工作方式　403
9.4.3　工作原理　404
9.4.4　更多信息　404
9.4.5　请参阅　405
9.5　清理和剥离HTML　405
9.5.1　准备工作　405
9.5.2　工作方式　405
9.5.3　工作原理　405
9.5.4　更多信息　406
9.5.5　请参阅　406
9.6　使用BeautifulSoup转换
HTML实体　406
9.6.1　准备工作　406
9.6.2　工作方式　406
9.6.3　工作原理　407
9.6.4　更多信息　407
9.6.5　请参阅　407
9.7　检测和转换字符编码　407
9.7.1　准备工作　408
9.7.2　工作方式　408
9.7.3　工作原理　409
9.7.4　更多信息　409
9.7.5　请参阅　410
附录A　宾州treebank词性标签　411
模块3　使用Python掌握自然语言处理
第　1章 使用字符串　417
1.1　标记化　417
1.1.1　将文本标记为句子　418
1.1.2　其他语言文字的标记化　418
1.1.3　将句子标记为单词　419
1.1.4　使用TreebankWordTokenizer
进行标记化　420
1.1.5　使用正则表达式进行
标记化　421
1.2　规范化　424
1.2.1　消除标点符号　424
1.2.2　转化为小写和大写　425
1.2.3　处理停用词　425
1.2.4　计算英语中的停用词　426
1.3　替代和纠正标记　427
1.3.1　使用正则表达式替换
单词　427
1.3.2　使用一个文本替换另一个
文本的示例　428
1.3.3　在标记化之前进行
替代　428
1.3.4　处理重复的字符　428
1.3.5　删除重复字符的示例　429
1.3.6　使用单词的同义词替换
单词　430
1.4　在文本上应用齐夫定律　431
1.5　相似性量度　431
1.5.1　使用编辑距离算法应用
相似性量度　432
1.5.2　使用杰卡德系数应用
相似性量度　434
1.5.3　使用史密斯-沃特曼算法
应用相似性量度　434
1.5.4　其他字符串相似性指标　435
1.6　本章小结　436
第　2章 统计语言模型　437
2.1　单词频率　437
2.1.1　对给定文本进行最大
似然估计　441
2.1.2　隐马尔可夫模型估计　448
2.2　在MLE模型上应用平滑　450
2.2.1　加一平滑法　450
2.2.2　古德-图灵算法　451
2.2.3　聂氏估计　456
2.2.4　威滕 贝尔估计　457
2.3　为MLE指定回退机制　457
2.4　应用数据插值获得混合和
匹配　458
2.5　应用困惑度评估语言模型　458
2.6　在建模语言中应用
梅特罗波利斯-黑斯廷斯算法　459
2.7　在语言处理中应用
吉布斯采样　459
2.8　本章小结　461
第3章　词语形态学―试一试　462
3.1　词语形态学　462
3.2　词根还原器　463
3.3　词形还原　466
3.4　开发用于非英语语言的词根
还原器　467
3.5　词语形态分析器　469
3.6　词语形态生成器　471
3.7　搜索引擎　471
3.8　本章小结　475
第4章　词性标注―识别单词　476
4.1　词性标注　476
4.2　创建POS标注的语料库　482
4.3　选择某个机器学习算法　484
4.4　涉及n元组方法的统计建模　486
4.5　使用POS标注的语料库开发
组块器　491
4.6　本章小结　494
第5章　解析―分析训练数据　495
5.1　解析　495
5.2　构建树库　496
5.3　从树库中提取上下文无关文法的
规则　501
5.4　从CFG中创建概率上下文无关的
文法　507
5.5　CYK图解析算法　509
5.6　厄雷图解析算法　510
5.7　本章小结　516
第6章　语义分析―意义重大　517
6.1　语义分析　517
6.1.1　NER简介　521
6.1.2　使用隐马尔可夫模型的
NER系统　525
6.1.3　使用机器学习工具包训练
NER　530
6.1.4　使用POS标注的
NER　531
6.2　从Wordnet中生成同义词集
ID　534
6.3　使用Wordnet消除歧义　537
6.4　本章小结　541
第7章　情感分析―我很高兴　542
7.1　情感分析　542
7.2　使用机器学习的情感分析　548
7.3　本章小结　572
第8章　信息检索―访问信息　573
8.1　信息检索　573
8.1.1　停用词删除　574
8.1.2　利用向量空间模型进行
信息检索　576
8.2　向量空间评分以及与查询
操作器交互　583
8.3　利用隐含语义索引开发IR
系统　586
8.4　文本摘要　587
8.5　问答系统　588
8.6　本章小结　589
第9章　话语分析―知识就是信仰　590
9.1　话语分析　590
9.1.1　使用定中心理论进行
话语分析　595
9.1.2　回指解析　596
9.2　本章小结　601
第　10章 NLP系统的评估―
性能分析　602
10.1　对NLP系统进行评估的
需求　602
10.1.1　NLP工具（POS标注器、
词干还原器和形态分析器）
的评估　603
10.1.2　使用黄金数据评估
解析器　613
10.2　IR系统的评估　614
10.3　错误识别的指标　614
10.4　基于词汇匹配的指标　615
10.5　基于语法匹配的指标　619
10.6　使用浅层语义匹配的
指标　620
10.7　本章小结　621
参考书目　622
・ ・ ・ ・ ・ ・ (收起)第1章 中文语言的机器处理 1
1.1 历史回顾 2
1.1.1 从科幻到现实 2
1.1.2 早期的探索 3
1.1.3 规则派还是统计派 3
1.1.4 从机器学习到认知计算 5
1.2 现代自然语言系统简介 6
1.2.1 NLP流程与开源框架 6
1.2.2 哈工大NLP平台及其演示环境 9
1.2.3 Stanford NLP团队及其演示环境 11
1.2.4 NLTK开发环境 13
1.3 整合中文分词模块 16
1.3.1 安装Ltp Python组件 17
1.3.2 使用Ltp 3.3进行中文分词 18
1.3.3 使用结巴分词模块 20
1.4 整合词性标注模块 22
1.4.1 Ltp 3.3词性标注 23
1.4.2 安装StanfordNLP并编写Python接口类 24
1.4.3 执行Stanford词性标注 28
1.5 整合命名实体识别模块 29
1.5.1 Ltp 3.3命名实体识别 29
1.5.2 Stanford命名实体识别 30
1.6 整合句法解析模块 32
1.6.1 Ltp 3.3句法依存树 33
1.6.2 Stanford Parser类 35
1.6.3 Stanford短语结构树 36
1.6.4 Stanford依存句法树 37
1.7 整合语义角色标注模块 38
1.8 结语 40
第2章 汉语语言学研究回顾 42
2.1 文字符号的起源 42
2.1.1 从记事谈起 43
2.1.2 古文字的形成 47
2.2 六书及其他 48
2.2.1 象形 48
2.2.2 指事 50
2.2.3 会意 51
2.2.4 形声 53
2.2.5 转注 54
2.2.6 假借 55
2.3 字形的流变 56
2.3.1 笔与墨的形成与变革 56
2.3.2 隶变的方式 58
2.3.3 汉字的符号化与结构 61
2.4 汉语的发展 67
2.4.1 完整语义的基本形式DD句子 68
2.4.2 语言的初始形态与文言文 71
2.4.3 白话文与复音词 73
2.4.4 白话文与句法研究 78
2.5 三个平面中的语义研究 80
2.5.1 词汇与本体论 81
2.5.2 格语法及其框架 84
2.6 结语 86
第3章 词汇与分词技术 88
3.1 中文分词 89
3.1.1 什么是词与分词规范 90
3.1.2 两种分词标准 93
3.1.3 歧义、机械分词、语言模型 94
3.1.4 词汇的构成与未登录词 97
3.2 系统总体流程与词典结构 98
3.2.1 概述 98
3.2.2 中文分词流程 99
3.2.3 分词词典结构 103
3.2.4 命名实体的词典结构 105
3.2.5 词典的存储结构 108
3.3 算法部分源码解析 111
3.3.1 系统配置 112
3.3.2 Main方法与例句 113
3.3.3 句子切分 113
3.3.4 分词流程 117
3.3.5 一元词网 118
3.3.6 二元词图 125
3.3.7 NShort算法原理 130
3.3.8 后处理规则集 136
3.3.9 命名实体识别 137
3.3.10 细分阶段与最短路径 140
3.4 结语 142
第4章 NLP中的概率图模型 143
4.1 概率论回顾 143
4.1.1 多元概率论的几个基本概念 144
4.1.2 贝叶斯与朴素贝叶斯算法 146
4.1.3 文本分类 148
4.1.4 文本分类的实现 151
4.2 信息熵 154
4.2.1 信息量与信息熵 154
4.2.2 互信息、联合熵、条件熵 156
4.2.3 交叉熵和KL散度 158
4.2.4 信息熵的NLP的意义 159
4.3 NLP与概率图模型 160
4.3.1 概率图模型的几个基本问题 161
4.3.2 产生式模型和判别式模型 162
4.3.3 统计语言模型与NLP算法设计 164
4.3.4 极大似然估计 167
4.4 隐马尔科夫模型简介 169
4.4.1 马尔科夫链 169
4.4.2 隐马尔科夫模型 170
4.4.3 HMMs的一个实例 171
4.4.4 Viterbi算法的实现 176
4.5 最大熵模型 179
4.5.1 从词性标注谈起 179
4.5.2 特征和约束 181
4.5.3 最大熵原理 183
4.5.4 公式推导 185
4.5.5 对偶问题的极大似然估计 186
4.5.6 GIS实现 188
4.6 条件随机场模型 193
4.6.1 随机场 193
4.6.2 无向图的团（Clique）与因子分解 194
4.6.3 线性链条件随机场 195
4.6.4 CRF的概率计算 198
4.6.5 CRF的参数学习 199
4.6.6 CRF预测标签 200
4.7 结语 201
第5章 词性、语块与命名实体识别 202
5.1 汉语词性标注 203
5.1.1 汉语的词性 203
5.1.2 宾州树库的词性标注规范 205
5.1.3 stanfordNLP标注词性 210
5.1.4 训练模型文件 213
5.2 语义组块标注 219
5.2.1 语义组块的种类 220
5.2.2 细说NP 221
5.2.3 细说VP 223
5.2.4 其他语义块 227
5.2.5 语义块的抽取 229
5.2.6 CRF的使用 232
5.3 命名实体识别 240
5.3.1 命名实体 241
5.3.2 分词架构与专名词典 243
5.3.3 算法的策略DD词典与统计相结合 245
5.3.4 算法的策略DD层叠式架构 252
5.4 结语 259
第6章 句法理论与自动分析 260
6.1 转换生成语法 261
6.1.1 乔姆斯基的语言观 261
6.1.2 短语结构文法 263
6.1.3 汉语句类 269
6.1.4 谓词论元与空范畴 274
6.1.5 轻动词分析理论 279
6.1.6 NLTK操作句法树 280
6.2 依存句法理论 283
6.2.1 配价理论 283
6.2.2 配价词典 285
6.2.3 依存理论概述 287
6.2.4 Ltp依存分析介绍 290
6.2.5 Stanford依存转换、解析 293
6.3 PCFG短语结构句法分析 298
6.3.1 PCFG短语结构 298
6.3.2 内向算法和外向算法 301
6.3.3 Viterbi算法 303
6.3.4 参数估计 304
6.3.5 Stanford 的PCFG算法训练 305
6.4 结语 310
第7章 建设语言资源库 311
7.1 语料库概述 311
7.1.1 语料库的简史 312
7.1.2 语言资源库的分类 314
7.1.3 语料库的设计实例：国家语委语料库 315
7.1.4 语料库的层次加工 321
7.2 语法语料库 323
7.2.1 中文分词语料库 323
7.2.2 中文分词的测评 326
7.2.3 宾州大学CTB简介 327
7.3 语义知识库 333
7.3.1 知识库与HowNet简介 333
7.3.2 发掘义原 334
7.3.3 语义角色 336
7.3.4 分类原则与事件分类 344
7.3.5 实体分类 347
7.3.6 属性与分类 352
7.3.7 相似度计算与实例 353
7.4 语义网与百科知识库 360
7.4.1 语义网理论介绍 360
7.4.2 维基百科知识库 364
7.4.3 DBpedia抽取原理 365
7.5 结语 368
第8章 语义与认知 370
8.1 回顾现代语义学 371
8.1.1 语义三角论 371
8.1.2 语义场论 373
8.1.3 基于逻辑的语义学 376
8.2 认知语言学概述 377
8.2.1 象似性原理 379
8.2.2 顺序象似性 380
8.2.3 距离象似性 380
8.2.4 重叠象似性 381
8.3 意象图式的构成 383
8.3.1 主观性与焦点 383
8.3.2 范畴化：概念的认知 385
8.3.3 主体与背景 390
8.3.4 意象图式 392
8.3.5 社交中的图式 396
8.3.6 完形：压缩与省略 398
8.4 隐喻与转喻 401
8.4.1 隐喻的结构 402
8.4.2 隐喻的认知本质 403
8.4.3 隐喻计算的系统架构 405
8.4.4 隐喻计算的实现 408
8.5 构式语法 412
8.5.1 构式的概念 413
8.5.2 句法与构式 415
8.5.3 构式知识库 417
8.6 结语 420
第9章 NLP中的深度学习 422
9.1 神经网络回顾 422
9.1.1 神经网络框架 423
9.1.2 梯度下降法推导 425
9.1.3 梯度下降法的实现 427
9.1.4 BP神经网络介绍和推导 430
9.2 Word2Vec简介 433
9.2.1 词向量及其表达 434
9.2.2 Word2Vec的算法原理 436
9.2.3 训练词向量 439
9.2.4 大规模上下位关系的自动识别 443
9.3 NLP与RNN 448
9.3.1 Simple-RNN 449
9.3.2 LSTM原理 454
9.3.3 LSTM的Python实现 460
9.4 深度学习框架与应用 467
9.4.1 Keras框架介绍 467
9.4.2 Keras序列标注 471
9.4.3 依存句法的算法原理 478
9.4.4 Stanford依存解析的训练过程 483
9.5 结语 488
第10章 语义计算的架构 490
10.1 句子的语义和语法预处理 490
10.1.1 长句切分和融合 491
10.1.2 共指消解 496
10.2 语义角色 502
10.2.1 谓词论元与语义角色 502
10.2.2 PropBank简介 505
10.2.3 CPB中的特殊句式 506
10.2.4 名词性谓词的语义角色 509
10.2.5 PropBank展开 512
10.3 句子的语义解析 517
10.3.1 语义依存 517
10.3.2 完整架构 524
10.3.3 实体关系抽取 527
10.4 结语 531
・ ・ ・ ・ ・ ・ (收起)第1章 起步　　1
1.1 NLP中的基本概念和术语　　1
1.1.1 文本语料库　　1
1.1.2 段落　　2
1.1.3 句子　　2
1.1.4 短语和单词　　2
1.1.5 n元语法　　2
1.1.6 词袋　　2
1.2 NLP技术的应用　　3
1.2.1 情感分析　　3
1.2.2 命名实体识别　　4
1.2.3 实体链接　　5
1.2.4 文本翻译　　6
1.2.5 自然语言推理　　6
1.2.6 语义角色标记　　6
1.2.7 关系提取　　7
1.2.8 SQL查询生成或语义解析　　8
1.2.9 机器阅读理解　　8
1.2.10 文字蕴含　　10
1.2.11 指代消解　　10
1.2.12 搜索　　11
1.2.13 问答和聊天机器人　　11
1.2.14 文本转语音　　12
1.2.15 语音转文本　　13
1.2.16 说话人识别　　14
1.2.17 口语对话系统　　14
1.2.18 其他应用　　14
1.3 小结　　15
第2章 使用NLTK进行文本分类和词性标注　　16
2.1 安装NLTK 及其模块　　16
2.2 文本预处理及探索性分析　　18
2.2.1 分词　　18
2.2.2 词干提取　　19
2.2.3 去除停用词　　20
2.2.4 探索性分析　　20
2.3 词性标注　　24
2.3.1 词性标注定义　　24
2.3.2 词性标注的应用　　25
2.3.3 训练词性标注器　　25
2.4 训练影评情感分类器　　29
2.5 训练词袋分类器　　32
2.6 小结　　34
第3章 深度学习和TensorFlow　　35
3.1 深度学习　　35
3.1.1 感知器　　35
3.1.2 激活函数　　36
3.1.3 神经网络　　38
3.1.4 训练神经网络　　40
3.1.5 卷积神经网络　　43
3.1.6 递归神经网络　　44
3.2 TensorFlow　　45
3.2.1 通用图形处理单元　　45
3.2.2 安装　　46
3.2.3 Hello world !　　47
3.2.4 两数相加　　47
3.2.5 TensorBoard　　48
3.2.6 Keras库　　49
3.3 小结　　49
第4章 使用浅层模型进行语义嵌入　　50
4.1 词向量　　50
4.1.1 经典方法　　50
4.1.2 Word2vec　　51
4.1.3 连续词袋模型　　52
4.1.4 跳字模型　　53
4.2 从单词到文档嵌入　　59
4.3 Sentence2vec　　59
4.4 Doc2vec　　60
4.5 小结　　63
第5章 使用LSTM进行文本分类　　64
5.1 文本分类数据　　64
5.2 主题建模　　65
5.3 用于文本分类的深度学习元架构　　68
5.3.1 嵌入层　　68
5.3.2 深层表示　　68
5.3.3 全连接部分　　68
5.4 使用RNN识别YouTube视频垃圾评论　　69
5.5 使用CNN对新闻主题分类　　73
5.6 使用GloVe嵌入进行迁移学习　　76
5.7 多标签分类　　79
5.7.1 二元关联　　80
5.7.2 用于多标签分类的深度学习　　80
5.7.3 用于文档分类的attention网络　　81
5.8 小结　　83
第6章 使用CNN进行搜索和去重　　84
6.1 数据　　84
6.2 模型训练　　85
6.2.1 文本编码　　86
6.2.2 建立CNN模型　　87
6.2.3 训练　　89
6.2.4 推理　　91
6.3 小结　　92
第7章 使用字符级LSTM进行命名实体识别　　93
7.1 使用深度学习实现NER　　93
7.1.1 数据　　94
7.1.2 模型　　95
7.1.3 代码详解　　96
7.1.4 不同预训练词嵌入的影响　　98
7.1.5 改进空间　　105
7.2 小结　　105
第8章 使用GRU 进行文本生成和文本摘要　　106
8.1 使用RNN进行文本生成　　106
8.2 文本摘要　　112
8.2.1 提取式摘要　　112
8.2.2 抽象式摘要　　114
8.2.3 最新抽象式文本摘要　　123
8.3 小结　　125
第9章 使用记忆网络完成问答任务和编写聊天机器人　　127
9.1 QA任务　　127
9.2 用于QA任务的记忆网络　　128
9.2.1 记忆网络管道概述　　128
9.2.2 使用TensorFlow写一个记忆网络　　129
9.3 拓展记忆网络以进行对话建模　　134
9.3.1 对话数据集　　134
9.3.2 使用TensorFlow编写一个聊天机器人　　137
9.3.3 记忆网络相关文献　　146
9.4 小结　　146
第10章 使用基于attention的模型进行机器翻译　　147
10.1 机器翻译概述　　147
10.1.1 统计机器翻译　　147
10.1.2 神经机器翻译　　150
10.2 小结　　163
第11章 使用DeepSpeech进行语音识别　　164
11.1 语音识别概述　　164
11.2 建立用于语音识别的RNN模型　　165
11.2.1 语音信号表示　　165
11.2.2 用于语音数字识别的LSTM模型　　167
11.2.3 TensorBoard可视化　　168
11.2.4 使用DeepSpeech架构的语音转文本模型　　169
11.2.5 语音识别最新技术　　178
11.3 小结　　179
第12章 使用Tacotron进行文本转语音　　180
12.1 TTS领域概述　　181
12.1.1 自然性与可懂性　　181
12.1.2 TTS系统表现的评估方式　　181
12.1.3 传统技术――级联模型和参数模型　　182
12.1.4 关于频谱图和梅尔标度的一些提醒　　182
12.2 深度学习中的TTS　　185
12.2.1 WaveNet简介　　186
12.2.2 Tacotron　　186
12.3 利用Keras的Tacotron实现　　191
12.3.1 数据集　　192
12.3.2 数据准备　　192
12.3.3 架构实现　　196
12.3.4 训练与测试　　200
12.4 小结　　201
第13章 部署训练好的模型　　202
13.1 性能提升　　202
13.1.1 量化权重　　202
13.1.2 MobileNets　　203
13.2 TensorFlow Serving　　205
13.2.1 导出训练好的模型　　206
13.2.2 把导出模型投入服务　　207
13.3 在云上部署　　207
13.3.1 Amazon Web Services　　207
13.3.2 Google Cloud Platform　　210
13.4 在移动设备上部署　　213
13.4.1 iPhone　　213
13.4.2 Android　　213
13.5 小结　　213
・ ・ ・ ・ ・ ・ (收起)序一
序二
前言
第1章 NLP基础 1
1.1 什么是NLP 1
1.1.1 NLP的概念 1
1.1.2 NLP的研究任务 3
1.2 NLP的发展历程 5
1.3 NLP相关知识的构成 7
1.3.1 基本术语 7
1.3.2 知识结构 9
1.4 语料库 10
1.5 探讨NLP的几个层面 11
1.6 NLP与人工智能 13
1.7 本章小结 15
第2章 NLP前置技术解析 16
2.1 搭建Python开发环境 16
2.1.1 Python的科学计算发行版――Anaconda 17
2.1.2 Anaconda的下载与安装 19
2.2 正则表达式在NLP的基本应用 21
2.2.1 匹配字符串 22
2.2.2 使用转义符 26
2.2.3 抽取文本中的数字 26
2.3 Numpy使用详解 27
2.3.1 创建数组 28
2.3.2 获取Numpy中数组的维度 30
2.3.3 获取本地数据 31
2.3.4 正确读取数据 32
2.3.5 Numpy数组索引 32
2.3.6 切片 33
2.3.7 数组比较 33
2.3.8 替代值 34
2.3.9 数据类型转换 36
2.3.10 Numpy的统计计算方法 36
2.4 本章小结 37
第3章 中文分词技术 38
3.1 中文分词简介 38
3.2 规则分词 39
3.2.1 正向最大匹配法 39
3.2.2 逆向最大匹配法 40
3.2.3 双向最大匹配法 41
3.3 统计分词 42
3.3.1 语言模型 43
3.3.2 HMM模型 44
3.3.3 其他统计分词算法 52
3.4 混合分词 52
3.5 中文分词工具――Jieba 53
3.5.1 Jieba的三种分词模式 54
3.5.2 实战之高频词提取 55
3.6 本章小结 58
第4章 词性标注与命名实体识别 59
4.1 词性标注 59
4.1.1 词性标注简介 59
4.1.2 词性标注规范 60
4.1.3 Jieba分词中的词性标注 61
4.2 命名实体识别 63
4.2.1 命名实体识别简介 63
4.2.2 基于条件随机场的命名实体识别 65
4.2.3 实战一：日期识别 69
4.2.4 实战二：地名识别 75
4.3 总结 84
第5章 关键词提取算法 85
5.1 关键词提取技术概述 85
5.2 关键词提取算法TF/IDF算法 86
5.3 TextRank算法 88
5.4 LSA/LSI/LDA算法 91
5.4.1 LSA/LSI算法 93
5.4.2 LDA算法 94
5.5 实战提取文本关键词 95
5.6 本章小结 105
第6章 句法分析 106
6.1 句法分析概述 106
6.2 句法分析的数据集与评测方法 107
6.2.1 句法分析的数据集 108
6.2.2 句法分析的评测方法 109
6.3 句法分析的常用方法 109
6.3.1 基于PCFG的句法分析 110
6.3.2 基于最大间隔马尔可夫网络的句法分析 112
6.3.3 基于CRF的句法分析 113
6.3.4 基于移进C归约的句法分析模型 113
6.4 使用Stanford Parser的PCFG算法进行句法分析 115
6.4.1 Stanford Parser 115
6.4.2 基于PCFG的中文句法分析实战 116
6.5 本章小结 119
第7章 文本向量化 120
7.1 文本向量化概述 120
7.2 向量化算法word2vec 121
7.2.1 神经网络语言模型 122
7.2.2 C&W模型 124
7.2.3 CBOW模型和Skip-gram模型 125
7.3 向量化算法doc2vec/str2vec 127
7.4 案例：将网页文本向量化 129
7.4.1 词向量的训练 129
7.4.2 段落向量的训练 133
7.4.3 利用word2vec和doc2vec计算网页相似度 134
7.5 本章小结 139
第8章 情感分析技术 140
8.1 情感分析的应用 141
8.2 情感分析的基本方法 142
8.2.1 词法分析 143
8.2.2 机器学习方法 144
8.2.3 混合分析 144
8.3 实战电影评论情感分析 145
8.3.1 卷积神经网络 146
8.3.2 循环神经网络 147
8.3.3 长短时记忆网络 148
8.3.4 载入数据 150
8.3.5 辅助函数 154
8.3.6 模型设置 155
8.3.7 调参配置 158
8.3.8 训练过程 159
8.4 本章小结 159
第9章 NLP中用到的机器学习算法 160
9.1 简介 160
9.1.1 机器学习训练的要素 161
9.1.2 机器学习的组成部分 162
9.2 几种常用的机器学习方法 166
9.2.1 文本分类 166
9.2.2 特征提取 168
9.2.3 标注 169
9.2.4 搜索与排序 170
9.2.5 推荐系统 170
9.2.6 序列学习 172
9.3 分类器方法 173
9.3.1 朴素贝叶斯Naive Bayesian 173
9.3.2 逻辑回归 174
9.3.3 支持向量机 175
9.4 无监督学习的文本聚类 177
9.5 文本分类实战：中文垃圾邮件分类 180
9.5.1 实现代码 180
9.5.2 评价指标 187
9.6 文本聚类实战：用K-means对豆瓣读书数据聚类 190
9.7 本章小结 194
第10章 基于深度学习的NLP算法 195
10.1 深度学习概述 195
10.1.1 神经元模型 196
10.1.2 激活函数 197
10.1.3 感知机与多层网络 198
10.2 神经网络模型 201
10.3 多输出层模型 203
10.4 反向传播算法 204
10.5 最优化算法 208
10.5.1 梯度下降 208
10.5.2 随机梯度下降 209
10.5.3 批量梯度下降 210
10.6 丢弃法 211
10.7 激活函数 211
10.7.1 tanh函数 212
10.7.2 ReLU函数 212
10.8 实现BP算法 213
10.9 词嵌入算法 216
10.9.1 词向量 217
10.9.2 word2vec简介 217
10.9.3 词向量模型 220
10.9.4 CBOW和Skip-gram模型 222
10.10 训练词向量实践 224
10.11 朴素Vanilla-RNN 227
10.12 LSTM网络 230
10.12.1 LSTM基本结构 230
10.12.2 其他LSTM变种形式 234
10.13 Attention机制 236
10.13.1 文本翻译 237
10.13.2 图说模型 237
10.13.3 语音识别 239
10.13.4 文本摘要 239
10.14 Seq2Seq模型 240
10.15 图说模型 242
10.16 深度学习平台 244
10.16.1 Tensorflow 245
10.16.2 Mxnet 246
10.16.3 PyTorch 246
10.16.4 Caffe 247
10.16.5 Theano 247
10.17 实战Seq2Seq问答机器人 248
10.18 本章小结 254
第11章 Solr搜索引擎 256
11.1 全文检索的原理 257
11.2 Solr简介与部署 258
11.3 Solr后台管理描述 263
11.4 配置schema 267
11.5 Solr管理索引库 270
11.5.1 创建索引 270
11.5.2 查询索引 276
11.5.3 删除文档 279
11.6 本章小结 281
・ ・ ・ ・ ・ ・ (收起)第 1章　什么是文本分析 1
1.1　什么是文本分析　1
1.2　搜集数据　5
1.3　若输入错误数据，则输出亦为错误数据（garbage in，garbage out）　8
1.4　为什么你需要文本分析　9
1.5　总结　11
第　2章 Python文本分析技巧　12
2.1　为什么用Python来做文本分析　12
2.2　用Python进行文本操作　14
2.3　总结　18
第3章　spaCy语言模型　19
3.1　spaCy库　19
3.2　spaCy的安装步骤　21
3.3　故障排除　22
3.4　语言模型　22
3.5　安装语言模型　23
3.6　安装语言模型的方式及原因　25
3.7　语言模型的基本预处理操作　25
3.8　分词　26
3.9　词性标注　28
3.10　命名实体识别　29
3.11　规则匹配　30
3.12　预处理　31
3.13　总结　33
第4章　Gensim：文本向量化、向量变换和n-grams的工具　34
4.1　Gensim库介绍　34
4.2　向量以及为什么需要向量化　35
4.3　词袋（bag-of-words）　36
4.4　TF-IDF（词频-反向文档频率）　37
4.5　其他表示方式　38
4.6　Gensim中的向量变换　38
4.7　n-grams及其预处理技术　42
4.8　总结　44
第5章　词性标注及其应用　45
5.1　什么是词性标注　45
5.2　使用Python实现词性标注　49
5.3　使用spaCy进行词性标注　50
5.4　从头开始训练一个词性标注模型　53
5.5　词性标注的代码示例　57
5.6　总结　59
第6章　NER标注及其应用　60
6.1　什么是NER标注　60
6.2　用Python实现NER标注　64
6.3　使用spaCy实现NER标注　67
6.4　从头开始训练一个NER标注器　72
6.5　NER标注应用实例和可视化　77
6.6　总结　79
第7章　依存分析　80
7.1　依存分析　80
7.2　用Python实现依存分析　85
7.3　用spaCy实现依存分析　87
7.4　从头开始训练一个依存分析器　91
7.5　总结　98
第8章　主题模型　99
8.1　什么是主题模型　99
8.2　使用Gensim构建主题模型　101
8.3　隐狄利克雷分配（Latent Dirichlet Allocation）　102
8.4　潜在语义索引（Latent Semantic Indexing）　104
8.5　分层狄利特雷过程（Hierarchical Dirichlet Process）　105
8.6　动态主题模型　108
8.7　使用scikit-learn构建主题模型　109
8.8　总结　112
第9章　高级主题建模　113
9.1　高级训练技巧　113
9.2　探索文档　117
9.3　主题一致性和主题模型的评估　121
9.4　主题模型的可视化　123
9.5　总结　127
第　10章 文本聚类和文本分类　128
10.1　文本聚类　128
10.2　聚类前的准备工作　129
10.3　K-means　132
10.4　层次聚类　134
10.5　文本分类　136
10.6　总结　138
第　11章 查询词相似度计算和文本摘要　139
11.1　文本距离的度量　139
11.2　查询词相似度计算　145
11.3　文本摘要　147
11.4　总结　153
第　12章 Word2Vec、Doc2Vec和Gensim　154
12.1　Word2Vec　154
12.2　用Gensim实现Word2Vec　155
12.3　Doc2Vec　160
12.4　其他词嵌入技术　166
12.5　总结　172
第　13章 使用深度学习处理文本　173
13.1　深度学习　173
13.2　深度学习在文本上的应用　174
13.3　文本生成　177
13.4　总结　182
第　14章 使用Keras和spaCy进行深度学习　183
14.1　Keras和spaCy　183
14.2　使用Keras进行文本分类　185
14.3　使用spaCy进行文本分类　191
14.4　总结　201
第　15章 情感分析与聊天机器人　202
15.1　情感分析　202
15.2　基于Reddit的新闻数据挖掘　205
15.3　基于Twitter的微博数据挖掘　207
15.4　聊天机器人　209
15.5　总结　217
・ ・ ・ ・ ・ ・ (收起)译者序
前言
作者名单
第1章 延迟解释、浅层处理和构式：“尽可能解释”原则的基础 1
1.1 引言 1
1.2 延迟处理 2
1.3 工作记忆 5
1.4 如何识别语块：分词操作 7
1.5 延迟架构 10
1.5.1 分段和存储 11
1.5.2 内聚聚集 12
1.6 结论 15
1.7 参考文献 16
第2章 人类关联规范能否评估机器制造的关联列表 19
2.1 引言 19
2.2 人类语义关联 20
2.2.1 单词关联测试 20
2.2.2 作者的实验 21
2.2.3 人类关联拓扑 22
2.2.4 人类关联具有可比性 24
2.3 算法效率比较 26
2.3.1 语料库 26
2.3.2 LSA源关联列表 27
2.3.3 LDA源列表 28
2.3.4 基于关联比率的列表 28
2.3.5 列表比较 29
2.4 结论 33
2.5 参考文献 34
第3章 文本词如何在人类关联网络中选择相关词 37
3.1 引言 37
3.2 网络 40
3.3 基于文本的激励驱动的网络提取 42
3.3.1 子图提取算法 42
3.3.2 控制流程 43
3.3.3 最短路径提取 44
3.3.4 基于语料库的子图 46
3.4 网络提取流程的测试 46
3.4.1 进行测试的语料库 46
3.4.2 提取子图的评估 46
3.4.3 有向和无向子图提取：对比 48
3.4.4 每个激励产生的结果 49
3.5 对结果和相关工作的简要讨论 54
3.6 参考文献 57
第4章 反向关联任务 59
4.1 引言 59
4.2 计算前向关联 63
4.2.1 步骤 63
4.2.2 结果和评估 65
4.3 计算反向关联 67
4.3.1 问题 67
4.3.2 步骤 67
4.3.3 结果和评估 71
4.4 人类的表现 73
4.4.1 数据集 73
4.4.2 测试流程 75
4.4.3 评估 76
4.5 机器性能 77
4.6 讨论、结果和展望 78
4.6.1 人类的反向关联 78
4.6.2 机器的反向关联 80
4.7 致谢 82
4.8 参考文献 82
第5章 词汇的隐藏结构与功能 85
5.1 引言 86
5.2 方法 86
5.2.1 词典图 86
5.2.2 心理语言学变量 90
5.2.3 数据分析 91
5.3 内核、卫星、核心、MinSet以及词典余下部分的心理语言学属性 93
5.4 讨论 96
5.5 未来工作 99
5.6 参考文献 101
第6章 用于词义消歧的直推式学习博弈 103
6.1 引言 103
6.2 基于图的词义消歧 104
6.3 半监督学习方法 107
6.3.1 基于图的半监督学习 107
6.3.2 博弈论和博弈动态 108
6.4 词义消歧博弈 110
6.4.1 图构造 110
6.4.2 策略空间 111
6.4.3 收益矩阵 111
6.4.4 系统动力学 112
6.5 评估 113
6.5.1 实验设置 113
6.5.2 评估结果 114
6.5.3 对比先进水平算法 116
6.6 结论 117
6.7 参考文献 117
第7章 用心学写：生成连贯文本的问题 121
7.1 问题 121
7.2 次优文本及其相关原因 123
7.2.1 缺乏连贯性或凝聚力 124
7.2.2 错误引用 125
7.2.3 无动机的主题转移 126
7.3 如何解决任务的复杂性 127
7.4 相关研究 128
7.5 关于构建辅助写作过程的工具的假设 130
7.6 方法论 133
7.6.1 句法结构的识别 135
7.6.2 语义种子词的识别 135
7.6.3 单词对齐 137
7.6.4 确定对齐单词的相似性值 137
7.6.5 确定句子之间的相似性 141
7.6.6 基于句子相似性值的聚类 142
7.7 实验结果和评估 142
7.8 展望和总结 145
7.9 参考文献 146
第8章 面向著述属性的基于序贯规则挖掘的文体特征 149
8.1 引言和研究动机 149
8.2 著述属性过程 151
8.3 著述属性的文体特征 152
8.4 针对文体分析的时序数据挖掘 154
8.5 实验设置 155
8.5.1 数据集 156
8.5.2 分类方案 157
8.6 结果和讨论 158
8.7 结论 162
8.8 参考文献 162
第9章 一种并行的、面向认知的基频估计算法 165
9.1 引言 165
9.2 语音信号分割 167
9.2.1 语音和停顿段 168
9.2.2 浊音和清音区 169
9.2.3 稳定和不稳定区间 170
9.3 稳定区间的F0估计 171
9.4 F0传播 173
9.4.1 控制流 174
9.4.2 峰值传播 175
9.5 不稳定的浊音区域 178
9.6 并行化 178
9.7 实验和结果 179
9.8 结论 180
9.9 致谢 181
9.10 参考文献 182
第10章 基于完形填充、脑电图和眼球运动数据对n元语言模型、主题模型和循环神经网络的基准测试 185
10.1 引言 186
10.2 相关工作 187
10.3 方法 188
10.3.1 人类绩效评估 188
10.3.2 语言模型的三种风格 189
10.4 实验设置 192
10.5 结果 193
10.5.1 可预测性结果 193
10.5.2 N400振幅结果 196
10.5.3 单一注视时延结果 198
10.6 讨论和结论 200
10.7 致谢 202
10.8 参考文献 202
术语表 207
・ ・ ・ ・ ・ ・ (收起)第1章　应用自然语言处理技术 1
1.1　付出与回报 2
1.1.1　如何开始 2
1.1.2 招聘人员 2
1.1.3 学习 3
1.2 开发环境 3
1.3 技术基础 4
1.3.1 Java 4
1.3.2 规则方法 5
1.3.3 统计方法 5
1.3.4 计算框架 5
1.3.5 文本挖掘 7
1.3.6 语义库 7
1.4 本章小结 9
1.5 专业术语 9
第2章　中文分词原理与实现 11
2.1 接口 12
2.1.1 切分方案 13
2.1.2 词特征 13
2.2 查找词典算法 13
2.2.1 标准Trie树 14
2.2.2 三叉Trie树 18
2.2.3 词典格式 26
2.3 最长匹配中文分词 27
2.3.1 正向最大长度匹配法 28
2.3.2 逆向最大长度匹配法 33
2.3.3 处理未登录串 39
2.3.4 开发分词 43
2.4 概率语言模型的分词方法 45
2.4.1 一元模型 47
2.4.2 整合基于规则的方法 54
2.4.3 表示切分词图 55
2.4.4 形成切分词图 62
2.4.5 数据基础 64
2.4.6 改进一元模型 75
2.4.7 二元词典 79
2.4.8 完全二叉树组 85
2.4.9 三元词典 89
2.4.10 N元模型 90
2.4.11 N元分词 91
2.4.12 生成语言模型 99
2.4.13 评估语言模型 100
2.4.14 概率分词的流程与结构 101
2.4.15 可变长N元分词 102
2.4.16 条件随机场 103
2.5 新词发现 103
2.5.1 成词规则 109
2.6 词性标注 109
2.6.1 数据基础 114
2.6.2 隐马尔可夫模型 115
2.6.3 存储数据 124
2.6.4 统计数据 131
2.6.5 整合切分与词性标注 133
2.6.6 大词表 138
2.6.7 词性序列 138
2.6.8 基于转换的错误学习方法 138
2.6.9 条件随机场 141
2.7 词类模型 142
2.8 未登录词识别 144
2.8.1 未登录人名 144
2.8.2 提取候选人名 145
2.8.3 最长人名切分 153
2.8.4 一元概率人名切分 153
2.8.5 二元概率人名切分 156
2.8.6 未登录地名 159
2.8.7 未登录企业名 160
2.9 平滑算法 160
2.10 机器学习的方法 164
2.10.1 最大熵 165
2.10.2 条件随机场 170
2.11 有限状态机 171
2.12 地名切分 178
2.12.1 识别未登录地名 179
2.12.2 整体流程 185
2.13 企业名切分 187
2.13.1 识别未登录词 188
2.13.2 整体流程 190
2.14 结果评测 190
2.15 本章小结 191
2.16 专业术语 193
第3章　英文分析 194
3.1 分词 194
3.1.1 句子切分 194
3.1.2 识别未登录串 197
3.1.3 切分边界 198
3.2 词性标注 199
3.3 重点词汇 202
3.4 句子时态 203
3.5 本章小结 204
第4章　依存文法分析 205
4.1 句法分析树 205
4.2 依存文法 211
4.2.1 中文依存文法 211
4.2.2 英文依存文法 220
4.2.3 生成依存树 232
4.2.4 遍历 235
4.2.5 机器学习的方法 237
4.3 小结 237
4.4 专业术语 238
第5章　文档排重 239
5.1 相似度计算 239
5.1.1 夹角余弦 239
5.1.2 最长公共子串 242
5.1.3 同义词替换 246
5.1.4 地名相似度 248
5.1.5 企业名相似度 251
5.2 文档排重 251
5.2.1 关键词排重 251
5.2.2 SimHash 254
5.2.3 分布式文档排重 268
5.2.4 使用文本排重 269
5.3 在搜索引擎中使用文本排重 269
5.4 本章小结 270
5.5 专业术语 270
第6章　信息提取 271
6.1 指代消解 271
6.2 中文关键词提取 273
6.2.1 关键词提取的基本方法 273
6.2.2 HITS算法应用于关键词提取 275
6.2.3 从网页中提取关键词 277
6.3 信息提取 278
6.3.1 提取联系方式 280
6.3.2 从互联网提取信息 281
6.3.3 提取地名 282
6.4 拼写纠错 283
6.4.1 模糊匹配问题 285
6.4.2 正确词表 296
6.4.3 英文拼写检查 298
6.4.4 中文拼写检查 300
6.5 输入提示 302
6.6 本章小结 303
6.7 专业术语 303
第7章　自动摘要 304
7.1 自动摘要技术 305
7.1.1 英文文本摘要 307
7.1.2 中文文本摘要 309
7.1.3 基于篇章结构的自动摘要 314
7.1.4 句子压缩 314
7.2 指代消解 314
7.3 Lucene中的动态摘要 314
7.4 本章小结 317
7.5 专业术语 318
第8章　文本分类 319
8.1 地名分类 321
8.2 错误类型分类 321
8.3 特征提取 322
8.4 关键词加权法 326
8.5 朴素贝叶斯 330
8.6 贝叶斯文本分类 336
8.7 支持向量机 336
8.7.1 多级分类 345
8.7.2 规则方法 347
8.7.3 网页分类 350
8.8 最大熵 351
8.9 信息审查 352
8.10 文本聚类 353
8.10.1 K均值聚类方法 353
8.10.2 K均值实现 355
8.10.3 深入理解DBScan算法 359
8.10.4 使用DBScan算法聚类实例 361
8.11 本章小结 363
8.12 专业术语 363
第9章　文本倾向性分析 364
9.1 确定词语的褒贬倾向 367
9.2 实现情感识别 368
9.3 本章小结 372
9.4 专业术语 373
第10章　问答系统 374
10.1 问答系统的结构 375
10.1.1 提取问答对 376
10.1.2 等价问题 376
10.2 问句分析 377
10.2.1 问题类型 377
10.2.2 句型 381
10.2.3 业务类型 381
10.2.4 依存树 381
10.2.5 指代消解 383
10.2.6 二元关系 383
10.2.7 逻辑表示 386
10.2.8 问句模板 386
10.2.9 结构化问句模板 389
10.2.10 检索方式 390
10.2.11 问题重写 395
10.2.12 提取事实 395
10.2.13 验证答案 398
10.2.14 无答案的处理 398
10.3 知识库 398
10.4 聊天机器人 399
10.4.1 交互式问答 401
10.4.2 垂直领域问答系统 402
10.4.3 语料库 405
10.4.4 客户端 405
10.5 自然语言生成 405
10.6 依存句法 406
10.7 提取同义词 410
10.7.1 流程 410
10.8 本章小结 411
10.9 术语表 412
第11章　语音识别 413
11.1 总体结构 414
11.1.1 识别中文 416
11.1.2 自动问答 417
11.2 语音库 418
11.3 语音合成 419
11.3.1 归一化 420
11.4 语音 420
11.4.1 标注 424
11.4.2 相似度 424
11.5 Sphinx 424
11.5.1 中文训练集 426
11.6 Julius 429
11.7 本章小结 429
11.8 术语表 429
参考资源 430
后记 431
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
关于作者
第一部分 理论
第1章 找出词的结构
1.1 词及其部件
1.1.1 词元
1.1.2 词形
1.1.3 词素
1.1.4 类型学
1.2 问题和挑战
1.2.1 不规则性
1.2.2 歧义性
1.2.3 能产性
1.3 形态模型
1.3.1 查词典
1.3.2 有限状态形态
1.3.3 基于合一的形态
1.3.4 函数式形态
1.3.5 形态归纳
1.4 总结
第2章 找出文档的结构
2.1 概述
2.1.1 句子边界检测
2.1.2 主题边界检测
2.2 方法
2.2.1 生成序列分类方法
2.2.2 判别性局部分类方法
2.2.3 判别性序列分类方法
2.2.4 混合方法
2.2.5 句子分割的全局建模扩展
2.3 方法的复杂度
2.4 方法的性能
2.5 特征
2.5.1 同时用于文本与语音的特征
2.5.2 只用于文本的特征
2.5.3 语音特征
2.6 处理阶段
2.7 讨论
2.8 总结
第3章 句法
3.1 自然语言分析
3.2 树库：句法分析的数据驱动方法
3.3 句法结构的表示
3.3.1 使用依存图的句法分析
3.3.2 使用短语结构树的句法分析
3.4 分析算法
3.4.1 移进归约分析
3.4.2 超图和线图分析
3.4.3 最小生成树和依存分析
3.5 分析中的歧义消解模型
3.5.1 概率上下文无关文法
3.5.2 句法分析的生成模型
3.5.3 句法分析的判别模型
3.6 多语言问题：什么是词元
3.6.1 词元切分、实例和编码
3.6.2 分词
3.6.3 形态学
3.7 总结
第4章 语义分析
4.1 概述
4.2 语义解释
4.2.1 结构歧义
4.2.2 词义
4.2.3 实体与事件消解
4.2.4 谓词　论元结构
4.2.5 意义表示
4.3 系统范式
4.4 词义
4.4.1 资源
4.4.2 系统
4.4.3 软件
4.5 谓词　论元结构
4.5.1 资源
4.5.2 系统
4.5.3 软件
4.6 意义表示
4.6.1 资源
4.6.2 系统
4.6.3 软件
4.7 总结
4.7.1 词义消歧
4.7.2 谓词　论元结构
4.7.3 意义表示
第5章 语言模型
5.1 概述
5.2 n元模型
5.3 语言模型评价
5.4 参数估计
5.4.1 最大似然估计和平滑
5.4.2 贝叶斯参数估计
5.4.3 大规模语言模型
5.5 语言模型适应
5.6 语言模型的类型
5.6.1 基于类的语言模型
5.6.2 变长语言模型
5.6.3 判别式语言模型
5.6.4 基于句法的语言模型
5.6.5 最大熵语言模型
5.6.6 因子化语言模型
5.6.7 其他基于树的语言模型
5.6.8 基于主题的贝叶斯语言模型
5.6.9 神经网络语言模型
5.7 特定语言建模问题
5.7.1 形态丰富语言的建模
5.7.2 亚词单元的选择
5.7.3 形态类别建模
5.7.4 无分词语言
5.7.5 口语与书面语言
5.8 多语言和跨语言建模
5.8.1 多语言建模
5.8.2 跨语言建模
5.9 总结
第6章 文本蕴涵识别
6.1 概述
6.2 文本识别蕴涵任务
6.2.1 问题定义
6.2.2 RTE的挑战
6.2.3 评估文本蕴涵系统性能
6.2.4 文本蕴涵解决方案的应用
6.2.5 其他语言中的RTE研究
6.3 文本蕴涵识别的框架
6.3.1 要求
6.3.2 分析
6.3.3 有用的组件
6.3.4 通用模型
6.3.5 实现
6.3.6 对齐
6.3.7 推理
6.3.8 训练
6.4 案例分析
6.4.1 抽取语篇约束
6.4.2 基于编辑距离的RTE
6.4.3 基于转换的方法
6.4.4 逻辑表示及推理
6.4.5 独立于蕴涵学习对齐
6.4.6 在RTE中利用多对齐
6.4.7 自然逻辑
6.4.8 句法树核
6.4.9 使用有限依存上下文的全局相似度
6.4.10 RTE的潜在对齐推理
6.5 RTE的进一步研究
6.5.1 改进分析器
6.5.2 发明或解决新问题
6.5.3 开发知识库
6.5.4 更好的RTE评价
6.6 有用资源
6.6.1 文献
6.6.2 知识库
6.6.3 自然语言处理包
6.7 总结
第7章 多语情感与主观性分析
7.1 概述
7.2 定义
7.3 英语中的情感及主观性分析
7.3.1 词典
7.3.2 语料库
7.3.3 工具
7.4 词级和短语级标注
7.4.1 基于字典的方法
7.4.2 基于语料库的方法
7.5 句子级标注
7.5.1 基于字典
7.5.2 基于语料库
7.6 文档级标注
7.6.1 基于字典
7.6.2 基于语料库
7.7 什么有效，什么无效
7.7.1 最佳情况：已有人工标注的语料库
7.7.2 次优情形：基于语料库的跨语言映射
7.7.3 第三优情形：孳衍词典
7.7.4 第四优情形：翻译词典
7.7.5 各种可行方法的比较
7.8 总结
第二部分 实践
第8章 实体检测和追踪
8.1 概述
8.2 提及检测
8.2.1 数据驱动的分类
8.2.2 搜索提及
8.2.3 提及检测特征
8.2.4 提及检测实验
8.3 共指消解
8.3.1 Bell树的构建
8.3.2 共指模型：链接和引入模型
8.3.3 最大熵链接模型
8.3.4 共指消解实验
8.4 总结
第9章 关系和事件
9.1 概述
9.2 关系与事件
9.3 关系类别
9.4 将关系抽取视为分类
9.4.1 算法
9.4.2 特征
9.4.3 分类器
9.5 关系抽取的其他方法
9.5.1 无监督和半监督方法
9.5.2 核方法
9.5.3 实体和关系检测的联合方法
9.6 事件
9.7 事件抽取方法
9.8 超句
9.9 事件匹配
9.10 事件抽取的未来方向
9.11 总结
第10章 机器翻译
10.1 机器翻译现状
10.2 机器翻译评测
10.2.1 人工评测
10.2.2 自动评测
10.2.3 WER、BLEU、METEOR等
10.3 词对齐
10.3.1 共现
10.3.2 IBM模型1
10.3.3 期望最大化
10.3.4 对齐模型
10.3.5 对称化
10.3.6 作为机器学习问题的词对齐
10.4 基于短语的翻译模型
10.4.1 模型
10.4.2 训练
10.4.3 解码
10.4.4 立方剪枝
10.4.5 对数线性模型和参数调节
10.4.6 控制模型的大小
10.5 基于树的翻译模型
10.5.1 层次短语翻译模型
10.5.2 线图解码
10.5.3 基于句法的模型
10.6 语言学挑战
10.6.1 译词选择
10.6.2 形态学
10.6.3 词序
10.7 工具和数据资源
10.7.1 基本工具
10.7.2 机器翻译系统
10.7.3 平行语料
10.8 未来的方向
10.9 总结
第11章 跨语言信息检索
11.1 概述
11.2 文档预处理
11.2.1 文档句法和编码
11.2.2 词元化
11.2.3 规范化
11.2.4 预处理最佳实践
11.3 单语信息检索
11.3.1 文档表示
11.3.2 索引结构
11.3.3 检索模型
11.3.4 查询扩展
11.3.5 文档先验模型
11.3.6 模型选择的最佳实践
11.4 CLIR
11.4.1 基于翻译的方法
11.4.2 机器翻译
11.4.3 中间语言文档表示
11.4.4 最佳实践
11.5 多语言信息检索
11.5.1 语言识别
11.5.2 MLIR的索引建立
11.5.3 翻译查询串
11.5.4 聚合模型
11.5.5 最佳实践
11.6 信息检索的评价
11.6.1 建立实验环境
11.6.2 相关性评估
11.6.3 评价指标
11.6.4 已有数据集
11.6.5 最佳实践
11.7 工具、软件和资源
11.8 总结
第12章 多语自动文摘
12.1 概述
12.2 自动文摘方法
12.2.1 传统方法
12.2.2 基于图的方法
12.2.3 学习如何做摘要
12.2.4 多语自动摘要
12.3 评测
12.3.1 人工评价
12.3.2 自动评价
12.3.3 自动文摘评测系统的近期发展
12.3.4 多语自动文摘的自动评测方法
12.4 如何搭建自动文摘系统
12.4.1 材料
12.4.2 工具
12.4.3 说明
12.5 评测竞赛和数据集
12.5.1 评测竞赛
12.5.2 数据集
12.6 总结
第13章 问答系统
13.1 概述和历史
13.2 架构
13.3 源获取和预处理
13.4 问题分析
13.5 搜索及候选抽取
13.5.1 非结构化资源搜索
13.5.2 非结构化源文本的候选抽取
13.5.3 结构化源文本的候选抽取
13.6 回答评分
13.6.1 方法概述
13.6.2 证据结合
13.6.3 扩展到列表型问题
13.7 跨语言问答
13.8 案例研究
13.9 评测
13.9.1 评测任务
13.9.2 判断答案正确性
13.9.3 性能度量
13.10 当前和未来的挑战
13.11 总结和进一步阅读
第14章 提炼
14.1 概述
14.2 示例
14.3 相关性和冗余性
14.4 Rosetta Consortium 提炼系统
14.4.1 文档和语料库准备
14.4.2 索引
14.4.3 查询回答
14.5 其他提炼方法
14.5.1 系统架构
14.5.2 相关度
14.5.3 冗余
14.5.4 多模态提炼
14.5.5 跨语言提炼
14.6 评测和指标
14.7 总结
第15章 口语对话系统
15.1 概述
15.2 口语对话系统
15.2.1 语音识别和理解
15.2.2 语音生成
15.2.3 对话管理器
15.2.4 语音用户接口
15.3 对话形式
15.4 自然语言呼叫路由选择
15.5 三代对话应用
15.6 持续的改进循环
15.7 口语句子的转录和标注
15.8 口语对话系统的本地化
15.8.1 呼叫流程本地化
15.8.2 提示本地化
15.8.3 文法的本地化
15.8.4 源端数据
15.8.5 训练
15.8.6 测试
15.9 总结
第16章 聚合自然语言处理引擎
16.1 概述
16.2 聚合语音和NLP引擎架构的期望属性
16.2.1 灵活的分布式组件化
16.2.2 计算效率
16.2.3 数据操作功能
16.2.4 鲁棒性处理
16.3 聚合的架构
16.3.1 UIMA
16.3.2 GATE
16.3.3 InfoSphere Streams
16.4 案例研究
16.4.1 GALE 互操作性演示系统
16.4.2 跨语言自动语言开发系统
16.4.3 实时翻译服务
16.5 经验教训
16.5.1 分割涉及延迟和精度之间的权衡
16.5.2 联合优化与互操作性
16.5.3 数据模型需要使用约定
16.5.4 性能评估的挑战
16.5.5 引擎的前向波训练
16.6 总结
16.7 UIMA样本代码
索引
・ ・ ・ ・ ・ ・ (收起)★★第1篇 入门――基础知识与编程框架
-
★第1章 BERT模型很强大，你值得拥有 /2
★1.1 全球欢腾，喜迎BERT模型 /2
★1.2 为什么BERT模型这么强 /3
★1.3 怎么学习BERT模型 /4
1.3.1 BERT模型的技术体系 /4
1.3.2 学好自然语言处理的4件套――神经网络的基础知识、NLP的基础知识、编程框架的使用、BERT模型的原理及应用 /4
1.3.3 学习本书的前提条件 /5
★1.4 自然语言处理的技术趋势 /5
1.4.1 基于超大规模的高精度模型 /6
1.4.2 基于超小规模的高精度模型 /6
1.4.3 基于小样本训练的模型 /6
-
★第2章 神经网络的基础知识――可能你掌握得也没有那么牢 /7
★2.1 什么是神经网络 /7
2.1.1 神经网络能解决哪些问题 /7
2.1.2 神经网络的发展 /7
2.1.3 什么是深度学习 /8
2.1.4 什么是图神经网络 /8
2.1.5 什么是图深度学习 /9
★2.2 神经网络的工作原理 /10
2.2.1 了解单个神经元 /10
2.2.2 生物神经元与计算机神经元模型的结构相似性 /12
2.2.3 生物神经元与计算机神经元模型的工作流程相似性 /12
2.2.4 神经网络的形成 /13
★2.3 深度学习中包含了哪些神经网络 /13
2.3.1 全连接神经网络 /13
2.3.2 卷积神经网络 /17
2.3.3 循环神经网络 /23
2.3.4 带有注意力机制的神经网络 /30
2.3.5 自编码神经网络 /34
★2.4 图深度学习中包含哪些神经网络 /36
2.4.1 同构图神经网络 /37
2.4.2 异构图神经网络 /37
★2.5 激活函数――加入非线性因素，以解决线性模型的缺陷 /38
2.5.1 常用的激活函数 /38
2.5.2 更好的激活函数――Swish()与Mish() /41
2.5.3 更适合NLP任务的激活函数――GELU() /43
2.5.4 激活函数总结 /44
2.5.5 分类任务与Softmax算法 /44
★2.6 训练模型的原理 /45
2.6.1 反向传播与BP算法 /47
2.6.2 神经网络模块中的损失函数 /49
2.6.3 学习率 /50
2.6.5 优化器 /51
2.6.6 训练模型的相关算法，会用就行 /52
★2.7 【实例】用循环神经网络实现退位减法 /52
★2.8 训练模型中的常见问题及优化技巧 /56
2.8.1 过拟合与欠拟合问题 /56
2.8.2 改善模型过拟合的方法 /56
2.8.3 了解正则化技巧 /57
2.8.4 了解Dropout技巧 /57
2.8.5 Targeted Dropout与Multi-sample Dropout /58
2.8.6 批量归一化（BN）算法 /59
2.8.7 多种BN算法的介绍与选取 /64
2.8.8 全连接网络的深浅与泛化能力的联系 /64
-
★第3章 NLP的基础知识――NLP没那么“玄” /65
★3.1 NLP的本质与原理 /65
3.1.1 情感分析、相似度分析等任务的本质 /65
3.1.2 完形填空、实体词识别等任务的本质 /66
3.1.3 文章摘要任务、问答任务、翻译任务的本质 /67
★3.2 NLP的常用工具 /68
3.2.1 自然语言处理工具包――SpaCy /68
3.2.2 中文分词工具――Jieba /69
3.2.3 中文转拼音工具――Pypinyin /69
3.2.4 评估翻译质量的算法库――SacreBLEU /70
★3.3 计算机中的字符编码 /70
3.3.1 什么是ASCII编码 /71
3.3.2 为什么会出现乱码问题 /71
3.3.3 什么是Unicode /71
3.3.4 借助Unicode 处理中文字符的常用操作 /73
★3.4 计算机中的词与句 /74
3.4.1 词表与词向量 /75
3.4.2 词向量的原理及意义 /75
3.4.3 多项式分布 /76
3.4.4 什么是依存关系分析 /77
3.4.5 什么是TF /79
3.4.6 什么是IDF /79
3.4.7 什么是TF-IDF /80
3.4.8 什么是BLEU /80
★3.5 什么是语言模型 /81
3.5.1 统计语言模型 /81
3.5.2 CBOW与Skip-Gram语言模型 /81
3.5.3 自编码（Auto Encoding，AE）语言模型 /82
3.5.4 自回归（Auto Regressive，AR）语言模型 /83
★3.6 文本预处理的常用方法 /83
3.6.1 NLP数据集的获取与清洗 /83
3.6.2 基于马尔可夫链的数据增强 /84
-
★第4章 搭建编程环境――从安装开始，更适合零基础入门 /87
★4.1 编程框架介绍 /87
4.1.1 PyTorch介绍 /87
4.1.2 DGL库介绍 /88
4.1.3 支持BERT模型的常用工具库介绍 /89
★4.2 搭建Python开发环境 /89
★4.3 搭建PyTorch开发环境 /91
★4.4 搭建DGL环境 /95
★4.5 安装Transformers库 /96
-
★★第2篇 基础――神经网络与BERT模型
★第5章 PyTorch编程基础 /100
★5.1 神经网络中的基础数据类型 /100
★5.2 矩阵运算的基础 /101
5.2.1 转置矩阵 /101
5.2.2 对称矩阵及其特性 /101
5.2.3 对角矩阵与单位矩阵 /101
5.2.4 阿达玛积（Hadamard Product） /102
5.2.5 点积（Dot Product） /102
5.2.6 对角矩阵的特性与操作方法 /103
★5.3 PyTorch中的张量 /104
5.3.1 定义张量的方法 /105
5.3.2 生成随机值张量 /107
5.3.3 张量的基本操作 /108
5.3.4 在CPU和GPU控制的内存中定义张量 /112
5.3.5 张量间的数据操作 /113
★5.4 Variable类型与自动微分模块 /118
5.4.1 Variable对象与Tensor对象之间的转换 /118
5.4.2 控制梯度计算的方法 /119
5.4.3 Variable对象的属性 /121
★5.5 【实例】用PyTorch实现一个简单模型 /124
5.5.1 准备可复现的随机数据 /124
5.5.2 实现并训练模型 /125
5.5.3 可视化模型能力 /128
★5.6 定义模型结构的常用方法 /129
5.6.1 Module类的使用方法 /129
5.6.2 模型中的参数（Parameters变量） /131
5.6.3 为模型添加参数 /132
5.6.4 从模型中获取参数 /133
5.6.5 激活模型接口 /135
5.6.6 L2正则化接口 /136
5.6.7 Dropout接口 /136
5.6.8 批量归一化接口 /137
5.6.9 【实例】手动实现BN的计算方法 /139
★5.7 保存与载入模型的常用方法 /141
★5.8 训练模型的接口与使用 /143
5.8.1 选取训练模型中的损失函数 /143
5.8.2 【实例】Softmax接口的使用 /144
5.8.3 优化器的使用与优化参数的查看 /146
5.8.4 用退化学习率训练模型 /147
5.8.5 为模型添加钩子函数 /152
5.8.6 多显卡的训练方法 /153
5.8.7 梯度累加的训练方法 /153
★5.9 处理数据集的接口与使用 /154
5.9.1 用DataLoader类实现自定义数据集 /155
5.9.2 DataLoader类中的多种采样器子类 /155
5.9.3 Torchtext工具与内置数据集 /156
★5.10 【实例】训练中文词向量 /157
5.10.1 用Jieba库进行中文样本预处理 /158
5.10.2 按照Skip-Gram规则制作数据集 /159
5.10.3 搭建模型并进行训练 /161
5.10.4 夹角余弦值介绍 /164
★5.11 卷积神经网络的实现 /166
5.11.1 了解卷积接口 /166
5.11.2 卷积操作的类型 /168
5.11.3 卷积参数与卷积结果的计算规则 /169
5.11.4 【实例】卷积函数的使用 /169
5.11.5 了解池化接口 /174
5.11.6 【实例】池化函数的使用 /175
★5.12 【实例】用卷积神经网络实现文本分类任务 /177
5.12.1 了解用于文本分类的卷积网络――TextCNN模型 /177
5.12.2 编写代码实现实例 /179
5.12.3 用多GPU并行训练模型 /184
5.12.4 在多GPU的训练过程中，保存/读取模型文件的注意事项 /185
5.12.5 处理显存残留问题 /186
★5.13 RNN的实现 /187
5.13.1 LSTM与GRU接口的实现 /187
5.13.2 多项式分布采样接口 /188
★5.14 【实例】用RNN训练语言模型 /189
5.14.1 实现语言模型的思路与步骤 /189
5.14.2 准备样本与代码实现 /189
★5.15 【实例】手动实现一个带有自注意力机制的模型 /192
★5.16 【实例】利用带注意力机制的循环神经网络对文本进行分类 /194
5.16.1 制作等长数据集并实现LSTM模型 /194
5.16.2 用梯度剪辑技巧优化训练过程 /195
-
★第6章 BERT模型的原理 /197
★6.1 BERT模型的起源――Transformer模型 /197
6.1.1 Transformer模型出现之前的主流模型 /197
6.1.2 Transformer模型的原理 /199
6.1.3 Transformer模型的优缺点 /204
★6.2 【实例】用Transformer模型进行中/英文翻译 /204
★6.3 BERT模型的原理 /206
6.3.1 BERT模型的训练过程 /207
6.3.2 BERT模型的预训练方法 /207
6.3.3 BERT模型的掩码机制 /208
6.3.4 BERT模型的训练参数 /210
6.3.5 BERT模型的缺点 /210
★6.4 高精度的BERTology系列模型 /211
6.4.1 适合生成文章的模型――GPT模型 /211
6.4.2 支持人机对话的模型――DialoGPT模型 /212
6.4.3 融合了BERT模型与GPT技术的模型――MASS模型 /212
6.4.4 支持长文本输入的模型――Transformer-XL模型 /212
6.4.5 支持更长文本的模型――XLNet模型 /213
6.4.6 弥补XLNet模型不足的模型――MPNet模型 /217
6.4.7 稳健性更好的模型――RoBERTa模型 /217
6.4.8 使用了稀疏注意力的模型――Longformer、BigBird模型 /218
6.4.9 基于词掩码的模型――BERT-WWM、Wo BERT等模型 /220
6.4.10 基于小段文字掩码的模型――SpanBERT模型 /220
6.4.11 适合翻译任务的模型――T5模型 /221
6.4.12 支持多种语言的翻译模型――XLM、XLM-Roberta模型 /222
6.4.13 既能阅读又能写作的模型――UniLM 2.0模型 /223
6.4.14 适用于语法纠错任务的模型――StructBERT、Bart模型 /224
6.4.15 可以进行定向写作的模型――CTRL模型 /225
6.4.16 适合摘要生成的模型――PEGASUS模型 /226
6.4.17 支持更多语言的模型――T-ULR v2模型 /227
★6.5 小规模的BERTology系列模型 /227
6.5.1 比RoBERTa模型训练速度更快的模型――ELECTRA模型 /228
6.5.2 适用于文本分类的超小模型――PRADO、pQRNN模型 /229
6.5.3 比BERT模型更适合于部署场景的模型――DistillBERT模型 /231
6.5.4 比BERT模型更快的模型――FastBERT模型 /232
6.5.5 带有通用蒸馏方案的模型――MiniLM模型 /233
6.5.6 精简版的BERT模型――ALBERT、ALBERT_tiny、ALBERT V2模型 /234
★6.6 BERTology系列模型的预训练方法总结 /237
6.6.1 AE式训练方法的常用策略 /237
6.6.2 更多的训练经验 /237
-
★第7章 BERT模型的快速应用――BERT模型虽然强大，使用却不复杂！ /239
★7.1 了解Transformers库 /239
★7.2 Transformers库的3层应用结构 /240
★7.3 【实例】用Transformers库的管道方式完成多种NLP任务 /241
7.3.1 在管道方式中指定NLP任务 /241
7.3.2 代码实现：完成文本分类任务 /242
7.3.3 代码实现：完成特征提取任务 /243
7.3.4 代码实现：完成完形填空任务 /244
7.3.5 代码实现：完成阅读理解任务 /245
7.3.6 代码实现：完成摘要生成任务 /247
7.3.7 预训练模型文件的组成与其加载时的固定名称 /248
7.3.8 代码实现：完成实体词识别任务 /248
7.3.9 管道方式的工作原理 /249
7.3.10 在管道方式中应用指定模型 /251
★7.4 Transformers库中的自动模型（AutoModel）类 /252
7.4.1 各种AutoModel类 /252
7.4.2 AutoModel类的模型加载机制 /253
7.4.3 Transformers库中的其他语言模型 /254
★7.5 Transformers库中的BERTology系列模型 /255
7.5.1 Transformers库的文件结构 /255
7.5.2 获取和加载预训练模型文件 /257
7.5.3 查找Transformers库中可以使用的模型 /260
7.5.4 【实例】用BERT模型实现完形填空任务 /261
7.5.5 【扩展实例】用自动模型类替换BertForMaskedLM类 /264
★7.6 Transformers库中的词表工具 /264
7.6.1 PreTrainedTokenizer类中的特殊词 /265
7.6.2 PreTrainedTokenizer类中的特殊词的使用 /266
7.6.3 向PreTrainedTokenizer类中添加词 /269
7.6.4 【实例】用手动加载GPT-2模型权重的方式将句子补充完整 /270
7.6.5 子词拆分 /274
★7.7 【实例】用迁移学习训练BERT模型来对中文分类 /275
7.7.1 NLP中的迁移学习 /275
7.7.2 构建数据集 /277
7.7.3 构建并加载BERT模型的预训练模型 /279
7.7.4 Transformers库中的底层类 /280
7.7.5 用退化学习率训练模型 /281
7.7.6 用数据增强方法训练模型 /282
-
★第8章 模型的可解释性――深入模型内部，探究其工作的根源
★8.1 模型的可解释库 /283
8.1.1 了解Captum库 /283
8.1.2 Captum库的可视化工具――Captum Insights /284
★8.2 什么是梯度积分方法 /284
★8.3 【实例】对NLP模型的可解释性分析 /284
8.3.1 分析词嵌入模型 /284
8.3.2 拆解NLP模型的处理过程 /285
8.3.3 用Captum库提取NLP模型的词嵌入层 /286
8.3.4 用梯度积分的方法计算模型的可解释性 /287
8.3.5 可视化模型的可解释性 /289
★8.4 【实例】BERT模型的可解释性分析 /291
8.4.1 了解BERT模型的可解释性工具――Bertviz /291
8.4.2 用Bertviz工具可视化BERT模型的权重 /292
8.4.3 分析BERT模型的权重参数 /296
★8.5 用图神经网络解释BERT模型 /300
8.5.1 点积计算与聚合计算的关系 /300
8.5.2 从图的角度思考BERT模型 /302
-
★★第3篇 BERT模型实战★★
★第9章 图神经网络与BERT模型的结合★
★9.1 图神经网络基础 /306
9.1.1 图的相关术语和操作 /306
9.1.2 图卷积神经网络 /310
★9.2 DGL库的使用方法 /314
9.2.1 创建图结构 /314
9.2.2 DGL库与NetWorkx库的相互转换 /316
9.2.3 图的基本操作 /319
9.2.4 图的消息传播机制 /322
9.2.5 DGL库中的多图处理 /324
★9.3 【实例】用图节点的聚合方法实现BERT模型 /325
9.3.1 基于Transformers库的BERT模型修改方案 /325
9.3.2 实现图节点聚合的核心代码 /326
9.3.3 将原BERT模型的权重应用到基于图节点聚合方法实现的BERT模型上 /327
★9.4 什么是关系图卷积网络（R-GCN）模型 /329
9.4.1 R-GCN模型的原理 /329
9.4.2 基于R-GCN模型的优化 /330
9.4.3 R-GCN模型的实现 /330
★9.5 【实例】用R-GCN模型理解文本中的代词 /332
9.5.1 代词数据集（GAP）介绍 /332
9.5.2 将GAP数据集转换成“图”结构数据的思路 /333
9.5.3 用BERT模型提取代词特征 /335
9.5.4 用BERT模型提取其他词特征 /337
9.5.5 用SpaCy工具和批次图方法构建图数据集 /339
9.5.6 搭建多层R-GCN模型 /344
9.5.7 用5折交叉验证方法训练模型 /345
-
★第10章 BERT模型的行业应用 ★
★10.1 BERT模型在文本纠错领域的应用 /347
10.1.1 文本纠错中的常见任务及解决办法 /347
10.1.2 理解BERT模型的纠错能力 /348
10.1.3 改进BERT模型使其具有更强的纠错能力 /348
10.1.4 专用于文本纠错的模型――Soft-Masked BERT模型 /349
10.1.5 基于图神经网络的文本纠错模型――SpellGCN模型 /350
10.1.6 【实例】用Transformers和DGL库实现SpellGCN模型 /351
★10.2 BERT技术在聊天机器人领域的应用 /352
10.2.1 聊天机器人的种类与实现技术 /353
10.2.2 基于BERT模型完成聊天任务的思路 /354
10.2.3 【实例】用累加梯度训练支持中文的DialoGPT模型 /355
10.2.4 更强大的多轮聊天模型――Meena模型 /360
★10.3 BERT模型在服务器端部署的应用 /360
10.3.1 用transformers-cli工具快速部署BERT模型 /360
10.3.2 用torchserve库部署BERT模型 /362
・ ・ ・ ・ ・ ・ (收起)第1篇 自然语言处理基础篇
第1章 自然语言处理概述 2
1.1 什么是自然语言处理 2
1.1.1 定义 2
1.1.2 常用术语 3
1.1.3 自然语言处理的任务 3
1.1.4 自然语言处理的发展历程 4
1.2 自然语言处理中的挑战 5
1.2.1 歧义问题 5
1.2.2 语言的多样性 6
1.2.3 未登录词 6
1.2.4 数据稀疏 6
1.3 自然语言处理中的常用技术 8
1.4 机器学习中的常见问题 10
1.4.1 Batch和Epoch 10
1.4.2 Batch Size的选择 11
1.4.3 数据集不平衡问题 11
1.4.4 预训练模型与数据安全 12
1.4.5 通过开源代码学习 12
1.5 小结 13
第2章 Python自然语言处理基础 14
2.1 搭建环境 14
2.1.1 选择Python版本 14
2.1.2 安装Python 15
2.1.3 使用pip包管理工具和Python虚拟环境 17
2.1.4 使用集成开发环境 18
2.1.5 安装Python自然语言处理常用的库 21
2.2 用Python处理字符串 25
2.2.1 使用str类型 25
2.2.2 使用StringIO类 29
2.3 用Python处理语料 29
2.3.1 从文件读取语料 29
2.3.2 去重 31
2.3.3 停用词 31
2.3.4 编辑距离 31
2.3.5 文本规范化 32
2.3.6 分词 34
2.3.7 词频-逆文本频率 35
2.3.8 One-Hot 编码 35
2.4 Python的一些特性 36
2.4.1 动态的解释型语言 36
2.4.2 跨平台 37
2.4.3 性能问题 37
2.4.4 并行和并发 37
2.5 在Python中调用其他语言 38
2.5.1 通过ctypes调用C/C++代码 38
2.5.2 通过网络接口调用其他语言 40
2.6 小结 41
第2篇 PyTorch入门篇
第3章 PyTorch介绍 44
3.1 概述 44
3.2 与其他框架的比较 45
3.2.1 TensorFlow 45
3.2.2 PaddlePaddle 45
3.2.3 CNTK 46
3.3 PyTorch环境配置 46
3.3.1 通过pip安装 46
3.3.2 配置GPU环境 47
3.3.3 其他安装方法 48
3.3.4 在PyTorch中查看GPU是否可用 49
3.4 Transformers简介及安装 49
3.5 Apex简介及安装 50
3.6 小结 50
第4章 PyTorch基本使用方法 51
4.1 张量的使用 51
4.1.1 创建张量 51
4.1.2 张量的变换 53
4.1.3 张量的索引 59
4.1.4 张量的运算 59
4.2 使用torch.nn 60
4.3 激活函数 63
4.3.1 Sigmoid函数 63
4.3.2 Tanh函数 64
4.3.3 ReLU函数 64
4.3.4 Softmax函数 65
4.3.5 Softmin函数 65
4.3.6 LogSoftmax函数 66
4.4 损失函数 66
4.4.1 0-1损失函数 66
4.4.2 平方损失函数 66
4.4.3 绝对值损失函数 68
4.4.4 对数损失函数 68
4.5 优化器 69
4.5.1 SGD优化器 69
4.5.2 Adam优化器 70
4.5.3 AdamW优化器 70
4.6 数据加载 70
4.6.1 Dataset 70
4.6.2 DataLoader 71
4.7 使用PyTorch实现逻辑回归 73
4.7.1 生成随机数据 73
4.7.2 数据可视化 73
4.7.3 定义模型 74
4.7.4 训练模型 75
4.8 TorchText 76
4.8.1 安装TorchText 76
4.8.2 Data类 76
4.8.3 Datasets类 78
4.8.4 Vocab 79
4.8.5 utils 80
4.9 使用TensorBoard 81
4.9.1 安装和启动TensorBoard 81
4.9.2 在PyTorch中使用TensorBoard 81
4.10 小结 81
第5章 热身：使用字符级RNN分类帖子 82
5.1 数据与目标 82
5.1.1 数据 82
5.1.2 目标 84
5.2 输入与输出 84
5.2.1 统计数据集中出现的字符数量 85
5.2.2 使用One-Hot编码表示标题数据 85
5.2.3 使用词嵌入表示标题数据 85
5.2.4 输出 86
5.3 字符级RNN 87
5.3.1 定义模型 87
5.3.2 运行模型 87
5.4 数据预处理 89
5.4.1 合并数据并添加标签 90
5.4.2 划分训练集和数据集 90
5.5 训练与评估 90
5.5.1 训练 91
5.5.2 评估 91
5.5.3 训练模型 91
5.6 保存和加载模型 93
5.6.1 仅保存模型参数 93
5.6.2 保存模型与参数 93
5.6.3 保存词表 94
5.7 开发应用 94
5.7.1 给出任意标题的建议分类 94
5.7.2 获取用户输入并返回结果 95
5.7.3 开发Web API和Web界面 96
5.8 小结 97
第3篇 用PyTorch完成自然语言处理任务篇
第6章 分词问题 100
6.1 中文分词 100
6.1.1 中文的语言结构 100
6.1.2 未收录词 101
6.1.3 歧义 101
6.2 分词原理 101
6.2.1 基于词典匹配的分词 101
6.2.2 基于概率进行分词 102
6.2.3 基于机器学习的分词 105
6.3 使用第三方工具分词 106
6.3.1 S-MSRSeg 106
6.3.2 ICTCLAS 107
6.3.3 结巴分词 107
6.3.4 pkuseg 107
6.4 实践 109
6.4.1 对标题分词 109
6.4.2 统计词语数量与模型训练 109
6.4.3 处理用户输入 110
6.5 小结 110
第7章 RNN 111
7.1 RNN的原理 111
7.1.1 原始RNN 111
7.1.2 LSTM 113
7.1.3 GRU 114
7.2 PyTorch中的RNN 115
7.2.1 使用RNN 115
7.2.2 使用LSTM和GRU 116
7.2.3 双向RNN和多层RNN 117
7.3 RNN可以完成的任务 117
7.3.1 输入不定长，输出与输入长度相同 117
7.3.2 输入不定长，输出定长 118
7.3.3 输入定长，输出不定长 118
7.4 实践：使用PyTorch自带的RNN完成帖子分类 118
7.4.1 载入数据 118
7.4.2 定义模型 119
7.4.3 训练模型 119
7.5 小结 121
第8章 词嵌入 122
8.1 概述 122
8.1.1 词表示 122
8.1.2 PyTorch中的词嵌入 124
8.2 Word2vec 124
8.2.1 Word2vec简介 124
8.2.2 CBOW 125
8.2.3 SG 126
8.2.4 在PyTorch中使用Word2vec 126
8.3 GloVe 127
8.3.1 GloVe的原理 127
8.3.2 在PyTorch中使用GloVe预训练词向量 127
8.4 实践：使用预训练词向量完成帖子标题分类 128
8.4.1 获取预训练词向量 128
8.4.2 加载词向量 128
8.4.3 方法一：直接使用预训练词向量 129
8.4.4 方法二：在Embedding层中载入预训练词向量 130
8.5 小结 131
第9章 Seq2seq 132
9.1 概述 132
9.1.1 背景 132
9.1.2 模型结构 133
9.1.3 训练技巧 134
9.1.4 预测技巧 134
9.2 使用PyTorch实现Seq2seq 134
9.2.1 编码器 134
9.2.2 解码器 135
9.2.3 Seq2seq 136
9.2.4 Teacher Forcing 137
9.2.5 Beam Search 138
9.3 实践：使用Seq2seq完成机器翻译任务 138
9.3.1 数据集 138
9.3.2 数据预处理 139
9.3.3 构建训练集和测试集 141
9.3.4 定义模型 143
9.3.5 初始化模型 145
9.3.6 定义优化器和损失函数 146
9.3.7 训练函数和评估函数 146
9.3.8 训练模型 147
9.3.9 测试模型 148
9.4 小结 149
第10章 注意力机制 150
10.1 注意力机制的起源 150
10.1.1 在计算机视觉中的应用 150
10.1.2 在自然语言处理中的应用 151
10.2 使用注意力机制的视觉循环模型 151
10.2.1 背景 151
10.2.2 实现方法 152
10.3 Seq2seq中的注意力机制 152
10.3.1 背景 152
10.3.2 实现方法 153
10.3.3 工作原理 154
10.4 自注意力机制 155
10.4.1 背景 155
10.4.2 自注意力机制相关的工作 156
10.4.3 实现方法与应用 156
10.5 其他注意力机制 156
10.6 小结 157
第11章 Transformer 158
11.1 Transformer的背景 158
11.1.1 概述 158
11.1.2 主要技术 159
11.1.3 优势和缺点 159
11.2 基于卷积网络的Seq2seq 159
11.3 Transformer的结构 159
11.3.1 概述 160
11.3.2 Transformer中的自注意力机制 160
11.3.3 Multi-head Attention 161
11.3.4 使用Positional Encoding 162
11.4 Transformer的改进 164
11.5 小结 164
第12章 预训练语言模型 165
12.1 概述 165
12.1.1 为什么需要预训练 165
12.1.2 预训练模型的工作方式 166
12.1.3 自然语言处理预训练的发展 166
12.2 ELMo 167
12.2.1 特点 167
12.2.2 模型结构 167
12.2.3 预训练过程 168
12.3 GPT 168
12.3.1 特点 168
12.3.2 模型结构 168
12.3.3 下游任务 169
12.3.4 预训练过程 169
12.3.5 GPT-2和GPT-3 169
12.4 BERT 170
12.4.1 背景 171
12.4.2 模型结构 171
12.4.3 预训练 171
12.4.4 RoBERTa和ALBERT 171
12.5 Hugging Face Transformers 171
12.5.1 概述 172
12.5.2 使用Transformers 172
12.5.3 下载预训练模型 173
12.5.4 Tokenizer 173
12.5.5 BERT的参数 175
12.5.6 BERT的使用 176
12.5.7 GPT-2的参数 180
12.5.8 常见错误及其解决方法 181
12.6 其他开源中文预训练模型 181
12.6.1 TAL-EduBERT 181
12.6.2 Albert 182
12.7 实践：使用Hugging Face Transformers中的BERT做帖子标题分类 182
12.7.1 读取数据 182
12.7.2 导入包和设置参数 183
12.7.3 定义Dataset和DataLoader 183
12.7.4 定义评估函数 184
12.7.5 定义模型 185
12.7.6 训练模型 185
12.8 小结 186
第4篇 实战篇
第13章 项目：中文地址解析 188
13.1 数据集 188
13.1.1 实验目标与数据集介绍 188
13.1.2 载入数据集 190
13.2 词向量 195
13.2.1 查看词向量文件 195
13.2.2 载入词向量 196
13.3 BERT 196
13.3.1 导入包和配置 196
13.3.2 Dataset和DataLoader 198
13.3.3 定义模型 199
13.3.4 训练模型 200
13.3.5 获取预测结果 202
13.4 HTML5演示程序开发 203
13.4.1 项目结构 203
13.4.2 HTML5界面 204
13.4.3 创建前端事件 206
13.4.4 服务器逻辑 207
13.5 小结 211
第14章 项目：诗句补充 212
14.1 了解chinese-poetry数据集 212
14.1.1 下载chinese-poetry数据集 212
14.1.2 探索chinese-poetry数据集 213
14.2 准备训练数据 214
14.2.1 选择数据源 214
14.2.2 载入内存 214
14.2.3 切分句子 215
14.2.4 统计字频 218
14.2.5 删除低频字所在诗句 220
14.2.6 词到ID的转换 221
14.3 实现基本的LSTM 222
14.3.1 把处理好的数据和词表存入文件 222
14.3.2 切分训练集和测试集 224
14.3.3 Dataset 224
14.3.4 DataLoader 225
14.3.5 创建Dataset和DataLoader对象 226
14.3.6 定义模型 226
14.3.7 测试模型 228
14.3.8 训练模型 228
14.4 根据句子长度分组 229
14.4.1 按照句子长度分割数据集 229
14.4.2 不用考虑填充的DataLoader 230
14.4.3 创建多个DataLoader对象 230
14.4.4 处理等长句子的LSTM 231
14.4.5 评估模型效果 231
14.4.6 训练模型 232
14.5 使用预训练词向量初始化Embedding层 235
14.5.1 根据词向量调整字表 235
14.5.2 载入预训练权重 240
14.5.3 训练模型 240
14.6 使用Transformer完成诗句生成 244
14.6.1 位置编码 245
14.6.2 使用Transformer 245
14.6.3 训练和评估 246
14.7 使用GPT-2完成对诗模型 247
14.7.1 预训练模型 248
14.7.2 评估模型 249
14.7.3 Fine-tuning 252
14.8 开发HTML5演示程序 257
14.8.1 目录结构 257
14.8.2 HTML5界面 257
14.8.3 创建前端事件 259
14.8.4 服务器逻辑 260
14.8.5 检验结果 263
14.9 小结 264
参考文献 265
・ ・ ・ ・ ・ ・ (收起)第1 章 深度学习――机器大脑的结构 1
1.1 概述 3
1.1.1 可以做酸奶的面包机――通用机器的概念 3
1.1.2 连接主义 5
1.1.3 用机器设计机器 6
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 7
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 13
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 19
1.6.1 深度学习带来了什么 19
1.6.2 深度学习尚未做到什么 20
1.7 内容回顾与推荐阅读 21
1.8 参考文献 21
第2 章 知识图谱――机器大脑中的知识库 23
2.1 什么是知识图谱 25
2.2 知识图谱的构建 27
2.2.1 大规模知识库 27
2.2.2 互联网链接数据 28
2.2.3 互联网网页文本数据 29
2.2.4 多数据源的知识融合 29
2.3 知识图谱的典型应用 30
2.3.1 查询理解（Query Understanding） 30
2.3.2 自动问答（Question Answering） 32
2.3.3 文档表示（Document Representation） 33
2.4 知识图谱的主要技术 34
2.4.1 实体链指（Entity Linking） 34
2.4.2 关系抽取（Relation Extraction） 35
2.4.3 知识推理（Knowledge Reasoning） 37
2.4.4 知识表示（Knowledge Representation） 38
2.5 前景与挑战 39
2.6 内容回顾与推荐阅读 40
2.7 参考文献 41
第3 章 大数据系统――大数据背后的支撑技术 43
3.1 概述 45
3.2 高性能计算技术 46
3.2.1 超级计算机的组成 47
3.2.2 并行计算的系统支持 48
3.3 虚拟化和云计算技术 52
3.3.1 虚拟化技术 52
3.3.2 云计算服务 54
3.4 基于分布式计算的大数据系统 55
3.4.1 Hadoop 生态系统 55
3.4.2 Spark 61
3.4.3 典型的大数据基础架构 63
3.5 大规模图计算 63
3.5.1 分布式图计算框架 64
3.5.2 高效的单机图计算框架 65
3.6 NoSQL 66
3.6.1 MongoDB 简介 67
3.7 内容回顾与推荐阅读 69
3.8 参考文献 70
第4 章 智能问答――智能助手是如何炼成的 71
4.1 概述 73
4.2 问答系统的主要组成 77
4.3 文本问答系统 78
4.3.1 问题理解 78
4.3.2 知识检索 81
4.3.3 答案生成 83
4.4 社区问答系统 84
4.4.1 社区问答系统的结构 85
4.4.2 相似问题检索 86
4.4.3 答案过滤 86
4.5 多媒体问答系统 87
4.6 大型问答系统案例：IBM 沃森问答系统 89
4.6.1 沃森的总体结构 89
4.6.2 问题解析 90
4.6.3 知识储备 90
4.6.4 检索和候选答案生成 91
4.6.5 可信答案确定 92
4.7 内容回顾与推荐阅读 93
4.8 参考文献 94
第5 章 主题模型――机器的智能摘要利器 97
5.1 概述 99
5.2 主题模型出现的背景 100
5.3 第一个主题模型潜在语义分析 102
5.4 第一个正式的概率主题模型 104
5.5 第一个正式的贝叶斯主题模型 105
5.6 LDA 的概要介绍 106
5.6.1 LDA 的延伸理解――主题模型广义理解 109
5.6.2 模型求解 111
5.6.3 模型评估 112
5.6.4 模型选择：主题数目的确定 113
5.7 主题模型的变形与应用 114
5.7.1 基于LDA 的模型变种 114
5.7.2 基于LDA 的典型应用 115
5.7.3 一个基于主题模型的新浪名人话题排行榜应用 118
5.8 内容回顾与推荐阅读 122
5.9 参考文献 123
第6 章 个性化推荐系统――如何了解电脑背后的TA 129
6.1 概述 131
6.1.1 推荐系统的发展历史 132
6.1.2 推荐无处不在 133
6.1.3 从千人一面到千人千面 133
6.2 个性化推荐的基本问题 134
6.2.1 推荐系统的输入 135
6.2.2 推荐系统的输出 137
6.2.3 个性化推荐的形式化 137
6.2.4 推荐系统的三大核心问题 138
6.3 典型推荐算法浅析 139
6.3.1 推荐算法的分类 139
6.3.2 典型推荐算法介绍 140
6.3.3 基于矩阵分解的打分预测 146
6.3.4 推荐的可解释性 151
6.3.5 推荐算法的评价 153
6.3.6 我们走了多远 156
6.4 参考文献 160
第7 章 情感分析与意见挖掘――计算机如何了解人类情感 165
7.1 概述 167
7.2 情感分析的主要研究问题 172
7.3 情感分析的主要方法 175
7.3.1 构成情感和观点的基本元素 175
7.3.2 情感极性与情感词典 177
7.3.3 属性－观点对 182
7.3.4 情感分析 184
7.4 主要的情感词典资源 188
7.5 内容回顾与推荐阅读 189
7.6 参考文献 190
第8 章 面向社会媒体大数据的语言使用分析及应用 195
8.1 概述 197
8.2 面向社会媒体的自然语言使用分析 197
8.2.1 词汇的时空传播与演化 198
8.2.2 语言使用与个体差异 200
8.2.3 语言使用与社会地位 202
8.2.4 语言使用与群体分析 203
8.3 面向社会媒体的自然语言分析应用 206
8.3.1 社会预测 206
8.3.2 霸凌现象定量分析 207
8.4 未来研究的挑战与展望 208
8.5 参考文献 209
后 记 214
国际学术组织、学术会议与学术论文 214
国内学术组织、学术会议与学术论文 216
如何快速了解某个领域的研究进展 217
・ ・ ・ ・ ・ ・ (收起)1 深度计算――机器大脑的结构 1
1.1 惊人的深度学习 1
1.1.1 可以做酸奶的面包机：通用机器的概念 2
1.1.2 连接主义 4
1.1.3 用机器设计机器 5
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 6
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 14
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 20
1.6.1 深度学习带来了什么 20
1.6.2 深度学习尚未做到什么 21
1.7 内容回顾与推荐阅读 . 22
1.8 参考文献 23
2 知识图谱――机器大脑中的知识库 25
2.1 什么是知识图谱 25
2.2 知识图谱的构建 28
2.2.1 大规模知识库 28
2.2.2 互联网链接数据 29
2.2.3 互联网网页文本数据 30
2.2.4 多数据源的知识融合 31
2.3 知识图谱的典型应用 32
2.3.1 查询理解 32
2.3.2 自动问答 34
2.3.3 文档表示 35
2.4 知识图谱的主要技术 36
2.4.1 实体链指 36
2.4.2 关系抽取 37
2.4.3 知识推理 39
2.4.4 知识表示 40
2.5 前景与挑战 42
2.6 内容回顾与推荐阅读 45
2.7 参考文献 45
3 大数据系统――大数据背后的支撑技术 47
3.1 大数据有多大 47
3.2 高性能计算技术 49
3.2.1 超级计算机的组成 49
3.2.2 并行计算的系统支持 51
3.3 虚拟化和云计算技术 55
3.3.1 虚拟化技术 56
3.3.2 云计算服务 58
3.4 基于分布式计算的大数据系统 59
3.4.1 Hadoop 生态系统 60
3.4.2 Spark 67
3.4.3 典型的大数据基础架构 68
3.5 大规模图计算 69
3.5.1 分布式图计算框架 70
3.5.2 高效的单机图计算框架 71
3.6 NoSQL 72
3.6.1 NoSQL 数据库的类别 72
3.6.2 MongoDB 简介 74
3.7 内容回顾与推荐阅读 76
3.8 参考文献 77
4 主题模型――机器的智能摘要利器 78
4.1 由文档到主题 78
4.2 主题模型出现的背景 80
4.3 第一个主题模型：潜在语义分析 81
4.4 第一个正式的概率主题模型 84
4.5 第一个正式的贝叶斯主题模型 85
4.6 LDA 的概要介绍 86
4.6.1 LDA 的延伸理解：主题模型广义理解 . 90
4.6.2 模型求解 92
4.6.3 模型评估 93
4.6.4 模型选择：主题数目的确定 94
4.7 主题模型的变形与应用 95
4.7.1 基于 LDA 的变种模型 95
4.7.2 基于 LDA 的典型应用 97
4.7.3 基于主题模型的新浪名人话题排行榜应用 100
4.8 内容回顾与推荐阅读 104
4.9 参考文献 105
5 机器翻译――机器如何跨越语言障碍 110
5.1 机器翻译的意义 110
5.2 机器翻译的发展历史 111
5.2.1 基于规则的机器翻译 112
5.2.2 基于语料库的机器翻译 112
5.2.3 基于神经网络的机器翻译 114
5.3 经典的神经网络机器翻译模型 114
5.3.1 基于循环神经网络的神经网络机器翻译 114
5.3.2 从卷积序列到序列模型 117
5.3.3 基于自注意力机制的 Transformer 模型 118
5.4 机器翻译译文质量评价 120
5.5 机器翻译面临的挑战 121
5.6 参考文献 123
6 情感分析与意见挖掘――机器如何了解人类情感 125
6.1 情感可以计算吗 125
6.2 哪里需要文本情感分析 . 126
6.2.1 情感分析的宏观反映 127
6.2.2 情感分析的微观特征 128
6.3 情感分析的主要研究问题 129
6.4 情感分析的主要方法 132
6.4.1 构成情感和观点的基本元素 132
6.4.2 情感极性与情感词典 134
6.4.3 属性－观点对 141
6.4.4 情感极性分析 143
6.5 主要的情感分析资源 148
6.6 前景与挑战 149
6.7 内容回顾与推荐阅读 150
6.8 参考文献 151
7 智能问答与对话系统――智能助手是如何炼成的 154
7.1 问答：图灵测试的基本形式 154
7.2 从问答到对话 155
7.2.1 对话系统的基本过程 156
7.2.2 文本对话系统的常见场景 157
7.3 问答系统的主要组成 159
7.4 文本问答系统 161
7.4.1 问题理解 161
7.4.2 知识检索 165
7.4.3 答案生成 169
7.5 端到端的阅读理解问答技术 169
7.5.1 什么是阅读理解任务 170
7.5.2 阅读理解任务的模型 172
7.5.3 阅读理解任务的其他工程技巧 173
7.6 社区问答系统 174
7.6.1 社区问答系统的结构 174
7.6.2 相似问题检索 175
7.6.3 答案过滤 177
7.6.4 社区问答的应用 177
7.7 多媒体问答系统 179
7.8 大型问答系统案例：IBM 沃森问答系统 181
7.8.1 沃森的总体结构 182
7.8.2 问题解析 182
7.8.3 知识储备 183
7.8.4 检索和候选答案生成 184
7.8.5 可信答案确定 184
7.9 前景与挑战 186
7.10 内容回顾与推荐阅读 186
7.11 参考文献 187
8 个性化推荐系统――如何了解计算机背后的他 190
8.1 什么是推荐系统 190
8.2 推荐系统的发展历史 191
8.2.1 推荐无处不在 192
8.2.2 从千人一面到千人千面 193
8.3 个性化推荐的基本问题 194
8.3.1 推荐系统的输入 194
8.3.2 推荐系统的输出 196
8.3.3 个性化推荐的基本形式 197
8.3.4 推荐系统的三大核心问题 198
8.4 典型推荐算法浅析 199
8.4.1 推荐算法的分类 199
8.4.2 典型推荐算法介绍 200
8.4.3 基于矩阵分解的打分预测 207
8.4.4 基于神经网络的推荐算法 213
8.5 推荐的可解释性 214
8.6 推荐算法的评价 217
8.6.1 评分预测的评价 218
8.6.2 推荐列表的评价 219
8.6.3 推荐理由的评价 220
8.7 前景与挑战：我们走了多远 221
8.7.1 推荐系统面临的问题 221
8.7.2 推荐系统的新方向 223
8.8 内容回顾与推荐阅读 225
8.9 参考文献 226
9 机器写作――从分析到创造 228
9.1 什么是机器写作 228
9.2 艺术写作 229
9.2.1 机器写诗 229
9.2.2 AI 对联 233
9.3 当代写作 236
9.3.1 机器写稿 236
9.3.2 机器故事生成 239
9.4 内容回顾 241
9.5 参考文献 242
10 社交商业数据挖掘――从用户数据挖掘到商业智能应用 243
10.1 社交媒体平台中的数据宝藏 . 243
10.2 打通网络社区的束缚：用户网络社区身份的链指与融合 245
10.3 揭开社交用户的面纱：用户画像的构建 247
10.3.1 基于显式社交属性的构建方法 247
10.3.2 基于网络表示学习的构建方法 249
10.3.3 产品受众画像的构建 250
10.4 了解用户的需求：用户消费意图的识别 254
10.4.1 个体消费意图识别 254
10.4.2 群体消费意图识别 256
10.5 精准的供需匹配：面向社交平台的产品推荐算法 258
10.5.1 候选产品列表生成 258
10.5.2 基于学习排序算法的推荐框架 259
10.5.3 基于用户属性的排序特征构建 260
10.5.4 推荐系统的整体设计概览 261
10.6 前景与挑战 262
10.7 内容回顾与推荐阅读 263
10.8 参考文献 264
11 智慧医疗――信息技术在医疗领域应用的结晶 265
11.1 智慧医疗的起源 265
11.2 智慧医疗的庐山真面目 267
11.3 智慧医疗中的人工智能应用 268
11.3.1 医疗过程中的人工智能应用 268
11.3.2 医疗研究中的人工智能应用 272
11.4 前景与挑战 273
11.5 内容回顾与推荐阅读 275
11.6 参考文献 275
12 智慧司法――智能技术促进司法公正 276
12.1 智能技术与法律的碰撞 . 276
12.2 智慧司法相关研究 . 277
12.2.1 法律智能的早期研究 278
12.2.2 判决预测：虚拟法官的诞生与未来 279
12.2.3 文书生成：司法过程简化 283
12.2.4 要素提取：司法结构化 285
12.2.5 类案匹配：解决一案多判 289
12.2.6 司法问答：让机器理解法律 292
12.3 智慧司法的期望偏差与应用挑战 293
12.3.1 智慧司法的期望偏差 293
12.3.2 智慧司法的应用挑战 294
12.4 内容回顾与推荐阅读 295
12.5 参考文献 295
13 智能金融――机器金融大脑 298
13.1 智能金融正当其时 298
13.1.1 什么是智能金融 298
13.1.2 智能金融与金融科技、互联网金融的异同 298
13.1.3 智能金融适时而生 299
13.2 智能金融技术 301
13.2.1 大数据的机遇与挑战 301
13.2.2 智能金融中的自然语言处理 303
13.2.3 金融事理图谱 307
13.2.4 智能金融中的深度学习 310
13.3 智能金融应用 314
13.3.1 智能投顾 314
13.3.2 智能研报 315
13.3.3 智能客服 316
13.4 前景与挑战 317
13.5 内容回顾与推荐阅读 319
13.6 参考文献 319
14 计算社会学――透过大数据了解人类社会 320
14.1 透过数据了解人类社会 320
14.2 面向社会媒体的自然语言使用分析 321
14.2.1 词汇的时空传播与演化 322
14.2.2 语言使用与个体差异 325
14.2.3 语言使用与社会地位 326
14.2.4 语言使用与群体分析 328
14.3 面向社会媒体的自然语言分析应用 330
14.3.1 社会预测 330
14.3.2 霸凌现象定量分析 331
14.4 未来研究的挑战与展望 332
14.5 参考文献 333
后记 334
・ ・ ・ ・ ・ ・ (收起)1 深度计算――机器大脑的结构 1
1.1 惊人的深度学习 1
1.1.1 可以做酸奶的面包机：通用机器的概念 2
1.1.2 连接主义 4
1.1.3 用机器设计机器 5
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 6
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 14
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 20
1.6.1 深度学习带来了什么 20
1.6.2 深度学习尚未做到什么 21
1.7 内容回顾与推荐阅读 . 22
1.8 参考文献 23
2 知识图谱――机器大脑中的知识库 25
2.1 什么是知识图谱 25
2.2 知识图谱的构建 28
2.2.1 大规模知识库 28
2.2.2 互联网链接数据 29
2.2.3 互联网网页文本数据 30
2.2.4 多数据源的知识融合 31
2.3 知识图谱的典型应用 32
2.3.1 查询理解 32
2.3.2 自动问答 34
2.3.3 文档表示 35
2.4 知识图谱的主要技术 36
2.4.1 实体链指 36
2.4.2 关系抽取 37
2.4.3 知识推理 39
2.4.4 知识表示 40
2.5 前景与挑战 42
2.6 内容回顾与推荐阅读 45
2.7 参考文献 45
3 大数据系统――大数据背后的支撑技术 47
3.1 大数据有多大 47
3.2 高性能计算技术 49
3.2.1 超级计算机的组成 49
3.2.2 并行计算的系统支持 51
3.3 虚拟化和云计算技术 55
3.3.1 虚拟化技术 56
3.3.2 云计算服务 58
3.4 基于分布式计算的大数据系统 59
3.4.1 Hadoop 生态系统 60
3.4.2 Spark 67
3.4.3 典型的大数据基础架构 68
3.5 大规模图计算 69
3.5.1 分布式图计算框架 70
3.5.2 高效的单机图计算框架 71
3.6 NoSQL 72
3.6.1 NoSQL 数据库的类别 72
3.6.2 MongoDB 简介 74
3.7 内容回顾与推荐阅读 76
3.8 参考文献 77
4 主题模型――机器的智能摘要利器 78
4.1 由文档到主题 78
4.2 主题模型出现的背景 80
4.3 第一个主题模型：潜在语义分析 81
4.4 第一个正式的概率主题模型 84
4.5 第一个正式的贝叶斯主题模型 85
4.6 LDA 的概要介绍 86
4.6.1 LDA 的延伸理解：主题模型广义理解 . 90
4.6.2 模型求解 92
4.6.3 模型评估 93
4.6.4 模型选择：主题数目的确定 94
4.7 主题模型的变形与应用 95
4.7.1 基于 LDA 的变种模型 95
4.7.2 基于 LDA 的典型应用 97
4.7.3 基于主题模型的新浪名人话题排行榜应用 100
4.8 内容回顾与推荐阅读 104
4.9 参考文献 105
5 机器翻译――机器如何跨越语言障碍 110
5.1 机器翻译的意义 110
5.2 机器翻译的发展历史 111
5.2.1 基于规则的机器翻译 112
5.2.2 基于语料库的机器翻译 112
5.2.3 基于神经网络的机器翻译 114
5.3 经典的神经网络机器翻译模型 114
5.3.1 基于循环神经网络的神经网络机器翻译 114
5.3.2 从卷积序列到序列模型 117
5.3.3 基于自注意力机制的 Transformer 模型 118
5.4 机器翻译译文质量评价 120
5.5 机器翻译面临的挑战 121
5.6 参考文献 123
6 情感分析与意见挖掘――机器如何了解人类情感 125
6.1 情感可以计算吗 125
6.2 哪里需要文本情感分析 . 126
6.2.1 情感分析的宏观反映 127
6.2.2 情感分析的微观特征 128
6.3 情感分析的主要研究问题 129
6.4 情感分析的主要方法 132
6.4.1 构成情感和观点的基本元素 132
6.4.2 情感极性与情感词典 134
6.4.3 属性－观点对 141
6.4.4 情感极性分析 143
6.5 主要的情感分析资源 148
6.6 前景与挑战 149
6.7 内容回顾与推荐阅读 150
6.8 参考文献 151
7 智能问答与对话系统――智能助手是如何炼成的 154
7.1 问答：图灵测试的基本形式 154
7.2 从问答到对话 155
7.2.1 对话系统的基本过程 156
7.2.2 文本对话系统的常见场景 157
7.3 问答系统的主要组成 159
7.4 文本问答系统 161
7.4.1 问题理解 161
7.4.2 知识检索 165
7.4.3 答案生成 169
7.5 端到端的阅读理解问答技术 169
7.5.1 什么是阅读理解任务 170
7.5.2 阅读理解任务的模型 172
7.5.3 阅读理解任务的其他工程技巧 173
7.6 社区问答系统 174
7.6.1 社区问答系统的结构 174
7.6.2 相似问题检索 175
7.6.3 答案过滤 177
7.6.4 社区问答的应用 177
7.7 多媒体问答系统 179
7.8 大型问答系统案例：IBM 沃森问答系统 181
7.8.1 沃森的总体结构 182
7.8.2 问题解析 182
7.8.3 知识储备 183
7.8.4 检索和候选答案生成 184
7.8.5 可信答案确定 184
7.9 前景与挑战 186
7.10 内容回顾与推荐阅读 186
7.11 参考文献 187
8 个性化推荐系统――如何了解计算机背后的他 190
8.1 什么是推荐系统 190
8.2 推荐系统的发展历史 191
8.2.1 推荐无处不在 192
8.2.2 从千人一面到千人千面 193
8.3 个性化推荐的基本问题 194
8.3.1 推荐系统的输入 194
8.3.2 推荐系统的输出 196
8.3.3 个性化推荐的基本形式 197
8.3.4 推荐系统的三大核心问题 198
8.4 典型推荐算法浅析 199
8.4.1 推荐算法的分类 199
8.4.2 典型推荐算法介绍 200
8.4.3 基于矩阵分解的打分预测 207
8.4.4 基于神经网络的推荐算法 213
8.5 推荐的可解释性 214
8.6 推荐算法的评价 217
8.6.1 评分预测的评价 218
8.6.2 推荐列表的评价 219
8.6.3 推荐理由的评价 220
8.7 前景与挑战：我们走了多远 221
8.7.1 推荐系统面临的问题 221
8.7.2 推荐系统的新方向 223
8.8 内容回顾与推荐阅读 225
8.9 参考文献 226
9 机器写作――从分析到创造 228
9.1 什么是机器写作 228
9.2 艺术写作 229
9.2.1 机器写诗 229
9.2.2 AI 对联 233
9.3 当代写作 236
9.3.1 机器写稿 236
9.3.2 机器故事生成 239
9.4 内容回顾 241
9.5 参考文献 242
10 社交商业数据挖掘――从用户数据挖掘到商业智能应用 243
10.1 社交媒体平台中的数据宝藏 . 243
10.2 打通网络社区的束缚：用户网络社区身份的链指与融合 245
10.3 揭开社交用户的面纱：用户画像的构建 247
10.3.1 基于显式社交属性的构建方法 247
10.3.2 基于网络表示学习的构建方法 249
10.3.3 产品受众画像的构建 250
10.4 了解用户的需求：用户消费意图的识别 254
10.4.1 个体消费意图识别 254
10.4.2 群体消费意图识别 256
10.5 精准的供需匹配：面向社交平台的产品推荐算法 258
10.5.1 候选产品列表生成 258
10.5.2 基于学习排序算法的推荐框架 259
10.5.3 基于用户属性的排序特征构建 260
10.5.4 推荐系统的整体设计概览 261
10.6 前景与挑战 262
10.7 内容回顾与推荐阅读 263
10.8 参考文献 264
11 智慧医疗――信息技术在医疗领域应用的结晶 265
11.1 智慧医疗的起源 265
11.2 智慧医疗的庐山真面目 267
11.3 智慧医疗中的人工智能应用 268
11.3.1 医疗过程中的人工智能应用 268
11.3.2 医疗研究中的人工智能应用 272
11.4 前景与挑战 273
11.5 内容回顾与推荐阅读 275
11.6 参考文献 275
12 智慧司法――智能技术促进司法公正 276
12.1 智能技术与法律的碰撞 . 276
12.2 智慧司法相关研究 . 277
12.2.1 法律智能的早期研究 278
12.2.2 判决预测：虚拟法官的诞生与未来 279
12.2.3 文书生成：司法过程简化 283
12.2.4 要素提取：司法结构化 285
12.2.5 类案匹配：解决一案多判 289
12.2.6 司法问答：让机器理解法律 292
12.3 智慧司法的期望偏差与应用挑战 293
12.3.1 智慧司法的期望偏差 293
12.3.2 智慧司法的应用挑战 294
12.4 内容回顾与推荐阅读 295
12.5 参考文献 295
13 智能金融――机器金融大脑 298
13.1 智能金融正当其时 298
13.1.1 什么是智能金融 298
13.1.2 智能金融与金融科技、互联网金融的异同 298
13.1.3 智能金融适时而生 299
13.2 智能金融技术 301
13.2.1 大数据的机遇与挑战 301
13.2.2 智能金融中的自然语言处理 303
13.2.3 金融事理图谱 307
13.2.4 智能金融中的深度学习 310
13.3 智能金融应用 314
13.3.1 智能投顾 314
13.3.2 智能研报 315
13.3.3 智能客服 316
13.4 前景与挑战 317
13.5 内容回顾与推荐阅读 319
13.6 参考文献 319
14 计算社会学――透过大数据了解人类社会 320
14.1 透过数据了解人类社会 320
14.2 面向社会媒体的自然语言使用分析 321
14.2.1 词汇的时空传播与演化 322
14.2.2 语言使用与个体差异 325
14.2.3 语言使用与社会地位 326
14.2.4 语言使用与群体分析 328
14.3 面向社会媒体的自然语言分析应用 330
14.3.1 社会预测 330
14.3.2 霸凌现象定量分析 331
14.4 未来研究的挑战与展望 332
14.5 参考文献 333
后记 334
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 数据革命 1
1.1 数据生成 1
1.2 Spark 2
1.2.1 Spark Core 3
1.2.2 Spark组件 4
1.3 设置环境 5
1.3.1 Windows 5
1.3.2 iOS 6
1.4 小结 7
第2章 机器学习简介 9
2.1 有监督机器学习 10
2.2 无监督机器学习 12
2.3 半监督机器学习 14
2.4 强化学习 14
2.5 小结 15
第3章 数据处理 17
3.1 加载和读取数据 17
3.2 添加一个新列 20
3.3 筛选数据 21
3.3.1 条件1 21
3.3.2 条件2 22
3.4 列中的非重复值 23
3.5 数据分组 23
3.6 聚合 25
3.7 用户自定义函数(UDF) 26
3.7.1 传统的Python函数 26
3.7.2 使用lambda函数 27
3.7.3 Pandas UDF(向量化的UDF) 28
3.7.4 Pandas UDF(多列) 29
3.8 去掉重复值 29
3.9 删除列 30
3.10 写入数据 30
3.10.1 csv 31
3.10.2 嵌套结构 31
3.11 小结 31
第4章 线性回归 33
4.1 变量 33
4.2 理论 34
4.3 说明 41
4.4 评估 42
4.5 代码 43
4.5.1 数据信息 43
4.5.2 步骤1：创建
SparkSession对象 44
4.5.3 步骤2：读取数据集 44
4.5.4 步骤3：探究式数据分析 44
4.5.5 步骤4：特征工程化 45
4.5.6 步骤5：划分数据集 47
4.5.7 步骤6：构建和训练线性回归模型 47
4.5.8 步骤7：在测试数据上评估线性回归模型 48
4.6 小结 48
第5章 逻辑回归 49
5.1 概率 49
5.1.1 使用线性回归 50
5.1.2 使用Logit 53
5.2 截距(回归系数) 54
5.3 虚变量 55
5.4 模型评估 56
5.4.1 正确的正面预测 56
5.4.2 正确的负面预测 57
5.4.3 错误的正面预测 57
5.4.4 错误的负面预测 57
5.4.5 准确率 57
5.4.6 召回率 57
5.4.7 精度 58
5.4.8 F1分数 58
5.4.9 截断/阈值概率 58
5.4.10 ROC曲线 58
5.5 逻辑回归代码 59
5.5.1 数据信息 59
5.5.2 步骤1：创建Spark会话对象 60
5.5.3 步骤2：读取数据集 60
5.5.4 步骤3：探究式数据分析 60
5.5.5 步骤4：特征工程 63
5.5.6 步骤5：划分数据集 68
5.5.7 步骤6：构建和训练逻辑回归模型 69
5.5.8 训练结果 69
5.5.9 步骤7：在测试数据上评估线性回归模型 70
5.5.10 混淆矩阵 71
5.6 小结 72
第6章 随机森林 73
6.1 决策树 73
6.1.1 熵 75
6.1.2 信息增益 76
6.2 随机森林 78
6.3 代码 80
6.3.1 数据信息 80
6.3.2 步骤1：创建SparkSession对象 81
6.3.3 步骤2：读取数据集 81
6.3.4 步骤3：探究式数据分析 81
6.3.5 步骤4：特征工程 85
6.3.6 步骤5：划分数据集 86
6.3.7 步骤6：构建和训练随机森林模型 87
6.3.8 步骤7：基于测试数据进行评估 87
6.3.9 准确率 89
6.3.10 精度 89
6.3.11 AUC曲线下的面积 89
6.3.12 步骤8：保存模型 90
6.4 小结 90
第7章 推荐系统 91
7.1 推荐 91
7.1.1 基于流行度的RS 92
7.1.2 基于内容的RS 93
7.1.3 基于协同过滤的RS 95
7.1.4 混合推荐系统 103
7.2 代码 104
7.2.1 数据信息 105
7.2.2 步骤1：创建SparkSession对象 105
7.2.3 步骤2：读取数据集 105
7.2.4 步骤3：探究式数据分析 105
7.2.5 步骤4：特征工程 108
7.2.6 步骤5：划分数据集 109
7.2.7 步骤6：构建和训练推荐系统模型 110
7.2.8 步骤7：基于测试数据进行预测和评估 110
7.2.9 步骤8：推荐活动用户可能会喜欢的排名靠前的电影 111
7.3 小结 114
第8章 聚类 115
8.1 初识聚类 115
8.2 用途 117
8.2.1 K-均值 117
8.2.2 层次聚类 127
8.3 代码 131
8.3.1 数据信息 131
8.3.2 步骤1：创建SparkSession对象 131
8.3.3 步骤2：读取数据集 131
8.3.4 步骤3：探究式数据分析 131
8.3.5 步骤4：特征工程 133
8.3.6 步骤5：构建K均值聚类模型 133
8.3.7 步骤6：聚类的可视化 136
8.4 小结 137
第9章 自然语言处理 139
9.1 引言 139
9.2 NLP涉及的处理步骤 139
9.3 语料 140
9.4 标记化 140
9.5 移除停用词 141
9.6 词袋 142
9.7 计数向量器 143
9.8 TF-IDF 144
9.9 使用机器学习进行文本分类 145
9.10 序列嵌入 151
9.11 嵌入 151
9.12 小结 160
・ ・ ・ ・ ・ ・ (收起)导读 1
Contributors 17
Preface 23
Part I Fundamental aspects 1
1 Ontology and the lexicon： a multidisciplinary perspective 3
1.1 Situating ontologies and lexical resources 3
1.2 The content of ontologies 10
1.3 Theoretical framework for the
ontologies／lexicons interface 14
1.4 From ontologies to the lexicon and back 21
1.5 Outline of chapters 23
2 Formal ontology as interlingua： the SUMO and
WordNet linking project and global WordNet 25
2.1 WordNet 25
2.2 Principles of construction of formal ontologies
and lexicons 29
2.3 Mappings 30
2.4 Interpreting language 32
2.5 Global WordNet 33
2.6 SUMO translation templates 35
3 Interfacing WordNet with DOLCE： towards OntoWordNet 36
3.1 Introduction 36
3.2 WordNet’s preliminary analysis 37
3.3 The DOLCE upper ontology 39
3.4 Mapping WordNet into DOLCE 48
3.5 Conclusion 52
4 Reasoning over natural language text by means of FrameNet and ontologies 53
4.1 Introduction 53
4.2 An introduction to the FrameNet lexicon 54
4.3 Linking FrameNet to ontologies for reasoning 56
4.4 Formalizing FrameNet in OWL DL 57
4.5 Reasoning over FrameNet―annotated text 62
4.6 Linking FrameNet to SUMO 66
4.7 Discussion 69
4.8 Conclusion and outlook 70
5 Synergizing ontologies and the lexicon： a roadmap 72
5.1 Formal mappings between ontologies 72
5.2 Evaluation of ontolex resources 73
5.3 Bridging different lexical models and resources 75
5.4 Technological framework 77
Part II Discovery and representation of conceptual systems 79
6 Experiments of ontology construction with Formal Concept Analysis 81
6.1 Introduction 81
6.2 Basic concepts and related work 82
6.3 Dataset selection and design of experiments 86
6.4 Evaluation and discussion 92
6.5 Conclusion and future work 96
7 Ontology， lexicon， and fact repository as leveraged to interpret events of change 98
7.1 Introduction 98
7.2 A snapshot of OntoSem 100
7.3 Motivation for pursuing deep analysis of events of change 101
7.4 Increase 102
7.5 Content divorced from its rendering 114
7.6 NLP with reasoning and for reasoning 117
7.7 Conclusion 118
8 Hantology： conceptual system discovery based on orthographic convention 122
8.1 Introduction： hanzi and conventionalized conceptualization 122
8.2 General framework 126
8.3 Conceptualization and classification of the radicals system 128
8.4 The ontology of a radical as a semantic symbol 132
8.5 The architecture of Hantology 133
8.6 OWL encoding of Hantology 137
8.7 Summary 139
8.8 Conclusion 142
9 What’s in a schema？ 144
9.1 Introduction 144
9.2 An ontology for cognitive linguistics 146
9.3 The c.DnS ontology 148
9.4 Schemata， mental spaces， and constructions 161
9.5 An embodied semiotic metamodel 166
9.6 Applying Semion to FrameNet and related resources 169
9.7 Conclusion 181
Part III Interfacing ontologies and lexical resources 183
10 Interfacing ontologies and lexical resources 185
10.1 Introduction 185
10.2 Classifying experiments in ontologies and lexical resources 185
10.3 Ontologies and their construction 188
10.4 How actual resources fit the classification 190
10.5 Two practical examples 194
10.6 Available tools for the ontology lexical resource interface 196
10.7 Conclusion 200
11 Sinica BOW （Bilingual Ontological WordNet）：integration of bilingual WordNet and SUMO 201
11.1 Background and motivation 201
11.2 Resources and structure required in the BOW approach 202
11.3 Interfacing multiple resources： a lexicon―driven approach 204
11.4 Integration of multiple knowledge sources 207
11.5 Updating and future improvements 209
11.6 Conclusion 210
12 Ontology―based semantic lexicons：mapping between terms and object descriptions 212
12.1 Introduction 212
12.2 Why we need semantic lexicons 213
12.3 More semantics than we need 215
12.4 The semantics we need is in ontologies 218
12.5 Conclusion 223
13 Merging global and specialized linguistic ontologies 224
13.1 Introduction 224
13.2 Linguistic ontologies versus formal ontologies 226
13.3 Specialized linguistic ontologies 229
13.4 The plug―in approach 230
13.5 Experiments 236
13.6 Applications and extensions 237
13.7 Conclusion 238
Part IV Learning and using ontological knowledge 239
14 The life cycle of knowledge 241
14.1 Introduction 241
14.2 Using ontolexical knowledge in NLP 242
14.3 Creating ontolexical knowledge with NLP 249
14.4 Conclusion 256
15 The Omega ontology 258
15.1 Introduction 258
15.2 Constituents of Omega 258
15.3 Structure of Omega 260
15.4 Construction of Omega via merging 263
15.5 Omega’s auxiliary knowledge sources 264
15.6 Applications 266
15.7 Omega 5 and the OntoNotes project 267
15.8 Discussion and future work 268
15.9 Conclusion 269
16 Automatic acquisition of lexico―semantic knowledge for question answering 271
16.1 Introduction 271
16.2 Lexico―semantic knowledge for QA 272
16.3 Related work 274
16.4 Extracting semantically similar words 275
16.5 Using automatically acquired role and function words 279
16.6 Using automatically acquired categorized NEs 280
16.7 Evaluation 283
16.8 Conclusion and future work 286
17 Agricultural ontology construction and maintenance in Thai 288
17.1 Introduction 288
17.2 A framework of ontology construction and maintenance 290
17.3 Ontology acquisition from texts 291
17.4 Ontology acquisitions from a dictionary and a thesaurus 301
17.5 Integration into an ontological tree 306
17.6 Conclusion 307
References 309
Index 335
・ ・ ・ ・ ・ ・ (收起)第 1 章 心爱的聊天机器人 .................................................................................................. 1
聊天机器人的受欢迎程度 .......................................................................................... 2
Python 之禅以及为什么它适用于聊天机器人 .......................................................... 3
对聊天机器人的需求 .................................................................................................. 4
商业视角 ............................................................................................................ 5
开发者视角 ........................................................................................................ 9
受聊天机器人影响的行业 ........................................................................................ 11
聊天机器人的发展历程 ............................................................................................ 12
1950 .................................................................................................................. 12
1966 .................................................................................................................. 12
1972 .................................................................................................................. 12
1981 .................................................................................................................. 12
1985 .................................................................................................................. 12
1992 .................................................................................................................. 13
1995 .................................................................................................................. 13
1996 .................................................................................................................. 13
2001 .................................................................................................................. 13
2006 .................................................................................................................. 13
2010 .................................................................................................................. 13
2012 .................................................................................................................. 14
2014 .................................................................................................................. 14
2015 .................................................................................................................. 14
2016 .................................................................................................................. 14
2017 .................................................................................................................. 14
我可以用聊天机器人解决什么样的问题 ................................................................ 15
这个问题能通过简单的问答或来回交流解决吗 ........................................... 15
这个工作是否有高度重复性，需要进行数据收集和分析 ........................... 15
你的机器人的任务可以自动化和固定化吗 ................................................... 16
一个 QnA 机器人 ...................................................................................................... 16
从聊天机器人开始 .................................................................................................... 17
聊天机器人中的决策树 ............................................................................................ 18
在聊天机器人中使用决策树 ........................................................................... 18
决策树如何起到作用 ....................................................................................... 18
最好的聊天机器人/机器人框架 ............................................................................... 21
聊天机器人组件和使用的相关术语 ........................................................................ 23
意图（Intent） ................................................................................................. 23
实体（Entities） .............................................................................................. 23
话术（Utterances） ......................................................................................... 24
训练机器人 ...................................................................................................... 24
置信度得分 ...................................................................................................... 24
第 2 章 聊天机器人中的自然语言处理 ............................................................................ 25
为什么我需要自然语言处理知识来搭建聊天机器人 ............................................ 25
spaCy 是什么 ............................................................................................................. 26
spaCy 的基准测试结果 .................................................................................... 27
spaCy 提供了什么能力 .................................................................................... 27
spaCy 的特性 ............................................................................................................. 28
安装和前置条件 .............................................................................................. 29
spaCy 模型是什么............................................................................................ 31
搭建聊天机器人所使用的自然语言处理基本方法 ................................................ 32
词性标注 .......................................................................................................... 32
词干提取和词性还原 ....................................................................................... 36
命名实体识别 .................................................................................................. 38
停用词 .............................................................................................................. 41
依存句法分析 .................................................................................................. 43
名词块 .............................................................................................................. 47
计算相似度 ...................................................................................................... 49
搭建聊天机器人时自然语言处理的一些好方法 .................................................... 51
分词 .................................................................................................................. 51
正则表达式 ...................................................................................................... 52
总结 ........................................................................................................................... 53
第 3 章 轻松搭建聊天机器人 ............................................................................................ 55
Dialogflow 简介 ........................................................................................................ 55
开始 ........................................................................................................................... 56
搭建一个点餐机器人 ....................................................................................... 57
确定范围 .......................................................................................................... 57
列举意图 .......................................................................................................... 57
列举实体 .......................................................................................................... 58
搭建点餐机器人 ........................................................................................................ 58
Dialogflow 入门 ............................................................................................... 59
创建意图的几大要点 ....................................................................................... 62
创建意图并添加自定义话术 ........................................................................... 62
为意图添加默认回复 ....................................................................................... 63
菜品描述意图及附属实体 ............................................................................... 64
理解用户需求并回复 ....................................................................................... 67
将 Dialogflow 聊天机器人发布到互联网上 ............................................................ 72
在 Facebook Messenger 上集成 Dialogflow 聊天机器人 ........................................ 75
设置 Facebook .................................................................................................. 76
创建一个 Facebook 应用程序 ......................................................................... 76
设置 Dialogflow 控制台 .................................................................................. 77
配置 Webhook .................................................................................................. 79
测试信使机器人 .............................................................................................. 80
Fulfillment .................................................................................................................. 83
启用 Webhook .................................................................................................. 85
检查响应数据 .................................................................................................. 87
总结 ........................................................................................................................... 89
第 4 章 从零开始搭建聊天机器人 .................................................................................... 91
Rasa NLU 是什么 ...................................................................................................... 92
我们为什么要使用 Rasa NLU ......................................................................... 92
深入了解 Rasa NLU ......................................................................................... 93
从零开始训练和搭建聊天机器人 ............................................................................ 94
搭建一个星座聊天机器人 ............................................................................... 94
星座机器人和用户之间的对话脚本 ............................................................... 95
为聊天机器人准备数据 ................................................................................... 96
训练聊天机器人模型 ..................................................................................... 101
从模型进行预测 ............................................................................................ 103
使用 Rasa Core 进行对话管理 ............................................................................... 105
深入了解 Rasa Core 及对话系统 .................................................................. 105
理解 Rasa 概念 ............................................................................................... 108
为聊天机器人创建域文件 ............................................................................. 111
为聊天机器人编写自定义动作 .............................................................................. 113
训练机器人的数据准备 .......................................................................................... 116
构造故事数据 ................................................................................................ 117
交互学习 ........................................................................................................ 119
将对话导出成故事......................................................................................... 132
测试机器人 .............................................................................................................. 133
测试用例一 .................................................................................................... 133
测试用例二 .................................................................................................... 134
总结 ......................................................................................................................... 135
第 5 章 部署自己的聊天机器人 ...................................................................................... 137
前提条件.................................................................................................................. 137
Rasa 的凭据管理 ..................................................................................................... 137
在 Facebook 上部署聊天机器人 ............................................................................ 139
在 Heroku 上创建一个应用 ........................................................................... 139
在本地系统中安装 Heroku ............................................................................ 140
在 Facebook 上创建和设置应用程序 ........................................................... 140
在 Heroku 上创建和部署 Rasa 动作服务器应用程序 ................................. 143
创建 Rasa 聊天机器人 API 应用程序........................................................... 144
创建一个用于 Facebook Messenger 聊天机器人的独立脚本 ..................... 144
验证对话管理应用程序在 Heroku 上的部署情况 ....................................... 147
集成 Facebook Webhook ................................................................................ 148
部署后验证：Facebook 聊天机器人 ............................................................ 149
在 Slack 上部署聊天机器人 ................................................................................... 151
为 Slack 创建独立脚本 .................................................................................. 151
编辑 Procfile ................................................................................................... 154
将 Slack 机器人最终部署到 Heroku 上 ........................................................ 154
订阅 Slack 事件 .............................................................................................. 155
订阅机器人事件 ............................................................................................ 156
部署后验证：Slack 机器人 ........................................................................... 156
独立部署聊天机器人 .............................................................................................. 157
编写脚本实现自己的聊天机器人通道 ......................................................... 158
编写 Procfile 并部署到 Web 上 ..................................................................... 159
验证你的聊天机器人 API ............................................................................. 160
绘制聊天机器人的图形界面 ......................................................................... 161
总结 ......................................................................................................................... 165
・ ・ ・ ・ ・ ・ (收起)自然语言处理Java实现 1
第1章 自然语言处理实践基础 - 1 -
1.1 准备开发环境 - 1 -
1.1.1 Windows命令行Cmd - 2 -
1.1.2 在Windows下使用Java - 3 -
1.1.3 Linux终端 - 5 -
1.1.4 在Linux下使用Java - 6 -
1.1.5 Eclipse集成开发环境 - 8 -
1.2 技术基础 - 8 -
1.2.1 机器学习 - 8 -
1.2.2 Java基础 - 9 -
1.2.3 信息采集 - 10 -
1.2.4 文本挖掘 - 11 -
1.2.5 SWIG扩展Java性能 - 11 -
1.2.6 代码移植 - 13 -
1.2.7 语义 - 15 -
1.2.8 Hadoop分布式计算框架 - 16 -
1.3 本章小结 - 19 -
1.4 专业术语 - 20 -
第2章 中文分词原理与实现 21
2.1 接口 21
2.1.1 切分方案 22
2.1.2 词典格式 23
2.2 散列表最长匹配中文分词 24
2.2.1 算法实现 24
2.2.2 使用Ant构建分词jar包 26
2.2.3 使用Maven构建分词jar包 28
2.2.4 使用Gradle构建分词jar包 30
2.2.5 生成JavaDoc 32
2.3 查找词典算法 32
2.3.1 标准Trie树 32
2.3.2 三叉Trie树 36
2.4 Trie树正向最大长度匹配法 43
2.4.1 逆向最大长度匹配法 47
2.4.2 有限状态机识别未登录串 52
2.5 概率语言模型的分词方法 59
2.5.1 一元模型 60
2.5.2 整合基于规则的方法 66
2.5.3 表示切分词图 67
2.5.4 形成切分词图 73
2.5.5 数据基础 75
2.5.6 改进一元模型 85
2.5.7 二元词典 88
2.5.8 完全二叉树组 93
2.5.9 三元词典 96
2.5.10 N元模型 97
2.5.11 N元分词 98
2.5.12 生成语言模型 105
2.5.13 评估语言模型 106
2.5.14 概率分词的流程与结构 107
2.5.15 可变长n元分词 108
2.5.16 条件随机场 108
2.6 新词发现 108
2.7 安卓中文输入法 113
2.8 词性标注 116
2.8.1 数据基础 119
2.8.2 隐马尔可夫模型 120
2.8.3 存储数据 127
2.8.4 统计数据 133
2.8.5 整合切分与词性标注 135
2.8.6 大词表 139
2.8.7 词性序列 140
2.8.8 基于转换的错误学习方法 140
2.8.9 条件随机场 142
2.9 词类模型 143
2.10 未登录词识别 144
2.10.1 未登录人名 144
2.10.2 提取候选人名 145
2.10.3 最长人名切分 151
2.10.4 一元概率人名切分 152
2.10.5 二元概率人名切分 153
2.10.6 未登录地名 156
2.10.7 未登录企业名 156
2.11 中文分词总体结构 156
2.12 平滑算法 158
2.12.1 最大熵 161
2.12.2 条件随机场 166
2.13 地名切分 167
2.13.1 识别未登录地名 167
2.13.2 整体流程 173
2.14 企业名切分 175
2.14.1 识别未登录词 175
2.14.2 整体流程 177
2.15 结果评测 177
2.16 本章小结 178
2.17 专业术语 179
第3章 语义分析 181
3.1 句法分析树 181
3.2 依存文法 186
3.2.1 中文依存文法 186
3.2.2 英文依存文法 193
3.2.3 生成依存树 201
3.2.4 机器学习的方法 205
3.3 依存语言模型 205
3.4 使用Java计算机语言的语义分析 206
3.5 小结 209
3.6 专业术语 210
第4章 文章分析与生成 211
4.1 分词 211
4.1.1 句子切分 211
4.1.2 识别未登录串 213
4.1.3 切分边界 214
4.2 词性标注 215
4.3 重点词汇 218
4.4 句子时态 219
4.5 自动写作 220
4.6 本章小结 221
第5章 文档排重 222
5.1 相似度计算 222
5.1.1 夹角余弦 222
5.1.2 最长公共子串 224
5.1.3 同义词替换 228
5.1.4 地名相似度 229
5.1.5 企业名相似度 231
5.2 文档排重 232
5.2.1 关键词排重 232
5.2.2 SimHash 235
5.2.3 分布式文档排重 246
5.2.4 使用文本排重 247
5.3 在搜索引擎中使用文本排重 247
5.4 本章小结 248
5.5 专业术语 248
第6章 信息提取 249
6.1 指代消解 249
6.2 中文关键词提取 250
6.2.1 关键词提取的基本方法 250
6.2.2 HITS算法应用于关键词提取 252
6.2.3 从网页中提取关键词 254
6.3 信息提取 254
6.3.1 提取联系方式 256
6.3.2 从互联网提取信息 256
6.3.3 提取地名 257
6.4 拼写纠错 258
6.4.1 模糊匹配问题 260
6.4.2 正确词表 269
6.4.3 英文拼写检查 271
6.4.4 中文拼写检查 272
6.5 输入提示 274
6.6 本章小结 275
6.7专业术语 275
第7章 自动摘要 276
7.1 自动摘要技术 276
7.1.1 英文文本摘要 278
7.1.2 中文文本摘要 280
7.1.3 基于篇章结构的自动摘要 284
7.1.4 句子压缩 284
7.2 指代消解 284
7.3 多文档摘要 284
7.4 分布式部署 285
7.5 本章小结 287
7.5专业术语 287
第8章 文本分类 288
8.1 地名分类 289
8.2 文本模板分类 289
8.3 特征提取 290
8.4 线性分类器 293
8.4.1 关键词加权法 294
8.4.2 朴素贝叶斯 297
8.4.3 贝叶斯文本分类 302
8.4.4 支持向量机 303
8.4.5 多级分类 311
8.4.6 使用sklearn实现文本分类 312
8.4.7 规则方法 312
8.4.8 网页分类 315
8.5 FastText文本分类 316
8.5.1 词向量 316
8.5.2 JavaCPP包装Java接口 316
8.5.3 使用JFastText 318
8.6 最大熵分类器 319
8.7 文本聚类 320
8.7.1 K均值聚类方法 320
8.7.2 K均值实现 322
8.7.3 深入理解DBScan算法 326
8.7.4 使用DBScan算法聚类实例 327
8.8 持续集成 329
8.9 本章小结 330
8.9专业术语 330
第9章 文本倾向性分析 331
9.1 确定词语的褒贬倾向 332
9.2 实现情感识别 334
9.3 本章小结 336
9.4专业术语 337
第10章 语音识别 338
10.1 总体结构 338
10.1.1 识别中文 340
10.1.2 自动问答 341
10.2 语音库 343
10.3 语音 344
10.3.1 标注语音 347
10.3.2 动态时间规整计算相似度 348
10.4 Sphinx语音识别 351
10.4.1 中文训练集 352
10.4.2 使用Sphinx4 353
10.4.3 ARPA文件格式 356
10.4.4 运行于Android的PocketSphinx 358
10.5 说话人识别 362
10.6 本章小结 363
10.7 术语表 363
第11章 问答系统 365
11.1 问答系统的结构 365
11.1.1 提取问答对 366
11.1.2 等价问题 366
11.2 问句分析 366
11.2.1 问题类型 367
11.2.2 句型 369
11.2.3 用户意图识别 370
11.2.4 业务类型 370
11.2.5 依存树 370
11.2.6 指代消解 371
11.2.7 二元关系 371
11.2.8 问句模板 373
11.2.9 结构化问句模板 375
11.2.10 检索方式 376
11.2.11 问题重写 380
11.2.12 提取事实 380
11.2.13 验证答案 382
11.3 知识库 382
11.3.1 语义库 383
11.4 AIML聊天机器人 385
11.4.1 交互式问答 387
11.4.2 垂直领域问答系统 388
11.4.3 语料库 391
11.4.4 客户端 391
11.5 自然语言生成 391
11.6 JavaFX开发界面 393
11.7 本章小结 394
11.8 术语表 394
第12章 机器翻译 395
12.1 使用机器翻译API 395
12.2 翻译日期 395
12.3 神经网络机器翻译 397
12.4 辅助机器翻译 399
12.5 机器翻译的评价 400
12.6 本章小结 400
参考资源 402
书籍 402
网址 402
后记 403
・ ・ ・ ・ ・ ・ (收起)译者序
第2版前言
第1版前言
第1版致谢
第1章　基础知识 1
1.1　概率测度 1
1.2　随机变量 2
1.2.1　连续随机变量和离散随机变量 2
1.2.2　多元随机变量的联合分布 3
1.3　条件分布 4
1.3.1　贝叶斯法则 5
1.3.2　独立随机变量与条件独立随机变量 6
1.3.3　可交换的随机变量 6
1.4　随机变量的期望 7
1.5　模型 9
1.5.1　参数模型与非参数模型 9
1.5.2　模型推断 10
1.5.3　生成模型 11
1.5.4　模型中的独立性假定 13
1.5.5　有向图模型 13
1.6　从数据场景中学习 15
1.7　贝叶斯学派和频率学派的哲学（冰山一角） 17
1.8　本章小结 17
1.9　习题 18
第2章　绪论 19
2.1　贝叶斯统计与自然语言处理的结合点概述 19
2.2　第一个例子：隐狄利克雷分配模型 22
2.2.1　狄利克雷分布 26
2.2.2　推断 28
2.2.3　总结 29
2.3　第二个例子：贝叶斯文本回归 30
2.4　本章小结 31
2.5　习题 31
第3章　先验 33
3.1　共轭先验 33
3.1.1　共轭先验和归一化常数 36
3.1.2　共轭先验在隐变量模型中的应用 37
3.1.3　混合共轭先验 38
3.1.4　重新归一化共轭分布 39
3.1.5　是否共轭的讨论 39
3.1.6　总结 40
3.2　多项式分布和类别分布的先验 40
3.2.1　再谈狄利克雷分布 41
3.2.2　Logistic正态分布 44
3.2.3　讨论 48
3.2.4　总结 49
3.3　非信息先验 49
3.3.1　均匀不正常先验 50
3.3.2　Jeffreys先验 51
3.3.3　讨论 51
3.4　共轭指数模型 52
3.5　模型中的多参数抽取 53
3.6　结构先验 54
3.7　本章小结 55
3.8　习题 56
第4章　贝叶斯估计 57
4.1　隐变量学习：两种观点 58
4.2　贝叶斯点估计 58
4.2.1　最大后验估计 59
4.2.2　基于最大后验解的后验近似 64
4.2.3　决策-理论点估计 65
4.2.4　总结 66
4.3　经验贝叶斯 66
4.4　后验的渐近行为 68
4.5　本章小结 69
4.6　习题 69
第5章　采样算法 70
5.1　MCMC算法：概述 71
5.2　MCMC推断的自然语言处理模型结构 71
5.3　吉布斯采样 73
5.3.1　坍塌吉布斯采样 76
5.3.2　运算符视图 79
5.3.3　并行化的吉布斯采样器 80
5.3.4　总结 81
5.4　Metropolis-Hastings算法 82
5.5　切片采样 84
5.5.1　辅助变量采样 85
5.5.2　切片采样和辅助变量采样在自然语言处理中的应用 85
5.6　模拟退火 86
5.7　MCMC算法的收敛性 86
5.8　马尔可夫链：基本理论 88
5.9　MCMC领域外的采样算法 89
5.10　蒙特卡罗积分 91
5.11　讨论 93
5.11.1　分布的可计算性与采样 93
5.11.2　嵌套的MCMC采样 93
5.11.3　MCMC方法的运行时间 93
5.11.4　粒子滤波 93
5.12　本章小结 95
5.13　习题 95
第6章　变分推断 97
6.1　边缘对数似然的变分界 97
6.2　平均场近似 99
6.3　平均场变分推断算法 100
6.3.1　狄利克雷-多项式变分推断 101
6.3.2　与期望最大化算法的联系 104
6.4　基于变分推断的经验贝叶斯 106
6.5　讨论 106
6.5.1　推断算法的初始化 107
6.5.2　收敛性诊断 107
6.5.3　变分推断在解码中的应用 107
6.5.4　变分推断最小化KL散度 108
6.5.5　在线的变分推断 109
6.6　本章小结 109
6.7　习题 109
第7章　非参数先验 111
7.1　狄利克雷过程：三种视角 112
7.1.1　折棍子过程 112
7.1.2　中餐馆过程 114
7.2　狄利克雷过程混合模型 115
7.2.1　基于狄利克雷过程混合模型的推断 116
7.2.2　狄利克雷过程混合是混合模型的极限 118
7.3　层次狄利克雷过程 119
7.4　Pitman?Yor过程 120
7.4.1　Pitman-Yor过程用于语言建模 121
7.4.2　Pitman-Yor过程的幂律行为 122
7.5　讨论 123
7.5.1　高斯过程 124
7.5.2　印度自助餐过程 124
7.5.3　嵌套的中餐馆过程 125
7.5.4　距离依赖的中餐馆过程 125
7.5.5　序列记忆器 126
7.6　本章小结 126
7.7　习题 127
第8章　贝叶斯语法模型 128
8.1　贝叶斯隐马尔可夫模型 129
8.2　概率上下文无关语法 131
8.2.1　作为多项式分布集的PCFG 133
8.2.2　PCFG的基本推断算法 133
8.2.3　作为隐马尔可夫模型的PCFG 136
8.3　贝叶斯概率上下文无关语法 137
8.3.1　PCFG的先验 137
8.3.2　贝叶斯PCFG的蒙特卡罗推断 138
8.3.3　贝叶斯PCFG的变分推断 139
8.4　适配器语法 140
8.4.1　Pitman-Yor适配器语法 141
8.4.2　PYAG的折棍子视角 142
8.4.3　基于PYAG的推断 143
8.5　层次狄利克雷过程PCFG 144
8.6　依存语法 147
8.7　同步语法 148
8.8　多语言学习 149
8.8.1　词性标注 149
8.8.2　语法归纳 151
8.9　延伸阅读 152
8.10　本章小结 153
8.11　习题 153
第9章　表征学习与神经网络 155
9.1　神经网络与表征学习：为什么是现在 155
9.2　词嵌入 158
9.2.1　词嵌入的skip-gram模型 158
9.2.2　贝叶斯skip-gram词嵌入 160
9.2.3　讨论 161
9.3　神经网络 162
9.3.1　频率论估计和反向传播算法 164
9.3.2　神经网络权值的先验 166
9.4　神经网络在自然语言处理中的现代应用 168
9.4.1　循环神经网络和递归神经网络 168
9.4.2　梯度消失与梯度爆炸问题 169
9.4.3　神经编码器-解码器模型 172
9.4.4　卷积神经网络 175
9.5　调整神经网络 177
9.5.1　正则化 177
9.5.2　超参数调整 178
9.6　神经网络生成建模 180
9.6.1　变分自编码器 180
9.6.2　生成对抗网络 185
9.7　本章小结 186
9.8　习题 187
结束语 189
附录A　基本概念 191
附录B　概率分布清单 197
参考文献 203
・ ・ ・ ・ ・ ・ (收起)第一部分 人工智能基础
第 1 章 绪论 2
1.1 什么是人工智能 2
1.1.1 类人行为：图灵测试方法 3
1.1.2 类人思考：认知建模方法 3
1.1.3 理性思考：“思维法则”方法 4
1.1.4 理性行为：理性智能体方法 4
1.1.5 益机 5
1.2 人工智能的基础 6
1.2.1 哲学 6
1.2.2 数学 8
1.2.3 经济学 9
1.2.4 神经科学 10
1.2.5 心理学 12
1.2.6 计算机工程 13
1.2.7 控制理论与控制论 14
1.2.8 语言学 15
1.3 人工智能的历史 16
1.3.1 人工智能的诞生（1943―1956） 16
1.3.2 早期热情高涨，期望无限（1952―1969） 17
1.3.3 一些现实（1966―1973） 19
1.3.4 专家系统（1969―1986） 20
1.3.5 神经网络的回归（1986―现在） 22
1.3.6 概率推理和机器学习（1987―现在） 22
1.3.7 大数据（2001―现在） 23
1.3.8 深度学习（2011―现在） 24
1.4 目前的先进技术 24
1.5 人工智能的风险和收益 27
小结 30
参考文献与历史注释 31
第 2 章 智能体 32
2.1 智能体和环境 32
2.2 良好行为：理性的概念 34
2.2.1 性能度量 34
2.2.2 理性 35
2.2.3 全知、学习和自主 36
2.3 环境的本质 37
2.3.1 指定任务环境 37
2.3.2 任务环境的属性 38
2.4 智能体的结构 41
2.4.1 智能体程序 41
2.4.2 简单反射型智能体 42
2.4.3 基于模型的反射型智能体 44
2.4.4 基于目标的智能体 45
2.4.5 基于效用的智能体 46
2.4.6 学习型智能体 47
2.4.7 智能体程序的组件如何工作 49
小结 50
参考文献与历史注释 51
第二部分 问题求解
第 3 章 通过搜索进行问题求解 54
3.1 问题求解智能体 54
3.1.1 搜索问题和解 55
3.1.2 问题形式化 56
3.2 问题示例 57
3.2.1 标准化问题 57
3.2.2 真实世界问题 59
3.3 搜索算法 61
3.3.1 最佳优先搜索 62
3.3.2 搜索数据结构 63
3.3.3 冗余路径 64
3.3.4 问题求解性能评估 65
3.4 无信息搜索策略 65
3.4.1 广度优先搜索 66
3.4.2 Dijkstra 算法或一致代价搜索 67
3.4.3 深度优先搜索与内存问题 68
3.4.4 深度受限和迭代加深搜索 69
3.4.5 双向搜索 712
3.4.6 无信息搜索算法对比 72
3.5 有信息（启发式）搜索策略 73
3.5.1 贪心最佳优先搜索 73
3.5.2 A* 搜索 75
3.5.3 搜索等值线 77
3.5.4 满意搜索：不可容许的启发式
函数与加权 A* 搜索 79
3.5.5 内存受限搜索 80
3.5.6 双向启发式搜索 83
3.6 启发式函数 85
3.6.1 启发式函数的准确性对性能的影响 85
3.6.2 从松弛问题出发生成启发式函数 86
3.6.3 从子问题出发生成启发式函数：模式数据库 87
3.6.4 使用地标生成启发式函数 88
3.6.5 学习以更好地搜索 90
3.6.6 从经验中学习启发式函数 90
小结 90
参考文献与历史注释 92
第 4 章 复杂环境中的搜索 95
4.1 局部搜索和最优化问题 95
4.1.1 爬山搜索 96
4.1.2 模拟退火 98
4.1.3 局部束搜索 99
4.1.4 进化算法 99
4.2 连续空间中的局部搜索 102
4.3 使用非确定性动作的搜索 104
4.3.1 不稳定的真空吸尘器世界 105
4.3.2 与或搜索树 106
4.3.3 反复尝试 107
4.4 部分可观测环境中的搜索 108
4.4.1 无观测信息的搜索 108
4.4.2 部分可观测环境中的搜索 111
4.4.3 求解部分可观测问题 112
4.4.4 部分可观测环境中的智能体 113
4.5 在线搜索智能体和未知环境 115
4.5.1 在线搜索问题 115
4.5.2 在线搜索智能体 117
4.5.3 在线局部搜索 118
4.5.4 在线搜索中的学习 119
小结 120
参考文献与历史注释 121
第 5 章 对抗搜索和博弈 124
5.1 博弈论 124
5.2 博弈中的优化决策 126
5.2.1 极小化极大搜索算法 127
5.2.2 多人博弈中的最优决策 128
5.2.3 α-β 剪枝 129
5.2.4 移动顺序 131
5.3 启发式 α-β 树搜索 132
5.3.1 评价函数 132
5.3.2 截断搜索 134
5.3.3 前向剪枝 135
5.3.4 搜索和查表 136
5.4 蒙特卡罗树搜索 136
5.5 随机博弈 139
5.6 部分可观测博弈 142
5.6.1 四国军棋：部分可观测的国际象棋 142
5.6.2 纸牌游戏 144
5.7 博弈搜索算法的局限性 146
小结 147
参考文献与历史注释 148
第 6 章 约束满足问题 152
6.1 定义约束满足问题 152
6.1.1 问题示例：地图着色 153
6.1.2 问题示例：车间作业调度 154
6.1.3 CSP 形式体系的变体 155
6.2 约束传播：CSP 中的推断 156
6.2.1 节点一致性 157
6.2.2 弧一致性 157
6.2.3 路径一致性 158
6.2.4 k 一致性 158
6.2.5 全局约束 159
6.2.6 数独 160
6.3 CSP 的回溯搜索 161
6.3.1 变量排序和值排序 163
6.3.2 交替进行搜索和推理 164
6.3.3 智能回溯：向后看 164
6.3.4 约束学习 166
6.4 CSP 的局部搜索 166
6.5 问题的结构 168
6.5.1 割集调整 169
6.5.2 树分解 170
6.5.3 值对称 171
小结 171
参考文献与历史注释 172
第三部分 知识、推理和规划
第 7 章 逻辑智能体 176
7.1 基于知识的智能体 176
7.2 wumpus 世界 178
7.3 逻辑 180
7.4 命题逻辑：一种非常简单的逻辑 183
7.4.1 语法 183
7.4.2 语义 184
7.4.3 一个简单的知识库 185
7.4.4 一个简单的推断过程 186
7.5 命题定理证明 187
7.5.1 推断与证明 188
7.5.2 通过归结证明 190
7.5.3 霍恩子句与确定子句 194
7.5.4 前向链接与反向链接 194
7.6 高效命题模型检验 196
7.6.1 完备的回溯算法 196
7.6.2 局部搜索算法 198
7.6.3 随机 SAT 问题概览 199
7.7 基于命题逻辑的智能体 200
7.7.1 世界的当前状态 200
7.7.2 混合智能体 203
7.7.3 逻辑状态估计 204
7.7.4 用命题推断进行规划 205
小结 207
参考文献与历史注释 208
第 8 章 一阶逻辑 211
8.1 回顾表示 211
8.1.1 思想的语言 212
8.1.2 结合形式语言和自然语言的优点 213
8.2 一阶逻辑的语法和语义 215
8.2.1 一阶逻辑模型 215
8.2.2 符号与解释 216
8.2.3 项 218
8.2.4 原子语句 218
8.2.5 复合语句 218
8.2.6 量词 219
8.2.7 等词 222
8.2.8 数据库语义 222
8.3 使用一阶逻辑 223
8.3.1 一阶逻辑的断言与查询 223
8.3.2 亲属关系论域 224
8.3.3 数、集合与列表 225
8.3.4 wumpus 世界 227
8.4 一阶逻辑中的知识工程 228
8.4.1 知识工程的过程 229
8.4.2 电子电路论域 230
小结 233
参考文献与历史注释 234
第 9 章 一阶逻辑中的推断 236
9.1 命题推断与一阶推断 236
9.2 合一与一阶推断 238
9.2.1 合一 239
9.2.2 存储与检索 240
9.3 前向链接 241
9.3.1 一阶确定子句 242
9.3.2 简单的前向链接算法 242
9.3.3 高效前向链接 244
9.4 反向链接 247
9.4.1 反向链接算法 247
9.4.2 逻辑编程 248
9.4.3 冗余推断和无限循环 249
9.4.4 Prolog 的数据库语义 251
9.4.5 约束逻辑编程 251
9.5 归结 252
9.5.1 一阶逻辑的合取范式 252
9.5.2 归结推断规则 253
9.5.3 证明范例 254
9.5.4 归结的完备性 256
9.5.5 等词 258
9.5.6 归结策略 260
小结 261
参考文献与历史注释 262
第 10 章 知识表示 265
10.1 本体论工程 265
10.2 类别与对象 267
10.2.1 物理组成 268
10.2.2 量度 269
10.2.3 对象：事物和物质 271
10.3 事件 272
10.3.1 时间 273
10.3.2 流和对象 275
10.4 精神对象和模态逻辑 275
10.5 类别的推理系统 278
10.5.1 语义网络 278
10.5.2 描述逻辑 280
10.6 用缺省信息推理 281
10.6.1 限定与缺省逻辑 281
10.6.2 真值维护系统 283
小结 284
参考文献与历史注释 285
第 11 章 自动规划 290
11.1 经典规划的定义 290
11.1.1 范例领域：航空货物运输 291
11.1.2 范例领域：备用轮胎问题 292
11.1.3 范例领域：积木世界 292
11.2 经典规划的算法 294
11.2.1 规划的前向状态空间搜索 294
11.2.2 规划的反向状态空间搜索 295
11.2.3 使用布尔可满足性规划 296
11.2.4 其他经典规划方法 296
11.3 规划的启发式方法 297
11.3.1 领域无关剪枝 299
11.3.2 规划中的状态抽象 300
11.4 分层规划 300
11.4.1 高层动作 301
11.4.2 搜索基元解 302
11.4.3 搜索抽象解 303
11.5 非确定性域的规划和行动 307
11.5.1 无传感器规划 309
11.5.2 应变规划 312
11.5.3 在线规划 313
11.6 时间、调度和资源 315
11.6.1 时间约束和资源约束的表示 315
11.6.2 解决调度问题 316
11.7 规划方法分析 318
小结 319
参考文献与历史注释 320
第四部分 不确定知识和不确定推理
第 12 章 不确定性的量化 326
12.1 不确定性下的动作 326
12.1.1 不确定性概述 327
12.1.2 不确定性与理性决策 328
12.2 基本概率记号 329
12.2.1 概率是关于什么的 329
12.2.2 概率断言中的命题语言 330
12.2.3 概率公理及其合理性 333
12.3 使用完全联合分布进行推断 334
12.4 独立性 336
12.5 贝叶斯法则及其应用 337
12.5.1 应用贝叶斯法则：简单实例 338
12.5.2 应用贝叶斯法则：合并证据 339
12.6 朴素贝叶斯模型 340
12.7 重游 wumpus 世界 342
小结 344
参考文献与历史注释 345
第 13 章 概率推理 348
13.1 不确定域的知识表示 348
13.2 贝叶斯网络的语义 350
13.2.1 贝叶斯网络中的条件独立性关系 353
13.2.2 条件分布的高效表示 354
13.2.3 连续变量的贝叶斯网络 356
13.2.4 案例研究：汽车保险 358
13.3 贝叶斯网络中的精确推断 360
13.3.1 通过枚举进行推断 361
13.3.2 变量消元算法 363
13.3.3 精确推断的复杂性 365
13.3.4 聚类算法 366
13.4 贝叶斯网络中的近似推理 367
13.4.1 直接采样方法 368
13.4.2 通过马尔可夫链模拟进行推断 372
13.4.3 编译近似推断 378
13.5 因果网络 379
13.5.1 表示动作：do 操作 380
13.5.2 后门准则 382
小结 382
参考文献与历史注释 383
第 14 章 时间上的概率推理 388
14.1 时间与不确定性 388
14.1.1 状态与观测 389
14.1.2 转移模型与传感器模型 389
14.2 时序模型中的推断 391
14.2.1 滤波与预测 392
14.2.2 平滑 394
14.2.3 寻找最可能序列 396
14.3 隐马尔可夫模型 398
14.3.1 简化矩阵算法 398
14.3.2 隐马尔可夫模型示例：定位 400
14.4 卡尔曼滤波器 403
14.4.1 更新高斯分布 403
14.4.2 简单的一维示例 404
14.4.3 一般情况 406
14.4.4 卡尔曼滤波的适用范围 407
14.5 动态贝叶斯网络 408
14.5.1 构建动态贝叶斯网络 409
14.5.2 动态贝叶斯网络中的精确推断 412
14.5.3 动态贝叶斯网络中的近似推断 413
小结 417
参考文献与历史注释 418
第 15 章 概率编程 421
15.1 关系概率模型 421
15.1.1 语法与语义 423
15.1.2 实例：评定玩家的技能等级 425
15.1.3 关系概率模型中的推断 426
15.2 开宇宙概率模型 427
15.2.1 语义与语法 428
15.2.2 开宇宙概率模型的推断 429
15.2.3 示例 430
15.3 追踪复杂世界 433
15.3.1 示例：多目标跟踪 433
15.3.2 示例：交通监控 436
15.4 作为概率模型的程序 436
15.4.1 示例：文本阅读 437
15.4.2 语法与语义 438
15.4.3 推断结果 438
15.4.4 结合马尔可夫模型改进生成程序 439
15.4.5 生成程序的推断 439
小结 440
参考文献与历史注释 440
第 16 章 做简单决策 444
16.1 在不确定性下结合信念与愿望 444
16.2 效用理论基础 445
16.2.1 理性偏好的约束 445
16.2.2 理性偏好导致效用 447
16.3 效用函数 448
16.3.1 效用评估和效用尺度 448
16.3.2 金钱的效用 449
16.3.3 期望效用与决策后失望 451
16.3.4 人类判断与非理性 452
16.4 多属性效用函数 454
16.4.1 占优 455
16.4.2 偏好结构与多属性效用 456
16.5 决策网络 458
16.5.1 使用决策网络表示决策问题 458
16.5.2 评估决策网络 460
16.6 信息价值 460
16.6.1 简单示例 460
16.6.2 完美信息的一般公式 461
16.6.3 价值信息的性质 462
16.6.4 信息收集智能体的实现 463
16.6.5 非短视信息收集 463
16.6.6 敏感性分析与健壮决策 464
16.7 未知偏好 465
16.7.1 个人偏好的不确定性 466
16.7.2 顺从人类 467
小结 468
参考文献与历史注释 469
第 17 章 做复杂决策 473
17.1 序贯决策问题 473
17.1.1 时间上的效用 475
17.1.2 最优策略与状态效用 477
17.1.3 奖励规模 479
17.1.4 表示 MDP 480
17.2 MDP 的算法 482
17.2.1 价值迭代 482
17.2.2 策略迭代 485
17.2.3 线性规划 487
17.2.4 MDP 的在线算法 487
17.3 老虎机问题 489
17.3.1 计算基廷斯指数 491
17.3.2 伯努利老虎机 492
17.3.3 近似最优老虎机策略 493
17.3.4 不可索引变体 493
17.4 部分可观测MDP 495
17.5 求解POMDP 的算法 497
17.5.1 POMDP的价值迭代 497
17.5.2 POMDP的在线算法 500
小结 501
参考文献与历史注释 502
第 18 章 多智能体决策 505
18.1 多智能体环境的特性 505
18.1.1 单个决策者 505
18.1.2 多决策者 506
18.1.3 多智能体规划 507
18.1.4 多智能体规划：合作与协调 509
18.2 非合作博弈论 510
18.2.1 单步博弈：正则形式博弈 510
18.2.2 社会福利 513
18.2.3 重复博弈 517
18.2.4 序贯博弈：扩展形式 520
18.2.5 不确定收益与辅助博弈 525
18.3 合作博弈论 527
18.3.1 联盟结构与结果 528
18.3.2 合作博弈中的策略 529
18.3.3 合作博弈中的计算 531
18.4 制定集体决策 533
18.4.1 在合同网中分配任务 533
18.4.2 通过拍卖分配稀缺资源 535
18.4.3 投票 539
18.4.4 议价 541
小结 544
参考文献与历史注释 545
第五部分 机器学习
第 19 章 样例学习 550
19.1 学习的形式 550
19.2 监督学习 552
19.3 决策树学习 555
19.3.1 决策树的表达能力 556
19.3.2 从样例中学习决策树 557
19.3.3 选择测试属性 559
19.3.4 泛化与过拟合 560
19.3.5 拓展决策树的适用范围 562
19.4 模型选择与模型优化 563
19.4.1 模型选择 564
19.4.2 从错误率到损失函数 566
19.4.3 正则化 567
19.4.4 超参数调整 568
19.5 学习理论 569
19.6 线性回归与分类 572
19.6.1 单变量线性回归 572
19.6.2 梯度下降 574
19.6.3 多变量线性回归 575
19.6.4 带有硬阈值的线性分类器 577
19.6.5 基于逻辑斯谛回归的线性分类器 579
19.7 非参数模型 581
19.7.1 最近邻模型 581
19.7.2 使用 k-d 树寻找最近邻 583
19.7.3 局部敏感哈希 584
19.7.4 非参数回归 585
19.7.5 支持向量机 586
19.7.6 核技巧 589
19.8 集成学习 589
19.8.1 自助聚合法 590
19.8.2 随机森林法 590
19.8.3 堆叠法 591
19.8.4 自适应提升法 592
19.8.5 梯度提升法 594
19.8.6 在线学习 595
19.9 开发机器学习系统 596
19.9.1 问题形式化 596
19.9.2 数据收集、评估和管理 597
19.9.3 模型选择与训练 601
19.9.4 信任、可解释性、可说明性 601
19.9.5 操作、监控和维护 603
小结 604
参考文献与历史注释 605
第 20 章 概率模型学习 610
20.1 统计学习 610
20.2 完全数据学习 613
20.2.1 最大似然参数学习：离散模型 613
20.2.2 朴素贝叶斯模型 615
20.2.3 生成模型和判别模型 616
20.2.4 最大似然参数学习：连续模型 616
20.2.5 贝叶斯参数学习 618
20.2.6 贝叶斯线性回归 620
20.2.7 贝叶斯网络结构学习 622
20.2.8 非参数模型密度估计 623
20.3 隐变量学习：EM 算法 624
20.3.1 无监督聚类：学习混合高斯 625
20.3.2 学习带隐变量的贝叶斯网络参数值 627
20.3.3 学习隐马尔可夫模型 630
20.3.4 EM 算法的一般形式 630
20.3.5 学习带隐变量的贝叶斯网络结构 631
小结 632
参考文献与历史注释 632
第 21 章 深度学习 635
21.1 简单前馈网络 636
21.1.1 网络作为复杂函数 636
21.1.2 梯度与学习 639
21.2 深度学习的计算图 640
21.2.1 输入编码 641
21.2.2 输出层与损失函数 641
21.2.3 隐藏层 642
21.3 卷积网络 643
21.3.1 池化与下采样 646
21.3.2 卷积神经网络的张量运算 646
21.3.3 残差网络 647
21.4 学习算法 648
21.4.1 计算图中的梯度计算 649
21.4.2 批量归一化 650
21.5 泛化 650
21.5.1 选择正确的网络架构 651
21.5.2 神经架构搜索 652
21.5.3 权重衰减 653
21.5.4 暂退法 653
21.6 循环神经网络 654
21.6.1 训练基本的循环神经网络 655
21.6.2 长短期记忆 RNN 656
21.7 无监督学习与迁移学习 657
21.7.1 无监督学习 657
21.7.2 迁移学习和多任务学习 661
21.8 应用 662
21.8.1 视觉 662
21.8.2 自然语言处理 663
21.8.3 强化学习 663
小结 664
参考文献与历史注释 664
第 22 章 强化学习 668
22.1 从奖励中学习 668
22.2 被动强化学习 670
22.2.1 直接效用估计 671
22.2.2 自适应动态规划 671
22.2.3 时序差分学习 672
22.3 主动强化学习 674
22.3.1 探索 675
22.3.2 安全探索 677
22.3.3 时序差分 Q 学习 678
22.4 强化学习中的泛化 680
22.4.1 近似直接效用估计 680
22.4.2 近似时序差分学习 681
22.4.3 深度强化学习 682
22.4.4 奖励函数设计 683
22.4.5 分层强化学习 683
22.5 策略搜索 686
22.6 学徒学习与逆强化学习 688
22.7 强化学习的应用 690
22.7.1 在电子游戏中的应用 690
22.7.2 在机器人控制中的应用 691
小结 692
参考文献与历史注释 693
第六部分 沟通、感知和行动
第 23 章 自然语言处理 698
23.1 语言模型 698
23.1.1 词袋模型 699
23.1.2 n 元单词模型 700
23.1.3 其他 n 元模型 701
23.1.4 n 元模型的平滑 701
23.1.5 单词表示 702
23.1.6 词性标注 703
23.1.7 语言模型的比较 706
23.2 文法 707
23.3 句法分析 709
23.3.1 依存分析 711
23.3.2 从样例中学习句法分析器 712
23.4 扩展文法 713
23.4.1 语义解释 715
23.4.2 学习语义文法 717
23.5 真实自然语言的复杂性 717
23.6 自然语言任务 720
小结 722
参考文献与历史注释 722
第 24 章 自然语言处理中的深度学习 727
24.1 词嵌入 727
24.2 自然语言处理中的循环神经网络 730
24.2.1 使用循环神经网络的语言模型 730
24.2.2 用循环神经网络进行分类 732
24.2.3 自然语言处理任务中的 LSTM模型 733
24.3 序列到序列模型 733
24.3.1 注意力 735
24.3.2 解码 736
24.4 Transformer 架构 737
24.4.1 自注意力 737
24.4.2 从自注意力到 Transformer 738
24.5 预训练和迁移学习 739
24.5.1 预训练词嵌入 740
24.5.2 预训练上下文表示 741
24.5.3 掩码语言模型 742
24.6 最高水平（SOTA） 742
小结 745
参考文献与历史注释 745
第 25 章 计算机视觉 748
25.1 引言 748
25.2 图像形成 749
25.2.1 无透镜成像：针孔照相机 749
25.2.2 透镜系统 751
25.2.3 缩放正交投影 752
25.2.4 光线与明暗 752
25.2.5 颜色 753
25.3 简单图像特征 754
25.3.1 边缘 755
25.3.2 纹理 757
25.3.3 光流 758
25.3.4 自然图像分割 759
25.4 图像分类 760
25.4.1 基于卷积神经网络的图像分类 761
25.4.2 卷积神经网络对图像分类问题
有效的原因 762
25.5 物体检测 763
25.6 三维世界 766
25.6.1 多个视图下的三维线索 766
25.6.2 双目立体视觉 766
25.6.3 移动摄像机给出的三维线索 768
25.6.4 单个视图的三维线索 769
25.7 计算机视觉的应用 769
25.7.1 理解人类行为 770
25.7.2 匹配图片与文字 772
25.7.3 多视图重建 773
25.7.4 单视图中的几何 774
25.7.5 生成图片 775
25.7.6 利用视觉控制运动 778
小结 780
参考文献与历史注释 781
第 26 章 机器人学 785
26.1 机器人 785
26.2 机器人硬件 786
26.2.1 机器人的硬件层面分类 786
26.2.2 感知世界 787
26.2.3 产生运动 789
26.3 机器人学解决哪些问题 789
26.4 机器人感知 790
26.4.1 定位与地图构建 791
26.4.2 其他感知类型 795
26.4.3 机器人感知中的监督学习与无监督学习 795
26.5 规划与控制 796
26.5.1 构形空间 796
26.5.2 运动规划 799
26.5.3 轨迹跟踪控制 806
26.5.4 最优控制 809
26.6 规划不确定的运动 810
26.7 机器人学中的强化学习 812
26.7.1 利用模型 812
26.7.2 利用其他信息 813
26.8 人类与机器人 814
26.8.1 协调 814
26.8.2 学习做人类期望的事情 817
26.9 其他机器人框架 820
26.9.1 反应式控制器 820
26.9.2 包容架构 821
26.10 应用领域 822
小结 825
参考文献与历史注释 826
第七部分 总结
第 27 章 人工智能的哲学、伦理和安全性 832
27.1 人工智能的极限 832
27.1.1 由非形式化得出的论据 832
27.1.2 由能力缺陷得出的论据 833
27.1.3 数学异议 833
27.1.4 衡量人工智能 834
27.2 机器能真正地思考吗 835
27.2.1 中文房间 835
27.2.2 意识与感质 836
27.3 人工智能的伦理 836
27.3.1 致命性自主武器 837
27.3.2 监控、安全与隐私 839
27.3.3 公平与偏见 841
27.3.4 信任与透明度 844
27.3.5 工作前景 845
27.3.6 机器人权利 847
27.3.7 人工智能安全性 848
小结 851
参考文献与历史注释 852
第 28 章 人工智能的未来 857
28.1 人工智能组件 857
28.2 人工智能架构 862
附录 A 数学背景知识 865
附录 B 关于语言与算法的说明 871
参考文献 873Ⅰ artificial intelligence
1 introduction
1.1what is al?
1.2the foundations of artificial intelligence
1.3the history of artificial intelligence
1.4the state of the art
1.5summary, bibliographical and historical notes, exercises
2 intelligent agents
2.1agents and environments
2.2good behavior: the concept of rationality
2.3the nature of environments
2.4the structure of agents
2.5summary, bibliographical and historical notes, exercises
Ⅱ problem-solving
3 solving problems by searching
3.1problem-solving agents
3.2example problems
3.3searching for solutions
3.4uninformed search strategies
3.5informed (heuristic) search strategies
3.6heuristic functions
3.7summary, bibliographical and historical notes, exercises
4 beyond classical search
4.1local search algorithms and optimization problems
4.2local search in continuous spaces
4.3searching with nondeterministic actions
4.4searching with partial observations
4.5online search agents and unknown environments
4.6summary, bibliographical and historical notes, exercises
5 adversarial search
5.1games
5.2optimal decisions in games
5.3alpha-beta pruning
5.4imperfect real-time decisions
5.5stochastic games
5.6partially observable games
5.7state-of-the-art game programs
5.8alternative approaches
5.9summary, bibliographical and historical notes, exercises
6 constraint satisfaction problems
6.1defining constraint satisfaction problems
6.2constraint propagation: inference in csps
6.3backtracking search for csps
6.4local search for csps
6.5the structure of problems
6.6summary, bibliographical and historical notes, exercises
Ⅲ knowledge, reasoning, and planning
7 logical agents
7.1knowledge-based agents
7.2the wumpus world
7.3logic
7.4propositional logic: a very simple logic
7.5propositional theorem proving
7.6effective propositional model checking
7.7agents based on propositional logic
7.8summary, bibliographical and historical notes, exercises
8 first-order logic
8.1representation revisited
8.2syntax and semantics of first-order logic
8.3using first-order logic
8.4knowledge engineering in first-order logic
8.5summary, bibliographical and historical notes, exercises
9 inference in first-order logic
9.1propositional vs. first-order inference
9.2unification and lifting
9.3forward chaining
9.4backward chaining
9.5resolution
9.6summary, bibliographical and historical notes, exercises
10 classical planning
10.1 definition of classical planning
10.2 algorithms for planning as state-space search
10.3 planning graphs
10.4 other classical planning approaches
10.5 analysis of planning approaches
10.6 summary, bibliographical and historical notes, exercises
11 planning and acting in the real world
11.1 time, schedules, and resources
11.2 hierarchical planning
11.3 planning and acting in nondeterministic domains
11.4 multiagent planning
11.5 summary, bibliographical and historical notes, exercises
12 knowledge representation
12.1 ontological engineering
12.2 categories and objects
12.3 events
12.4 mental events and mental objects
12.5 reasoning systems for categories
12.6 reasoning with default information
12.7 the intemet shopping world
12.8 summary, bibliographical and historical notes, exercises
Ⅳ uncertain knowledge and reasoning
13 quantifying uncertainty
13.1 acting under uncertainty
13.2 basic probability notation
13.3 inference using full joint distributions
13.4 independence
13.5 bayes' rule and its use
13.6 the wumpus world revisited
13.7 summary, bibliographical and historical notes, exercises
14 probabilistic reasoning
14.1 representing knowledge in an uncertain domain
14.2 the semantics of bayesian networks
14.3 efficient representation of conditional distributions
14.4 exact inference in bayesian networks
14.5 approximate inference in bayesian networks
14.6 relational and first-order probability models
14.7 other approaches to uncertain reasoning
14.8 summary, bibliographical and historical notes, exercises
15 probabilistic reasoning over time
15.1 time and uncertainty
15.2 inference in temporal models
15.3 hidden markov models
15.4 kalman filters
15.5 dynamic bayesian networks
15.6 keeping track of many objects
15.7 summary, bibliographical and historical notes, exercises
16 making simple decisions
16.1 combining beliefs and desires under uncertainty
16.2 the basis of utility theory
16.3 utility functions
16.4 multiattribute utility functions
16.5 decision networks
16.6 the value of information
16.7 decision-theoretic expert systems
16.8 summary, bibliographical and historical notes, exercises
17 making complex decisions
17.1 sequential decision problems
17.2 value iteration
17.3 policy iteration
17.4 partially observable mdps
17.5 decisions with multiple agents: game theory
17.6 mechanism design
17.7 summary, bibliographical and historical notes, exercises
V learning
18 learning from examples
18.1 forms of learning
18.2 supervised learning
18.3 leaming decision trees
18.4 evaluating and choosing the best hypothesis
18.5 the theory of learning
18.6 regression and classification with linear models
18.7 artificial neural networks
18.8 nonparametric models
18.9 support vector machines
18.10 ensemble learning
18.11 practical machine learning
18.12 summary, bibliographical and historical notes, exercises
19 knowledge in learning
19.1 a logical formulation of learning
19.2 knowledge in learning
19.3 explanation-based learning
19.4 learning using relevance information
19.5 inductive logic programming
19.6 summary, bibliographical and historical notes, exercis
20 learning probabilistic models
20.1 statistical learning
20.2 learning with complete data
20.3 learning with hidden variables: the em algorithm.
20.4 summary, bibliographical and historical notes, exercis
21 reinforcement learning
21. l introduction
21.2 passive reinforcement learning
21.3 active reinforcement learning
21.4 generalization in reinforcement learning
21.5 policy search
21.6 applications of reinforcement learning
21.7 summary, bibliographical and historical notes, exercis
VI communicating, perceiving, and acting
22 natural language processing
22.1 language models
22.2 text classification
22.3 information retrieval
22.4 information extraction
22.5 summary, bibliographical and historical notes, exercis
23 natural language for communication
23.1 phrase structure grammars
23.2 syntactic analysis (parsing)
23.3 augmented grammars and semantic interpretation
23.4 machine translation
23.5 speech recognition
23.6 summary, bibliographical and historical notes, exercis
24 perception
24.1 image formation
24.2 early image-processing operations
24.3 object recognition by appearance
24.4 reconstructing the 3d world
24.5 object recognition from structural information
24.6 using vision
24.7 summary, bibliographical and historical notes, exercises
25 robotics
25.1 introduction
25.2 robot hardware
25.3 robotic perception
25.4 planning to move
25.5 planning uncertain movements
25.6 moving
25.7 robotic software architectures
25.8 application domains
25.9 summary, bibliographical and historical notes, exercises
VII conclusions
26 philosophical foundations
26.1 weak ai: can machines act intelligently?
26.2 strong ai: can machines really think?
26.3 the ethics and risks of developing artificial intelligence
26.4 summary, bibliographical and historical notes, exercises
27 al: the present and future
27.1 agent components
27.2 agent architectures
27.3 are we going in the right direction?
27.4 what if ai does succeed?
a mathematical background
a. 1complexity analysis and o0 notation
a.2 vectors, matrices, and linear algebra
a.3 probability distributions
b notes on languages and algorithms
b.1defining languages with backus-naur form (bnf)
b.2describing algorithms with pseudocode
b.3online help
bibliography
index
・ ・ ・ ・ ・ ・ (收起)第1章 AIGC为何引发关注
1.1　《太空歌剧院》带来的冲击和影响　002
1.2　“生成”所引发的创意性工作革新　004
1.3　内容生成方式进入新阶段　005
1.4　AIGC在绘画领域率先破圈　006
1.5　典型的AIGC模型　008
海外模型　008
国内模型　010
第2章 模型即服务时代的到来
2.1　模型即服务的历史进程　017
早期人工智能在曲折中探索　017
深度学习引发关注　019
2.2　典型的深度学习网络　021
生成对抗网络　021
Transformer　024
2.3　大公司探索之路　026
DeepMind　026
OpenAI　027
2.4　基础模型普及的关键节点　028
基础模型的能力与服务　028
曾经热议的云，今后的基础模型　031
基础模型的通用性　033
2.5　人工智能的未来何在　033
人工智能逐步接近人类的思考模式　033
未来人工智能的发展特点　035
第3章 ChatGPT引发的潮流与思考
3.1　ChatGPT会成为人工智能的拐点吗　038
引发全球关注的ChatGPT　038
ChatGPT潜在的应用领域　039
3.2　ChatGPT能力大揭秘　040
3.3　ChatGPT是OpenAI对大模型的坚定实践　042
3.4　ChatGPT的局限性及其引发的思考　043
技术创新性与工程创新性　043
知识局限性　044
盈利与成本之间的平衡　044
应用落地所面临的困境　045
法律合规与应用抵制　045
网络安全风险　046
能耗挑战　047
3.5　ChatGPT引发的思考　048
如何看待人类创新与机器创新　048
ChatGPT在哪些方面值得我们学习　049
3.6?GPT-4未来已来，奇点时刻该如何面对　049
多模态　050
提示工程的价值　050
安全隐忧　050
第4章 大模型驱动的人工智能绘画“创作”
4.1　AI绘画的先驱――AARON　053
4.2　人工智能绘画的原理　054
神经网络是如何模仿人类思考的　054
如何让神经网络画一幅画　055
4.3　人工智能学习如何画一只猫　057
教会你的神经网络认识“猫咪”　057
人工智能真的画出了猫咪　058
4.4　DALL-E的初次尝试与突破　059
4.5　人工智能绘画的技术创新点　061
CLIP实现跨模态创新，打造图文匹配　061
用Diffusion加速AIGC落地普及　063
Diffusion模型为AIGC写下的注脚　064
Stable Diffusion岂止于开源　065
AIGC进一步降低模型的使用门槛　066
4.6　使AIGC绘画技术成熟的重要因素　068
提示词的重要性　068
算力资源的关键支撑　071
第5章 人类的创新能力会被AIGC替代吗
5.1　艺术创作会被AIGC取代吗　073
用户的猎奇与创作者的抵触　073
AIGC不会取代艺术创作工作　074
使用AIGC，需要具备什么能力　077
AIGC是直接消费品还是工具　078
5.2　创作者如何通过AIGC获得更大的收益　080
如何将AIGC应用于创作　080
创意工作者的收益探索　084
未来人工智能创作艺术的5个层次　085
5.3　AIGC――你的“达・芬奇”　089
内容输出的“平民化”　089
大众与艺术家“直连”　090
实时互动和精准化构建的“即时满足”　091
社区与共创的“想象力”　092
基于生成全新内容的平台　093
5.4　抓住AIGC的机遇　094
AIGC时代，做“短信”还是“微信”　094
AIGC的发展仍无法脱离技术周期　097
第6章 开源成就行业发展的未来
6.1　开源让我们站在巨人的肩膀上　099
6.2　开源成为引爆AIGC的导火索　099
6.3　大模型的开源之路　101
第7章 AIGC与商业化
7.1　AIGC商业化的3个阶段　106
感知冲击――尝鲜阶段　107
认知领悟――协助阶段　107
新生态链――原创阶段　108
7.2　AI领域的企业发展　108
平台型企业　109
应用型企业　111
现有产品的智能化　112
7.3　当下典型的AIGC变现手段　114
按照计算量收费　114
按照输出图像数量收费　114
软件按月付费　115
模型训练费　116
7.4　AIGC商业模式的困境　116
AIGC Inside的商业化并不容易　116
难以建立技术壁垒　117
探索自主的大模型及应用　118
第8章 AIGC的典型应用
8.1　文字创作　121
主要特点　121
典型应用　122
8.2　音频生成　126
主要特点　126
典型应用　127
8.3　视频生成　131
主要特点　131
典型应用　131
8.4　3D模型生成　135
主要特点　135
典型应用　135
8.5　编写代码　137
主要特点　137
典型应用　137
8.6　游戏创作开发　139
主要特点　139
典型应用　140
8.7　绘画产品　143
典型绘画产品的AIGC应用　144
AIGC绘画与NFT结合　148
8.8　建筑设计　149
将AIGC融入建筑设计　149
用AIGC实现装修设计　151
8.9　其他应用　152
DIY设计　152
儿童创意实现　155
内容营销　156
诊疗与心灵慰藉　156
第9章 AIGC的不足与挑战
9.1　技术与产业方面的不足与挑战　159
细节仍需打磨　159
成本问题　161
输出结果不一致　162
大模型到大应用的挑战　162
通用性较差　163
9.2　在确权方面面临的挑战　163
AIGC作品的著作权归属　163
著作权争议的潜在解决方案　165
法律监管出现争议　166
企业态度不统一　166
伦理与安全风险　167
第10章 业界和学界的专家洞察
10.1　AIGC可扩展潜力巨大，可能掀起新一波创新创业浪潮　170
从AIGC到AIGS，“服务规模化的个性化”时代到来　170
从科技圈体验到全民使用，AI首次成功破圈　171
OpenAI已经成功探索出AI领域科技创新落地的新模式　173
中国需要自主大模型，也有可能探索出自己的创新　175
10.2　AIGC火热的背后，需要深度思考治理难题　179
破解“克林格里奇困境”，要靠更敏捷的治理思路　179
加强对弱势群体的保护，平台应该做好“守门人”　180
AIGC内容知识产权还没有定论，但业界已有基本共识　182
探索人工智能领域“数据合作”新范式　183
10.3　AIGC火热背后的业界冷思考：中国AI行业的未来发展，需要有自己的思路　185
ChatGPT的流畅对话来源于预训练大模型　185
“AI幻觉”仍是阻碍产业发展的难题　186
大规模预训练技术仍处于早期探索阶段，人工智能公司还需耐心打磨　188
在AIGC技术浪潮中，一些行业将迎来全新挑战　189
中国AI行业的未来发展，需要有自己的思考和思路　191
・ ・ ・ ・ ・ ・ (收起)第 1章　从数学建模到人工智能 1
1.1　数学建模　1
1.1.1　数学建模与人工智能　1
1.1.2　数学建模中的常见问题　4
1.2　人工智能下的数学　12
1.2.1　统计量　12
1.2.2　矩阵概念及运算　13
1.2.3　概率论与数理统计　16
1.2.4　高等数学――导数、微分、不定积分、定积分　19
第　2章 Python快速入门　24
2.1　安装Python　24
2.1.1　Python安装步骤　24
2.1.2　IDE的选择　27
2.2　Python基本操作　28
2.2.1　第 一个小程序　28
2.2.2　注释与格式化输出　28
2.2.3　列表、元组、字典　34
2.2.4　条件语句与循环语句　37
2.2.5　break、continue、pass　40
2.3　Python高级操作　41
2.3.1　lambda　41
2.3.2　map　42
2.3.3　filter　43
第3章　Python科学计算库NumPy　45
3.1　NumPy简介与安装　45
3.1.1　NumPy简介　45
3.1.2　NumPy安装　45
3.2　基本操作　46
3.2.1　初识NumPy　46
3.2.2　NumPy数组类型　47
3.2.3　NumPy创建数组　49
3.2.4　索引与切片　56
3.2.5　矩阵合并与分割　60
3.2.6　矩阵运算与线性代数　62
3.2.7　NumPy的广播机制　69
3.2.8　NumPy统计函数　71
3.2.9　NumPy排序、搜索　75
3.2.10　NumPy数据的保存　79
第4章　常用科学计算模块快速入门　80
4.1　Pandas科学计算库　80
4.1.1　初识Pandas　80
4.1.2　Pandas基本操作　82
4.2　Matplotlib可视化图库　94
4.2.1　初识Matplotlib　94
4.2.2　Matplotlib基本操作　96
4.2.3　Matplotlib绘图案例　98
4.3　SciPy科学计算库　100
4.3.1　初识SciPy　100
4.3.2　SciPy基本操作　101
4.3.3　SciPy图像处理案例　103
第5章　Python网络爬虫　106
5.1　爬虫基础　106
5.1.1　初识爬虫　106
5.1.2　网络爬虫的算法　107
5.2　爬虫入门实战　107
5.2.1　调用API　107
5.2.2　爬虫实战　112
5.3　爬虫进阶―高效率爬虫　113
5.3.1　多进程　113
5.3.2　多线程　114
5.3.3　协程　115
5.3.4　小结　116
第6章　Python数据存储　117
6.1　关系型数据库MySQL　117
6.1.1　初识MySQL　117
6.1.2　Python操作MySQL　118
6.2　NoSQL之MongoDB　120
6.2.1　初识NoSQL　120
6.2.2　Python操作MongoDB　121
6.3　本章小结　123
6.3.1　数据库基本理论　123
6.3.2　数据库结合　124
6.3.3　结束语　125
第7章　Python数据分析　126
7.1　数据获取　126
7.1.1　从键盘获取数据　126
7.1.2　文件的读取与写入　127
7.1.3　Pandas读写操作　129
7.2　数据分析案例　130
7.2.1　普查数据统计分析案例　130
7.2.2　小结　139
第8章　自然语言处理　140
8.1　Jieba分词基础　140
8.1.1　Jieba中文分词　140
8.1.2　Jieba分词的3种模式　141
8.1.3　标注词性与添加定义词　142
8.2　关键词提取　144
8.2.1　TF-IDF关键词提取　145
8.2.2　TextRank关键词提取　147
8.3　word2vec介绍　150
8.3.1　word2vec基础原理简介　150
8.3.2　word2vec训练模型　153
8.3.3　基于gensim的word2vec实战　154
第9章　从回归分析到算法基础　160
9.1　回归分析简介　160
9.1.1　“回归”一词的来源　160
9.1.2　回归与相关　161
9.1.3　回归模型的划分与应用　161
9.2　线性回归分析实战　162
9.2.1　线性回归的建立与求解　162
9.2.2　Python求解回归模型案例　164
9.2.3　检验、预测与控制　166
第　10章 从K-Means聚类看算法调参　171
10.1　K-Means基本概述　171
10.1.1　K-Means简介　171
10.1.2　目标函数　171
10.1.3　算法流程　172
10.1.4　算法优缺点分析　174
10.2　K-Means实战　174
第　11章 从决策树看算法升级　180
11.1　决策树基本简介　180
11.2　经典算法介绍　181
11.2.1　信息熵　181
11.2.2　信息增益　182
11.2.3　信息增益率184
11.2.4　基尼系数　185
11.2.5　小结　185
11.3　决策树实战　186
11.3.1　决策树回归　186
11.3.2　决策树的分类　188
第　12章 从朴素贝叶斯看算法多变　193
12.1　朴素贝叶斯简介　193
12.1.1　认识朴素贝叶斯　193
12.1.2　朴素贝叶斯分类的工作过程　194
12.1.3　朴素贝叶斯算法的优缺点　195
12.2　3种朴素贝叶斯实战　195
第　13章 从推荐系统看算法场景　200
13.1　推荐系统简介　200
13.1.1　推荐系统的发展　200
13.1.2　协同过滤　201
13.2　基于文本的推荐　208
13.2.1　标签与知识图谱推荐案例　209
13.2.2　小结　217
第　14章 从TensorFlow开启深度学习之旅　218
14.1　初识TensorFlow　218
14.1.1　什么是TensorFlow　218
14.1.2　安装TensorFlow　219
14.1.3　TensorFlow基本概念与原理　219
14.2　TensorFlow数据结构　221
14.2.1　阶　221
14.2.2　形状　221
14.2.3　数据类型　221
14.3　生成数据十二法　222
14.3.1　生成Tensor　222
14.3.2　生成序列　224
14.3.3　生成随机数　225
14.4　TensorFlow实战　225
参考文献　230
・ ・ ・ ・ ・ ・ (收起)目 录

第1章 人工智能初印象 1
1.1 什么是人工智能？ 1
1.1.1 定义AI 2
1.1.2 理解数据是智能算法的核心 3
1.1.3 把算法看作“菜谱” 4
1.2 人工智能简史 6
1.3 问题类型与问题解决范式 7
1.4 人工智能概念的直观印象 9
1.5 人工智能算法的用途 13
1.5.1 农业：植物种植优化 13
1.5.2 银行业：欺诈检测 14
1.5.3 网络安全：攻击检测与处理 14
1.5.4 医疗：智能诊断 14
1.5.5 物流：路径规划与优化 15
1.5.6 通信：网络优化 16
1.5.7 游戏：主体创造 16
1.5.8 艺术：创造杰出作品 17
1.6 本章小结 17
第2章 搜索算法基础 21
2.1 什么是规划与搜索？ 21
2.2 计算成本：需要智能算法的原因 23
2.3 适合用搜索算法的问题 24
2.4 表示状态：创建一个表示问题空间与解的框架 26
2.4.1 图：表示搜索问题与解 28
2.4.2 用具体的数据结构表示图 28
2.4.3 树：表示搜索结果的具体结构 29
2.5 无知搜索：盲目地找寻解 31
2.6 广度优先搜索：先看广度，再看深度 33
2.7 深度优先搜索：先看深度，再看广度 39
2.8 盲目搜索算法的用例 45
2.9 可选：关于图的类别 46
2.10 可选：其他表示图的方法 47
2.10.1 关联矩阵 47
2.10.2 邻接表 48
2.11 本章小结 48
第3章 智能搜索 51
3.1 定义启发式方法：设计有根据的猜测 51
3.2 知情搜索：在指导下寻求解决方案 54
3.2.1 A*搜索 54
3.2.2 知情搜索算法的用例 61
3.3 对抗性搜索：在不断变化的环境中寻找解决方案 62
3.3.1 一个简单的对抗性问题 62
3.3.2 最小-最大搜索：模拟行动并选择最好的未来 63
3.3.3 启发式 64
3.3.4 阿尔法-贝塔剪枝：仅探索合理的路径 72
3.3.5 对抗搜索算法的典型案例 75
3.4 本章小结 75
第4章 进化算法 77
4.1 什么是进化？ 77
4.2 适合用进化算法的问题 80
4.3 遗传算法的生命周期 84
4.4 对解空间进行编码 86
4.5 创建解决方案种群 89
4.6 衡量种群中个体的适应度 91
4.7 根据适应度得分筛选亲本 93
4.8 由亲本繁殖个体 96
4.8.1 单点交叉：从每个亲本继承一部分 97
4.8.2 两点交叉：从每个亲本继承多个部分 98
4.8.3 均匀交叉：从每个亲本继承多个部分 98
4.8.4 二进制编码的位串突变 100
4.8.5 二进制编码的翻转位突变 101
4.9 繁衍下一代 101
4.9.1 探索与挖掘 102
4.9.2 停止条件 102
4.10 遗传算法的参数配置 104
4.11 进化算法的用例 105
4.12 本章小结 105
第5章 进化算法(高级篇) 107
5.1 进化算法的生命周期 107
5.2 其他筛选策略 109
5.2.1 排序筛选法：均分赛场 109
5.2.2 联赛筛选法：分组对抗 110
5.2.3 精英筛选法：只选最好的 111
5.3 实值编码：处理真实数值 111
5.3.1 实值编码的核心概念 112
5.3.2 算术交叉：数学化繁殖 113
5.3.3 边界突变 113
5.3.4 算术突变 114
5.4 顺序编码：处理序列 114
5.4.1 适应度函数的重要性 116
5.4.2 顺序编码的核心概念 116
5.4.3 顺序突变：适用于顺序编码 116
5.5 树编码：处理层次结构 117
5.5.1 树编码的核心概念 118
5.5.2 树交叉：继承树的分支 119
5.5.3 节点突变：更改节点的值 120
5.6 常见进化算法 120
5.6.1 遗传编程 120
5.6.2 进化编程 121
5.7 进化算法术语表 121
5.8 进化算法的其他用例 121
5.9 本章小结 122
第6章 群体智能：蚁群优化 125
6.1 什么是群体智能？ 125
6.2 适合用蚁群优化算法的问题 127
6.3 状态表达：如何表达蚂蚁和路径？ 130
6.4 蚁群优化算法的生命周期 134
6.4.1 初始化信息素印迹 135
6.4.2 建立蚂蚁种群 136
6.4.3 为蚂蚁选择下一个访问项目 138
6.4.4 更新信息素印迹 145
6.4.5 更新最佳解决方案 149
6.4.6 确定终止条件 150
6.5 蚁群优化算法的用例 152
6.6 本章小结 153
第7章 群体智能：粒子群优化 155
7.1 什么是粒子群优化？ 155
7.2 优化问题：略偏技术性的观点 157
7.3 适合用粒子群优化算法的问题 160
7.4 状态表达：粒子是什么样的？ 162
7.5 粒子群优化的生命周期 163
7.5.1 初始化粒子群 164
7.5.2 计算粒子的适应度 166
7.5.3 更新粒子的位置 169
7.5.4 确定终止条件 180
7.6 粒子群优化算法的用例 181
7.7 本章小结 183
第8章 机器学习 185
8.1 什么是机器学习？ 185
8.2 适合用机器学习的问题 187
8.2.1 监督学习 188
8.2.2 非监督学习 188
8.2.3 强化学习 188
8.3 机器学习的工作流程 188
8.3.1 收集和理解数据：掌握数据背景 189
8.3.2 准备数据：清洗和整理 191
8.3.3 训练模型：用线性回归预测 196
8.3.4 测试模型：验证模型精度 205
8.3.5 提高准确性 208
8.4 分类问题：决策树 210
8.4.1 分类问题：非此即彼 210
8.4.2 决策树的基础知识 211
8.4.3 训练决策树 213
8.4.4 用决策树对实例进行分类 223
8.5 其他常见的机器学习算法 226
8.6 机器学习算法的用例 227
8.7 本章小结 228
第9章 人工神经网络 231
9.1 什么是人工神经网络？ 231
9.2 感知器：表征神经元 234
9.3 定义人工神经网络 237
9.4 前向传播：使用训练好的人工神经网络 243
9.5 反向传播：训练人工神经网络 250
9.6 激活函数一览 259
9.7 设计人工神经网络 260
9.8 人工神经网络的类型和用例 263
9.8.1 卷积神经网络 263
9.8.2 递归神经网络 264
9.8.3 生成对抗网络 264
9.9 本章小结 266
第10章 基于Q-learning的强化学习 269
10.1 什么是强化学习？ 269
10.2 适合用强化学习的问题 272
10.3 强化学习的生命周期 273
10.3.1 模拟与数据：环境重现 274
10.3.2 使用Q-learning模拟训练 278
10.3.3 模拟并测试Q表 287
10.3.4 衡量训练的性能 287
10.3.5 无模型和基于模型的学习 288
10.4 强化学习的深度学习方法 289
10.5 强化学习的用例 289
10.5.1 机器人技术 290
10.5.2 推荐引擎 290
10.5.3 金融贸易 290
10.5.4 电子游戏 291
10.6 本章小结 291
・ ・ ・ ・ ・ ・ (收起)第一部分引言
第　1章人工智能概述　2
1.0　引言　2
1.0.1　人工智能的定义　3
1.0.2　思维是什么？智能是什么？　3
1.1　图灵测试　5
1.1.1　图灵测试的定义　6
1.1.2　图灵测试的争议和批评　8
1.2　强人工智能与弱人工智能　9
1.3　启发法　11
1.3.1　长方体的对角线：解决一个相对简单但相关的
问题　11
1.3.2　水壶问题：向后倒推　12
1.4　识别适用人工智能来求解的问题　13
1.5　应用和方法　15
1.5.1　搜索算法和拼图　16
1.5.2　二人博弈　18
1.5.3　自动推理　18
1.5.4　产生式规则和专家系统　19
1.5.5　细胞自动机　20
1.5.6　神经计算　21
1.5.7　遗传算法　23
1.5.8　知识表示　23
1.5.9　不确定性推理　24
1.6　人工智能的早期历史　25
1.7　人工智能的近期历史到现在　29
1.7.1　博弈　29
1.7.2　专家系统　30
1.7.3　神经计算　31
1.7.4　进化计算　31
1.7.5　自然语言处理　32
1.7.6　生物信息学　34
1.8　新千年人工智能的发展　34
1.9　本章小结　36
第二部分　基础知识
第　2章盲目搜索　46
2.0　简介：智能系统中的搜索　46
2.1　状态空间图　47
2.2　生成与测试范式　49
2.2.1　回溯　50
2.2.2　贪婪算法　54
2.2.3　旅行销售员问题　56
2.3　盲目搜索算法　58
2.3.1　深度优先搜索　58
2.3.2　广度优先搜索　60
2.4　盲目搜索算法的实现和比较　63
2.4.1　实现深度优先搜索　63
2.4.2　实现广度优先搜索　65
2.4.3　问题求解性能的测量指标　65
2.4.4　DFS和BFS的比较　66
2.5　本章小结　68
第3章　知情搜索　74
3.0　引言　74
3.1　启发法　76
3.2　知情搜索（第一部分）――找到任何解　81
3.2.1　爬山法　81
3.2.2　最陡爬坡法　82
3.3　最佳优先搜索　84
3.4　集束搜索　87
3.5　搜索算法的其他指标　89
3.6　知情搜索（第二部分）――找到最佳解　90
3.6.1　分支定界法　90
3.6.2　使用低估值的分支定界法　95
3.6.3　采用动态规划的分支定界法　98
3.6.4　A*搜索　99
3.7　知情搜索（第三部分）―高级搜索算法　100
3.7.1　约束满足搜索　100
3.7.2　与或树　101
3.7.3　双向搜索　102
3.8　本章小结　104
第4章　博弈中的搜索　109
4.0　引言　109
4.1　博弈树和极小化极大评估　110
4.1.1　启发式评估　112
4.1.2　博弈树的极小化极大评估　112
4.2　具有α-剪枝的极小化极大算法　115
4.3　极小化极大算法的变体和改进　120
4.3.1　负极大值算法　120
4.3.2　渐进深化法　122
4.3.3　启发式续篇和地平线效应　122
4.4　概率游戏和预期极小化极大值算法　123
4.5　博弈理论　125
迭代的囚徒困境　126
4.6　本章小结　127
第5章　人工智能中的逻辑　133
5.0　引言　133
5.1　逻辑和表示　134
5.2　命题逻辑　135
5.2.1　命题逻辑―基础　136
5.2.2　命题逻辑中的论证　140
5.2.3　证明命题逻辑论证有效的第二种方法　141
5.3　谓词逻辑――简要介绍　143
5.3.1　谓词逻辑中的合一　144
5.3.2　谓词逻辑中的反演　146
5.3.3　将谓词表达式转换为子句形式　148
5.4　其他一些逻辑　151
5.4.1　二阶逻辑　151
5.4.2　非单调逻辑　152
5.4.3　模糊逻辑　152
5.4.4　模态逻辑　153
5.5　本章小结　153
第6章　知识表示　160
6.0　引言　160
6.1　图形草图和人类视窗　163
6.2　图和哥尼斯堡桥问题　166
6.3　搜索树　167
6.4　表示方法的选择　169
6.5　产生式系统　172
6.6　面向对象　172
6.7　框架法　173
6.8　脚本和概念依赖系统　176
6.9　语义网络　179
6.10　关联　181
6.11　新近的方法　182
6.11.1　概念地图　182
6.11.2　概念图　184
6.11.3　Baecker的工作　184
6.12　智能体：智能或其他　185
6.12.1　智能体的一些历史　188
6.12.2　当代智能体　189
6.12.3　语义网　191
6.12.4　IBM眼中的未来世界　191
6.12.5　作者的观点　192
6.13　本章小结　192
第7章　产生式系统　199
7.0　引言　199
7.1　背景　199
7.2　基本示例　202
7.3　CARBUYER系统　204
7.4　产生式系统和推导方法　208
7.4.1　冲突消解　211
7.4.2　正向链接　213
7.4.3　反向链接　214
7.5　产生式系统和细胞自动机　219
7.6　随机过程与马尔可夫链　221
7.7　本章小结　222
第三部分　基于知识的系统
第8章　人工智能中的不确定性　228
8.0　引言　228
8.1　模糊集　229
8.2　模糊逻辑　231
8.3　模糊推理　232
8.4　概率理论和不确定性　235
8.5　本章小结　239
第9章　专家系统　242
9.0　引言　242
9.1　背景　242
9.2　专家系统的特点　249
9.3　知识工程　250
9.4　知识获取　252
9.5　经典的专家系统　254
9.5.1　DENDRAL　254
9.5.2　MYCIN　255
9.5.3　EMYCIN　258
9.5.4　PROSPECTOR　259
9.5.5　模糊知识和贝叶斯规则　261
9.6　提高效率的方法　262
9.6.1　守护规则　262
9.6.2　Rete算法　263
9.7　基于案例的推理　264
9.8　更多最新的专家系统　269
9.8.1　改善就业匹配系统　269
9.8.2　振动故障诊断的专家系统　270
9.8.3　自动牙科识别　270
9.8.4　更多采用案例推理的专家系统　271
9.9　本章小结　271
第　10章机器学习第一部分　277
10.0　引言　277
10.1　机器学习：简要概述　277
10.2　机器学习系统中反馈的作用　279
10.3　归纳学习　280
10.4　利用决策树进行学习　282
10.5　适用于决策树的问题　283
10.6　熵　284
10.7　使用ID3构建决策树　285
10.8　其余问题　287
10.9　本章小结　288
第　11章机器学习第二部分：神经网络　291
11.0　引言　291
11.1　人工神经网络的研究　292
11.2　麦卡洛克-皮茨网络　294
11.3　感知器学习规则　295
11.4　增量规则　303
11.5　反向传播　308
11.6　实现关注点　313
11.6.1　模式分析　316
11.6.2　训练方法　317
11.7　离散型霍普菲尔德网络　318
11.8　应用领域　323
11.9　本章小结　330
第　12章受到自然启发的搜索　337
12.0　引言　337
12.1　模拟退火　338
12.2　遗传算法　341
12.3　遗传规划　349
12.4　禁忌搜索　353
12.5　蚂蚁聚居地优化　356
12.6　本章小结　359
第四部分　高级专题
第　13章自然语言处理　368
13.0　引言　368
13.1　概述：语言的问题和可能性　368
13.2　自然语言处理的历史　371
13.2.1　基础期（20世纪40年代和50年代）　371
13.2.2　符号与随机方法（1957―1970）　372
13.2.3　４种范式（1970―1983）　372
13.2.4　经验主义和有限状态模型（1983―1993）　373
13.2.5　大融合（1994―1999）　373
13.2.6　机器学习的兴起（2000―2008）　374
13.3　句法和形式语法　374
13.3.1　语法类型　374
13.3.2　句法解析：CYK算法　379
13.4　语义分析和扩展语法　380
13.4.1　转换语法　381
13.4.2　系统语法　381
13.4.3　格语法　382
13.4.4　语义语法　383
13.4.5　Schank系统　383
13.5　NLP中的统计方法　387
13.5.1　统计解析　387
13.5.2　机器翻译（回顾）和IBM的Candide系统　388
13.5.3　词义消歧　389
13.6　统计NLP的概率模型　390
13.6.1　隐马尔可夫模型　390
13.6.2　维特比算法　391
13.7　统计NLP语言数据集　392
13.7.1　宾夕法尼亚州树库项目　392
13.7.2　WordNet　394
13.7.3　NLP中的隐喻模型　394
13.8　应用：信息提取和问答系统　396
13.8.1　问答系统　396
13.8.2　信息提取　401
13.9　现在和未来的研究（基于CHARNIAK的工作）　401
13.10　语音理解　402
13.11　语音理解技术的应用　405
13.12　本章小结　410
第　14章自动规划　417
14.0　引言　417
14.1　规划问题　418
14.1.1　规划术语　418
14.1.2　规划应用示例　419
14.2　一段简短的历史和一个著名的问题　424
14.3　规划方法　426
14.3.1　规划即搜索　426
14.3.2　部分有序规划　430
14.3.3　分级规划　432
14.3.4　基于案例的规划　433
14.3.5　规划方法集锦　434
14.4　早期规划系统　435
14.4.1　STRIPS　435
14.4.2　NOAH　436
14.4.3　NONLIN　436
14.5　更多现代规划系统　437
14.5.1　O-PLAN　438
14.5.2　Graphplan　439
14.5.3　规划系统集锦　441
14.5.4　学习系统的规划方法　441
14.5.5　SCIBox自动规划器　442
14.6　本章小结　444
第五部分　现在和未来
第　15章机器人技术　452
15.0　引言　452
15.1　历史：服务人类、仿效人类、增强人类和替代人类　455
15.1.1　早期机械机器人　455
15.1.2　电影与文学中的机器人　458
15.1.3　20世纪早期的机器人　458
15.2　技术问题　464
15.2.1　机器人的组件　464
15.2.2　运动　467
15.2.3　点机器人的路径规划　468
15.2.4　移动机器人运动学　469
15.3　应用：21世纪的机器人　471
15.4　本章小结　479
第　16章高级计算机博弈　482
16.0　引言　482
16.1　跳棋：从塞缪尔到舍弗尔　483
16.1.1　在跳棋博弈中用于机器学习的启发式方法　486
16.1.2　填鸭式学习与概括　488
16.1.3　签名表评估和棋谱学习　489
16.1.4　含有奇诺克程序的世界跳棋锦标赛　490
16.1.5　彻底解决跳棋游戏　491
16.2　国际象棋：人工智能的“果蝇”　494
16.2.1　计算机国际象棋的历史背景　495
16.2.2　编程方法　496
16.2.3　超越地平线效应　505
16.2.4　DeepThought和DeepBlue与特级大师的比赛（1988―1995年）　505
16.3　计算机国际象棋对人工智能的贡献　507
16.3.1　在机器中的搜索　507
16.3.2　在搜索方面，人与机器的对比　508
16.3.3　启发式、知识和问题求解　509
16.3.4　蛮力：知识vs.搜索；表现vs.能力　510
16.3.5　残局数据库和并行计算　511
16.3.6　本书第一作者的贡献　514
16.4　其他博弈　514
16.4.1　奥赛罗　515
16.4.2　西洋双陆棋　516
16.4.3　桥牌　518
16.4.4　扑克　519
16.5　围棋：人工智能的“新果蝇”？　520
16.6　本章小结　523
第　17章大事记　532
17.0　引言　532
17.1　提纲挈领――概述　532
17.2　普罗米修斯归来　534
17.3　提纲挈领――介绍人工智能的成果　535
17.4　IBM的沃森-危险边缘挑战赛　539
17.5　21世纪的人工智能　543
17.6　本章小结　545
附录A　CLIPS示例：专家系统外壳　548
附录B　用于隐马尔可夫链的维特比算法的实现（由HarunIftikhar提供）　552
附录C　对计算机国际象棋的贡献：令人惊叹的WalterShawnBrowne　555
附录D　应用程序和数据　559
附录E　部分练习的答案　560
・ ・ ・ ・ ・ ・ (收起)第0章 绪论
第1章 数学基础
1.1 导数
1.2 概率论基础
1.3 矩阵基础
习题
第2章 搜索
引言
2.1 搜索问题的定义
2.2 搜索算法基础
2.3 盲目搜索
2.4 启发式搜索
2.5 局部搜索
2.6 对抗搜索
本章总结
历史回顾
习题
第3章 机器学习
引言
3.1 监督学习的概念
3.2 数据集与损失函数
3.3 泛化
3.4 过拟合与欠拟合
3.5 创建数据集
3.6 无监督学习与半监督学习
本章总结
历史回顾
习题
参考文献
第4章 线性回归
引言
4.1 线性回归
4.2 优化方法
4.3 二分类问题
4.4 多分类问题
4.5 岭回归
4.6 套索回归
4.7 支持向量机算法
本章总结
习题
第5章 决策树模型
引言
5.1 决策树的例子
5.2 决策树的定义
5.3 决策树的训练算法
本章总结
历史回顾
习题
参考文献
第6章 集成学习
引言
6.1 集成学习
6.2 随机森林
6.3 梯度提升
本章总结
历史回顾
习题
参考文献
第7章 神经网络初步
引言
7.1 深度线性网络
7.2 非线性神经网络
7.3 反向传播计算导数
7.4 优化器
7.5 权值初始化
7.6 权值衰减
7.7 权值共享与卷积
7.8 循环神经网络
本章总结
历史回顾
习题
第8章 计算机视觉
引言
8.1 什么是计算机视觉
8.2 图像的形成
8.3 线性滤波器
8.4 边缘检测
8.5 立体视觉
8.6 卷积神经网络
8.7 物体检测
8.8 语义分割
本章总结
历史回顾
习题
参考文献
第9章 自然语言处理
引言
9.1 语言模型
9.2 向量语义
9.3 基于神经网络的语言模型处理
9.4 基于神经网络的机器翻译
9.5 语言模型预训练
本章总结
历史回顾
习题
第10章 马尔可夫决策过程与强化学习
引言
10.1 马尔可夫链
10.2 马尔可夫决策过程
10.3 马尔可夫决策过程的求解算法及分析
10.4 强化学习
本章总结
历史回顾
参考文献
习题
附录A 数学基础
A.1 导数
A.2 概率
A.3 矩阵
・ ・ ・ ・ ・ ・ (收起)第1 章 人工智能与数学基础..........１
1.1 什么是人工智能............................ 2
1.2 人工智能的发展 ............................ 2
1.3 人工智能的应用 ............................ 4
1.4 学习人工智能需要哪些知识 ............. 5
1.5 为什么要学习数学 ......................... 7
1.6 本书包括的数学知识 ...................... 8
第 1 篇
基础篇................................................................. 9
第 2 章 高等数学基础 ................. １0
2.1 函数.......................................... 11
2.2 极限..........................................13
2.3 无穷小与无穷大...........................17
2.4 连续性与导数..............................19
2.5 偏导数...................................... 24
2.6 方向导数................................... 27
2.7 梯度......................................... 29
2.8 综合实例―梯度下降法求函数的最小值.......................................31
2.9 高手点拨................................... 35
2.10 习题....................................... 38
第 3 章 微积分..............................39
3.1 微积分的基本思想 ....................... 40
3.2 微积分的解释..............................41
3.3 定积分...................................... 42
3.4 定积分的性质............................. 44
3.5 牛顿―莱布尼茨公式.................... 45
3.6 综合实例―Python 中常用的定积分求解方法................................... 49
3.7 高手点拨....................................51
3.8 习题 ........................................ 52
第 4 章 泰勒公式与拉格朗日乘子法..............................53
4.1 泰勒公式出发点.......................... 54
4.2 一点一世界................................ 54
4.3 阶数和阶乘的作用....................... 59
4.4 麦克劳林展开式的应用..................61
4.5 拉格朗日乘子法.......................... 63
4.6 求解拉格朗日乘子法.................... 64
4.7 综合实例―编程模拟实现 sinx 的n 阶泰勒多项式并验证结果.................. 67
4.8 高手点拨 ................................... 68
4.9 习题 ......................................... 68
第2 篇
核心篇............................................................... 69
第 5 章 将研究对象形式化―线性代数基础 ..........................70
5.1 向量..........................................71
5.2 矩阵......................................... 73
5.3 矩阵和向量的创建....................... 77
5.4 特殊的矩阵................................ 85
5.5 矩阵基本操作..............................91
5.6 转置矩阵和逆矩阵....................... 96
5.7 行列式..................................... 101
5.8 矩阵的秩..................................104
5.9 内积与正交...............................108
5.10 综合实例―线性代数在实际问题中的应用 ....................................... 114
5.11 高手点拨 ................................ 121
5.12 习题......................................126
第 6 章 从数据中提取重要信息―特征值与矩阵分解..........127
6.1 特征值与特征向量 .....................128
6.2 特征空间..................................133
6.3 特征值分解...............................133
6.4 SVD 解决的问题.......................135
6.5 奇异值分解（SVD）..................136
6.6 综合实例 1―利用 SVD 对图像进行压缩 .......................................140
6.7 综合实例 2―利用 SVD 推荐商品 .......................................143
6.8 高手点拨..................................150
6.9 习题 .......................................154
第 7 章 描述统计规律 1―概率论基础................................155
7.1 随机事件及其概率 ......................156
7.2 条件概率.................................. 161
7.3 独立性.....................................162
7.4 随机变量..................................165
7.5 二维随机变量............................173
7.6 边缘分布..................................177
7.7 综合实例―概率的应用.............180
7.8 高手点拨.................................. 181
7.9 习题........................................184
第 8 章 描述统计规律 2―随机变量与概率估计........................185
8.1 随机变量的数字特征 ..................186
8.2 大数定律和中心极限定理.............193
8.3 数理统计基本概念......................199
8.4 最大似然估计........................... 203
8.5 最大后验估计........................... 206
8.6 综合实例 1―贝叶斯用户满意度预测 ...................................... 209
8.7 综合实例 2―最大似然法求解模型参数 .......................................217
8.8 高手点拨 ................................ 222
8.9 习题 ....................................... 224
第 3 篇
提高篇............................................................. 225
第 9 章 随机变量的几种分布...... 226
9.1 正态分布 ................................ 227
9.2 二项分布................................. 240
9.3 泊松分布................................. 250
9.4 均匀分布..................................261
9.5 卡方分布................................. 266
9.6 Beta 分布 .............................. 273
9.7 综合实例―估算棒球运动员的击中率 ...................................... 283
9.8 高手点拨 ................................ 285
9.9 习题 ...................................... 286
第 10 章 数据的空间变换―核函数变换............................. 287
10.1 相关知识简介 ......................... 288
10.2 核函数的引入 ......................... 290
10.3 核函数实例............................ 290
10.4 常用核函数.............................291
10.5 核函数的选择......................... 294
10.6 SVM 原理 ............................ 295
10.7 非线性 SVM 与核函数的引入.... 305
10.8 综合实例―利用 SVM 构建分类
问题......................................310
10.9 高手点拨................................315
10.10 习题 ................................... 322
第 11 章 熵与激活函数 .............. 323
11.1 熵和信息熵............................ 324
11.2 激活函数 ............................... 328
11.3 综合案例―分类算法中信息熵的应用...................................... 339
11.4 高手点拨 ................................341
11.5 习题 ..................................... 342
第4 篇
应用篇............................................................. 333
第 12 章 假设检验 ..................... 344
12.1 假设检验的基本概念................. 345
12.2 Z 检验 ...................................351
12.3 t 检验 ................................... 353
12.4 卡方检验............................... 358
12.5 假设检验中的两类错误 ..............361
12.6 综合实例 1―体检数据中的假设检验问题..................................... 363
12.7 综合实例 2―种族对求职是否有影响..................................... 369
12.8 高手点拨............................... 372
12.9 习题..................................... 374
13 章 相关分析...................... 375
13.1 相关分析概述.......................... 376
13.2 皮尔森相关系数....................... 378
13.3 相关系数的计算与假设检验........ 379
13.4 斯皮尔曼等级相关.................... 385
13.5 肯德尔系数............................. 392
13.6 质量相关分析.......................... 396
13.7 品质相关分析.......................... 400
13.8 偏相关与复相关....................... 403
13.9 综合实例―相关系数计算........ 405
13.10 高手点拨.............................. 407
13.11 习题..................................... 408
第 14 章 回归分析......................409
14.1 回归分析概述...........................410
14.2 回归方程推导及应用..................412
14.3 回归直线拟合优度.....................416
14.4 线性回归的模型检验..................417
14.5 利用回归直线进行估计和预测......419
14.6 多元与曲线回归问题..................421
14.7 Python 工具包....................... 426
14.8 综合实例―个人医疗保费预测任务...................................... 432
14.9 高手点拨................................ 444
14.10 习题..................................... 446
第 15 章 方差分析......................449
15.1 方差分析概述.......................... 448
15.2 方差的比较............................. 450
15.3 方差分析.................................451
15.4 综合实例―连锁餐饮用户评级分析...................................... 460
15.5 高手点拨................................ 464
15.6 习题...................................... 466
第 16 章 聚类分析......................469
16.1 聚类分析概述.......................... 468
16.2 层次聚类................................ 470
16.3 K-Means 聚类...................... 484
16.4 DBSCAN 聚类....................... 494
16.5 综合实例―聚类分析.............. 499
16.6 高手点拨.................................512
16.7 习题.......................................512
第 17 章 贝叶斯分析....................513
17.1 贝叶斯分析概述........................514
17.2 MCMC 概述.......................... 520
17.3 MCMC 采样 ......................... 525
17.4 Gibbs 采样........................... 529
17.5 综合实例―利用 PyMC3 实现随机模拟样本分布......................... 532
17.6 高手点拨............................... 539
17.7 习题..................................... 540
・ ・ ・ ・ ・ ・ (收起)出版者的话
专家指导委员会
译者序
序
第2版序
致谢
第1章 基于知识的智能系统概述
1.1 智能机器概述
1.2 人工智能发展历史
1.3 小结
复习题
参考文献
第2章 基于规则的专家系统
2.1 知识概述
2.2 规则是一种知识表达技术
2.3 专家系统研发团队中的主要参与者
2.4 基于规则的专家系统的结构
2.5 专家系统的基本特征
2.6 前向链接和后向链接推理技术
2.7 实例
2.8 冲突的解决方案
2.9 基于规则的专家系统的优缺点
2.10 小结
复习题
参考文献
第3章 基于规则的专家系统的不确定管理
3.1 不确定性简介
3.2 基本概率论
3.3 贝叶斯推理
3.4 FORECAST：贝叶斯证据累积
3.5 贝叶斯方法的偏差
3.6 确定因子理论和证据推理
3.7 FORECAST：确定因子的应用
3.8 贝叶斯推理和确定因子的比较
3.9 小结
复习题
参考文献
第4章 模糊专家系统
4.1 概述
4.2 模糊集
4.3 语言变量和模糊限制语
4.4 模糊集的操作
4.5 模糊规则
4.6 模糊推理
4.7 建立模糊专家系统
4.8 小结
复习题
参考文献
参考书目
第5章 基于框架的专家系统
5.1 框架简介
5.2 作为知识表达技术的框架
5.3 基于框架系统中的继承
5.4 方法和守护程序
5.5 框架和规则的交互
5.6 基于框架的专家系统实例：Buy Smart
5.7 小结
复习题
参考文献
参考书目
第6章 人工神经网络
6.1 人脑工作机制简介
6.2 作为简单计算元素的神经元
6.3 感知器
6.4 多层神经网络
6.5 多层神经网络的加速学习
6.6 Hopfield神经网络
6.7 双向相关记忆
6.8 自组织神经网络
6.9 小结
复习题
参考文献
第7章 进化计算
7.1 进化是智能的吗
7.2 模拟自然进化
7.3 遗传算法
7.4 遗传算法如何工作
7.5 实例：用遗传算法来维护计划
7.6 进化策略
7.7 遗传编程
7.8 小结
复习题
参考文献
参考书目
第8章 混合智能系统
8.1 概述
8.2 神经专家系统
8.3 神经模糊系统
8.4 ANFIS：自适应性神经模糊推理系统
8.5 进化神经网络
8.6 模糊进化系统
8.7 小结
复习题
参考文献
第9章 知识工程和数据挖掘
9.1 知识工程简介
9.2 专家系统可以解决的问题
9.3 模糊专家系统可以解决的问题
9.4 神经网络可以解决的问题
9.5 遗传算法可以解决的问题
9.6 混合智能系统可以解决的问题
9.7 数据挖掘和知识发现
9.8 小结
复习题
参考文献
术语表
附录 人工智能工具和厂商
・ ・ ・ ・ ・ ・ (收起)推荐序 情感机器离我们有多远
李德毅
中国人工智能学会理事长
中国工程院院士
引言 人类思维与人工智能的未来
第一部分 情感，另一种人类思维方式
01 坠入爱河
我们的每一种主要的“情感状态”都是因为激活了一些资源,同时关闭了另外一些资源――大脑的运行方式由此改变了。如果每次这种改变都会激活更多其他资源,那么最终将导致资源的大规模“级联”。
“爱”的手提箱
精神奥秘之海
情绪与情感
本能机，让婴儿情感更好捉摸
云认知型思维
成人精神活动的6大层级
情感“瀑布”
思维维度的多样性
02 依恋与目标
人类的一些目标是天生的本能, 是由我们的基因决定的; 另一些目标则是通过“尝试和错误”学习，来实现已有目标的次级目标；而高层次目标, 则是由一种特殊的机器体系形成的。这种特殊的机器体系是指我们对身为依恋对象的父母、朋友或亲人的价值观的继承, 这些价值观积极地响应了我们的需要, 在我们体内产生了“自我意识”情感。
沙子游戏 ：从叉子到勺子
依恋与目标
印刻者
依恋性学习模式
学习、快乐和信用赋能
价值体系的塑造
幼儿和动物的依恋
谁是我们的印刻者
自律，构建目标一致的自我模型
公众印刻
03 从疼痛到煎熬
任何疼痛都会激活“摆脱疼痛”这一目标, 而这个目标的实现将有助于目标本身的消失。然而, 如果疼痛强烈而又持久, 就会激发其他大脑资源, 进而压制其他目标。如果这种情况级联式地爆发下去, 那么大脑
的大部分区域都会被痛苦占据 。可见，在处于某种精神状态中时, 我们也就失去了“选择的自由”。
疼痛之中
煎熬，大脑失去自由选择权
苦难机器
致命性的痛苦
心智“批评家”：纠正性警告、外显抑制和内隐束缚
弗洛伊德的思维“三明治”
控制我们的情绪和性情
情感利用
第二部分 洞悉思维本质，创建情感机器的6大维度
04 意识
“意识”是一个“手提箱”式词汇, 它被我们用来表示许多不同的精神活动。而这些精神活动并没有单一的原因或起源, 当然, 这也正是为何人们发现很难“理解意识是什么”的原因所在。心灵的每个阶段都
是一个同时存在多种可能性的剧场，而意识则将这些可能性相互比较, 通过注意力的强化和抑制作用, 选择一些可能性、抑制其他可能性。
什么是意识
打开意识的手提箱
A 脑、B 脑和C 脑
对意识的高估
如何开启意识
主观体验，心理学中的无解难题
自我模型与自我意识
笛卡儿剧场
不间断的意识流
05 精神活动层级
我们的大脑是如何产生如此多新事物和新想法的? 资源可以分为 6 种不同的层级――本能反应、后天反应、沉思、反思、自我反思、自我意识，以对想法和思维机制进行衡量。每一个层级模式都建立在下一个
层级模式的基础之上, 最上层的模式表现的是人们的最高理想和个人目标。
本能反应
后天反应
沉思
反思
自我反思
自我意识
想象
想象场景
预测机器
06 常识
我们所做的许多常识性事情和常识性推理，要比吸引更多关注、获得令人敬仰的专业技能复杂得多。你所“看到”的并不完全来自视觉, 还来自这些视觉引发的其他知识。常识性知识的主体, 即人类需要在文明
世界中相处下去会涉及的许多问题, 如我们所说的常识性问题, 目标是什么以及它们是如何实现的，我们平常是如何通过类比来推理, 以及我们如何猜测哪一项知识等，可能与我们的决策方式相关联。
什么是常识
常识性知识和推理
意图和目标
差异的“幻想”世界
在不确定性中，作出最优决策
相似推理
正面经验和负面经验的博弈
07 思维
我们几乎从未认识到常识性思考所创造的奇迹。人人都有不同的思维方式。在众多的兴趣爱好当中, 是什么选择了我们下一步将要思考的内容？每一种兴趣又会持续多久? 批评家又是如何选择所使用思维方式
的？事实上，工作被隐藏在“脑后”, 仍在继续运行。
是什么选择了我们思考的主题
批评家-选择器模型，思维跳跃之源
情感化思维
人类的19大思维方式
6 大批评家，选择最合适的思维方式
先有情感，还是先有行为
庞加莱无意识过程的4 大阶段
认知语境下的批评家选择
人类心理学的核心问题
08 智能
每个物种的个体智力都会从愚笨逐渐发展到优秀, 即使最高级的人类思维也本应从这个过程发展而来。我们可以通过多种视角来观察事物，我们拥有快速进行视角转换的方法、拥有高效学习的特殊方式、拥有获
得相关知识的有效方式并可以不断扩大思维方式的范围、拥有表征事物的多种方式。正是这种多样性造就了人类思维的多功能。
预估距离
平行类比
高效率学习的奥秘
信用赋能
创造力和天才
记忆与表征结构
表征等级
09 自我
是什么让人类变得独一无二? 任何其他动物都无法像人类这样拥有各种各样的人格。其中一些性格是与生俱来的, 而另一些性格则来自个人经验, 但在每一种情况中, 我们都具有各异
的特征。每当想尝试理解自己时, 我们都可能需要采取多种角度来看待自己。
多样的“自我”
人格特质
“自我”观念的魅力
为什么我们喜欢快乐
情感描述难题
发现感觉中独特的“质”
人类思维的组织方式
复杂的尊严
人类智能的3大时间跨度
致 谢
注 释
译者后记
・ ・ ・ ・ ・ ・ (收起)第 1 章 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 本书面向的读者 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7
1.2 深度学习的历史趋势 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.1 神经网络的众多名称和命运变迁 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.2 与日俱增的数据量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2.3 与日俱增的模型规模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13
1.2.4 与日俱增的精度、复杂度和对现实世界的冲击 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
第 1 部分 应用数学与机器学习基础
第 2 章 线性代数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.1 标量、向量、矩阵和张量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.2 矩阵和向量相乘. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21
2.3 单位矩阵和逆矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4 线性相关和生成子空间 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 范数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24
2.6 特殊类型的矩阵和向量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.7 特征分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.8 奇异值分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.9 Moore-Penrose 伪逆 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.10 迹运算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.11 行列式 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.12 实例：主成分分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30
第 3 章 概率与信息论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34
3.1 为什么要使用概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2 随机变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.3 概率分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.3.1 离散型变量和概率质量函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.3.2 连续型变量和概率密度函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4 边缘概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.5 条件概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.6 条件概率的链式法则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.7 独立性和条件独立性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.8 期望、方差和协方差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.9 常用概率分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.9.1 Bernoulli 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.2 Multinoulli 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.3 高斯分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.9.4 指数分布和 Laplace 分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.9.5 Dirac 分布和经验分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.9.6 分布的混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.10 常用函数的有用性质. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
3.11 贝叶斯规则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.12 连续型变量的技术细节 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.13 信息论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.14 结构化概率模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
第 4 章 数值计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.1 上溢和下溢 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.2 病态条件 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3 基于梯度的优化方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3.1 梯度之上：Jacobian 和 Hessian 矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4 约束优化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.5 实例：线性最小二乘 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
第 5 章 机器学习基础. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63
5.1 学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.1 任务 T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.2 性能度量 P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1.3 经验 E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1.4 示例：线性回归 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
5.2 容量、过拟合和欠拟合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2.1 没有免费午餐定理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.2.2 正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3 超参数和验证集. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .76
5.3.1 交叉验证 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.4 估计、偏差和方差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .77
5.4.1 点估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.4.2 偏差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.4.3 方差和标准差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.4.4 权衡偏差和方差以最小化均方误差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.4.5 一致性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.5 最大似然估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.5.1 条件对数似然和均方误差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84
5.5.2 最大似然的性质 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.6 贝叶斯统计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.6.1 最大后验 (MAP) 估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.7 监督学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.1 概率监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.2 支持向量机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7.3 其他简单的监督学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .90
5.8 无监督学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .91
5.8.1 主成分分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.8.2 k-均值聚类 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94
5.9 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.10 构建机器学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.11 促使深度学习发展的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.11.1 维数灾难 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.11.2 局部不变性和平滑正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.11.3 流形学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
第 2 部分 深度网络：现代实践
第 6 章 深度前馈网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.1 实例：学习 XOR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.2 基于梯度的学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.2.1 代价函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.2.2 输出单元 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.3 隐藏单元. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .119
6.3.1 整流线性单元及其扩展 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3.2 logistic sigmoid 与双曲正切函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.3.3 其他隐藏单元 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.4 架构设计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123
6.4.1 万能近似性质和深度. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123
6.4.2 其他架构上的考虑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126
6.5 反向传播和其他的微分算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126
6.5.1 计算图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.5.2 微积分中的链式法则. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .128
6.5.3 递归地使用链式法则来实现反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
6.5.4 全连接 MLP 中的反向传播计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
6.5.5 符号到符号的导数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .131
6.5.6 一般化的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .133
6.5.7 实例：用于 MLP 训练的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135
6.5.8 复杂化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5.9 深度学习界以外的微分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5.10 高阶微分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6.6 历史小记. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .139
第 7 章 深度学习中的正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.1 参数范数惩罚 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.1.1 L2 参数正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.1.2 L1 正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.2 作为约束的范数惩罚. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .146
7.3 正则化和欠约束问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .147
7.4 数据集增强 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.5 噪声鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
7.5.1 向输出目标注入噪声. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .150
7.6 半监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.7 多任务学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.8 提前终止. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .151
7.9 参数绑定和参数共享. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .156
7.9.1 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
7.10 稀疏表示. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .157
7.11 Bagging 和其他集成方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .158
7.12 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .159
7.13 对抗训练. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .165
7.14 切面距离、正切传播和流形正切分类器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
第 8 章 深度模型中的优化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .169
8.1 学习和纯优化有什么不同 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.1 经验风险最小化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.2 代理损失函数和提前终止 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.1.3 批量算法和小批量算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.2 神经网络优化中的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.1 病态 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.2 局部极小值 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.2.3 高原、鞍点和其他平坦区域 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .175
8.2.4 悬崖和梯度爆炸 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.2.5 长期依赖 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.2.6 非精确梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
8.2.7 局部和全局结构间的弱对应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
8.2.8 优化的理论限制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
8.3 基本算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .180
8.3.1 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
8.3.2 动量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
8.3.3 Nesterov 动量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183
8.4 参数初始化策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
8.5 自适应学习率算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
8.5.1 AdaGrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
8.5.2 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.5.3 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
8.5.4 选择正确的优化算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .190
8.6 二阶近似方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.6.1 牛顿法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.6.2 共轭梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.6.3 BFGS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
8.7 优化策略和元算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
8.7.1 批标准化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
8.7.2 坐标下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
8.7.3 Polyak 平均 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
8.7.4 监督预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
8.7.5 设计有助于优化的模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
8.7.6 延拓法和课程学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .199
第 9 章 卷积网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
9.1 卷积运算. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .201
9.2 动机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.3 池化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
9.4 卷积与池化作为一种无限强的先验 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
9.5 基本卷积函数的变体. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .211
9.6 结构化输出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
9.7 数据类型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .219
9.8 高效的卷积算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
9.9 随机或无监督的特征. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .220
9.10 卷积网络的神经科学基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
9.11 卷积网络与深度学习的历史 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
第 10 章 序列建模：循环和递归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
10.1 展开计算图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
10.2 循环神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .230
10.2.1 导师驱动过程和输出循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
10.2.2 计算循环神经网络的梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
10.2.3 作为有向图模型的循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
10.2.4 基于上下文的 RNN 序列建模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
10.3 双向 RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
10.4 基于编码 - 解码的序列到序列架构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
10.5 深度循环网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242
10.6 递归神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243
10.7 长期依赖的挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
10.8 回声状态网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .245
10.9 渗漏单元和其他多时间尺度的策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
10.9.1 时间维度的跳跃连接. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .247
10.9.2 渗漏单元和一系列不同时间尺度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
10.9.3 删除连接 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10 长短期记忆和其他门控 RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10.1 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.10.2 其他门控 RNN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .250
10.11 优化长期依赖. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .251
10.11.1 截断梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
10.11.2 引导信息流的正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
10.12 外显记忆 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
第 11 章 实践方法论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
11.1 性能度量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .256
11.2 默认的基准模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
11.3 决定是否收集更多数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
11.4 选择超参数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
11.4.1 手动调整超参数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .259
11.4.2 自动超参数优化算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .262
11.4.3 网格搜索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
11.4.4 随机搜索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
11.4.5 基于模型的超参数优化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
11.5 调试策略. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .264
11.6 示例：多位数字识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
第 12 章 应用. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .269
12.1 大规模深度学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.1 快速的 CPU 实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.2 GPU 实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.1.3 大规模的分布式实现. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .271
12.1.4 模型压缩 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
12.1.5 动态结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
12.1.6 深度网络的专用硬件实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
12.2 计算机视觉 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
12.2.1 预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
12.2.2 数据集增强 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
12.3 语音识别. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .278
12.4 自然语言处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .279
12.4.1 n-gram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .280
12.4.2 神经语言模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
12.4.3 高维输出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
12.4.4 结合 n-gram 和神经语言模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
12.4.5 神经机器翻译 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
12.4.6 历史展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
12.5 其他应用. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .290
12.5.1 推荐系统 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
12.5.2 知识表示、推理和回答 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
第 3 部分 深度学习研究
第 13 章 线性因子模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
13.1 概率 PCA 和因子分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
13.2 独立成分分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .298
13.3 慢特征分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
13.4 稀疏编码. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .301
13.5 PCA 的流形解释 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
第 14 章 自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.1 欠完备自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.2 正则自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .307
14.2.1 稀疏自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
14.2.2 去噪自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
14.2.3 惩罚导数作为正则. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .309
14.3 表示能力、层的大小和深度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
14.4 随机编码器和解码器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .310
14.5 去噪自编码器详解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
14.5.1 得分估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.5.2 历史展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.6 使用自编码器学习流形 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.7 收缩自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .317
14.8 预测稀疏分解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .319
14.9 自编码器的应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
第 15 章 表示学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
15.1 贪心逐层无监督预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
15.1.1 何时以及为何无监督预训练有效有效 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
15.2 迁移学习和领域自适应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
15.3 半监督解释因果关系. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .329
15.4 分布式表示 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
15.5 得益于深度的指数增益 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
15.6 提供发现潜在原因的线索 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
第 16 章 深度学习中的结构化概率模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
16.1 非结构化建模的挑战. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .339
16.2 使用图描述模型结构. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342
16.2.1 有向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
16.2.2 无向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
16.2.3 配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
16.2.4 基于能量的模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .346
16.2.5 分离和 d-分离 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .347
16.2.6 在有向模型和无向模型中转换 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
16.2.7 因子图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
16.3 从图模型中采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.4 结构化建模的优势 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.5 学习依赖关系 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .354
16.6 推断和近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
16.7 结构化概率模型的深度学习方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .355
16.7.1 实例：受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
第 17 章 蒙特卡罗方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
17.1 采样和蒙特卡罗方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.1.1 为什么需要采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.1.2 蒙特卡罗采样的基础. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .359
17.2 重要采样. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .360
17.3 马尔可夫链蒙特卡罗方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
17.4 Gibbs 采样. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .365
17.5 不同的峰值之间的混合挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
17.5.1 不同峰值之间通过回火来混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
17.5.2 深度也许会有助于混合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
第 18 章 直面配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
18.1 对数似然梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .369
18.2 随机最大似然和对比散度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
18.3 伪似然 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
18.4 得分匹配和比率匹配. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .376
18.5 去噪得分匹配 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378
18.6 噪声对比估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378
18.7 估计配分函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380
18.7.1 退火重要采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
18.7.2 桥式采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
第 19 章 近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
19.1 把推断视作优化问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .385
19.2 期望最大化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
19.3 最大后验推断和稀疏编码 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
19.4 变分推断和变分学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .389
19.4.1 离散型潜变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
19.4.2 变分法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
19.4.3 连续型潜变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
19.4.4 学习和推断之间的相互作用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
19.5 学成近似推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397
19.5.1 醒眠算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
19.5.2 学成推断的其他形式. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .398
第 20 章 深度生成模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
20.1 玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
20.2 受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
20.2.1 条件分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
20.2.2 训练受限玻尔兹曼机. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .402
20.3 深度信念网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .402
20.4 深度玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
20.4.1 有趣的性质 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
20.4.2 DBM 均匀场推断 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
20.4.3 DBM 的参数学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
20.4.4 逐层预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
20.4.5 联合训练深度玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
20.5 实值数据上的玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
20.5.1 Gaussian-Bernoulli RBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
20.5.2 条件协方差的无向模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
20.6 卷积玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
20.7 用于结构化或序列输出的玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
20.8 其他玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
20.9 通过随机操作的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
20.9.1 通过离散随机操作的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
20.10 有向生成网络. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422
20.10.1 sigmoid 信念网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
20.10.2 可微生成器网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423
20.10.3 变分自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .425
20.10.4 生成式对抗网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .427
20.10.5 生成矩匹配网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .429
20.10.6 卷积生成网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .430
20.10.7 自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
20.10.8 线性自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .430
20.10.9 神经自回归网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .431
20.10.10 NADE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
20.11 从自编码器采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
20.11.1 与任意去噪自编码器相关的马尔可夫链 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
20.11.2 夹合与条件采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .434
20.11.3 回退训练过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .435
20.12 生成随机网络. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .435
20.12.1 判别性 GSN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
20.13 其他生成方案. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .436
20.14 评估生成模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .437
20.15 结论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
参考文献. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .439
索引 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
・ ・ ・ ・ ・ ・ (收起)译者序　　xiii
前言　　xv
第1章　Python入门　　1
1.1 Python是什么　　1
1.2 Python的安装　　2
1.2.1　Python版本　　2
1.2.2　使用的外部库　　2
1.2.3　Anaconda发行版　　3
1.3 Python解释器　　4
1.3.1　算术计算　　4
1.3.2　数据类型　　5
1.3.3　变量　　5
1.3.4　列表　　6
1.3.5　字典　　7
1.3.6　布尔型　　7
1.3.7　if 语句　　8
1.3.8　for 语句　　8
1.3.9　函数　　9
1.4 Python脚本文件　　9
1.4.1　保存为文件　　9
1.4.2　类　　10
1.5 NumPy　　11
1.5.1　导入NumPy　　11
1.5.2　生成NumPy数组　　12
1.5.3　NumPy 的算术运算　　12
1.5.4　NumPy的N维数组　　13
1.5.5　广播　　14
1.5.6　访问元素　　15
1.6 Matplotlib　　16
1.6.1　绘制简单图形　　16
1.6.2　pyplot 的功能　　17
1.6.3　显示图像　　18
1.7 小结　　19
第2章　感知机　　21
2.1 感知机是什么　　21
2.2 简单逻辑电路　　23
2.2.1　与门　　23
2.2.2　与非门和或门　　23
2.3 感知机的实现　　25
2.3.1　简单的实现　　25
2.3.2　导入权重和偏置　　26
2.3.3　使用权重和偏置的实现　　26
2.4 感知机的局限性　　28
2.4.1　异或门　　28
2.4.2　线性和非线性　　30
2.5 多层感知机　　31
2.5.1　已有门电路的组合　　31
2.5.2　异或门的实现　　33
2.6 从与非门到计算机　　35
2.7 小结　　36
第3章　神经网络　　37
3.1 从感知机到神经网络　　37
3.1.1　神经网络的例子　　37
3.1.2　复习感知机　　38
3.1.3　激活函数登场　　40
3.2 激活函数　　42
3.2.1　sigmoid 函数　　42
3.2.2　阶跃函数的实现　　43
3.2.3　阶跃函数的图形　　44
3.2.4　sigmoid 函数的实现　　45
3.2.5　sigmoid 函数和阶跃函数的比较　　46
3.2.6　非线性函数　　48
3.2.7　ReLU函数　　49
3.3 多维数组的运算　　50
3.3.1　多维数组　　50
3.3.2　矩阵乘法　　51
3.3.3　神经网络的内积　　55
3.4　　3 层神经网络的实现　　56
3.4.1　符号确认　　57
3.4.2　各层间信号传递的实现　　58
3.4.3　代码实现小结　　62
3.5 输出层的设计　　63
3.5.1　恒等函数和softmax 函数　　64
3.5.2　实现softmax 函数时的注意事项　　66
3.5.3　softmax 函数的特征　　67
3.5.4　输出层的神经元数量　　68
3.6 手写数字识别　　69
3.6.1　MNIST数据集　　70
3.6.2　神经网络的推理处理　　73
3.6.3　批处理　　75
3.7 小结　　79
第4章　神经网络的学习　　81
4.1 从数据中学习　　81
4.1.1　数据驱动　　82
4.1.2　训练数据和测试数据　　84
4.2 损失函数　　85
4.2.1　均方误差　　85
4.2.2　交叉熵误差　　87
4.2.3　mini-batch 学习　　88
4.2.4　mini-batch 版交叉熵误差的实现　　91
4.2.5　为何要设定损失函数　　92
4.3 数值微分　　94
4.3.1　导数　　94
4.3.2　数值微分的例子　　96
4.3.3　偏导数　　98
4.4 梯度　　100
4.4.1　梯度法　　102
4.4.2　神经网络的梯度　　106
4.5 学习算法的实现　　109
4.5.1　2 层神经网络的类　　110
4.5.2　mini-batch 的实现　　114
4.5.3　基于测试数据的评价　　116
4.6 小结　　118
第5章　误差反向传播法　　121
5.1 计算图　　121
5.1.1　用计算图求解　　122
5.1.2　局部计算　　124
5.1.3　为何用计算图解题　　125
5.2 链式法则　　126
5.2.1　计算图的反向传播　　127
5.2.2　什么是链式法则　　127
5.2.3　链式法则和计算图　　129
5.3 反向传播　　130
5.3.1　加法节点的反向传播　　130
5.3.2　乘法节点的反向传播　　132
5.3.3　苹果的例子　　133
5.4 简单层的实现　　135
5.4.1　乘法层的实现　　135
5.4.2　加法层的实现　　137
5.5 激活函数层的实现　　139
5.5.1　ReLU层　　139
5.5.2　Sigmoid 层　　141
5.6 AffineSoftmax层的实现　　144
5.6.1　Affine层　　144
5.6.2　批版本的Affine层　　148
5.6.3　Softmax-with-Loss 层　　150
5.7 误差反向传播法的实现　　154
5.7.1　神经网络学习的全貌图　　154
5.7.2　对应误差反向传播法的神经网络的实现　　155
5.7.3　误差反向传播法的梯度确认　　158
5.7.4　使用误差反向传播法的学习　　159
5.8 小结　　161
第6章　与学习相关的技巧　　163
6.1 参数的更新　　163
6.1.1　探险家的故事　　164
6.1.2　SGD　　164
6.1.3　SGD的缺点　　166
6.1.4　Momentum　　168
6.1.5　AdaGrad　　170
6.1.6　Adam　　172
6.1.7　使用哪种更新方法呢　　174
6.1.8　基于MNIST数据集的更新方法的比较　　175
6.2 权重的初始值　　176
6.2.1　可以将权重初始值设为0 吗　　176
6.2.2　隐藏层的激活值的分布　　177
6.2.3　ReLU的权重初始值　　181
6.2.4　基于MNIST数据集的权重初始值的比较　　183
6.3 Batch Normalization　　184
6.3.1　Batch Normalization 的算法　　184
6.3.2　Batch Normalization 的评估　　186
6.4 正则化　　188
6.4.1　过拟合　　189
6.4.2　权值衰减　　191
6.4.3　Dropout　　192
6.5 超参数的验证　　195
6.5.1　验证数据　　195
6.5.2　超参数的最优化　　196
6.5.3　超参数最优化的实现　　198
6.6 小结　　200
第7章　卷积神经网络　　201
7.1 整体结构　　201
7.2 卷积层　　202
7.2.1　全连接层存在的问题　　203
7.2.2　卷积运算　　203
7.2.3　填充　　206
7.2.4　步幅　　207
7.2.5　3 维数据的卷积运算　　209
7.2.6　结合方块思考　　211
7.2.7　批处理　　213
7.3 池化层　　214
7.4 卷积层和池化层的实现　　216
7.4.1　4 维数组　　216
7.4.2　基于im2col 的展开　　217
7.4.3　卷积层的实现　　219
7.4.4　池化层的实现　　222
7.5 CNN的实现　　224
7.6 CNN的可视化　　228
7.6.1　第1 层权重的可视化　　228
7.6.2　基于分层结构的信息提取　　230
7.7 具有代表性的CNN　　231
7.7.1　LeNet　　231
7.7.2　AlexNet　　232
7.8 小结　　233
第8章　深度学习　　235
8.1 加深网络　　235
8.1.1　向更深的网络出发　　235
8.1.2　进一步提高识别精度　　238
8.1.3　加深层的动机　　240
8.2 深度学习的小历史　　242
8.2.1　ImageNet　　243
8.2.2　VGG　　244
8.2.3　GoogLeNet　　245
8.2.4　ResNet　　246
8.3 深度学习的高速化　　248
8.3.1　需要努力解决的问题　　248
8.3.2　基于GPU的高速化　　249
8.3.3　分布式学习　　250
8.3.4　运算精度的位数缩减　　252
8.4 深度学习的应用案例　　253
8.4.1　物体检测　　253
8.4.2　图像分割　　255
8.4.3　图像标题的生成　　256
8.5 深度学习的未来　　258
8.5.1　图像风格变换　　258
8.5.2　图像的生成　　259
8.5.3　自动驾驶　　261
8.5.4　Deep Q-Network（强化学习）　　262
8.6 小结　　264
附录A　Softmax-with-Loss 层的计算图　　267
A.1 正向传播　　268
A.2 反向传播　　270
A.3 小结　　277
参考文献　　279
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
前言
译者简介
学习环境配置
资源与支持
主要符号表
第 1章　引言　1
1.1　日常生活中的机器学习　2
1.2　机器学习中的关键组件　3
1.2.1　数据　3
1.2.2　模型　4
1.2.3　目标函数　4
1.2.4　优化算法　5
1.3　各种机器学习问题　5
1.3.1　监督学习　5
1.3.2　无监督学习　11
1.3.3　与环境互动　11
1.3.4　强化学习　12
1.4　起源　13
1.5　深度学习的发展　15
1.6　深度学习的成功案例　16
1.7　特点　17
第 2章　预备知识　20
2.1　数据操作　20
2.1.1　入门　21
2.1.2　运算符　22
2.1.3　广播机制　23
2.1.4　索引和切片　24
2.1.5　节省内存　24
2.1.6　转换为其他Python对象　25
2.2　数据预处理　26
2.2.1　读取数据集　26
2.2.2　处理缺失值　26
2.2.3　转换为张量格式　27
2.3　线性代数　27
2.3.1　标量　28
2.3.2　向量　28
2.3.3　矩阵　29
2.3.4　张量　30
2.3.5　张量算法的基本性质　31
2.3.6　降维　32
2.3.7　点积　33
2.3.8　矩阵-向量积　33
2.3.9　矩阵-矩阵乘法　34
2.3.10　范数　35
2.3.11　关于线性代数的更多信息　36
2.4　微积分　37
2.4.1　导数和微分　37
2.4.2　偏导数　40
2.4.3　梯度　41
2.4.4　链式法则　41
2.5　自动微分　42
2.5.1　一个简单的例子　42
2.5.2　非标量变量的反向传播　43
2.5.3　分离计算　43
2.5.4　Python控制流的梯度计算　44
2.6　概率　44
2.6.1　基本概率论　45
2.6.2　处理多个随机变量　48
2.6.3　期望和方差　50
2.7　查阅文档　51
2.7.1　查找模块中的所有函数和类　51
2.7.2　查找特定函数和类的用法　52
第3章　线性神经网络　54
3.1　线性回归　54
3.1.1　线性回归的基本元素　54
3.1.2　向量化加速　57
3.1.3　正态分布与平方损失　58
3.1.4　从线性回归到深度网络　60
3.2　线性回归的从零开始实现　61
3.2.1　生成数据集　62
3.2.2　读取数据集　63
3.2.3　初始化模型参数　63
3.2.4　定义模型　64
3.2.5　定义损失函数　64
3.2.6　定义优化算法　64
3.2.7　训练　64
3.3　线性回归的简洁实现　66
3.3.1　生成数据集　66
3.3.2　读取数据集　66
3.3.3　定义模型　67
3.3.4　初始化模型参数　67
3.3.5　定义损失函数　68
3.3.6　定义优化算法　68
3.3.7　训练　68
3.4　softmax回归　69
3.4.1　分类问题　69
3.4.2　网络架构　70
3.4.3　全连接层的参数开销　70
3.4.4　softmax运算　71
3.4.5　小批量样本的向量化　71
3.4.6　损失函数　72
3.4.7　信息论基础　73
3.4.8　模型预测和评估　74
3.5　图像分类数据集　74
3.5.1　读取数据集　75
3.5.2　读取小批量　76
3.5.3　整合所有组件　76
3.6　softmax回归的从零开始实现　77
3.6.1　初始化模型参数　77
3.6.2　定义softmax操作　78
3.6.3　定义模型　78
3.6.4　定义损失函数　79
3.6.5　分类精度　79
3.6.6　训练　80
3.6.7　预测　82
3.7　softmax回归的简洁实现　83
3.7.1　初始化模型参数　83
3.7.2　重新审视softmax的实现　84
3.7.3　优化算法　84
3.7.4　训练　84
第4章　多层感知机　86
4.1　多层感知机　86
4.1.1　隐藏层　86
4.1.2　激活函数　88
4.2　多层感知机的从零开始实现　92
4.2.1　初始化模型参数　92
4.2.2　激活函数　93
4.2.3　模型　93
4.2.4　损失函数　93
4.2.5　训练　93
4.3　多层感知机的简洁实现　94
模型　94
4.4　模型选择、欠拟合和过拟合　95
4.4.1　训练误差和泛化误差　96
4.4.2　模型选择　97
4.4.3　欠拟合还是过拟合　98
4.4.4　多项式回归　99
4.5　权重衰减　103
4.5.1　范数与权重衰减　103
4.5.2　高维线性回归　104
4.5.3　从零开始实现　104
4.5.4　简洁实现　106
4.6　暂退法　108
4.6.1　重新审视过拟合　108
4.6.2　扰动的稳健性　108
4.6.3　实践中的暂退法　109
4.6.4　从零开始实现　110
4.6.5　简洁实现　111
4.7　前向传播、反向传播和计算图　112
4.7.1　前向传播　113
4.7.2　前向传播计算图　113
4.7.3　反向传播　114
4.7.4　训练神经网络　115
4.8　数值稳定性和模型初始化　115
4.8.1　梯度消失和梯度爆炸　116
4.8.2　参数初始化　117
4.9　环境和分布偏移　119
4.9.1　分布偏移的类型　120
4.9.2　分布偏移示例　121
4.9.3　分布偏移纠正　122
4.9.4　学习问题的分类法　125
4.9.5　机器学习中的公平、责任和透明度　126
4.10　实战Kaggle比赛：预测房价　127
4.10.1　下载和缓存数据集　127
4.10.2　Kaggle　128
4.10.3　访问和读取数据集　129
4.10.4　数据预处理　130
4.10.5　训练　131
4.10.6　K折交叉验证　132
4.10.7　模型选择　133
4.10.8　提交Kaggle预测　133
第5章　深度学习计算　136
5.1　层和块　136
5.1.1　自定义块　138
5.1.2　顺序块　139
5.1.3　在前向传播函数中执行代码　139
5.1.4　效率　140
5.2　参数管理　141
5.2.1　参数访问　141
5.2.2　参数初始化　143
5.2.3　参数绑定　145
5.3　延后初始化　145
实例化网络　146
5.4　自定义层　146
5.4.1　不带参数的层　146
5.4.2　带参数的层　147
5.5　读写文件　148
5.5.1　加载和保存张量　148
5.5.2　加载和保存模型参数　149
5.6　GPU　150
5.6.1　计算设备　151
5.6.2　张量与GPU　152
5.6.3　神经网络与GPU　153
第6章　卷积神经网络　155
6.1　从全连接层到卷积　155
6.1.1　不变性　156
6.1.2　多层感知机的限制　157
6.1.3　卷积　158
6.1.4　“沃尔多在哪里”回顾　158
6.2　图像卷积　159
6.2.1　互相关运算　159
6.2.2　卷积层　161
6.2.3　图像中目标的边缘检测　161
6.2.4　学习卷积核　162
6.2.5　互相关和卷积　162
6.2.6　特征映射和感受野　163
6.3　填充和步幅　164
6.3.1　填充　164
6.3.2　步幅　165
6.4　多输入多输出通道　166
6.4.1　多输入通道　167
6.4.2　多输出通道　167
6.4.3　1×1卷积层　168
6.5　汇聚层　170
6.5.1　最大汇聚和平均汇聚　170
6.5.2　填充和步幅　171
6.5.3　多个通道　172
6.6　卷积神经网络（LeNet）　173
6.6.1　LeNet　173
6.6.2　模型训练　175
第7章　现代卷积神经网络　178
7.1　深度卷积神经网络（AlexNet）　178
7.1.1　学习表征　179
7.1.2　AlexNet　181
7.1.3　读取数据集　183
7.1.4　训练AlexNet　183
7.2　使用块的网络（VGG）　184
7.2.1　VGG块　184
7.2.2　VGG网络　185
7.2.3　训练模型　186
7.3　网络中的网络（NiN）　187
7.3.1　NiN块　187
7.3.2　NiN模型　188
7.3.3　训练模型　189
7.4　含并行连接的网络（GoogLeNet）　190
7.4.1　Inception块　190
7.4.2　GoogLeNet模型　191
7.4.3　训练模型　193
7.5　批量规范化　194
7.5.1　训练深层网络　194
7.5.2　批量规范化层　195
7.5.3　从零实现　196
7.5.4　使用批量规范化层的 LeNet　197
7.5.5　简明实现　198
7.5.6　争议　198
7.6　残差网络（ResNet）　200
7.6.1　函数类　200
7.6.2　残差块　201
7.6.3　ResNet模型　202
7.6.4　训练模型　204
7.7　稠密连接网络（DenseNet）　205
7.7.1　从ResNet到DenseNet　205
7.7.2　稠密块体　206
7.7.3　过渡层　206
7.7.4　DenseNet模型　207
7.7.5　训练模型　207
第8章　循环神经网络　209
8.1　序列模型　209
8.1.1　统计工具　210
8.1.2　训练　212
8.1.3　预测　213
8.2　文本预处理　216
8.2.1　读取数据集　216
8.2.2　词元化　217
8.2.3　词表　217
8.2.4　整合所有功能　219
8.3　语言模型和数据集　219
8.3.1　学习语言模型　220
8.3.2　马尔可夫模型与n元语法　221
8.3.3　自然语言统计　221
8.3.4　读取长序列数据　223
8.4　循环神经网络　226
8.4.1　无隐状态的神经网络　227
8.4.2　有隐状态的循环神经网络　227
8.4.3　基于循环神经网络的字符级语言模型　228
8.4.4　困惑度　229
8.5　循环神经网络的从零开始实现　230
8.5.1　独热编码　231
8.5.2　初始化模型参数　231
8.5.3　循环神经网络模型　232
8.5.4　预测　232
8.5.5　梯度截断　233
8.5.6　训练　234
8.6　循环神经网络的简洁实现　237
8.6.1　定义模型　237
8.6.2　训练与预测　238
8.7　通过时间反向传播　239
8.7.1　循环神经网络的梯度分析　239
8.7.2　通过时间反向传播的细节　241
第9章　现代循环神经网络　244
9.1　门控循环单元（GRU）　244
9.1.1　门控隐状态　245
9.1.2　从零开始实现　247
9.1.3　简洁实现　248
9.2　长短期记忆网络（LSTM）　249
9.2.1　门控记忆元　249
9.2.2　从零开始实现　252
9.2.3　简洁实现　253
9.3　深度循环神经网络　254
9.3.1　函数依赖关系　255
9.3.2　简洁实现　255
9.3.3　训练与预测　255
9.4　双向循环神经网络　256
9.4.1　隐马尔可夫模型中的动态规划　256
9.4.2　双向模型　258
9.4.3　双向循环神经网络的错误应用　259
9.5　机器翻译与数据集　260
9.5.1　下载和预处理数据集　261
9.5.2　词元化　262
9.5.3　词表　263
9.5.4　加载数据集　263
9.5.5　训练模型　264
9.6　编码器-解码器架构　265
9.6.1　编码器　265
9.6.2　解码器　266
9.6.3　合并编码器和解码器　266
9.7　序列到序列学习（seq2seq）　267
9.7.1　编码器　268
9.7.2　解码器　269
9.7.3　损失函数　270
9.7.4　训练　271
9.7.5　预测　272
9.7.6　预测序列的评估　273
9.8　束搜索　275
9.8.1　贪心搜索　275
9.8.2　穷举搜索　276
9.8.3　束搜索　276
第 10章　注意力机制　278
10.1　注意力提示　278
10.1.1　生物学中的注意力提示　279
10.1.2　查询、键和值　280
10.1.3　注意力的可视化　280
10.2　注意力汇聚：Nadaraya-Watson 核回归　281
10.2.1　生成数据集　282
10.2.2　平均汇聚　282
10.2.3　非参数注意力汇聚　283
10.2.4　带参数注意力汇聚　284
10.3　注意力评分函数　287
10.3.1　掩蔽softmax操作　288
10.3.2　加性注意力　289
10.3.3　缩放点积注意力　290
10.4　Bahdanau 注意力　291
10.4.1　模型　291
10.4.2　定义注意力解码器　292
10.4.3　训练　293
10.5　多头注意力　295
10.5.1　模型　295
10.5.2　实现　296
10.6　自注意力和位置编码　298
10.6.1　自注意力　298
10.6.2　比较卷积神经网络、循环神经网络和自注意力　298
10.6.3　位置编码　299
10.7　Transformer　302
10.7.1　模型　302
10.7.2　基于位置的前馈网络　303
10.7.3　残差连接和层规范化　304
10.7.4　编码器　304
10.7.5　解码器　305
10.7.6　训练　307
第 11章　优化算法　311
11.1　优化和深度学习　311
11.1.1　优化的目标　311
11.1.2　深度学习中的优化挑战　312
11.2　凸性　315
11.2.1　定义　315
11.2.2　性质　317
11.2.3　约束　319
11.3　梯度下降　322
11.3.1　一维梯度下降　322
11.3.2　多元梯度下降　324
11.3.3　自适应方法　326
11.4　随机梯度下降　329
11.4.1　随机梯度更新　329
11.4.2　动态学习率　331
11.4.3　凸目标的收敛性分析　332
11.4.4　随机梯度和有限样本　333
11.5　小批量随机梯度下降　334
11.5.1　向量化和缓存　335
11.5.2　小批量　336
11.5.3　读取数据集　337
11.5.4　从零开始实现　337
11.5.5　简洁实现　340
11.6　动量法　341
11.6.1　基础　341
11.6.2　实际实验　345
11.6.3　理论分析　346
11.7　AdaGrad算法　348
11.7.1　稀疏特征和学习率　348
11.7.2　预处理　349
11.7.3　算法　350
11.7.4　从零开始实现　351
11.7.5　简洁实现　352
11.8　RMSProp算法　353
11.8.1　算法　353
11.8.2　从零开始实现　354
11.8.3　简洁实现　355
11.9　Adadelta算法　356
11.9.1　算法　356
11.9.2　实现　356
11.10　Adam算法　358
11.10.1　算法　358
11.10.2　实现　359
11.10.3　Yogi　360
11.11　学习率调度器　361
11.11.1　一个简单的问题　361
11.11.2　学习率调度器　363
11.11.3　策略　364
第 12章　计算性能　369
12.1　编译器和解释器　369
12.1.1　符号式编程　370
12.1.2　混合式编程　371
12.1.3　Sequential的混合式编程　371
12.2　异步计算　372
通过后端异步处理　373
12.3　自动并行　375
12.3.1　基于GPU的并行计算　375
12.3.2　并行计算与通信　376
12.4　硬件　378
12.4.1　计算机　378
12.4.2　内存　379
12.4.3　存储器　380
12.4.4　CPU　381
12.4.5　GPU和其他加速卡　383
12.4.6　网络和总线　385
12.4.7　更多延迟　386
12.5　多GPU训练　388
12.5.1　问题拆分　388
12.5.2　数据并行性　390
12.5.3　简单网络　390
12.5.4　数据同步　391
12.5.5　数据分发　392
12.5.6　训练　392
12.6　多GPU的简洁实现　394
12.6.1　简单网络　394
12.6.2　网络初始化　395
12.6.3　训练　395
12.7　参数服务器　397
12.7.1　数据并行训练　397
12.7.2　环同步（ring
synchronization）　399
12.7.3　多机训练　400
12.7.4　键-值存储　402
第 13章　计算机视觉　404
13.1　图像增广　404
13.1.1　常用的图像增广方法　404
13.1.2　使用图像增广进行训练　408
13.2　微调　410
13.2.1　步骤　410
13.2.2　热狗识别　411
13.3　目标检测和边界框　415
边界框　415
13.4　锚框　417
13.4.1　生成多个锚框　417
13.4.2　交并比（IoU）　419
13.4.3　在训练数据中标注锚框　420
13.4.4　使用非极大值抑制预测
边界框　424
13.5　多尺度目标检测　427
13.5.1　多尺度锚框　427
13.5.2　多尺度检测　429
13.6　目标检测数据集　430
13.6.1　下载数据集　430
13.6.2　读取数据集　431
13.6.3　演示　432
13.7　单发多框检测（SSD）　433
13.7.1　模型　433
13.7.2　训练模型　437
13.7.3　预测目标　439
13.8　区域卷积神经网络（R-CNN）系列　441
13.8.1　R-CNN　441
13.8.2　Fast R-CNN　442
13.8.3　Faster R-CNN　443
13.8.4　Mask R-CNN　444
13.9　语义分割和数据集　445
13.9.1　图像分割和实例分割　445
13.9.2　Pascal VOC2012 语义分割数据集　446
13.10　转置卷积　450
13.10.1　基本操作　450
13.10.2　填充、步幅和多通道　451
13.10.3　与矩阵变换的联系　452
13.11　全卷积网络　453
13.11.1　构建模型　454
13.11.2　初始化转置卷积层　455
13.11.3　读取数据集　456
13.11.4　训练　456
13.11.5　预测　457
13.12　风格迁移　458
13.12.1　方法　459
13.12.2　阅读内容和风格图像　460
13.12.3　预处理和后处理　460
13.12.4　提取图像特征　461
13.12.5　定义损失函数　461
13.12.6　初始化合成图像　463
13.12.7　训练模型　463
13.13　实战 Kaggle竞赛：图像分类（CIFAR-10）　464
13.13.1　获取并组织数据集　465
13.13.2　图像增广 　467
13.13.3　读取数据集　468
13.13.4　定义模型　468
13.13.5　定义训练函数　468
13.13.6　训练和验证模型　469
13.13.7　在Kaggle上对测试集进行分类并提交结果　469
13.14　实战Kaggle竞赛：狗的品种识别（ImageNet Dogs）　470
13.14.1　获取和整理数据集　471
13.14.2　图像增广　472
13.14.3　读取数据集　472
13.14.4　微调预训练模型　473
13.14.5　定义训练函数　473
13.14.6　训练和验证模型　474
13.14.7　对测试集分类并在Kaggle提交结果　475
第 14章　自然语言处理：预训练　476
14.1　词嵌入（word2vec）　477
14.1.1　为何独热向量是一个糟糕的选择　477
14.1.2　自监督的word2vec　477
14.1.3　跳元模型　477
14.1.4　连续词袋模型　478
14.2　近似训练　480
14.2.1　负采样　480
14.2.2　层序softmax　481
14.3　用于预训练词嵌入的数据集　482
14.3.1　读取数据集　482
14.3.2　下采样　483
14.3.3　中心词和上下文词的提取　484
14.3.4　负采样　485
14.3.5　小批量加载训练实例　486
14.3.6　整合代码　487
14.4　预训练word2vec　488
14.4.1　跳元模型　488
14.4.2　训练　489
14.4.3　应用词嵌入　491
14.5　全局向量的词嵌入（GloVe）　491
14.5.1　带全局语料库统计的跳元模型　492
14.5.2　GloVe模型　492
14.5.3　从共现概率比值理解GloVe模型　493
14.6　子词嵌入　494
14.6.1　fastText模型　494
14.6.2　字节对编码　495
14.7　词的相似度和类比任务　497
14.7.1　加载预训练词向量　497
14.7.2　应用预训练词向量　499
14.8　来自Transformer的双向编码器表示（BERT）　500
14.8.1　从上下文无关到上下文敏感　500
14.8.2　从特定于任务到不可知任务　501
14.8.3　BERT：将ELMo与GPT结合起来　501
14.8.4　输入表示　502
14.8.5　预训练任务　504
14.8.6　整合代码　506
14.9　用于预训练BERT的数据集　507
14.9.1　为预训练任务定义辅助函数　508
14.9.2　将文本转换为预训练数据集　509
14.10　预训练BERT　512
14.10.1　预训练BERT　512
14.10.2　用BERT表示文本　514
第 15章　自然语言处理：应用　515
15.1　情感分析及数据集　516
15.1.1　读取数据集　516
15.1.2　预处理数据集　517
15.1.3　创建数据迭代器　517
15.1.4　整合代码　518
15.2　情感分析：使用循环神经网络　518
15.2.1　使用循环神经网络表示单个文本　519
15.2.2　加载预训练的词向量　520
15.2.3　训练和评估模型　520
15.3　情感分析：使用卷积神经网络　521
15.3.1　一维卷积　522
15.3.2　最大时间汇聚层　523
15.3.3　textCNN模型　523
15.4　自然语言推断与数据集　526
15.4.1　自然语言推断　526
15.4.2　斯坦福自然语言推断（SNLI）数据集　527
15.5　自然语言推断：使用注意力　530
15.5.1　模型　530
15.5.2　训练和评估模型　533
15.6　针对序列级和词元级应用微调BERT　535
15.6.1　单文本分类　535
15.6.2　文本对分类或回归　536
15.6.3　文本标注　537
15.6.4　问答　537
15.7　自然语言推断：微调BERT　538
15.7.1　加载预训练的BERT　539
15.7.2　微调BERT的数据集　540
15.7.3　微调BERT　541
附录A　深度学习工具　543
A.1　使用Jupyter记事本　543
A.1.1　在本地编辑和运行代码　543
A.1.2　高级选项　545
A.2　使用Amazon SageMaker　546
A.2.1　注册　547
A.2.2　创建SageMaker实例　547
A.2.3　运行和停止实例　548
A.2.4　更新Notebook　548
A.3　使用Amazon EC2实例　549
A.3.1　创建和运行EC2实例　549
A.3.2　安装CUDA　553
A.3.3　安装库以运行代码　553
A.3.4　远程运行Jupyter记事本　554
A.3.5　关闭未使用的实例　554
A.4　选择服务器和GPU　555
A.4.1　选择服务器　555
A.4.2　选择GPU　556
A.5　为本书做贡献　558
A.5.1　提交微小更改　558
A.5.2　大量文本或代码修改　559
A.5.3　提交主要更改　559
参考文献　562
・ ・ ・ ・ ・ ・ (收起)第1章　什么是深度学习 1
1.1　人工智能、机器学习和深度学习 1
1.1.1　人工智能 2
1.1.2　机器学习 2
1.1.3　从数据中学习规则与表示 3
1.1.4　深度学习之“深度” 5
1.1.5　用三张图理解深度学习的工作原理 7
1.1.6　深度学习已取得的进展 8
1.1.7　不要相信短期炒作 9
1.1.8　人工智能的未来 10
1.2　深度学习之前：机器学习简史 10
1.2.1　概率建模 11
1.2.2　早期神经网络 11
1.2.3　核方法 11
1.2.4　决策树、随机森林和梯度提升机 12
1.2.5　回到神经网络 13
1.2.6　深度学习有何不同 14
1.2.7　机器学习现状 14
1.3　为什么要用深度学习，为什么是现在 16
1.3.1　硬件 17
1.3.2　数据 17
1.3.3　算法 18
1.3.4　新一轮投资热潮 18
1.3.5　深度学习的普及 19
1.3.6　这种趋势会持续下去吗 20
第2章　神经网络的数学基础 21
2.1　初识神经网络 21
2.2　神经网络的数据表示 25
2.2.1　标量（0阶张量） 25
2.2.2　向量（1阶张量） 25
2.2.3　矩阵（2阶张量） 26
2.2.4　3阶张量与更高阶的张量 26
2.2.5　关键属性 26
2.2.6　在NumPy中操作张量 28
2.2.7　数据批量的概念 28
2.2.8　现实世界中的数据张量实例 29
2.2.9　向量数据 29
2.2.10　时间序列数据或序列数据 29
2.2.11　图像数据 30
2.2.12　视频数据 31
2.3　神经网络的“齿轮”：张量运算 31
2.3.1　逐元素运算 32
2.3.2　广播 33
2.3.3　张量积 34
2.3.4　张量变形 36
2.3.5　张量运算的几何解释 37
2.3.6　深度学习的几何解释 40
2.4　神经网络的“引擎”：基于梯度的优化 40
2.4.1　什么是导数 41
2.4.2　张量运算的导数：梯度 42
2.4.3　随机梯度下降 44
2.4.4　链式求导：反向传播算法 46
2.5　回顾第一个例子 51
2.5.1　用TensorFlow 从头开始重新实现第一个例子 52
2.5.2　完成一次训练步骤 54
2.5.3　完整的训练循环 55
2.5.4　评估模型 55
2.6　本章总结 56
第3章　Keras 和TensorFlow 入门 57
3.1　TensorFlow 简介 57
3.2　Keras 简介 58
3.3　Keras 和TensorFlow 简史 59
3.4　建立深度学习工作区 60
3.4.1　Jupyter笔记本：运行深度学习实验的首选方法 60
3.4.2　使用Colaboratory 61
3.5　TensorFlow入门 63
3.5.1　常数张量和变量 64
3.5.2　张量运算：用TensorFlow进行数学运算 66
3.5.3　重温GradientTape API 66
3.5.4　一个端到端的例子：用TensorFlow编写线性分类器 67
3.6　神经网络剖析：了解核心Keras API 71
3.6.1　层：深度学习的基础模块 71
3.6.2　从层到模型 74
3.6.3　编译步骤：配置学习过程 75
3.6.4　选择损失函数 77
3.6.5　理解fit()方法 77
3.6.6　监控验证数据上的损失和指标 78
3.6.7　推断：在训练后使用模型 79
3.7　本章总结 80
第4章　神经网络入门：分类与回归 81
4.1　影评分类：二分类问题示例 82
4.1.1　IMDB 数据集 82
4.1.2　准备数据 83
4.1.3　构建模型 84
4.1.4　验证你的方法 87
4.1.5　利用训练好的模型对新数据进行预测 90
4.1.6　进一步实验 90
4.1.7　小结 90
4.2　新闻分类：多分类问题示例 91
4.2.1　路透社数据集 91
4.2.2　准备数据 92
4.2.3　构建模型 92
4.2.4　验证你的方法 93
4.2.5　对新数据进行预测 96
4.2.6　处理标签和损失的另一种方法 96
4.2.7　拥有足够大的中间层的重要性 96
4.2.8　进一步实验 97
4.2.9　小结 97
4.3　预测房价：标量回归问题示例 97
4.3.1　波士顿房价数据集 98
4.3.2　准备数据 98
4.3.3　构建模型 99
4.3.4　利用K折交叉验证来验证你的方法 99
4.3.5　对新数据进行预测 103
4.3.6　小结 103
4.4　本章总结 104
第5章　机器学习基础 105
5.1　泛化：机器学习的目标 105
5.1.1　欠拟合与过拟合 105
5.1.2　深度学习泛化的本质 110
5.2　评估机器学习模型 115
5.2.1　训练集、验证集和测试集 115
5.2.2　超越基于常识的基准 118
5.2.3　模型评估的注意事项 119
5.3　改进模型拟合 119
5.3.1　调节关键的梯度下降参数 119
5.3.2　利用更好的架构预设 121
5.3.3　提高模型容量 121
5.4　提高泛化能力 123
5.4.1　数据集管理 123
5.4.2　特征工程 124
5.4.3　提前终止 125
5.4.4　模型正则化 125
5.5　本章总结 132
第6章　机器学习的通用工作流程 133
6.1　定义任务 134
6.1.1　定义问题 134
6.1.2　收集数据集 135
6.1.3　理解数据 138
6.1.4　选择衡量成功的指标 139
6.2　开发模型 139
6.2.1　准备数据 139
6.2.2　选择评估方法 140
6.2.3　超越基准 141
6.2.4　扩大模型规模：开发一个过拟合的模型 142
6.2.5　模型正则化与调节超参数 142
6.3　部署模型 143
6.3.1　向利益相关者解释你的工作并设定预期 143
6.3.2　部署推断模型 143
6.3.3　监控模型在真实环境中的性能 146
6.3.4　维护模型 146
6.4　本章总结 147
第7章　深入Keras 148
7.1　Keras 工作流程 148
7.2　构建Keras 模型的不同方法 149
7.2.1　序贯模型 149
7.2.2　函数式API 152
7.2.3　模型子类化 157
7.2.4　混合使用不同的组件 159
7.2.5　用正确的工具完成工作 160
7.3　使用内置的训练循环和评估循环 160
7.3.1　编写自定义指标 161
7.3.2　使用回调函数 162
7.3.3　编写自定义回调函数 164
7.3.4　利用TensorBoard进行监控和可视化 165
7.4　编写自定义的训练循环和评估循环 167
7.4.1　训练与推断 168
7.4.2　指标的低阶用法 169
7.4.3　完整的训练循环和评估循环 169
7.4.4　利用tf.function加快运行速度 171
7.4.5　在fit()中使用自定义训练循环 172
7.5　本章总结 174
第8章　计算机视觉深度学习入门 175
8.1　卷积神经网络入门 176
8.1.1　卷积运算 178
8.1.2　最大汇聚运算 182
8.2　在小型数据集上从头开始训练一个卷积神经网络 184
8.2.1　深度学习对数据量很小的问题的适用性 184
8.2.2　下载数据 185
8.2.3　构建模型 . 187
8.2.4　数据预处理 189
8.2.5　使用数据增强 193
8.3　使用预训练模型 196
8.3.1　使用预训练模型做特征提取 197
8.3.2　微调预训练模型 204
8.4　本章总结 208
第9章　计算机视觉深度学习进阶 209
9.1　三项基本的计算机视觉任务 209
9.2　图像分割示例 210
9.3　现代卷积神经网络架构模式 218
9.3.1　模块化、层次结构和复用 218
9.3.2　残差连接 221
9.3.3　批量规范化 224
9.3.4　深度可分离卷积 226
9.3.5　综合示例：一个类似Xception的迷你模型 227
9.4　解释卷积神经网络学到的内容 229
9.4.1　中间激活值的可视化 230
9.4.2　卷积神经网络滤波器的可视化 235
9.4.3　类激活热力图的可视化 241
9.5　本章总结 246
第10章　深度学习处理时间序列 247
10.1　不同类型的时间序列任务 247
10.2　温度预测示例 248
10.2.1　准备数据 251
10.2.2　基于常识、不使用机器学习的基准 254
10.2.3　基本的机器学习模型 254
10.2.4　一维卷积模型 256
10.2.5　第一个RNN 基准 258
10.3　理解RNN 259
10.4　RNN 的高级用法 265
10.4.1　利用循环dropout 降低过拟合 265
10.4.2　循环层堆叠 268
10.4.3　使用双向RNN 269
10.4.4　进一步实验 271
10.5　本章总结 272
第11章　深度学习处理文本 273
11.1　自然语言处理概述 273
11.2　准备文本数据 274
11.2.1　文本标准化 275
11.2.2　文本拆分（词元化） 276
11.2.3　建立词表索引 277
11.2.4　使用TextVectorization层 278
11.3　表示单词组的两种方法：集合和序列 282
11.3.1　准备IMDB 影评数据 282
11.3.2　将单词作为集合处理：词袋方法 284
11.3.3　将单词作为序列处理：序列模型方法 289
11.4　Transformer架构 298
11.4.1　理解自注意力 298
11.4.2　多头注意力 302
11.4.3　Transformer编码器 303
11.4.4　何时使用序列模型而不是词袋模型 309
11.5　超越文本分类：序列到序列学习 310
11.5.1　机器翻译示例 312
11.5.2　RNN 的序列到序列学习 314
11.5.3　使用Transformer 进行序列到序列学习 318
11.6　本章总结 323
第12章　生成式深度学习 324
12.1　文本生成 325
12.1.1　生成式深度学习用于序列生成的简史 325
12.1.2　如何生成序列数据 326
12.1.3　采样策略的重要性 327
12.1.4　用Keras 实现文本生成 328
12.1.5　带有可变温度采样的文本生成回调函数 331
12.1.6　小结 334
12.2　DeepDream 334
12.2.1　用Keras 实现DeepDream 335
12.2.2　小结 341
12.3　　神经风格迁移 341
12.3.1　内容损失 342
12.3.2　风格损失 342
12.3.3　用Keras 实现神经风格迁移 343
12.3.4　小结 348
12.4　用变分自编码器生成图像 348
12.4.1　从图像潜在空间中采样 348
12.4.2　图像编辑的概念向量 350
12.4.3　变分自编码器 350
12.4.4　用Keras 实现变分自编码器 352
12.4.5　小结 357
12.5　生成式对抗网络入门 358
12.5.1　简要实现流程 359
12.5.2　诸多技巧 360
12.5.3　CelebA 数据集 360
12.5.4　判别器 361
12.5.5　生成器 362
12.5.6　对抗网络 364
12.5.7　小结 366
12.6　本章总结 367
第13章　适合现实世界的最佳实践 368
13.1　将模型性能发挥到极致 368
13.1.1　超参数优化 368
13.1.2　模型集成 375
13.2　加速模型训练 376
13.2.1　使用混合精度加快GPU上的训练速度 377
13.2.2　多GPU训练 380
13.2.3　TPU训练 382
13.3　本章总结 384
第14章　总结 385
14.1　重点概念回顾 385
14.1.1　人工智能的多种方法 385
14.1.2　深度学习在机器学习领域中的特殊之处 386
14.1.3　如何看待深度学习 386
14.1.4　关键的推动技术 387
14.1.5　机器学习的通用工作流程 388
14.1.6　关键网络架构 388
14.1.7　可能性空间 392
14.2　深度学习的局限性 394
14.2.1　将机器学习模型拟人化的风险 394
14.2.2　自动机与智能体 396
14.2.3　局部泛化与极端泛化 397
14.2.4　智能的目的 399
14.2.5　逐步提高泛化能力 400
14.3　如何实现更加通用的人工智能 401
14.3.1　设定正确目标的重要性：捷径法则 401
14.3.2　新目标 402
14.4　实现智能：缺失的内容 403
14.4.1　智能是对抽象类比的敏感性 404
14.4.2　两种抽象 405
14.4.3　深度学习所缺失的那一半 407
14.5　深度学习的未来 408
14.5.1　模型即程序 408
14.5.2　将深度学习与程序合成融合 409
14.5.3　终身学习和模块化子程序复用 411
14.5.4　长期愿景 412
14.6　了解快速发展的领域的最新进展 413
14.6.1　在Kaggle 上练习解决现实世界的问题 413
14.6.2　在arXiv上了解最新进展 414
14.6.3　探索Keras 生态系统 414
14.7　结束语 414
・ ・ ・ ・ ・ ・ (收起)第一部分　深度学习基础
第1章　什么是深度学习　　2
1.1　人工智能、机器学习与深度学习　　2
1.1.1　人工智能　　3
1.1.2　机器学习　　3
1.1.3　从数据中学习表示　　4
1.1.4　深度学习之“深度”　　6
1.1.5　用三张图理解深度学习的工作原理　　7
1.1.6　深度学习已经取得的进展　　9
1.1.7　不要相信短期炒作　　9
1.1.8　人工智能的未来　　10
1.2　深度学习之前：机器学习简史　　11
1.2.1　概率建模　　11
1.2.2　早期神经网络　　11
1.2.3　核方法　　12
1.2.4　决策树、随机森林与梯度提升机　　13
1.2.5　回到神经网络　　14
1.2.6　深度学习有何不同　　14
1.2.7　机器学习现状　　15
1.3　为什么是深度学习，为什么是现在　　15
1.3.1　硬件　　16
1.3.2　数据　　17
1.3.3　算法　　17
1.3.4　新的投资热潮　　17
1.3.5　深度学习的大众化　　18
1.3.6　这种趋势会持续吗　　18
第2章　神经网络的数学基础　　20
2.1　初识神经网络　　20
2.2　神经网络的数据表示　　23
2.2.1　标量（0D张量）　　23
2.2.2　向量（1D张量）　　24
2.2.3　矩阵（2D张量）　　24
2.2.4　3D张量与更高维张量　　24
2.2.5　关键属性　　25
2.2.6　在Numpy中操作张量　　26
2.2.7　数据批量的概念　　27
2.2.8　现实世界中的数据张量　　27
2.2.9　向量数据　　27
2.2.10　时间序列数据或序列数据　　28
2.2.11　图像数据　　28
2.2.12　视频数据　　29
2.3　神经网络的“齿轮”：张量运算　　29
2.3.1　逐元素运算　　30
2.3.2　广播　　31
2.3.3　张量点积　　32
2.3.4　张量变形　　34
2.3.5　张量运算的几何解释　　34
2.3.6　深度学习的几何解释　　35
2.4　神经网络的“引擎”：基于梯度的优化　　36
2.4.1　什么是导数　　37
2.4.2　张量运算的导数：梯度　　38
2.4.3　随机梯度下降　　38
2.4.4　链式求导：反向传播算法　　41
2.5　回顾第一个例子　　41
本章小结　　42
第3章　神经网络入门　　43
3.1　神经网络剖析　　43
3.1.1　层：深度学习的基础组件　　44
3.1.2　模型：层构成的网络　　45
3.1.3　损失函数与优化器：配置学习过程的关键　　45
3.2　Keras简介　　46
3.2.1　Keras、TensorFlow、Theano 和CNTK　　47
3.2.2　使用Keras 开发：概述　　48
3.3　建立深度学习工作站　　49
3.3.1　Jupyter笔记本：运行深度学习实验的首选方法　　49
3.3.2　运行Keras：两种选择　　50
3.3.3　在云端运行深度学习任务：优点和缺点　　50
3.3.4　深度学习的最佳GPU　　50
3.4　电影评论分类：二分类问题　　51
3.4.1　IMDB 数据集　　51
3.4.2　准备数据　　52
3.4.3　构建网络　　52
3.4.4　验证你的方法　　56
3.4.5　使用训练好的网络在新数据上生成预测结果　　59
3.4.6　进一步的实验　　59
3.4.7　小结　　59
3.5　新闻分类：多分类问题　　59
3.5.1　路透社数据集　　60
3.5.2　准备数据　　61
3.5.3　构建网络　　61
3.5.4　验证你的方法　　62
3.5.5　在新数据上生成预测结果　　65
3.5.6　处理标签和损失的另一种方法　　65
3.5.7　中间层维度足够大的重要性　　65
3.5.8　进一步的实验　　66
3.5.9　小结　　66
3.6　预测房价：回归问题　　66
3.6.1　波士顿房价数据集　　67
3.6.2　准备数据　　67
3.6.3　构建网络　　68
3.6.4　利用K折验证来验证你的方法　　68
3.6.5　小结　　72
本章小结　　73
第4章　机器学习基础　　74
4.1　机器学习的四个分支　　74
4.1.1　监督学习　　74
4.1.2　无监督学习　　75
4.1.3　自监督学习　　75
4.1.4　强化学习　　75
4.2　评估机器学习模型　　76
4.2.1　训练集、验证集和测试集　　77
4.2.2　评估模型的注意事项　　80
4.3　数据预处理、特征工程和特征学习　　80
4.3.1　神经网络的数据预处理　　80
4.3.2　特征工程　　81
4.4　过拟合与欠拟合　　83
4.4.1　减小网络大小　　83
4.4.2　添加权重正则化　　85
4.4.3　添加dropout正则化　　87
4.5　机器学习的通用工作流程　　89
4.5.1　定义问题，收集数据集　　89
4.5.2　选择衡量成功的指标　　89
4.5.3　确定评估方法　　90
4.5.4　准备数据　　90
4.5.5　开发比基准更好的模型　　90
4.5.6　扩大模型规模：开发过拟合的模型　　91
4.5.7　模型正则化与调节超参数　　92
本章小结　　92
第二部分　深度学习实践
第5章　深度学习用于计算机视觉　　94
5.1　卷积神经网络简介　　94
5.1.1　卷积运算　　96
5.1.2　最大池化运算　　101
5.2　在小型数据集上从头开始训练一个卷积神经网络　　102
5.2.1　深度学习与小数据问题的相关性　　103
5.2.2　下载数据　　103
5.2.3　构建网络　　106
5.2.4　数据预处理　　107
5.2.5　使用数据增强　　111
5.3　使用预训练的卷积神经网络　　115
5.3.1　特征提取　　116
5.3.2　微调模型　　124
5.3.3　小结　　130
5.4　卷积神经网络的可视化　　130
5.4.1　可视化中间激活　　131
5.4.2　可视化卷积神经网络的过滤器　　136
5.4.3　可视化类激活的热力图　　142
本章小结　　146
第6章　深度学习用于文本和序列　　147
6.1　处理文本数据　　147
6.1.1　单词和字符的one-hot编码　　149
6.1.2　使用词嵌入　　151
6.1.3　整合在一起：从原始文本到词嵌入　　155
6.1.4　小结　　162
6.2　理解循环神经网络　　162
6.2.1　Keras中的循环层　　164
6.2.2　理解LSTM层和GRU层　　168
6.2.3　Keras中一个LSTM的具体例子　　170
6.2.4　小结　　172
6.3　循环神经网络的高级用法　　172
6.3.1　温度预测问题　　172
6.3.2　准备数据　　175
6.3.3　一种基于常识的、非机器学习的基准方法　　177
6.3.4　一种基本的机器学习方法　　178
6.3.5　第一个循环网络基准　　180
6.3.6　使用循环dropout来降低过拟合　　181
6.3.7　循环层堆叠　　182
6.3.8　使用双向RNN　　184
6.3.9　更多尝试　　187
6.3.10　小结　　187
6.4　用卷积神经网络处理序列　　188
6.4.1　理解序列数据的一维卷积　　188
6.4.2　序列数据的一维池化　　189
6.4.3　实现一维卷积神经网络　　189
6.4.4　结合CNN和RNN来处理长序列　　191
6.4.5　小结　　195
本章总结　　195
第7章　高级的深度学习最佳实践　　196
7.1　不用Sequential模型的解决方案：Keras 函数式API　　196
7.1.1　函数式API简介　　199
7.1.2　多输入模型　　200
7.1.3　多输出模型　　202
7.1.4　层组成的有向无环图　　204
7.1.5　共享层权重　　208
7.1.6　将模型作为层　　208
7.1.7　小结　　209
7.2　使用Keras回调函数和TensorBoard来检查并监控深度学习模型　　210
7.2.1　训练过程中将回调函数作用于模型　　210
7.2.2　TensorBoard简介：TensorFlow的可视化框架　　212
7.2.3　小结　　219
7.3　让模型性能发挥到极致　　219
7.3.1　高级架构模式　　219
7.3.2　超参数优化　　222
7.3.3　模型集成　　223
7.3.4　小结　　224
本章总结　　225
第8章　生成式深度学习　　226
8.1　使用LSTM生成文本　　227
8.1.1　生成式循环网络简史　　227
8.1.2　如何生成序列数据　　228
8.1.3　采样策略的重要性　　229
8.1.4　实现字符级的LSTM文本生成　　230
8.1.5　小结　　234
8.2　DeepDream　　235
8.2.1　用Keras实现DeepDream　　236
8.2.2　小结　　241
8.3　神经风格迁移　　241
8.3.1　内容损失　　242
8.3.2　风格损失　　243
8.3.3　用Keras实现神经风格迁移　　243
8.3.4　小结　　249
8.4　用变分自编码器生成图像　　249
8.4.1　从图像的潜在空间中采样　　249
8.4.2　图像编辑的概念向量　　250
8.4.3　变分自编码器　　251
8.4.4　小结　　256
8.5　生成式对抗网络简介　　257
8.5.1　GAN 的简要实现流程　　258
8.5.2　大量技巧　　259
8.5.3　生成器　　260
8.5.4　判别器　　261
8.5.5　对抗网络　　261
8.5.6　如何训练DCGAN　　262
8.5.7　小结　　264
本章总结　　264
第9章　总结　　265
9.1　重点内容回顾　　265
9.1.1　人工智能的各种方法　　265
9.1.2　深度学习在机器学习领域中的特殊之处　　266
9.1.3　如何看待深度学习　　266
9.1.4　关键的推动技术　　267
9.1.5　机器学习的通用工作流程　　268
9.1.6　关键网络架构　　268
9.1.7　可能性空间　　272
9.2　深度学习的局限性　　273
9.2.1　将机器学习模型拟人化的风险　　273
9.2.2　局部泛化与极端泛化　　275
9.2.3　小结　　276
9.3　深度学习的未来　　277
9.3.1　模型即程序　　277
9.3.2　超越反向传播和可微层　　278
9.3.3　自动化机器学习　　279
9.3.4　终身学习与模块化子程序复用　　279
9.3.5　长期愿景　　281
9.4　了解一个快速发展领域的最新进展　　281
9.4.1　使用Kaggle练习解决现实世界的问题　　281
9.4.2　在arXiv阅读最新进展　　282
9.4.3　探索Keras生态系统　　282
9.5　结束语　　282
附录A　在Ubuntu上安装Keras及其依赖　　283
附录B　在EC2 GPU实例上运行Jupyter笔记本　　287
・ ・ ・ ・ ・ ・ (收起)序
前言
常用符号表
第一部分 机器学习基础
第1章 绪论3
1.1人工智能...............................4
1.1.1人工智能的发展历史....................5
1.1.2人工智能的流派.......................7
1.2机器学习...............................7
1.3表示学习...............................8
1.3.1局部表示和分布式表示...................9
1.3.2表示学习...........................11
1.4深度学习...............................11
1.4.1端到端学习..........................12
1.5神经网络...............................13
1.5.1人脑神经网络........................13
1.5.2人工神经网络........................14
1.5.3神经网络的发展历史....................15
1.6本书的知识体系...........................17
1.7常用的深度学习框架.........................18
1.8总结和深入阅读...........................20
第2章 机器学习概述23
2.1基本概念...............................24
2.2机器学习的三个基本要素......................26
2.2.1模型..............................26
2.2.2学习准则...........................27
2.2.3优化算法...........................30
2.3机器学习的简单示例――线性回归.................33
2.3.1参数学习...........................34
2.4偏差-方差分解............................38
2.5机器学习算法的类型.........................41
2.6数据的特征表示...........................43
2.6.1传统的特征学习.......................44
2.6.2深度学习方法........................46
2.7评价指标...............................46
2.8理论和定理..............................49
2.8.1PAC学习理论........................49
2.8.2没有免费午餐定理......................50
2.8.3奥卡姆剃刀原理.......................50
2.8.4丑小鸭定理..........................51
2.8.5归纳偏置...........................51
2.9总结和深入阅读...........................51
第3章 线性模型
3.1线性判别函数和决策边界......................56
3.1.1二分类............................56
3.1.2多分类............................58
3.2Logistic回归.............................59
3.2.1参数学习...........................60
3.3Softmax回归.............................61
3.3.1参数学习...........................62
3.4感知器.................................64
3.4.1参数学习...........................64
3.4.2感知器的收敛性.......................66
3.4.3参数平均感知器.......................67
3.4.4扩展到多分类........................69
3.5支持向量机..............................71
3.5.1参数学习...........................73
3.5.2核函数............................74
3.5.3软间隔............................74
3.6损失函数对比.............................75
3.7总结和深入阅读...........................76
第二部分 基础模型
第4章 前馈神经网络81
4.1神经元.................................82
4.1.1Sigmoid型函数.......................83
4.1.2ReLU函数..........................86
4.1.3Swish函数..........................88
4.1.4GELU函数..........................89
4.1.5Maxout单元.........................89
4.2网络结构...............................90
4.2.1前馈网络...........................90
4.2.2记忆网络...........................90
4.2.3图网络............................90
4.3前馈神经网络.............................91
4.3.1通用近似定理........................93
4.3.2应用到机器学习.......................94
4.3.3参数学习...........................95
4.4反向传播算法.............................95
4.5自动梯度计算.............................98
4.5.1数值微分...........................99
4.5.2符号微分...........................99
4.5.3自动微分...........................100
4.6优化问题...............................103
4.6.1非凸优化问题........................103
4.6.2梯度消失问题........................104
4.7总结和深入阅读...........................104
第5章 卷积神经网络109
5.1卷积..................................110
5.1.1卷积的定义..........................110
5.1.2互相关............................112
5.1.3卷积的变种..........................113
5.1.4卷积的数学性质.......................114
5.2卷积神经网络.............................115
5.2.1用卷积来代替全连接....................115
5.2.2卷积层............................116
5.2.3汇聚层............................118
5.2.4卷积网络的整体结构....................119
5.3参数学习...............................120
5.3.1卷积神经网络的反向传播算法...............120
5.4几种典型的卷积神经网络......................121
5.4.1LeNet-5............................122
5.4.2AlexNet...........................123
5.4.3Inception网络........................125
5.4.4残差网络...........................126
5.5其他卷积方式.............................127
5.5.1转置卷积...........................127
5.5.2空洞卷积...........................129
5.6总结和深入阅读...........................130
第6章 循环神经网络133
6.1给网络增加记忆能力.........................134
6.1.1延时神经网络........................134
6.1.2有外部输入的非线性自回归模型..............134
6.1.3循环神经网络........................135
6.2简单循环网络.............................135
6.2.1循环神经网络的计算能力..................136
6.3应用到机器学习...........................138
6.3.1序列到类别模式.......................138
6.3.2同步的序列到序列模式...................139
6.3.3异步的序列到序列模式...................139
6.4参数学习...............................140
6.4.1随时间反向传播算法....................141
6.4.2实时循环学习算法......................142
6.5长程依赖问题.............................143
6.5.1改进方案...........................144
6.6基于门控的循环神经网络......................145
6.6.1长短期记忆网络.......................145
6.6.2LSTM网络的各种变体...................147
6.6.3门控循环单元网络......................148
6.7深层循环神经网络..........................149
6.7.1堆叠循环神经网络......................150
6.7.2双向循环神经网络......................150
6.8扩展到图结构.............................151
6.8.1递归神经网络........................151
6.8.2图神经网络..........................152
6.9总结和深入阅读...........................153
第7章 网络优化与正则化157
7.1网络优化...............................157
7.1.1网络结构多样性.......................158
7.1.2高维变量的非凸优化....................158
7.1.3神经网络优化的改善方法..................160
7.2优化算法...............................160
7.2.1小批量梯度下降.......................160
7.2.2批量大小选择........................161
7.2.3学习率调整..........................162
7.2.4梯度估计修正........................167
7.2.5优化算法小结........................170
7.3参数初始化..............................171
7.3.1基于固定方差的参数初始化.................172
7.3.2基于方差缩放的参数初始化.................173
7.3.3正交初始化..........................175
7.4数据预处理..............................176
7.5逐层归一化..............................178
7.5.1批量归一化..........................179
7.5.2层归一化...........................181
7.5.3权重归一化..........................182
7.5.4局部响应归一化.......................182
7.6超参数优化..............................183
7.6.1网格搜索...........................183
7.6.2随机搜索...........................184
7.6.3贝叶斯优化..........................184
7.6.4动态资源分配........................185
7.6.5神经架构搜索........................186
7.7网络正则化..............................186
7.7.1?1和?2正则化........................187
7.7.2权重衰减...........................188
7.7.3提前停止...........................188
7.7.4丢弃法............................189
7.7.5数据增强...........................191
7.7.6标签平滑...........................191
7.8总结和深入阅读...........................192
第8章 注意力机制与外部记忆197
8.1认知神经学中的注意力.......................198
8.2注意力机制..............................199
8.2.1注意力机制的变体......................201
8.3自注意力模型.............................203
8.4人脑中的记忆.............................205
8.5记忆增强神经网络..........................207
8.5.1端到端记忆网络.......................208
8.5.2神经图灵机..........................210
8.6基于神经动力学的联想记忆.....................211
8.6.1Hopfiel网络........................212
8.6.2使用联想记忆增加网络容量.................215
8.7总结和深入阅读...........................215
第9章 无监督学习219
9.1无监督特征学习...........................220
9.1.1主成分分析..........................220
9.1.2稀疏编码...........................222
9.1.3自编码器...........................224
9.1.4稀疏自编码器........................225
9.1.5堆叠自编码器........................226
9.1.6降噪自编码器........................226
9.2概率密度估计.............................227
9.2.1参数密度估计........................227
9.2.2非参数密度估计.......................229
9.3总结和深入阅读...........................232
第10章 模型独立的学习方式235
10.1集成学习...............................235
10.1.1AdaBoost算法........................237
10.2自训练和协同训练..........................240
10.2.1自训练............................240
10.2.2协同训练...........................240
10.3多任务学习..............................242
10.4迁移学习...............................245
10.4.1归纳迁移学习........................246
10.4.2转导迁移学习........................247
10.5终身学习...............................249
10.6元学习.................................252
10.6.1基于优化器的元学习....................253
10.6.2模型无关的元学习......................254
10.7总结和深入阅读...........................255
第三部分 进阶模型
第11章 概率图模型261
11.1模型表示...............................262
11.1.1有向图模型..........................263
11.1.2常见的有向图模型......................264
11.1.3无向图模型..........................267
11.1.4无向图模型的概率分解...................267
11.1.5常见的无向图模型......................269
11.1.6有向图和无向图之间的转换.................270
11.2学习..................................271
11.2.1不含隐变量的参数估计...................271
11.2.2含隐变量的参数估计....................273
11.3推断..................................279
11.3.1精确推断...........................279
11.3.2近似推断...........................282
11.4变分推断...............................283
11.5基于采样法的近似推断.......................285
11.5.1采样法............................285
11.5.2拒绝采样...........................287
11.5.3重要性采样..........................288
11.5.4马尔可夫链蒙特卡罗方法..................289
11.6总结和深入阅读...........................292
第12章 深度信念网络297
12.1玻尔兹曼机..............................297
12.1.1生成模型...........................299
12.1.2能量最小化与模拟退火...................301
12.1.3参数学习...........................302
12.2受限玻尔兹曼机...........................304
12.2.1生成模型...........................305
12.2.2参数学习...........................307
12.2.3受限玻尔兹曼机的类型...................308
12.3深度信念网络.............................309
12.3.1生成模型...........................310
12.3.2参数学习...........................310
12.4总结和深入阅读...........................313
第13章 深度生成模型317
13.1概率生成模型.............................318
13.1.1密度估计...........................318
13.1.2生成样本...........................319
13.1.3应用于监督学习.......................319
13.2变分自编码器.............................319
13.2.1含隐变量的生成模型....................319
13.2.2推断网络...........................321
13.2.3生成网络...........................323
13.2.4模型汇总...........................323
13.2.5再参数化...........................325
13.2.6训练..............................325
13.3生成对抗网络.............................327
13.3.1显式密度模型和隐式密度模型...............327
13.3.2网络分解...........................327
13.3.3训练..............................329
13.3.4一个生成对抗网络的具体实现：DCGAN..........330
13.3.5模型分析...........................330
13.3.6改进模型...........................333
13.4总结和深入阅读...........................336
第14章 深度强化学习339
14.1强化学习问题.............................340
14.1.1典型例子...........................340
14.1.2强化学习定义........................340
14.1.3马尔可夫决策过程......................341
14.1.4强化学习的目标函数....................343
14.1.5值函数............................344
14.1.6深度强化学习........................345
14.2基于值函数的学习方法.......................346
14.2.1动态规划算法........................346
14.2.2蒙特卡罗方法........................349
14.2.3时序差分学习方法......................350
14.2.4深度Q网络..........................353
14.3基于策略函数的学习方法......................354
14.3.1REINFORCE算法......................356
14.3.2带基准线的REINFORCE算法...............356
14.4演员-评论员算法...........................358
14.5总结和深入阅读...........................360
第15章 序列生成模型365
15.1序列概率模型.............................366
15.1.1序列生成...........................367
15.2N元统计模型.............................368
15.3深度序列模型.............................370
15.3.1模型结构...........................370
15.3.2参数学习...........................373
15.4评价方法...............................373
15.4.1困惑度............................373
15.4.2BLEU算法..........................374
15.4.3ROUGE算法.........................375
15.5序列生成模型中的学习问题.....................375
15.5.1曝光偏差问题........................376
15.5.2训练目标不一致问题....................377
15.5.3计算效率问题........................377
15.6序列到序列模型...........................385
15.6.1基于循环神经网络的序列到序列模型...........386
15.6.2基于注意力的序列到序列模型...............387
15.6.3基于自注意力的序列到序列模型..............388
15.7总结和深入阅读...........................390
附录数学基础 393
附录A 线性代数 394
附录B 微积分 404
附录C 数学优化 413
附录D 概率论 420
附录E 信息论 433
索引 439
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
前言
如何使用本书
资源与支持
主要符号表
第1 章　深度学习简介… ………………… 1
1.1　起源…………………………………………… 2
1.2　发展…………………………………………… 4
1.3　成功案例……………………………………… 6
1.4　特点………………………………………… 7
小结…………………………………………… 8
练习…………………………………………… 8
第2 章　预备知识… ……………………… 9
2.1　获取和运行本书的代码……………………… 9
2.1.1　获取代码并安装运行环境 … ……… 9
2.1.2　更新代码和运行环境 … …………… 11
2.1.3　使用GPU版的MXNet … ………… 11
小结……………………………………………12
练习……………………………………………12
2.2　数据操作… ……………………………… 12
2.2.1　创建NDArray ………………………12
2.2.2　运算 …………………………………14
2.2.3　广播机制 ……………………………16
2.2.4　索引 …………………………………17
2.2.5　运算的内存开销 ……………………17
2.2.6　NDArray和NumPy相互变换………18
小结……………………………………………19
练习……………………………………………19
2.3　自动求梯度… …………………………… 19
2.3.1　简单例子 … …………………………19
2.3.2　训练模式和预测模式 …………… 20
2.3.3　对Python控制流求梯度 … …… 20
小结……………………………………………21
练习……………………………………………21
2.4　查阅文档… ……………………………… 21
2.4.1　查找模块里的所有函数和类 … ……21
2.4.2　查找特定函数和类的使用 ……… 22
2.4.3　在MXNet网站上查阅 …………… 23
小结………………………………………… 24
练习………………………………………… 24
第3 章　深度学习基础… ……………… 25
3.1　线性回归…………………………………… 25
3.1.1　线性回归的基本要素 … ………… 25
3.1.2　线性回归的表示方法 … ………… 28
小结………………………………………… 30
练习………………………………………… 30
3.2　线性回归的从零开始实现… …………… 30
3.2.1　生成数据集 … …………………… 30
3.2.2　读取数据集 ……………………… 32
3.2.3　初始化模型参数 ………………… 32
3.2.4　定义模型 ………………………… 33
3.2.5　定义损失函数 …………………… 33
3.2.6　定义优化算法 …………………… 33
3.2.7　训练模型 ………………………… 33
小结………………………………………… 34
练习………………………………………… 34
3.3　线性回归的简洁实现… ………………… 35
3.3.1　生成数据集 … …………………… 35
3.3.2　读取数据集 ……………………… 35
3.3.3　定义模型 ………………………… 36
3.3.4　初始化模型参数 ………………… 36
3.3.5　定义损失函数 …………………… 37
3.3.6　定义优化算法 …………………… 37
3.3.7　训练模型 ………………………… 37
小结………………………………………… 38
练习………………………………………… 38
3.4　softmax回归… ………………………… 38
3.4.1　分类问题 … ……………………… 38
3.4.2　softmax回归模型… …………… 39
3.4.3　单样本分类的矢量计算表达式…… 40
3.4.4　小批量样本分类的矢量计算表达式 …………………………… 40
3.4.5　交叉熵损失函数 ……………………41
3.4.6　模型预测及评价 ………………… 42
小结………………………………………… 42
练习………………………………………… 42
3.5　图像分类数据集（Fashion-MNIST）… ……………… 42
3.5.1　获取数据集 … …………………… 42
3.5.2　读取小批量 ……………………… 44
小结………………………………………… 45
练习………………………………………… 45
3.6　softmax回归的从零开始实现… ……… 45
3.6.1　读取数据集 … …………………… 45
3.6.2　初始化模型参数 ………………… 45
3.6.3　实现softmax运算 … …………… 46
3.6.4　定义模型 ………………………… 46
3.6.5　定义损失函数 …………………… 47
3.6.6　计算分类准确率 ………………… 47
3.6.7　训练模型 ………………………… 48
3.6.8　预测… …………………………… 48
小结………………………………………… 49
练习………………………………………… 49
3.7　softmax回归的简洁实现… …………… 49
3.7.1　读取数据集 … …………………… 49
3.7.2　定义和初始化模型 ……………… 50
3.7.3　softmax和交叉熵损失函数 … … 50
3.7.4　定义优化算法 …………………… 50
3.7.5　训练模型 ………………………… 50
小结………………………………………… 50
练习………………………………………… 50
3.8　多层感知机… …………………………… 51
3.8.1　隐藏层 … ……………………………51
3.8.2　激活函数 ………………………… 52
3.8.3　多层感知机 ……………………… 55
小结………………………………………… 55
练习………………………………………… 55
3.9　多层感知机的从零开始实现… ………… 56
3.9.1　读取数据集 … …………………… 56
3.9.2　定义模型参数 …………………… 56
3.9.3　定义激活函数 …………………… 56
3.9.4　定义模型 ………………………… 56
3.9.5　定义损失函数 …………………… 57
3.9.6　训练模型 ………………………… 57
小结………………………………………… 57
练习………………………………………… 57
3.10　多层感知机的简洁实现………………… 57
3.10.1　定义模型 ………………………… 58
3.10.2　训练模型 … …………………… 58
小结………………………………………… 58
练习………………………………………… 58
3.11　模型选择、欠拟合和过拟合… ………… 58
3.11.1　训练误差和泛化误差 …………… 59
3.11.2　模型选择 ………………………… 59
3.11.3　欠拟合和过拟合 ………………… 60
3.11.4　多项式函数拟合实验 ……………61
小结………………………………………… 65
练习………………………………………… 65
3.12　权重衰减………………………………… 65
3.12.1　方法 ……………………………… 65
3.12.2　高维线性回归实验 … ………… 66
3.12.3　从零开始实现 … ……………… 66
3.12.4　简洁实现 … …………………… 68
小结………………………………………… 70
练习………………………………………… 70
3.13　丢弃法…………………………………… 70
3.13.1　方法 ……………………………… 70
3.13.2　从零开始实现 … …………………71
3.13.3　简洁实现 … …………………… 73
小结………………………………………… 74
练习………………………………………… 74
3.14　正向传播、反向传播和计算图………… 74
3.14.1　正向传播 ……………………… 74
3.14.2　正向传播的计算图 … ………… 75
3.14.3　反向传播 … …………………… 75
3.14.4　训练深度学习模型 … ………… 76
小结………………………………………… 77
练习………………………………………… 77
3.15　数值稳定性和模型初始化……………… 77
3.15.1　衰减和爆炸 ……………………… 77
3.15.2　随机初始化模型参数 … ……… 78
小结………………………………………… 78
练习………………………………………… 79
3.16　实战Kaggle比赛：房价预测… ……… 79
3.16.1　Kaggle比赛 … ………………… 79
3.16.2　读取数据集 … ………………… 80
3.16.3　预处理数据集 … …………………81
3.16.4　训练模型 … …………………… 82
3.16.5　k 折交叉验证 …………………… 82
3.16.6　模型选择 … …………………… 83
3.16.7　预测并在Kaggle提交结果… … 84
小结………………………………………… 85
练习………………………………………… 85
第4 章　深度学习计算… ……………… 86
4.1　模型构造………………………………… 86
4.1.1　继承Block类来构造模型 … …… 86
4.1.2　Sequential类继承自Block类…………………………… 87
4.1.3　构造复杂的模型… ……………… 88
小结………………………………………… 89
练习………………………………………… 90
4.2　模型参数的访问、初始化和共享… …… 90
4.2.1　访问模型参数 … ………………… 90
4.2.2　初始化模型参数 ………………… 92
4.2.3　自定义初始化方法 ……………… 93
4.2.4　共享模型参数 …………………… 94
小结………………………………………… 94
练习………………………………………… 94
4.3　模型参数的延后初始化… ……………… 95
4.3.1　延后初始化 … …………………… 95
4.3.2　避免延后初始化 ………………… 96
小结………………………………………… 96
练习………………………………………… 97
4.4　自定义层… ……………………………… 97
4.4.1　不含模型参数的自定义层 … …… 97
4.4.2　含模型参数的自定义层 ………… 98
小结………………………………………… 99
练习………………………………………… 99
4.5　读取和存储… …………………………… 99
4.5.1　读写NDArray… ………………… 99
4.5.2　读写Gluon模型的参数… ……… 100
小结………………………………………… 101
练习………………………………………… 101
4.6　GPU计算………………………………… 101
4.6.1　计算设备 … ……………………… 102
4.6.2　NDArray的GPU计算…………… 102
4.6.3　Gluon的GPU计算 ……………… 104
小结………………………………………… 105
练习………………………………………… 105
第5 章　卷积神经网络… ……………… 106
5.1　二维卷积层………………………………… 106
5.1.1　二维互相关运算 … ……………… 106
5.1.2　二维卷积层 … …………………… 107
5.1.3　图像中物体边缘检测 … ………… 108
5.1.4　通过数据学习核数组 … ………… 109
5.1.5　互相关运算和卷积运算 … ……… 109
5.1.6　特征图和感受野… ……………… 110
小结………………………………………… 110
练习………………………………………… 110
5.2　填充和步幅… …………………………… 111
5.2.1　填充 … …………………………… 111
5.2.2　步幅 ……………………………… 112
小结………………………………………… 113
练习………………………………………… 113
5.3　多输入通道和多输出通道… …………… 114
5.3.1　多输入通道 … …………………… 114
5.3.2　多输出通道… …………………… 115
5.3.3　1×1卷积层 ……………………… 116
小结………………………………………… 117
练习………………………………………… 117
5.4　池化层… ………………………………… 117
5.4.1　二维最大池化层和平均池化层 … ………………………… 117
5.4.2　填充和步幅 ……………………… 119
5.4.3　多通道 …………………………… 120
小结………………………………………… 120
练习………………………………………… 121
5.5　卷积神经网络（LeNet）… …………… 121
5.5.1　LeNet模型 … …………………… 121
5.5.2　训练模型… ……………………… 122
小结………………………………………… 124
练习………………………………………… 124
5.6　深度卷积神经网络（AlexNet）… …… 124
5.6.1　学习特征表示 … ………………… 125
5.6.2　AlexNet… ……………………… 126
5.6.3　读取数据集 ……………………… 127
5.6.4　训练模型 ………………………… 128
小结………………………………………… 128
练习………………………………………… 129
5.7　使用重复元素的网络（VGG）………… 129
5.7.1　VGG块 …………………………… 129
5.7.2　VGG网络 … …………………… 129
5.7.3　训练模型… ……………………… 130
小结………………………………………… 131
练习………………………………………… 131
5.8　网络中的网络（NiN）… ……………… 131
5.8.1　NiN块 … ………………………… 131
5.8.2　NiN模型 … ……………………… 132
5.8.3　训练模型… ……………………… 133
小结………………………………………… 134
练习………………………………………… 134
5.9　含并行连结的网络（GoogLeNet）…… 134
5.9.1　Inception块 ……………………… 134
5.9.2　GoogLeNet模型 … …………… 135
5.9.3　训练模型 ………………………… 137
小结………………………………………… 137
练习………………………………………… 137
5.10　批量归一化……………………………… 138
5.10.1　批量归一化层 ………………… 138
5.10.2　从零开始实现 … ……………… 139
5.10.3　使用批量归一化层的LeNet … … 140
5.10.4　简洁实现 … …………………… 141
小结………………………………………… 142
练习………………………………………… 142
5.11　残差网络（ResNet） ……………… 143
5.11.1　残差块 …………………………… 143
5.11.2　ResNet模型… ………………… 145
5.11.3　训练模型………………………… 146
小结………………………………………… 146
练习………………………………………… 146
5.12　稠密连接网络（DenseNet）………… 147
5.12.1　稠密块 …………………………… 147
5.12.2　过渡层 … ……………………… 148
5.12.3　DenseNet模型 ………………… 148
5.12.4　训练模型 … …………………… 149
小结………………………………………… 149
练习………………………………………… 149
第6 章　循环神经网络… ……………… 150
6.1　语言模型………………………………… 150
6.1.1　语言模型的计算 … ……………… 151
6.1.2　n 元语法 … ……………………… 151
小结………………………………………… 152
练习………………………………………… 152
6.2　循环神经网络… ………………………… 152
6.2.1　不含隐藏状态的神经网络 … …… 152
6.2.2　含隐藏状态的循环神经网络… … 152
6.2.3　应用：基于字符级循环神经网络的语言模型 … ……………………… 154
小结………………………………………… 155
练习………………………………………… 155
6.3　语言模型数据集（歌词）…… 155
6.3.1　读取数据集 … …………………… 155
6.3.2　建立字符索引 …………………… 156
6.3.3　时序数据的采样 ………………… 156
小结………………………………………… 158
练习………………………………………… 159
6.4　循环神经网络的从零开始实现… ……… 159
6.4.1　one-hot向量 … ………………… 159
6.4.2　初始化模型参数 ………………… 160
6.4.3　定义模型 ………………………… 160
6.4.4　定义预测函数 …………………… 161
6.4.5　裁剪梯度 ………………………… 161
6.4.6　困惑度 …………………………… 162
6.4.7　定义模型训练函数 ……………… 162
6.4.8　训练模型并创作歌词 …………… 163
小结………………………………………… 164
练习………………………………………… 164
6.5　循环神经网络的简洁实现… …………… 165
6.5.1　定义模型 … ……………………… 165
6.5.2　训练模型 ………………………… 166
小结………………………………………… 168
练习………………………………………… 168
6.6　通过时间反向传播… …………………… 168
6.6.1　定义模型 … ……………………… 168
6.6.2　模型计算图 ……………………… 169
6.6.3　方法 ……………………………… 169
小结………………………………………… 170
练习………………………………………… 170
6.7　门控循环单元（GRU）………………… 170
6.7.1　门控循环单元 … ………………… 171
6.7.2　读取数据集 ……………………… 173
6.7.3　从零开始实现 …………………… 173
6.7.4　简洁实现 ………………………… 175
小结………………………………………… 176
练习………………………………………… 176
6.8　长短期记忆（LSTM）… ……………… 176
6.8.1　长短期记忆 … …………………… 176
6.8.2　读取数据集 ……………………… 179
6.8.3　从零开始实现 …………………… 179
6.8.4　简洁实现 ………………………… 181
小结………………………………………… 181
练习………………………………………… 182
6.9　深度循环神经网络… …………………… 182
小结………………………………………… 183
练习………………………………………… 183
6.10　双向循环神经网络……………………… 183
小结………………………………………… 184
练习………………………………………… 184
第7 章　优化算法… …………………… 185
7.1　优化与深度学习…………………………… 185
7.1.1　优化与深度学习的关系 … ……… 185
7.1.2　优化在深度学习中的挑战 … …… 186
小结………………………………………… 188
练习………………………………………… 189
7.2　梯度下降和随机梯度下降… …………… 189
7.2.1　一维梯度下降 … ………………… 189
7.2.2　学习率 …………………………… 190
7.2.3　多维梯度下降 …………………… 191
7.2.4　随机梯度下降 …………………… 193
小结………………………………………… 194
练习………………………………………… 194
7.3　小批量随机梯度下降… ………………… 194
7.3.1　读取数据集 … …………………… 195
7.3.2　从零开始实现 …………………… 196
7.3.3　简洁实现 ………………………… 198
小结………………………………………… 199
练习………………………………………… 199
7.4　动量法… …………………………………200
7.4.1　梯度下降的问题 … ……………… 200
7.4.2　动量法 …………………………… 201
・6・　目　　录
7.4.3　从零开始实现 …………………… 203
7.4.4　简洁实现 ………………………… 205
小结………………………………………… 205
练习………………………………………… 205
7.5　AdaGrad算法……………………………206
7.5.1　算法 … …………………………… 206
7.5.2　特点 ……………………………… 206
7.5.3　从零开始实现 …………………… 208
7.5.4　简洁实现 ………………………… 209
小结………………………………………… 209
练习………………………………………… 209
7.6　RMSProp算法… ………………………209
7.6.1　算法 … …………………………… 210
7.6.2　从零开始实现 …………………… 211
7.6.3　简洁实现 ………………………… 212
小结………………………………………… 212
练习………………………………………… 212
7.7　AdaDelta算法… ……………………… 212
7.7.1　算法… …………………………… 212
7.7.2　从零开始实现 …………………… 213
7.7.3　简洁实现 ………………………… 214
小结………………………………………… 214
练习………………………………………… 214
7.8　Adam算法… …………………………… 215
7.8.1　算法 … …………………………… 215
7.8.2　从零开始实现 …………………… 216
7.8.3　简洁实现 ………………………… 216
小结………………………………………… 217
练习………………………………………… 217
第8 章　计算性能… …………………… 218
8.1　命令式和符号式混合编程… …………… 218
8.1.1　混合式编程取两者之长 … ……… 220
8.1.2　使用HybridSequential类构造模型 … …………………………… 220
8.1.3　使用HybridBlock类构造模型… …………………………… 222
小结………………………………………… 224
练习………………………………………… 224
8.2　异步计算… ………………………………224
8.2.1　MXNet中的异步计算 …………… 224
8.2.2　用同步函数让前端等待计算结果 … …………………………… 226
8.2.3　使用异步计算提升计算性能 …… 226
8.2.4　异步计算对内存的影响 ………… 227
小结………………………………………… 229
练习………………………………………… 229
8.3　自动并行计算… …………………………229
8.3.1　CPU和GPU的并行计算 … …… 230
8.3.2　计算和通信的并行计算 ………… 231
小结………………………………………… 231
练习………………………………………… 231
8.4　多GPU计算……………………………… 232
8.4.1　数据并行 … ……………………… 232
8.4.2　定义模型 ………………………… 233
8.4.3　多GPU之间同步数据 … ……… 234
8.4.4　单个小批量上的多GPU训练 … …………………………… 236
8.4.5　定义训练函数 …………………… 236
8.4.6　多GPU训练实验 … …………… 237
小结………………………………………… 237
练习………………………………………… 237
8.5　多GPU计算的简洁实现………………… 237
8.5.1　多GPU上初始化模型参数……… 238
8.5.2　多GPU训练模型 … …………… 239
小结………………………………………… 241
练习………………………………………… 241
第9 章　计算机视觉… ………………… 242
9.1　图像增广…………………………………242
9.1.1　常用的图像增广方法 … ………… 243
9.1.2　使用图像增广训练模型 … ……… 246
小结………………………………………… 250
练习………………………………………… 250
9.2　微调… ……………………………………250
热狗识别 … ……………………………… 251
小结………………………………………… 255
练习………………………………………… 255
目　　录　・7・
9.3　目标检测和边界框… ……………………255
边界框 … ………………………………… 256
小结………………………………………… 257
练习………………………………………… 257
9.4　锚框… …………………………………… 257
9.4.1　生成多个锚框… ………………… 257
9.4.2　交并比 …………………………… 259
9.4.3　标注训练集的锚框 ……………… 260
9.4.4　输出预测边界框… ……………… 263
小结………………………………………… 265
练习………………………………………… 265
9.5　多尺度目标检测… ………………………265
小结………………………………………… 268
练习………………………………………… 268
9.6　目标检测数据集（皮卡丘）… …………268
9.6.1　获取数据集 … …………………… 269
9.6.2　读取数据集… …………………… 269
9.6.3　图示数据 ………………………… 270
小结………………………………………… 270
练习………………………………………… 271
9.7　单发多框检测（SSD）… ……………… 271
9.7.1　定义模型… ……………………… 271
9.7.2　训练模型 ………………………… 275
9.7.3　预测目标 ………………………… 277
小结………………………………………… 278
练习………………………………………… 278
9.8　区域卷积神经网络（R-CNN）系列……280
9.8.1　R-CNN … ……………………… 280
9.8.2　Fast R-CNN …………………… 281
9.8.3　Faster R-CNN ………………… 283
9.8.4　Mask R-CNN … ……………… 284
小结………………………………………… 285
练习………………………………………… 285
9.9　语义分割和数据集… ……………………285
9.9.1　图像分割和实例分割 … ………… 285
9.9.2　Pascal VOC2012语义分割数据集 … ………………………… 286
小结………………………………………… 290
练习………………………………………… 290
9.10　全卷积网络（FCN）… ………………290
9.10.1　转置卷积层 …………………… 291
9.10.2　构造模型 … …………………… 292
9.10.3　初始化转置卷积层……………… 294
9.10.4　读取数据集 … ………………… 295
9.10.5　训练模型………………………… 296
9.10.6　预测像素类别…………………… 296
小结………………………………………… 297
练习………………………………………… 297
9.11　样式迁移… ………………………………298
9.11.1　方法 ……………………………… 298
9.11.2　读取内容图像和样式图像……… 299
9.11.3　预处理和后处理图像 ………… 300
9.11.4　抽取特征 ……………………… 301
9.11.5　定义损失函数 ………………… 302
9.11.6　创建和初始化合成图像 ……… 303
9.11.7　训练模型………………………… 304
小结………………………………………… 306
练习………………………………………… 306
9.12　实战Kaggle比赛：图像
分类（CIFAR-10）……………………306
9.12.1　获取和整理数据集 ……………… 307
9.12.2　图像增广 … …………………… 310
9.12.3　读取数据集 … ………………… 310
9.12.4　定义模型………………………… 311
9.12.5　定义训练函数 … ……………… 312
9.12.6　训练模型 … …………………… 312
9.12.7　对测试集分类并在Kaggle
提交结果 … …………………… 313
小结………………………………………… 313
练习………………………………………… 313
9.13　实战Kaggle比赛：狗的品种
识别（ImageNet Dogs）…………… 314
9.13.1　获取和整理数据集 …………… 315
9.13.2　图像增广 … …………………… 316
9.13.3　读取数据集 … ………………… 317
9.13.4　定义模型 … …………………… 318
9.13.5　定义训练函数 … ……………… 318
9.13.6　训练模型 … …………………… 319
・8・　目　　录
9.13.7　对测试集分类并在Kaggle提交结果 … …………………… 319
小结………………………………………… 320
练习………………………………………… 320
第10 章　自然语言处理………………… 321
10.1　词嵌入（word2vec）………………… 321
10.1.1　为何不采用one-hot向量… …… 321
10.1.2　跳字模型 ………………………… 322
10.1.3　连续词袋模型 …………………… 323
小结………………………………………… 325
练习………………………………………… 325
10.2　近似训练…………………………………325
10.2.1　负采样 …………………………… 325
10.2.2　层序softmax …………………… 326
小结………………………………………… 327
练习………………………………………… 328
10.3　word2vec的实现………………………328
10.3.1　预处理数据集 …………………… 328
10.3.2　负采样 … ……………………… 331
10.3.3　读取数据集 … ………………… 331
10.3.4　跳字模型 … …………………… 332
10.3.5　训练模型 … …………………… 333
10.3.6　应用词嵌入模型 … …………… 335
小结………………………………………… 336
练习………………………………………… 336
10.4　子词嵌入（fastText）… ……………336
小结………………………………………… 337
练习………………………………………… 337
10.5　全局向量的词嵌入（GloVe）…………337
10.5.1　GloVe模型 …………………… 338
10.5.2　从条件概率比值理解GloVe模型……………………… 339
小结………………………………………… 340
练习………………………………………… 340
10.6　求近义词和类比词………………………340
10.6.1　使用预训练的词向量 ………… 340
10.6.2　应用预训练词向量 … ………… 341
小结………………………………………… 343
练习………………………………………… 343
10.7　文本情感分类：使用循环神经网络…… 343
10.7.1　文本情感分类数据集 ………… 343
10.7.2　使用循环神经网络的模型……… 345
小结………………………………………… 347
练习………………………………………… 347
10.8　文本情感分类：使用卷积神经网络（textCNN）… …………………347
10.8.1　一维卷积层 … ………………… 348
10.8.2　时序最大池化层 … …………… 349
10.8.3　读取和预处理IMDb数据集 … ……………………… 350
10.8.4　textCNN模型 … ……………… 350
小结………………………………………… 353
练习………………………………………… 353
10.9　编码器-解码器（seq2seq）…………353
10.9.1　编码器 ………………………… 354
10.9.2　解码器 … ……………………… 354
10.9.3　训练模型………………………… 355
小结………………………………………… 355
练习………………………………………… 355
10.10　 束搜索… ………………………………355
10.10.1　贪婪搜索 … …………………… 356
10.10.2　穷举搜索 ……………………… 357
10.10.3　束搜索 ………………………… 357
小结………………………………………… 358
练习………………………………………… 358
10.11　注意力机制… …………………………358
10.11.1　计算背景变量 … ……………… 359
10.11.2　更新隐藏状态 … ……………… 360
10.11.3　发展… ………………………… 361
小结………………………………………… 361
练习………………………………………… 361
10.12　机器翻译… …………………………… 361
10.12.1　读取和预处理数据集… ……… 361
10.12.2　含注意力机制的编码器-解码器 … …………… 363
10.12.3　训练模型 ……………………… 365
10.12.4　预测不定长的序列… ………… 367
10.12.5　评价翻译结果 ………………… 367
小结………………………………………… 369
练习………………………………………… 369
附录A　数学基础… …………………… 370
附录B　使用 Jupyter 记事本… ……… 376
附录C　使用 AWS 运行代码…………… 381
附录D　GPU 购买指南………………… 388
附录E　如何为本书做贡献… ………… 391
附录F　d2lzh 包索引…………………… 395
附录G　中英文术语对照表… ………… 397
参考文献………………………………… 402
索引……………………………………… 407
・ ・ ・ ・ ・ ・ (收起)前言
第1阶段 自动微分 1
步骤1 作为“箱子”的变量 3
1.1 什么是变量 3
1.2 实现Variable类 4
1.3 （补充）NumPy的多维数组 6
步骤2 创建变量的函数 8
2.1 什么是函数 8
2.2 Function类的实现 9
2.3 使用Function类 10
步骤3 函数的连续调用 13
3.1 Exp函数的实现 13
3.2 函数的连续调用 14
步骤4 数值微分 16
4.1 什么是导数 16
4.2 数值微分的实现 17
4.3 复合函数的导数 20
4.4 数值微分存在的问题 21
步骤5 反向传播的理论知识 22
5.1 链式法则 22
5.2 反向传播的推导 23
5.3 用计算图表示 25
步骤6 手动进行反向传播 27
6.1 Variable类的功能扩展 27
6.2 Function类的功能扩展 28
6.3 Square类和Exp类的功能扩展 28
6.4 反向传播的实现 29
步骤7 反向传播的自动化 32
7.1 为反向传播的自动化创造条件 33
7.2 尝试反向传播 36
7.3 增加backward方法 38
步骤8 从递归到循环 40
8.1 现在的Variable类 40
8.2 使用循环实现 41
8.3 代码验证 42
步骤9 让函数更易用 43
9.1 作为Python函数使用 43
9.2 简化backward方法 45
9.3 只支持ndarray 46
步骤10 测试 50
10.1 Python的单元测试 50
10.2 square函数反向传播的测试 52
10.3 通过梯度检验来自动测试 53
10.4 测试小结 54
第2阶段 用自然的代码表达 59
步骤11 可变长参数（正向传播篇) 61
11.1 修改Function类 62
11.2 Add类的实现 64
步骤12 可变长参数（改进篇) 65
12.1 第1项改进：使函数更容易使用 65
12.2 第2项改进：使函数更容易实现 67
12.3 add函数的实现 69
步骤13 可变长参数（反向传播篇) 70
13.1 支持可变长参数的Add类的反向传播 70
13.2 修改Variable类 71
13.3 Square类的实现 73
步骤14 重复使用同一个变量 75
14.1 问题的原因 76
14.2 解决方案 77
14.3 重置导数 79
步骤15 复杂的计算图（理论篇）81
15.1 反向传播的正确顺序 82
15.2 当前的DeZero 84
15.3 函数的优先级 87
步骤16 复杂的计算图（实现篇）88
16.1 增加“辈分”变量 88
16.2 按照“辈分”顺序取出元素 90
16.3 Variable类的backward 92
16.4 代码验证 93
步骤17 内存管理和循环引用 97
17.1 内存管理 97
17.2 引用计数方式的内存管理 98
17.3 循环引用 100
17.4 weakref模块 102
17.5 代码验证 104
步骤18 减少内存使用量的模式 106
18.1 不保留不必要的导数 106
18.2 回顾Function类 109
18.3 使用Confifig类进行切换 110
18.4 模式的切换 111
18.5 使用with语句切换 112
步骤19 让变量更易用 116
19.1 命名变量 116
19.2 实例变量ndarray 117
19.3 len函数和print函数 119
步骤20 运算符重载（1）122
20.1 Mul类的实现 122
20.2 运算符重载 125
步骤21 运算符重载（2）128
21.1 与ndarray一起使用 128
21.2 与flfloat和int一起使用 130
21.3 问题1：左项为flfloat或int的情况 131
21.4 问题2：左项为ndarray实例的情况 133
步骤22 运算符重载（3）134
22.1 负数 135
22.2 减法 136
22.3 除法 138
22.4 幂运算 139
步骤23 打包 141
23.1 文件结构 142
23.2 将代码移到核心类 142
23.3 运算符重载 144
23.4 实际的_ _init_ _.py文件 146
23.5 导入dezero 147
步骤24 复杂函数的求导 149
24.1 Sphere函数 150
24.2 matyas函数 151
24.3 GoldsteinPrice函数 152
第3阶段 实现高阶导数 161
步骤25 计算图的可视化（1） 163
25.1 安装Graphviz 163
25.2 使用DOT语言描述图形 165
25.3 指定节点属性 165
25.4 连接节点 167
步骤26 计算图的可视化（2）169
26.1 可视化代码的使用示例 169
26.2 从计算图转换为DOT语言 171
26.3 从DOT语言转换为图像 174
26.4 代码验证 176
步骤27 泰勒展开的导数 178
27.1 sin函数的实现 178
27.2 泰勒展开的理论知识 179
27.3 泰勒展开的实现 180
27.4 计算图的可视化 182
步骤28 函数优化 184
28.1 Rosenbrock函数 184
28.2 求导 185
28.3 梯度下降法的实现 186
步骤29 使用牛顿法进行优化（手动计算）190
29.1 使用牛顿法进行优化的理论知识 191
29.2 使用牛顿法实现优化 195
步骤30 高阶导数（准备篇） 197
30.1 确认工作①：Variable实例变量 197
30.2 确认工作②：Function类 199
30.3 确认工作③：Variable类的反向传播 201
步骤31 高阶导数（理论篇） 204
31.1 在反向传播时进行的计算 204
31.2 创建反向传播的计算图的方法 206
步骤32 高阶导数（实现篇） 209
32.1 新的DeZero 209
32.2 函数类的反向传播 210
32.3 实现更有效的反向传播（增加模式控制代码）211
32.4 修改_ _init_ _.py 213
步骤33 使用牛顿法进行优化（自动计算） 215
33.1 求二阶导数 215
33.2 使用牛顿法进行优化 217
步骤34 sin函数的高阶导数 219
34.1 sin函数的实现 219
34.2 cos函数的实现 220
34.3 sin函数的高阶导数 221
步骤35 高阶导数的计算图 225
35.1 tanh函数的导数 226
35.2 tanh函数的实现 226
35.3 高阶导数的计算图可视化 227
步骤36 DeZero的其他用途 234
36.1 double backprop的用途 234
36.2 深度学习研究中的应用示例 236
第4阶段 创建神经网络 243
步骤37 处理张量 245
37.1 对各元素进行计算 245
37.2 使用张量时的反向传播 247
37.3 使用张量时的反向传播（补充内容）249
步骤38 改变形状的函数 254
38.1 reshape函数的实现 254
38.2 从Variable对象调用reshape 258
38.3 矩阵的转置 259
38.4 实际的transpose函数（补充内容）262
步骤39 求和的函数 264
39.1 sum函数的反向传播 264
39.2 sum函数的实现 266
39.3 axis和keepdims 268
步骤40 进行广播的函数 272
40.1 broadcast_to函数和sum_to函数 272
40.2 DeZero的broadcast_to函数和sum_to函数 275
40.3 支持广播 277
步骤41 矩阵的乘积 280
41.1 向量的内积和矩阵的乘积 280
41.2 检查矩阵的形状 282
41.3 矩阵乘积的反向传播 282
步骤42 线性回归 288
42.1 玩具数据集 288
42.2 线性回归的理论知识 289
42.3 线性回归的实现 291
42.4 DeZero的mean_squared_error函数（补充内容） 295
步骤43 神经网络 298
43.1 DeZero中的linear函数 298
43.2 非线性数据集 301
43.3 激活函数和神经网络 302
43.4 神经网络的实现 303
步骤44 汇总参数的层 307
44.1 Parameter类的实现 307
44.2 Layer类的实现 309
44.3 Linear类的实现 312
44.4 使用Layer实现神经网络 314
步骤45 汇总层的层 316
45.1 扩展Layer类 316
45.2 Model类 319
45.3 使用Model来解决问题 321
45.4 MLP类 323
步骤46 通过Optimizer更新参数 325
46.1 Optimizer类 325
46.2 SGD类的实现 326
46.3 使用SGD类来解决问题 327
46.4 SGD以外的优化方法 328
步骤47 softmax函数和交叉熵误差 331
47.1 用于切片操作的函数 331
47.2 softmax函数 334
47.3 交叉熵误差 337
步骤48 多分类 340
48.1 螺旋数据集 340
48.2 用于训练的代码 341
步骤49 Dataset类和预处理 346
49.1 Dataset类的实现 346
49.2 大型数据集的情况 348
49.3 数据的连接 349
49.4 用于训练的代码 350
49.5 数据集的预处理 351
步骤50 用于取出小批量数据的DataLoader 354
50.1 什么是迭代器 354
50.2 使用DataLoader 358
50.3 accuracy函数的实现 359
50.4 螺旋数据集的训练代码 360
步骤51 MINST的训练 363
51.1 MNIST数据集 364
51.2 训练MNIST 366
51.3 改进模型 368
第5阶段 DeZero高级挑战 377
步骤52 支持GPU 379
52.1 CuPy的安装和使用方法 379
52.2 cuda模块 382
52.3 向Variable / Layer / DataLoader类添加代码 383
52.4 函数的相应修改 386
52.5 在GPU上训练MNIST 388
步骤53 模型的保存和加载 391
53.1 NumPy的save函数和load函数 391
53.2 Layer类参数的扁平化 394
53.3 Layer类的save函数和load函数 395
步骤54 Dropout和测试模式 398
54.1 什么是Dropout 398
54.2 Inverted Dropout 401
54.3 增加测试模式 401
54.4 Dropout的实现 402
步骤55 CNN的机制（1） 404
55.1 CNN的网络结构 404
55.2 卷积运算 405
55.3 填充 407
55.4 步幅 408
55.5 输出大小的计算方法 409
步骤56 CNN的机制（2）411
56.1 三阶张量 411
56.2 结合方块进行思考 412
56.3 小批量处理 414
56.4 池化层 415
步骤57 conv2d函数和pooling函数 418
57.1 使用im2col展开 418
57.2 conv2d函数的实现 420
57.3 Conv2d层的实现 425
57.4 pooling函数的实现 426
步骤58 具有代表性的CNN（VGG16）429
58.1 VGG16的实现 429
58.2 已训练的权重数据 431
58.3 使用已训练的VGG16 435
步骤59 使用RNN处理时间序列数据 438
59.1 RNN层的实现 438
59.2 RNN模型的实现 442
59.3 切断连接的方法 445
59.4 正弦波的预测 446
步骤60 LSTM与数据加载器 451
60.1 用于时间序列数据的数据加载器 451
60.2 LSTM层的实现 453
附录A inplace运算（步骤14的补充内容）463
A.1 问题确认 463
A.2 关于复制和覆盖 464
A.3 DeZero的反向传播 465
附录B 实现get_item函数（步骤47的补充内容）466
附录C 在Google Colaboratory上运行 469
后 记 473
参考文献 477
・ ・ ・ ・ ・ ・ (收起)推 荐 序 面对科技拐点，我们的判断与选择
中文版序 人工智能会放大认知能力
前 言 深度学习与智能的本质
第一部分 智能的新构想
01 机器学习的崛起
汽车新生态：无人驾驶将全面走入人们生活
自然语言翻译：从语言到句子的飞跃
语音识别：实时跨文化交流不再遥远
AI医疗：医学诊断将更加准确
金融科技：利用数据和算法获取最佳回报
深度法律：效率的提高与费用的降低
德州扑克：当机器智能学会了虚张声势
AlphaGo奇迹：神经科学与人工智能的协同
弗林效应：深度学习让人类更加智能
新教育体系：每个人都需要终身学习
正面影响：新兴技术不是生存威胁
回到未来：当人类智能遇到人工智能
02 人工智能的重生
看似简单的视觉识别
计算机视觉的进步
早期人工智能发展缓慢
从神经网络到人工智能
03 神经网络的黎明
深度学习的起点
从样本中学习
利用感知器区分性别
被低估的神经网络
04 大脑式的计算
网络模型能够模仿智能行为
神经网络先驱者
乔治・布尔与机器学习
利用神经科学理解大脑
大脑如何处理问题
计算神经科学的兴起
05 洞察视觉系统
人眼是如何看到东西的
大脑皮层中的视觉
突触的可塑性
通过阴影脑补立体全貌
视觉区域的层级结构
认知神经科学的诞生
第二部分 深度学习的演进
06 语音识别的突破
在嘈杂中找到你的声音
将独立分量分析应用于大脑
什么在操控我们的言行
07 霍普菲尔德网络和玻尔兹曼机
约翰・霍普菲尔德的伟大之处
内容可寻址存储器
局部最小值与全局最小值
玻尔兹曼机
赫布理论
学习识别镜像对称
学习识别手写数字
无监督学习和皮层发育
08 反向传播算法
算法的优化
语音合成的突破
神经网络的重生
理解真正的深度学习
神经网络的局限性
09 卷积学习
机器学习的稳步发展
卷积网络的渐进式改进
当深度学习遇到视觉层级结构
有工作记忆的神经网络
生成式对抗网络
应对现实社会的复杂性
10 奖励学习
机器如何学会下棋
大脑的奖励机制
用“感知-行动”框架提高绩效
学习如何翱翔
学习如何歌唱
人工智能的可塑性
更多需要被解决的问题
11 火爆的NIPS
为什么NIPS如此受欢迎
谁拥有最多数据，谁就是赢家
为未来做准备
第三部分 人类，智能与未来
12 智能时代
21世纪的生活
未来的身份认证
社交机器人的崛起
机器已经会识别人类面部表情
新技术改变教育方式
成为更好的学习者
训练你的大脑
智能商业
13 算法驱动
用算法把复杂问题简单化
理解、分析复杂系统
大脑的逻辑深度
尝试所有可能的策略
14 芯片崛起
神经形态芯片
视网膜芯片
神经形态工程
摩尔定律的终结
15 信息科学
用字节丈量世界
用数学思维解决通信难题
预测是如何产生的
深度理解大脑
大脑的操作系统
生物学与计算科学
人工智能能拥有媲美人类大脑的操作系统
16 生命与意识
视觉意识
视觉感知的过程
视觉感知的时机
视觉感知的部位
视觉搜索的机理
创造意识比理解意识更容易
17 进化的力量
大自然比我们聪明
认知科学的兴起
不能把语言问题只留给语言学家
难预测的行为规律
神经网络的寒冬
从深度学习到通用人工智能
18 深度智能
遗传密码
每个物种都有智能
进化的起源
人类终将解决智能难题
・ ・ ・ ・ ・ ・ (收起)第 1部分　PyTorch核心
第　1章 深度学习和PyTorch库简介　3
1．1　深度学习革命　4
1．2　PyTorch深度学习　5
1．3　为什么用PyTorch　6
1．4　PyTorch如何支持深度学习概述　8
1．5　硬件和软件要求　10
1．6　练习题　12
1．7　本章小结　13
第　2章 预训练网络　14
2．1　一个识别图像主体的预训练网络　15
2．1．1　获取一个预先训练好的网络用于图像识别　16
2．1．2　AlexNet　17
2．1．3　ResNet　19
2．1．4　准备运行　19
2．1．5　运行模型　21
2．2　一个足以以假乱真的预训练模型　23
2．2．1　GAN游戏　24
2．2．2　CycleGAN　25
2．2．3　一个把马变成斑马的网络　26
2．3　一个描述场景的预训练网络　29
2．4　Torch Hub　31
2．5　总结　32
2．6　练习题　32
2．7　本章小结　33
第3章　从张量开始　34
3．1　实际数据转为浮点数　34
3．2　张量：多维数组　36
3．2．1　从Python列表到PyTorch张量　36
3．2．2　构造第 1个张量　37
3．2．3　张量的本质　37
3．3　索引张量　40
3．4　命名张量　40
3．5　张量的元素类型　43
3．5．1　使用dtype指定数字类型　43
3．5．2　适合任何场合的dtype　44
3．5．3　管理张量的dtype属性　44
3．6　张量的API　45
3．7　张量的存储视图　46
3．7．1　索引存储区　47
3．7．2　修改存储值：就地操作　48
3．8　张量元数据：大小、偏移量和步长　48
3．8．1　另一个张量的存储视图　49
3．8．2　无复制转置　51
3．8．3　高维转置　52
3．8．4　连续张量　53
3．9　将张量存储到GPU　55
3．10　NumPy互操作性　57
3．11　广义张量也是张量　57
3．12　序列化张量　58
3．13　总结　60
3．14　练习题　60
3．15　本章小结　60
第4章　使用张量表征真实数据　61
4．1　处理图像　62
4．1．1　添加颜色通道　62
4．1．2　加载图像文件　63
4．1．3　改变布局　63
4．1．4　正规化数据　64
4．2　三维图像：体数据　65
4．3　表示表格数据　66
4．3．1　使用真实的数据集　67
4．3．2　加载葡萄酒数据张量　68
4．3．3　表示分数　70
4．3．4　独热编码　70
4．3．5　何时分类　72
4．3．6　寻找阈值　73
4．4　处理时间序列　75
4．4．1　增加时间维度　76
4．4．2　按时间段调整数据　77
4．4．3　准备训练　79
4．5　表示文本　81
4．5．1　将文本转化为数字　81
4．5．2　独热编码字符　82
4．5．3　独热编码整个词　83
4．5．4　文本嵌入　85
4．5．5　作为蓝图的文本嵌入　87
4．6　总结　88
4．7　练习题　88
4．8　本章小结　88
第5章　学习的机制　90
5．1　永恒的建模经验　90
5．2　学习就是参数估计　92
5．2．1　一个热点问题　93
5．2．2　收集一些数据　93
5．2．3　可视化数据　94
5．2．4　选择线性模型首试　94
5．3　减少损失是我们想要的　95
5．4　沿着梯度下降　98
5．4．1　减小损失　99
5．4．2　进行分析　99
5．4．3　迭代以适应模型　101
5．4．4　归一化输入　104
5．4．5　再次可视化数据　106
5．5　PyTorch自动求导：反向传播的一切　107
5．5．1　自动计算梯度　107
5．5．2　优化器　111
5．5．3　训练、验证和过拟合　115
5．5．4　自动求导更新及关闭　120
5．6　总结　121
5．7　练习题　122
5．8　本章小结　122
第6章　使用神经网络拟合数据　123
6．1　人工神经网络　124
6．1．1　组成一个多层网络　125
6．1．2　理解误差函数　125
6．1．3　我们需要的只是激活函数　126
6．1．4　更多激活函数　128
6．1．5　选择最佳激活函数　128
6．1．6　学习对于神经网络意味着什么　129
6．2　PyTorch nn模块　131
6．2．1　使用__call__()而不是forward()　132
6．2．2　回到线性模型　133
6．3　最终完成一个神经网络　137
6．3．1　替换线性模型　137
6．3．2　检查参数　138
6．3．3　与线性模型对比　141
6．4　总结　142
6．5　练习题　142
6．6　本章小结　142
第7章　区分鸟和飞机：从图像学习　143
7．1　微小图像数据集　143
7．1．1　下载CIFAR-10　144
7．1．2　Dataset类　145
7．1．3　Dataset变换　146
7．1．4　数据归一化　149
7．2　区分鸟和飞机　150
7．2．1　构建数据集　151
7．2．2　一个全连接模型　152
7．2．3　分类器的输出　153
7．2．4　用概率表示输出　154
7．2．5　分类的损失　157
7．2．6　训练分类器　159
7．2．7　全连接网络的局限　165
7．3　总结　167
7．4　练习题　167
7．5　本章小结　168
第8章　使用卷积进行泛化　169
8．1　卷积介绍　169
8．2　卷积实战　172
8．2．1　填充边界　173
8．2．2　用卷积检测特征　175
8．2．3　使用深度和池化技术进一步研究　177
8．2．4　为我们的网络整合一切　179
8．3　子类化nn．Module　181
8．3．1　将我们的网络作为一个nn．Module　182
8．3．2　PyTorch如何跟踪参数和子模块　183
8．3．3　函数式API　184
8．4　训练我们的convnet　185
8．4．1　测量精度　187
8．4．2　保存并加载我们的模型　188
8．4．3　在GPU上训练　188
8．5　模型设计　190
8．5．1　增加内存容量：宽度　191
8．5．2　帮助我们的模型收敛和泛化：正则化　192
8．5．3　深入学习更复杂的结构：深度　195
8．5．4　本节设计的比较　200
8．5．5　已经过时了　201
8．6　总结　201
8．7　练习题　201
8．8　本章小结　202
第　2部分 从现实世界的图像中学习：肺癌的早期检测
第9章　使用PyTorch来检测癌症　205
9．1　用例简介　205
9．2　为一个大型项目做准备　206
9．3　到底什么是CT扫描　207
9．4　项目：肺癌的端到端检测仪　210
9．4．1　为什么我们不把数据扔给神经网络直到它起作用呢　213
9．4．2　什么是结节　216
9．4．3　我们的数据来源：LUNA大挑战赛　217
9．4．4　下载LUNA数据集　218
9．5　总结　219
9．6　本章小结　219
第　10章 将数据源组合成统一的数据集　220
10．1　原始CT数据文件　222
10．2　解析LUNA的标注 数据　222
10．2．1　训练集和验证集　224
10．2．2　统一标注和候选 数据　225
10．3　加载单个CT扫描　227
10．4　使用病人坐标系定位结节　230
10．4．1　病人坐标系　230
10．4．2　CT扫描形状和体素大小　232
10．4．3　毫米和体素地址之间的转换　233
10．4．4　从CT扫描中取出一个结节　234
10．5　一个简单的数据集实现　235
10．5．1　使用getCtRawCandidate()函数缓存候选数组　238
10．5．2　在LunaDataset．__init__()中构造我们的数据集　238
10．5．3　分隔训练集和验证集　239
10．5．4　呈现数据　240
10．6　总结　241
10．7　练习题　241
10．8　本章小结　242
第　11章 训练分类模型以检测可疑肿瘤　243
11．1　一个基本的模型和训练循环　243
11．2　应用程序的主入口点　246
11．3　预训练和初始化　247
11．3．1　初始化模型和优化器　247
11．3．2　数据加载器的维护和供给　249
11．4　我们的首次神经网络设计　251
11．4．1　核心卷积　251
11．4．2　完整模型　254
11．5　训练和验证模型　257
11．5．1　computeBatchLoss()函数　258
11．5．2　类似的验证循环　260
11．6　输出性能指标　261
11．7　运行训练脚本　265
11．7．1　训练所需的数据　266
11．7．2　插曲：enumerateWithEstimate()函数　266
11．8　评估模型：得到99．7%的正确率是否意味着我们完成了任务　268
11．9　用TensorBoard绘制训练指标　269
11．9．1　运行TensorBoard　269
11．9．2　增加TensorBoard对指标记录函数的支持　272
11．10　为什么模型不学习检测结节　274
11．11　总结　275
11．12　练习题　275
11．13　本章小结　275
第　12章 通过指标和数据增强来提升训练　277
12．1　高级改进计划　278
12．2　好狗与坏狗：假阳性与假阴性　279
12．3　用图表表示阳性与阴性　280
12．3．1　召回率是Roxie的强项　282
12．3．2　精度是Preston的强项　283
12．3．3　在logMetrics()中实现精度和召回率　284
12．3．4　我们的终极性能指标：F1分数　285
12．3．5　我们的模型在新指标下表现如何　289
12．4　理想的数据集是什么样的　290
12．4．1　使数据看起来更理想化　292
12．4．2　使用平衡的LunaDataset与之前的数据集运行情况对比　296
12．4．3　认识过拟合　298
12．5　重新审视过拟合的问题　300
12．6　通过数据增强防止过拟合　300
12．6．1　具体的数据增强技术　301
12．6．2　看看数据增强带来的改进　306
12．7　总结　308
12．8　练习题　308
12．9　本章小结　309
第　13章 利用分割法寻找可疑结节　310
13．1　向我们的项目添加第 2个模型　310
13．2　各种类型的分割　312
13．3　语义分割：逐像素分类　313
13．4　更新分割模型　317
13．5　更新数据集以进行分割　319
13．5．1　U-Net有非常具体的对输入大小的要求　320
13．5．2　U-Net对三维和二维数据的权衡　320
13．5．3　构建真实、有效的数据集　321
13．5．4　实现Luna2dSegmentationDataset　327
13．5．5　构建训练和验证数据　331
13．5．6　实现TrainingLuna2dSegmentationDataset　332
13．5．7　在GPU上增强数据　333
13．6　更新用于分割的训练脚本　335
13．6．1　初始化分割和增强模型　336
13．6．2　使用Adam优化器　336
13．6．3　骰子损失　337
13．6．4　将图像导入TensorBoard　340
13．6．5　更新指标日志　343
13．6．6　保存模型　344
13．7　结果　345
13．8　总结　348
13．9　练习题　348
13．10　本章小结　349
第　14章 端到端的结节分析及下一步的方向　350
14．1　接近终点线　350
14．2　验证集的独立性　352
14．3　连接CT分割和候选结节分类　353
14．3．1　分割　354
14．3．2　将体素分组为候选结节　355
14．3．3　我们发现结节了吗？分类以减少假阳性　357
14．4　定量验证　360
14．5　预测恶性肿瘤　361
14．5．1　获取恶性肿瘤信息　361
14．5．2　曲线基线下的区域：按直径分类　362
14．5．3　重用预先存在的权重：微调　365
14．5．4　TensorBoard中的输出　370
14．6　在诊断时所见的内容　374
14．7　接下来呢？其他灵感和数据的来源　376
14．7．1　防止过拟合：更好的正则化　377
14．7．2　精细化训练数据　379
14．7．3　竞赛结果及研究论文　380
14．8　总结　381
14．9　练习题　382
14．10　本章小结　383
第3部分　部署
第　15章 部署到生产环境　387
15．1　PyTorch模型的服务　388
15．1．1　支持Flask服务的模型　388
15．1．2　我们想从部署中得到的东西　390
15．1．3　批处理请求　391
15．2　导出模型　395
15．2．1　PyTorch与ONNX的互操作性　396
15．2．2　PyTorch自己的导出：跟踪　397
15．2．3　具有跟踪模型的服务器　398
15．3　与PyTorch JIT编译器交互　398
15．3．1　超越经典Python/PyTorch的期望是什么　399
15．3．2　PyTorch作为接口和后端的双重特性　400
15．3．3　TorchScript　400
15．3．4　为可追溯的差异编写脚本　404
15．4　LibTorch：C++中的PyTorch　405
15．4．1　从C++中运行JITed模型　405
15．4．2　从C++ API开始　408
15．5　部署到移动设备　411
15．6　新兴技术：PyTorch
模型的企业服务　416
15．7　总结　416
15．8　练习题　416
15．9　本章小结　416
・ ・ ・ ・ ・ ・ (收起)第1章 互联网的增长引擎――推荐系统
1.1 为什么推荐系统是互联网的增长引擎
1.1.1 推荐系统的作用和意义
1.1.2 推荐系统与YouTube的观看时长增长
1.1.3 推荐系统与电商网站的收入增长
1.2 推荐系统的架构
1.2.1 推荐系统的逻辑框架
1.2.2 推荐系统的技术架构
1.2.3 推荐系统的数据部分
1.2.4 推荐系统的模型部分
1.2.5 深度学习对推荐系统的革命性贡献
1.2.6 把握整体，补充细节
1.3 本书的整体结构
第2章 前深度学习时代――推荐系统的进化之路
2.1 传统推荐模型的演化关系图
2.2 协同过滤――经典的推荐算法
2.2.1 什么是协同过滤
2.2.2 用户相似度计算
2.2.3 终结果的排序
2.2.4 ItemCF
2.2.5 UserCF与ItemCF的应用场景
2.2.6 协同过滤的下一步发展
2.3 矩阵分解算法――协同过滤的进化
2.3.1 矩阵分解算法的原理
2.3.2 矩阵分解的求解过程
2.3.3 消除用户和物品打分的偏差
2.3.4 矩阵分解的优点和局限性
2.4 逻辑回归――融合多种特征的推荐模型
2.4.1 基于逻辑回归模型的推荐流程
2.4.2 逻辑回归模型的数学形式
2.4.3 逻辑回归模型的训练方法
2.4.4 逻辑回归模型的优势
2.4.5 逻辑回归模型的局限性
2.5 从FM到FFM――自动特征交叉的解决方案
2.5.1 POLY2模型――特征交叉的开始
2.5.2 FM模型――隐向量特征交叉
2.5.3 FFM模型――引入特征域的概念
2.5.4 从POLY2到FFM的模型演化过程
2.6 GBDT+LR――特征工程模型化的开端
2.6.1 GBDT+LR组合模型的结构
2.6.2 GBDT进行特征转换的过程
2.6.3 GBDT+LR 组合模型开启的特征工程新趋势
2.7 LS-PLM――阿里巴巴曾经的主流推荐模型
2.7.1 LS-PLM 模型的主要结构
2.7.2 LS-PLM模型的优点
2.7.3 从深度学习的角度重新审视LS-PLM模型
2.8 总结――深度学习推荐系统的前夜
第3章 浪潮之巅――深度学习在推荐系统中的应用
3.1 深度学习推荐模型的演化关系图
3.2 AutoRec――单隐层神经网络推荐模型
3.2.1 AutoRec模型的基本原理
3.2.2 AutoRec模型的结构
3.2.3 基于AutoRec模型的推荐过程
3.2.4 AutoRec模型的特点和局限性
3.3 Deep Crossing模型――经典的深度学习架构
3.3.1 Deep Crossing模型的应用场景
3.3.2 Deep Crossing模型的网络结构
3.3.3 Deep Crossing模型对特征交叉方法的革命
3.4 NeuralCF模型――CF与深度学习的结合
3.4.1 从深度学习的视角重新审视矩阵分解模型
3.4.2 NeuralCF模型的结构
3.4.3 NeuralCF模型的优势和局限性
3.5 PNN模型――加强特征交叉能力
3.5.1 PNN模型的网络架构
3.5.2 Product层的多种特征交叉方式
3.5.3 PNN模型的优势和局限性
3.6 Wide&Deep 模型――记忆能力和泛化能力的综合
3.6.1 模型的记忆能力与泛化能力
3.6.2 Wide&Deep模型的结构
3.6.3 Wide&Deep模型的进化――Deep&Cross模型
3.6.4 Wide&Deep模型的影响力
3.7 FM与深度学习模型的结合
3.7.1 FNN――用FM的隐向量完成Embedding层初始化
3.7.2 DeepFM――用FM代替Wide部分
3.7.3 NFM――FM的神经网络化尝试
3.7.4 基于FM的深度学习模型的优点和局限性
3.8 注意力机制在推荐模型中的应用
3.8.1 AFM――引入注意力机制的FM
3.8.2 DIN――引入注意力机制的深度学习网络
3.8.3 注意力机制对推荐系统的启发
3.9 DIEN――序列模型与推荐系统的结合
3.9.1 DIEN的“进化”动机
3.9.2 DIEN模型的架构
3.9.3 兴趣抽取层的结构
3.9.4 兴趣进化层的结构
3.9.5 序列模型对推荐系统的启发
3.10 强化学习与推荐系统的结合
3.10.1 深度强化学习推荐系统框架
3.10.2 深度强化学习推荐模型
3.10.3 DRN的学习过程
3.10.4 DRN的在线学习方法――竞争梯度下降算法
3.10.5 强化学习对推荐系统的启发
3.11 总结――推荐系统的深度学习时代
第4章 Embedding技术在推荐系统中的应用
4.1 什么是Embedding
4.1.1 词向量的例子
4.1.2 Embedding 技术在其他领域的扩展
4.1.3 Embedding 技术对于深度学习推荐系统的重要性
4.2 Word2vec――经典的Embedding方法
4.2.1 什么是Word2vec
4.2.2 Word2vec模型的训练过程
4.2.3 Word2vec的“负采样”训练方法
4.2.4 Word2vec对Embedding技术的奠基性意义
4.3 Item2vec――Word2vec 在推荐系统领域的推广
4.3.1 Item2vec的基本原理
4.3.2 “广义”的Item2vec
4.3.3 Item2vec方法的特点和局限性
4.4 Graph Embedding――引入更多结构信息的图嵌入技术
4.4.1 DeepWalk――基础的Graph Embedding方法
4.4.2 Node2vec――同质性和结构性的权衡
4.4.3 EGES――阿里巴巴的综合性Graph Embedding方法
4.5 Embedding与深度学习推荐系统的结合
4.5.1 深度学习网络中的Embedding层
4.5.2 Embedding的预训练方法
4.5.3 Embedding作为推荐系统召回层的方法
4.6 局部敏感哈希――让Embedding插上翅膀的快速搜索方法
4.6.1 “快速”Embedding近邻搜索
4.6.2 局部敏感哈希的基本原理
4.6.3 局部敏感哈希多桶策略
4.7 总结――深度学习推荐系统的核心操作
第5章 多角度审视推荐系统
5.1 推荐系统的特征工程
5.1.1 构建推荐系统特征工程的原则
5.1.2 推荐系统中的常用特征
5.1.3 常用的特征处理方法
5.1.4 特征工程与业务理解
5.2 推荐系统召回层的主要策略
5.2.1 召回层和排序层的功能特点
5.2.2 多路召回策略
5.2.3 基于Embedding的召回方法
5.3 推荐系统的实时性
5.3.1 为什么说推荐系统的实时性是重要的
5.3.2 推荐系统“特征”的实时性
5.3.3 推荐系统“模型”的实时性
5.3.4 用“木桶理论”看待推荐系统的迭代升级
5.4 如何合理设定推荐系统中的优化目标
5.4.1 YouTube以观看时长为优化目标的合理性
5.4.2 模型优化和应用场景的统一性
5.4.3 优化目标是和其他团队的接口性工作
5.5 推荐系统中比模型结构更重要的是什么
5.5.1 有解决推荐问题的“银弹”吗
5.5.2 Netflix对用户行为的观察
5.5.3 观察用户行为，在模型中加入有价值的用户信息
5.5.4 DIN模型的改进动机
5.5.5 算法工程师不能只是一个“炼金术士”
5.6 冷启动的解决办法
5.6.1 基于规则的冷启动过程
5.6.2 丰富冷启动过程中可获得的用户和物品特征
5.6.3 利用主动学习、迁移学习和“探索与利用”机制
5.6.4 “巧妇难为无米之炊”的困境
5.7 探索与利用
5.7.1 传统的探索与利用方法
5.7.2 个性化的探索与利用方法
5.7.3 基于模型的探索与利用方法
5.7.4 “探索与利用”机制在推荐系统中的应用
第6章 深度学习推荐系统的工程实现
6.1 推荐系统的数据流
6.1.1 批处理大数据架构
6.1.2 流计算大数据架构
6.1.3 Lambda架构
6.1.4 Kappa架构
6.1.5 大数据平台与推荐系统的整合
6.2 推荐模型离线训练之Spark MLlib
6.2.1 Spark的分布式计算原理
6.2.2 Spark MLlib的模型并行训练原理
6.2.3 Spark MLlib并行训练的局限性
6.3 推荐模型离线训练之Parameter Server
6.3.1 Parameter Server的分布式训练原理
6.3.2 一致性与并行效率之间的取舍
6.3.3 多server节点的协同和效率问题
6.3.4 Parameter Server技术要点总结
6.4 推荐模型离线训练之TensorFlow
6.4.1 TensorFlow的基本原理
6.4.2 TensorFlow基于任务关系图的并行训练过程
6.4.3 TensorFlow的单机训练与分布式训练模式
6.4.4 TensorFlow技术要点总结
6.5 深度学习推荐模型的上线部署
6.5.1 预存推荐结果或Embedding结果
6.5.2 自研模型线上服务平台
6.5.3 预训练Embedding+轻量级线上模型
6.5.4 利用PMML转换并部署模型
6.5.5 TensorFlow Serving
6.5.6 灵活选择模型服务方法
6.6 工程与理论之间的权衡
6.6.1 工程师职责的本质
6.6.2 Redis容量和模型上线方式之间的权衡
6.6.3 研发周期限制和技术选型的权衡
6.6.4 硬件平台环境和模型结构间的权衡
6.6.5 处理好整体和局部的关系
第7章 推荐系统的评估
7.1 离线评估方法与基本评价指标
7.1.1 离线评估的主要方法
7.1.2 离线评估的指标
7.2 直接评估推荐序列的离线指标
7.2.1 P-R曲线
7.2.2 ROC曲线
7.2.3 平均精度均值
7.2.4 合理选择评估指标
7.3 更接近线上环境的离线评估方法――Replay
7.3.1 模型评估的逻辑闭环
7.3.2 动态离线评估方法
7.3.3 Netflix的Replay评估方法实践
7.4 A/B测试与线上评估指标
7.4.1 什么是A/B测试
7.4.2 A/B测试的“分桶”原则
7.4.3 线上A/B测试的评估指标
7.5 快速线上评估方法――Interleaving
7.5.1 传统A/B测试存在的统计学问题
7.5.2 Interleaving方法的实现
7.5.3 Interleaving方法与传统A/B测试的灵敏度比较
7.5.4 Interleaving方法指标与A/B测试指标的相关性
7.5.5 Interleaving方法的优点与缺点
7.6 推荐系统的评估体系
第8章 深度学习推荐系统的前沿实践
8.1 Facebook的深度学习推荐系统
8.1.1 推荐系统应用场景
8.1.2 以GBDT+LR组合模型为基础的CTR预估模型
8.1.3 实时数据流架构
8.1.4 降采样和模型校正
8.1.5 Facebook GBDT+LR组合模型的工程实践
8.1.6 Facebook的深度学习模型DLRM
8.1.7 DLRM模型并行训练方法
8.1.8 DLRM模型的效果
8.1.9 Facebook深度学习推荐系统总结
8.2 Airbnb基于Embedding的实时搜索推荐系统
8.2.1 推荐系统应用场景
8.2.2 基于短期兴趣的房源Embedding方法
8.2.3 基于长期兴趣的用户Embedding和房源Embedding
8.2.4 Airbnb搜索词的Embedding
8.2.5 Airbnb的实时搜索排序模型及其特征工程
8.2.6 Airbnb实时搜索推荐系统总结
8.3 YouTube深度学习视频推荐系统
8.3.1 推荐系统应用场景
8.3.2 YouTube推荐系统架构
8.3.3 候选集生成模型
8.3.4 候选集生成模型独特的线上服务方法
8.3.5 排序模型
8.3.6 训练和测试样本的处理
8.3.7 如何处理用户对新视频的偏好
8.3.8 YouTube深度学习视频推荐系统总结
8.4 阿里巴巴深度学习推荐系统的进化
8.4.1 推荐系统应用场景
8.4.2 阿里巴巴的推荐模型体系
8.4.3 阿里巴巴深度学习推荐模型的进化过程
8.4.4 模型服务模块的技术架构
8.4.5 阿里巴巴推荐技术架构总结
第9章 构建属于你的推荐系统知识框架
9.1 推荐系统的整体知识架构图
9.2 推荐模型发展的时间线
9.3 如何成为一名优秀的推荐工程师
9.3.1 推荐工程师的4项能力
9.3.2 能力的深度和广度
9.3.3 推荐工程师的能力总结
后记
・ ・ ・ ・ ・ ・ (收起)第1 章绪论1
1.1 简介2
1.2 图深度学习的动机2
1.3 本书内容4
1.4 本书读者定位6
1.5 图特征学习的简要发展史7
1.5.1 图特征选择8
1.5.2 图表示学习9
1.6 小结10
1.7 扩展阅读11
第1 篇基础理论
第2 章图论基础15
2.1 简介16
2.2 图的表示16
2.3 图的性质17
2.3.1 度17
2.3.2 连通度19
2.3.3 中心性21
2.4 谱图论24
2.4.1 拉普拉斯矩阵24
2.4.2 拉普拉斯矩阵的特征值和特征向量26
2.5 图信号处理27
2.6 复杂图30
2.6.1 异质图30
2.6.2 二分图30
2.6.3 多维图31
2.6.4 符号图32
2.6.5 超图33
2.6.6 动态图33
2.7 图的计算任务34
2.7.1 侧重于节点的任务35
2.7.2 侧重于图的任务36
2.8 小结37
2.9 扩展阅读37
第3 章深度学习基础39
3.1 简介40
3.2 深度前馈神经网络41
3.2.1 网络结构42
3.2.2 激活函数43
3.2.3 输出层和损失函数45
3.3 卷积神经网络47
3.3.1 卷积操作和卷积层48
3.3.2 实际操作中的卷积层51
3.3.3 非线性激活层52
3.3.4 池化层53
3.3.5 卷积神经网络总体框架53
3.4 循环神经网络54
3.4.1 传统循环神经网络的网络结构55
3.4.2 长短期记忆网络56
3.4.3 门控循环单元58
3.5 自编码器59
3.5.1 欠完备自编码器59
3.5.2 正则化自编码器60
3.6 深度神经网络的训练61
3.6.1 梯度下降61
3.6.2 反向传播62
3.6.3 预防过拟合64
3.7 小结65
3.8 扩展阅读65
第2 篇模型方法
第4 章图嵌入69
4.1 简介70
4.2 简单图的图嵌入71
4.2.1 保留节点共现71
4.2.2 保留结构角色80
4.2.3 保留节点状态83
4.2.4 保留社区结构84
4.3 复杂图的图嵌入86
4.3.1 异质图嵌入87
4.3.2 二分图嵌入89
4.3.3 多维图嵌入90
4.3.4 符号图嵌入91
4.3.5 超图嵌入93
4.3.6 动态图嵌入95
4.4 小结96
4.5 扩展阅读97
第5 章图神经网络99
5.1 简介100
5.2 图神经网络基本框架102
5.2.1 侧重于节点的任务的图神经网络框架102
5.2.2 侧重于图的任务的图神经网络框架103
5.3 图滤波器104
5.3.1 基于谱的图滤波器104
5.3.2 基于空间的图滤波器114
5.4 图池化120
5.4.1 平面图池化120
5.4.2 层次图池化121
5.5 图卷积神经网络的参数学习125
5.5.1 节点分类中的参数学习126
5.5.2 图分类中的参数学习126
5.6 小结127
5.7 扩展阅读128
第6 章图神经网络的健壮性129
6.1 简介130
6.2 图对抗攻击130
6.2.1 图对抗攻击的分类131
6.2.2 白盒攻击132
6.2.3 灰盒攻击135
6.2.4 黑盒攻击139
6.3 图对抗防御142
6.3.1 图对抗训练142
6.3.2 图净化144
6.3.3 图注意力机制144
6.3.4 图结构学习148
6.4 小结149
6.5 扩展阅读149
第7 章可扩展图神经网络151
7.1 简介152
7.2 逐点采样法155
7.3 逐层采样法158
7.4 子图采样法162
7.5 小结164
7.6 扩展阅读164
第8 章复杂图神经网络165
8.1 简介166
8.2 异质图神经网络166
8.3 二分图神经网络168
8.4 多维图神经网络168
8.5 符号图神经网络170
8.6 超图神经网络173
8.7 动态图神经网络174
8.8 小结175
8.9 扩展阅读175
第9 章图上的其他深度模型177
9.1 简介178
9.2 图上的自编码器178
9.3 图上的循环神经网络180
9.4 图上的变分自编码器182
9.4.1 用于节点表示学习的变分自编码器184
9.4.2 用于图生成的变分自编码器184
9.4.3 编码器：推论模型185
9.4.4 解码器: 生成模型186
9.4.5 重建的损失函数186
9.5 图上的生成对抗网络187
9.5.1 用于节点表示学习的生成对抗网络188
9.5.2 用于图生成的生成对抗网络189
9.6 小结191
9.7 扩展阅读191
第3 篇实际应用
第10 章自然语言处理中的图神经网络195
10.1 简介196
10.2 语义角色标注196
10.3 神经机器翻译199
10.4 关系抽取199
10.5 问答系统200
10.5.1 多跳问答任务201
10.5.2 Entity-GCN 202
10.6 图到序列学习203
10.7 知识图谱中的图神经网络205
10.7.1 知识图谱中的图滤波205
10.7.2 知识图谱到简单图的转换206
10.7.3 知识图谱补全207
10.8 小结208
10.9 扩展阅读208
第11 章计算机视觉中的图神经网络209
11.1 简介210
11.2 视觉问答210
11.2.1 图像表示为图211
11.2.2 图像和问题表示为图212
11.3 基于骨架的动作识别214
11.4 图像分类215
11.4.1 零样本图像分类216
11.4.2 少样本图像分类217
11.4.3 多标签图像分类218
11.5 点云学习219
11.6 小结220
11.7 扩展阅读220
第12 章数据挖掘中的图神经网络221
12.1 简介222
12.2 万维网数据挖掘222
12.2.1 社交网络分析222
12.2.2 推荐系统225
12.3 城市数据挖掘229
12.3.1 交通预测229
12.3.2 空气质量预测231
12.4 网络安全数据挖掘231
12.4.1 恶意账户检测231
12.4.2 虚假新闻检测233
12.5 小结234
12.6 扩展阅读234
第13 章生物化学和医疗健康中的
图神经网络235
13.1 简介236
13.2 药物开发与发现236
13.2.1 分子表示学习236
13.2.2 蛋白质相互作用界面预测237
13.2.3 药物C靶标结合亲和力预测239
13.3 药物相似性整合240
13.4 复方药物副作用预测242
13.5 疾病预测244
13.6 小结245
13.7 扩展阅读245
第4 篇前沿进展
第14 章图神经网络的高级方法249
14.1 简介250
14.2 深层图神经网络250
14.2.1 Jumping Knowledge 252
14.2.2 DropEdge 253
14.2.3 PairNorm 253
14.3 通过自监督学习探索未标记数据253
14.3.1 侧重于节点的任务254
14.3.2 侧重于图的任务256
14.4 图神经网络的表达能力257
14.4.1 WL 测试258
14.4.2 表达能力259
14.5 小结260
14.6 扩展阅读260
第15 章图神经网络的高级应用261
15.1 简介262
15.2 图的组合优化262
15.3 学习程序表示264
15.4 物理学中相互作用的动力系统推断265
15.5 小结266
15.6 扩展阅读266
参考文献267
索引295
・ ・ ・ ・ ・ ・ (收起)Contents 目　　录
前言
第一部分　PyTorch基础
第1章　Numpy基础2
1.1　生成Numpy数组3
1.1.1　从已有数据中创建数组3
1.1.2　利用random模块生成数组4
1.1.3　创建特定形状的多维数组5
1.1.4　利用arange、linspace函数生成数组6
1.2　获取元素7
1.3　Numpy的算术运算9
1.3.1　对应元素相乘9
1.3.2　点积运算10
1.4　数组变形11
1.4.1　更改数组的形状11
1.4.2　合并数组14
1.5　批量处理16
1.6　通用函数17
1.7　广播机制19
1.8　小结20
第2章　PyTorch基础21
2.1　为何选择PyTorch？21
2.2　安装配置22
2.2.1　安装CPU版PyTorch22
2.2.2　安装GPU版PyTorch24
2.3　Jupyter Notebook环境配置26
2.4　Numpy与Tensor28
2.4.1　Tensor概述28
2.4.2　创建Tensor28
2.4.3　修改Tensor形状30
2.4.4　索引操作31
2.4.5　广播机制32
2.4.6　逐元素操作32
2.4.7　归并操作33
2.4.8　比较操作34
2.4.9　矩阵操作35
2.4.10　PyTorch与Numpy比较35
2.5　Tensor与Autograd36
2.5.1　自动求导要点36
2.5.2　计算图37
2.5.3　标量反向传播38
2.5.4　非标量反向传播39
2.6　使用Numpy实现机器学习41
2.7　使用Tensor及Antograd实现机器学习44
2.8　使用TensorFlow架构46
2.9　小结48
第3章　PyTorch神经网络工具箱49
3.1　神经网络核心组件49
3.2　实现神经网络实例50
3.2.1　背景说明51
3.2.2　准备数据52
3.2.3　可视化源数据53
3.2.4　构建模型53
3.2.5　训练模型54
3.3　如何构建神经网络？56
3.3.1　构建网络层56
3.3.2　前向传播57
3.3.3　反向传播57
3.3.4　训练模型58
3.4　神经网络工具箱nn58
3.4.1　nn.Module58
3.4.2　nn.functional58
3.5　优化器59
3.6　动态修改学习率参数60
3.7　优化器比较60
3.8　小结62
第4章　PyTorch数据处理工具箱63
4.1　数据处理工具箱概述63
4.2　utils.data简介64
4.3　torchvision简介66
4.3.1　transforms67
4.3.2　ImageFolder67
4.4　可视化工具69
4.4.1　tensorboardX简介69
4.4.2　用tensorboardX可视化神经网络71
4.4.3　用tensorboardX可视化损失值72
4.4.4　用tensorboardX可视化特征图73
4.5　本章小结74
第二部分　深度学习基础
第5章　机器学习基础76
5.1　机器学习的基本任务76
5.1.1　监督学习77
5.1.2　无监督学习77
5.1.3　半监督学习78
5.1.4　强化学习78
5.2　机器学习一般流程78
5.2.1　明确目标79
5.2.2　收集数据79
5.2.3　数据探索与预处理79
5.2.4　选择模型及损失函数80
5.2.5　评估及优化模型81
5.3　过拟合与欠拟合81
5.3.1　权重正则化82
5.3.2　Dropout正则化83
5.3.3　批量正则化86
5.3.4　权重初始化88
5.4　选择合适激活函数89
5.5　选择合适的损失函数90
5.6　选择合适优化器92
5.6.1　传统梯度优化的不足93
5.6.2　动量算法94
5.6.3　AdaGrad算法96
5.6.4　RMSProp算法97
5.6.5　Adam算法98
5.7　GPU加速99
5.7.1　单GPU加速100
5.7.2　多GPU加速101
5.7.3　使用GPU注意事项104
5.8　本章小结104
第6章　视觉处理基础105
6.1　卷积神经网络简介105
6.2　卷积层107
6.2.1　卷积核108
6.2.2　步幅109
6.2.3　填充111
6.2.4　多通道上的卷积111
6.2.5　激活函数113
6.2.6　卷积函数113
6.2.7　转置卷积114
6.3　池化层115
6.3.1　局部池化116
6.3.2　全局池化117
6.4　现代经典网络119
6.4.1　LeNet-5模型119
6.4.2　AlexNet模型120
6.4.3　VGG模型121
6.4.4　GoogleNet模型122
6.4.5　ResNet模型123
6.4.6　胶囊网络简介124
6.5　PyTorch实现CIFAR-10多分类125
6.5.1　数据集说明125
6.5.2　加载数据125
6.5.3　构建网络127
6.5.4　训练模型128
6.5.5　测试模型129
6.5.6　采用全局平均池化130
6.5.7　像Keras一样显示各层参数131
6.6　模型集成提升性能133
6.6.1　使用模型134
6.6.2　集成方法134
6.6.3　集成效果135
6.7　使用现代经典模型提升性能136
6.8　本章小结137
第7章　自然语言处理基础138
7.1　循环神经网络基本结构138
7.2　前向传播与随时间反向传播140
7.3　循环神经网络变种143
7.3.1　LSTM144
7.3.2　GRU145
7.3.3　Bi-RNN146
7.4　循环神经网络的PyTorch实现146
7.4.1　RNN实现147
7.4.2　LSTM实现149
7.4.3　GRU实现151
7.5　文本数据处理152
7.6　词嵌入153
7.6.1　Word2Vec原理154
7.6.2　CBOW模型155
7.6.3　Skip-Gram模型155
7.7　PyTorch实现词性判别156
7.7.1　词性判别主要步骤156
7.7.2　数据预处理157
7.7.3　构建网络157
7.7.4　训练网络158
7.7.5　测试模型160
7.8　用LSTM预测股票行情160
7.8.1　 导入数据160
7.8.2　数据概览161
7.8.3　预处理数据162
7.8.4　定义模型163
7.8.5　训练模型163
7.8.6　测试模型164
7.9　循环神经网络应用场景165
7.10　小结166
第8章　生成式深度学习167
8.1　用变分自编码器生成图像167
8.1.1　自编码器168
8.1.2　变分自编码器168
8.1.3　用变分自编码器生成图像169
8.2　GAN简介173
8.2.1　GAN架构173
8.2.2　GAN的损失函数174
8.3　用GAN生成图像175
8.3.1　判别器175
8.3.2　生成器175
8.3.3　训练模型175
8.3.4　可视化结果177
8.4　VAE与GAN的优缺点178
8.5　ConditionGAN179
8.5.1　CGAN的架构179
8.5.2　CGAN生成器180
8.5.3　CGAN判别器180
8.5.4　CGAN损失函数181
8.5.5　CGAN可视化181
8.5.6　查看指定标签的数据182
8.5.7　可视化损失值182
8.6　DCGAN183
8.7　提升GAN训练效果的一些技巧184
8.8　小结185
第三部分　深度学习实践
第9章　人脸检测与识别188
9.1　人脸识别一般流程188
9.2　人脸检测189
9.2.1　目标检测189
9.2.2　人脸定位191
9.2.3　人脸对齐191
9.2.4　MTCNN算法192
9.3　特征提取193
9.4　人脸识别198
9.4.1　人脸识别主要原理198
9.4.2　人脸识别发展198
9.5　PyTorch实现人脸检测与识别199
9.5.1　验证检测代码199
9.5.2　检测图像200
9.5.3　检测后进行预处理200
9.5.4　查看经检测后的图像201
9.5.5　人脸识别202
9.6　小结202
第10章　迁移学习实例203
10.1　迁移学习简介203
10.2　特征提取204
10.2.1　PyTorch提供的预处理模块205
10.2.2　特征提取实例206
10.3　数据增强209
10.3.1　按比例缩放209
10.3.2　裁剪210
10.3.3　翻转210
10.3.4　改变颜色211
10.3.5　组合多种增强方法211
10.4　微调实例212
10.4.1　数据预处理212
10.4.2　加载预训练模型213
10.4.3　修改分类器213
10.4.4　选择损失函数及优化器213
10.4.5　训练及验证模型214
10.5　清除图像中的雾霾214
10.6　小结217
第11章　神经网络机器翻译实例218
11.1　Encoder-Decoder模型原理218
11.2　注意力框架220
11.3　PyTorch实现注意力Decoder224
11.3.1　构建Encoder224
11.3.2　构建简单Decoder225
11.3.3　构建注意力Decoder226
11.4　用注意力机制实现中英文互译227
11.4.1　导入需要的模块228
11.4.2　数据预处理228
11.4.3　构建模型231
11.4.4　训练模型234
11.4.5　随机采样，对模型进行测试235
11.4.6　可视化注意力236
11.5　小结237
第12章　实战生成式模型238
12.1　DeepDream模型238
12.1.1　Deep Dream原理238
12.1.2　DeepDream算法流程239
12.1.3　用PyTorch实现Deep Dream240
12.2　风格迁移243
12.2.1　内容损失244
12.2.2　风格损失245
12.2.3　用PyTorch实现神经网络风格迁移247
12.3　PyTorch实现图像修复252
12.3.1　网络结构252
12.3.2　损失函数252
12.3.3　图像修复实例253
12.4　PyTorch实现DiscoGAN255
12.4.1　DiscoGAN架构256
12.4.2　损失函数258
12.4.3　DiscoGAN实现258
12.4.4　用PyTorch实现从边框生成鞋子260
12.5　小结262
第13章　Caffe2模型迁移实例263
13.1　Caffe2简介263
13.2　Caffe如何升级到Caffe2264
13.3　PyTorch如何迁移到Caffe2265
13.4　小结268
第14章　AI新方向：对抗攻击269
14.1　对抗攻击简介269
14.1.1　白盒攻击与黑盒攻击270
14.1.2　无目标攻击与有目标攻击270
14.2　常见对抗样本生成方式271
14.2.1　快速梯度符号法271
14.2.2　快速梯度算法271
14.3　PyTorch实现对抗攻击272
14.3.1　实现无目标攻击272
14.3.2　实现有目标攻击274
14.4　对抗攻击和防御措施276
14.4.1　对抗攻击276
14.4.2　常见防御方法分类276
14.5　总结277
第15章　强化学习278
15.1　强化学习简介278
15.2　Q-Learning原理281
15.2.1　Q-Learning主要流程281
15.2.2　Q函数282
15.2.3　贪婪策略283
15.3　用PyTorch实现Q-Learning283
15.3.1　定义Q-Learing主函数283
15.3.2　执行Q-Learing284
15.4　SARSA算法285
15.4.1　SARSA算法主要步骤285
15.4.2　用PyTorch实现SARSA算法286
15.5　小结287
第16章　深度强化学习288
16.1　DQN算法原理288
16.1.1　Q-Learning方法的局限性289
16.1.2　用DL处理RL需要解决的问题289
16.1.3　用DQN解决方法289
16.1.4　定义损失函数290
16.1.5　DQN的经验回放机制290
16.1.6　目标网络290
16.1.7　网络模型291
16.1.8　DQN算法291
16.2　用PyTorch实现DQN算法292
16.3　小结295
附录A　PyTorch0.4版本变更296
附录B　AI在各行业的最新应用301
・ ・ ・ ・ ・ ・ (收起)目录

第1章 深度学习简介：为什么应该学习深度学习 1
1.1　欢迎阅读《深度学习图解》 1
1.2　为什么要学习深度学习 2
1.3　这很难学吗? 3
1.4　为什么要阅读本书 3
1.5　准备工作 4
1.6　你可能需要掌握一部分Python知识 5
1.7　本章小结 6
第2章 基本概念：机器该如何学习？ 7
2.1　什么是深度学习? 7
2.2　什么是机器学习？ 8
2.3　监督机器学习 9
2.4　无监督机器学习 10
2.5　参数学习和非参数学习 10
2.6　监督参数学习 11
2.7　无监督参数学习 13
2.8　非参数学习 14
2.9　本章小结 15
第3章 神经网络预测导论：前向传播 17
3.1　什么是预测 17
3.2　能够进行预测的简单神经网络 19
3.3　什么是神经网络? 20
3.4　这个神经网络做了什么? 21
3.5　使用多个输入进行预测 23
3.6　多个输入：这个神经网络做了什么? 24
3.7　多个输入：完整的可运行代码 29
3.8　预测多个输出 30
3.9　使用多个输入和输出进行预测 32
3.10　多输入多输出神经网络的工作原理 33
3.11　用预测结果进一步预测 35
3.12　NumPy快速入门 37
3.13　本章小结 40
第4章 神经网络学习导论：梯度下降 41
4.1　预测、比较和学习 41
4.2　什么是比较 42
4.3　学习 42
4.4　比较：你的神经网络是否做出了好的预测？ 43
4.5　为什么需要测量误差？ 44
4.6　最简单的神经学习形式是什么？ 45
4.7　冷热学习 46
4.8　冷热学习的特点 47
4.9　基于误差调节权重 48
4.10　梯度下降的一次迭代 50
4.11　学习就是减少误差 52
4.12　回顾学习的步骤 54
4.13　权重增量到底是什么? 55
4.14　狭隘的观点 57
4.15　插着小棍的盒子 58
4.16　导数：两种方式 59
4.17　你真正需要知道的 60
4.18　你不需要知道的 60
4.19　如何使用导数来学习 61
4.20　看起来熟悉吗? 62
4.21　破坏梯度下降 63
4.22　过度修正的可视化 64
4.23　发散 65
4.24　引入α 66
4.25　在代码中实现α 66
4.26　记忆背诵 67
第5章 通用梯度下降：一次学习多个权重 69
5.1　多输入梯度下降学习 69
5.2　多输入梯度下降详解 71
5.3　回顾学习的步骤 75
5.4　单项权重冻结：它有什么作用? 77
5.5　具有多个输出的梯度下降学习 79
5.6　具有多个输入和输出的梯度下降 81
5.7　这些权重学到了什么? 83
5.8　权重可视化 85
5.9　点积(加权和)可视化 86
5.10　本章小结 87
第6章 建立你的第一个深度神经网络：反向传播 89
6.1　交通信号灯问题 89
6.2　准备数据 91
6.3　矩阵和矩阵关系 92
6.4　使用Python创建矩阵 95
6.5　建立神经网络 96
6.6　学习整个数据集 97
6.7　完全、批量和随机梯度下降 97
6.8　神经网络对相关性的学习 98
6.9　向上与向下的压力 99
6.10　边界情况：过拟合 101
6.11　边界情况：压力冲突 101
6.12　学习间接相关性 103
6.13　创建关联 104
6.14　堆叠神经网络：回顾 105
6.15　反向传播：远程错误归因 106
6.16　反向传播：为什么有效? 107
6.17　线性与非线性 107
6.18　为什么神经网络仍然不起作用 109
6.19　选择性相关的秘密 110
6.20　快速冲刺 111
6.21　你的第一个深度神经网络 111
6.22　反向传播的代码 112
6.23　反向传播的一次迭代 114
6.24　整合代码 116
6.25　为什么深度网络这么重要? 117
第7章 如何描绘神经网络：在脑海里，在白纸上 119
7.1　到了简化的时候了 119
7.2　关联抽象 120
7.3　旧的可视化方法过于复杂 121
7.4　简化版可视化 122
7.5　进一步简化 123
7.6　观察神经网络是如何进行预测的 124
7.7　用字母而不是图片来进行可视化 125
7.8　连接变量 126
7.9　信息整合 127
7.10　可视化工具的重要性 127
第8章 学习信号，忽略噪声：正则化和批处理介绍 129
8.1　用在MNIST上的三层网络 129
8.2　好吧，这很简单 131
8.3　记忆与泛化 132
8.4　神经网络中的过拟合 133
8.5　过拟合从何而来 134
8.6　最简单的正则化：提前停止 135
8.7　行业标准正则化：dropout 136
8.8　为什么dropout有效：整合是有效的 137
8.9　dropout的代码 137
8.10　在MNIST数据集上对dropout进行测试 139
8.11　批量梯度下降 140
8.12　本章小结 143
第9章 概率和非线性建模：激活函数 145
9.1　什么是激活函数? 145
9.2　标准隐藏层激活函数 148
9.3　标准输出层激活函数 149
9.4　核心问题：输入具有
相似性 151
9.5　计算softmax 152
9.6　激活函数使用说明 153
9.7　将增量与斜率相乘 156
9.8　将输出转换为斜率(导数) 157
9.9　升级MNIST网络 157
第10章 卷积神经网络概论：关于边与角的神经学习 161
10.1　在多个位置复用权重 161
10.2　卷积层 162
10.3　基于NumPy的简单实现 164
10.4　本章小结 167
第11章 能够理解自然语言的神经网络：国王-男人+女人=？ 169
11.1　理解语言究竟是指什么? 170
11.2　自然语言处理(NLP) 170
11.3　监督NLP学习 171
11.4　IMDB电影评论数据集 172
11.5　在输入数据中提取单词相关性 173
11.6　对影评进行预测 174
11.7　引入嵌入层 175
11.8　解释输出 177
11.9　神经网络结构 178
11.10　单词嵌入表达的对比 180
11.11　神经元是什么意思? 181
11.12　完形填空 182
11.13　损失函数的意义 183
11.14　国王-男人+女人~=女王 186
11.15　单词类比 187
11.16　本章小结 188
第12章 像莎士比亚一样写作的神经网络：变长数据的递归层 189
12.1　任意长度的挑战 189
12.2　做比较真的重要吗？ 190
12.3　平均词向量的神奇力量 191
12.4　信息是如何存储在这些向量嵌入中的？ 192
12.5　神经网络是如何使用嵌入的？ 193
12.6　词袋向量的局限 194
12.7　用单位向量求词嵌入之和 195
12.8　不改变任何东西的矩阵 196
12.9　学习转移矩阵 197
12.10　学习创建有用的句子向量 198
12.11　Python下的前向传播 199
12.12　如何反向传播？ 200
12.13　让我们训练它！ 201
12.14　进行设置 201
12.15　任意长度的前向传播 202
12.16　任意长度的反向传播 203
12.17　任意长度的权重更新 204
12.18　运行代码，并分析输出 205
12.19　本章小结 207
第13章 介绍自动优化：搭建深度学习框架 209
13.1　深度学习框架是什么？ 209
13.2　张量介绍 210
13.3　自动梯度计算(autograd)介绍 211
13.4　快速检查 213
13.5　多次使用的张量 214
13.6　升级autograd以支持多次使用的张量 215
13.7　加法的反向传播如何工作？ 217
13.8　增加取负值操作的支持 218
13.9　添加更多函数的支持 219
13.10　使用autograd训练神经网络 222
13.11　增加自动优化 224
13.12　添加神经元层类型的支持 225
13.13　包含神经元层的神经元层 226
13.14　损失函数层 227
13.15　如何学习一个框架 228
13.16　非线性层 228
13.17　嵌入层 230
13.18　将下标操作添加到
autograd 231
13.19　再看嵌入层 232
13.20　交叉熵层 233
13.21　递归神经网络层 235
13.22　本章小结 238
第14章 像莎士比亚一样写作：长短期记忆网络 239
14.1　字符语言建模 239
14.2　截断式反向传播的必要性 240
14.3　截断式反向传播 241
14.4　输出样例 244
14.5　梯度消失与梯度激增 245
14.6　RNN反向传播的小例子 246
14.7　长短期记忆(LSTM)元胞 247
14.8　关于LSTM门限的直观理解 248
14.9　长短期记忆层 249
14.10　升级字符语言模型 250
14.11　训练LSTM字符语言模型 251
14.12　调优LSTM字符语言模型 252
14.13　本章小结 253
第15章 在看不见的数据上做深度学习：联邦学习导论 255
15.1　深度学习的隐私问题 255
15.2　联邦学习 256
15.3　学习检测垃圾邮件 257
15.4　让我们把它联邦化 259
15.5　深入联邦学习 260
15.6　安全聚合 261
15.7　同态加密 262
15.8　同态加密联邦学习 263
15.9　本章小结 264
第16章 往哪里去：简要指引 265
・ ・ ・ ・ ・ ・ (收起)第　一部分 基础知识
第　1章 走近深度学习：机器学习入门　3
1．1　什么是机器学习　4
1．1．1　机器学习与AI的关系　5
1．1．2　机器学习能做什么，不能做什么　6
1．2　机器学习示例　7
1．2．1　在软件应用中使用机器学习　9
1．2．2　监督学习　11
1．2．3　无监督学习　12
1．2．4　强化学习　12
1．3　深度学习　13
1．4　阅读本书能学到什么　14
1．5　小结　15
第　2章 围棋与机器学习　16
2．1　为什么选择游戏　16
2．2　围棋快速入门　17
2．2．1　了解棋盘　17
2．2．2　落子与吃子　18
2．2．3　终盘与胜负计算　19
2．2．4　理解劫争　20
2．2．5　让子　20
2．3　更多学习资源　20
2．4　我们可以教会计算机什么　21
2．4．1　如何开局　21
2．4．2　搜索游戏状态　21
2．4．3　减少需要考虑的动作数量　22
2．4．4　评估游戏状态　22
2．5　如何评估围棋AI的能力　23
2．5．1　传统围棋评级　23
2．5．2　对围棋AI进行基准测试　24
2．6　小结　24
第3章　实现第 一个围棋机器人　25
3．1　在Python中表达围棋游戏　25
3．1．1　实现围棋棋盘　28
3．1．2　在围棋中跟踪相连的棋组：棋链　28
3．1．3　在棋盘上落子和提子　30
3．2　跟踪游戏状态并检查非法动作　32
3．2．1　自吃　33
3．2．2　劫争　34
3．3　终盘　36
3．4　创建自己的第 一个机器人：理论上最弱的围棋AI　37
3．5　使用Zobrist哈希加速棋局　41
3．6　人机对弈　46
3．7　小结　47
第二部分　机器学习和游戏AI
第4章　使用树搜索下棋　51
4．1　游戏分类　52
4．2　利用极小化极大搜索预测对手　53
4．3　井字棋推演：一个极小化极大算法的示例　56
4．4　通过剪枝算法缩减搜索空间　58
4．4．1　通过棋局评估减少搜索深度　60
4．4．2　利用α-β剪枝缩减搜索宽度　63
4．5　使用蒙特卡洛树搜索评估游戏状态　66
4．5．1　在Python中实现蒙特卡洛树搜索　69
4．5．2　如何选择继续探索的分支　72
4．5．3　将蒙特卡洛树搜索应用于围棋　74
4．6　小结　76
第5章　神经网络入门　77
5．1　一个简单的用例：手写数字分类　78
5．1．1　MNIST手写数字数据集　78
5．1．2　MNIST数据的预处理　79
5．2　神经网络基础　85
5．2．1　将对率回归描述为简单的神经网络　85
5．2．2　具有多个输出维度的神经网络　85
5．3　前馈网络　86
5．4　我们的预测有多好？损失函数及优化　89
5．4．1　什么是损失函数　89
5．4．2　均方误差　89
5．4．3　在损失函数中找极小值　90
5．4．4　使用梯度下降法找极小值　91
5．4．5　损失函数的随机梯度下降算法　92
5．4．6　通过网络反向传播梯度　93
5．5　在Python中逐步训练神经网络　95
5．5．1　Python中的神经网络层　96
5．5．2　神经网络中的激活层　97
5．5．3　在Python中实现稠密层　98
5．5．4　Python顺序神经网络　100
5．5．5　将网络集成到手写数字分类应用中　102
5．6　小结　103
第6章　为围棋数据设计神经网络　105
6．1　为神经网络编码围棋棋局　107
6．2　生成树搜索游戏用作网络训练数据　109
6．3　使用Keras深度学习库　112
6．3．1　了解Keras的设计原理　112
6．3．2　安装Keras深度学习库　113
6．3．3　热身运动：在Keras中运行一个熟悉的示例　113
6．3．4　使用Keras中的前馈神经网络进行动作预测　115
6．4　使用卷积网络分析空间　119
6．4．1　卷积的直观解释　119
6．4．2　用Keras构建卷积神经网络　122
6．4．3　用池化层缩减空间　123
6．5　预测围棋动作概率　124
6．5．1　在最后一层使用softmax激活函数　125
6．5．2　分类问题的交叉熵损失函数　126
6．6　使用丢弃和线性整流单元构建更深的网络　127
6．6．1　通过丢弃神经元对网络进行正则化　128
6．6．2　线性整流单元激活函数　129
6．7　构建更强大的围棋动作预测网络　130
6．8　小结　133
第7章　从数据中学习：构建深度学习机器人　134
7．1　导入围棋棋谱　135
7．1．1　SGF文件格式　136
7．1．2　从KGS下载围棋棋谱并复盘　136
7．2　为深度学习准备围棋数据　137
7．2．1　从SGF棋谱中复盘围棋棋局　138
7．2．2　构建围棋数据处理器　139
7．2．3　构建可以高效地加载数据的围棋数据生成器　146
7．2．4　并行围棋数据处理和生成器　147
7．3　基于真实棋局数据训练深度学习模型　148
7．4　构建更逼真的围棋数据编码器　152
7．5　使用自适应梯度进行高效的训练　155
7．5．1　在SGD中采用衰减和动量　155
7．5．2　使用Adagrad优化神经网络　156
7．5．3　使用Adadelta优化自适应梯度　157
7．6　运行自己的实验并评估性能　157
7．6．1　测试架构与超参数的指南　158
7．6．2　评估训练与测试数据的性能指标　159
7．7　小结　160
第8章　实地部署围棋机器人　162
8．1　用深度神经网络创建动作预测代理　163
8．2　为围棋机器人提供Web前端　165
8．3　在云端训练与部署围棋机器人　169
8．4　与其他机器人对话：围棋文本协议　170
8．5　在本地与其他机器人对弈　172
8．5．1　机器人应该何时跳过回合或认输　172
8．5．2　让机器人与其他围棋程序进行对弈　173
8．6　将围棋机器人部署到在线围棋服务器　178
8．7　小结　182
第9章　通过实践学习：强化学习　183
9．1　强化学习周期　184
9．2　经验包括哪些内容　185
9．3　建立一个有学习能力的代理　188
9．3．1　从某个概率分布中进行抽样　189
9．3．2　剪裁概率分布　190
9．3．3　初始化一个代理实例　191
9．3．4　在磁盘上加载并保存代理　191
9．3．5　实现动作选择　193
9．4　自我对弈：计算机程序进行实践训练的方式　194
9．4．1　经验数据的表示　194
9．4．2　模拟棋局　197
9．5　小结　199
第　10章 基于策略梯度的强化学习　200
10．1　如何在随机棋局中识别更佳的决策　201
10．2　使用梯度下降法修改神经网络的策略　204
10．3　使用自我对弈进行训练的几个小技巧　208
10．3．1　评估学习的进展　208
10．3．2　衡量强度的细微差别　209
10．3．3　SGD优化器的微调　210
10．4　小结　213
第　11章 基于价值评估方法的强化学习　214
11．1　使用Q学习进行游戏　214
11．2　在Keras中实现Q学习　218
11．2．1　在Keras中构建双输入网络　218
11．2．2　用Keras实现ε贪婪策略　222
11．2．3　训练一个行动-价值函数　225
11．3　小结　226
第　12章 基于演员-评价方法的强化学习　227
12．1　优势能够告诉我们哪些决策更加重要　227
12．1．1　什么是优势　228
12．1．2　在自我对弈过程中计算优势值　230
12．2　为演员-评价学习设计神经网络　232
12．3　用演员-评价代理下棋　234
12．4　用经验数据训练一个演员-评价代理　235
12．5　小结　240
第三部分　一加一大于二
第　13章 AlphaGo：全部集结　243
13．1　为AlphaGo训练深度神经网络　245
13．1．1　AlphaGo的网络架构　246
13．1．2　AlphaGo棋盘编码器　248
13．1．3　训练AlphaGo风格的策略网络　250
13．2　用策略网络启动自我对弈　252
13．3　从自我对弈数据衍生出一个价值网络　254
13．4　用策略网络和价值网络做出更好的搜索　254
13．4．1　用神经网络改进蒙特卡洛推演　255
13．4．2　用合并价值函数进行树搜索　256
13．4．3　实现AlphaGo的搜索算法　258
13．5　训练自己的AlphaGo可能遇到的实践问题　263
13．6　小结　265
第　14章 AlphaGo Zero：将强化学习集成到树搜索中　266
14．1　为树搜索构建一个神经网络　267
14．2　使用神经网络来指导树搜索　268
14．2．1　沿搜索树下行　271
14．2．2　扩展搜索树　274
14．2．3　选择一个动作　276
14．3　训练　277
14．4　用狄利克雷噪声改进探索　281
14．5　处理超深度神经网络的相关最新技术　282
14．5．1　批量归一化　282
14．5．2　残差网络　283
14．6　探索额外资源　284
14．7　结语　285
14．8　小结　285
附录A　数学基础　286
附录B　反向传播算法　293
附录C　围棋程序与围棋服务器　297
附录D　用AWS来训练和部署围棋程序与围棋服务器　300
附录E　将机器人发布到OGS　307
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习简介 1
1.1 人工智能、机器学习与深度学习 2
1.2 深度学习的发展历程 7
1.3 深度学习的应用 10
1.3.1 计算机视觉 10
1.3.2 语音识别 14
1.3.3 自然语言处理 15
1.3.4 人机博弈 18
1.4 深度学习工具介绍和对比 19
小结 23
第2章 TensorFlow环境搭建 25
2.1 TensorFlow的主要依赖包 25
2.1.1 Protocol Buffer 25
2.1.2 Bazel 27
2.2 TensorFlow安装 29
2.2.1 使用Docker安装 30
2.2.2 使用pip安装 32
2.2.3 从源代码编译安装 33
2.3 TensorFlow测试样例 37
小结 38
第3章 TensorFlow入门 40
3.1 TensorFlow计算模型――计算图 40
3.1.1 计算图的概念 40
3.1.2 计算图的使用 41
3.2 TensorFlow数据模型――张量 43
3.2.1 张量的概念 43
3.2.2 张量的使用 45
3.3 TensorFlow运行模型――会话 46
3.4 TensorFlow实现神经网络 48
3.4.1 TensorFlow游乐场及神经网络简介 48
3.4.2 前向传播算法简介 51
3.4.3 神经网络参数与TensorFlow变量 54
3.4.4 通过TensorFlow训练神经网络模型 58
3.4.5 完整神经网络样例程序 62
小结 65
第4章 深层神经网络 66
4.1 深度学习与深层神经网络 66
4.1.1 线性模型的局限性 67
4.1.2 激活函数实现去线性化 70
4.1.3 多层网络解决异或运算 73
4.2 损失函数定义 74
4.2.1 经典损失函数 75
4.2.2 自定义损失函数 79
4.3 神经网络优化算法 81
4.4 神经网络进一步优化 84
4.4.1 学习率的设置 85
4.4.2 过拟合问题 87
4.4.3 滑动平均模型 90
小结 92
第5章 MNIST数字识别问题 94
5.1 MNIST数据处理 94
5.2 神经网络模型训练及不同模型结果对比 97
5.2.1 TensorFlow训练神经网络 97
5.2.2 使用验证数据集判断模型效果 102
5.2.3 不同模型效果比较 103
5.3 变量管理 107
5.4 TensorFlow模型持久化 112
5.4.1 持久化代码实现 112
5.4.2 持久化原理及数据格式 117
5.5 TensorFlow最佳实践样例程序 126
小结 132
第6章 图像识别与卷积神经网络 134
6.1 图像识别问题简介及经典数据集 135
6.2 卷积神经网络简介 139
6.3 卷积神经网络常用结构 142
6.3.1 卷积层 142
6.3.2 池化层 147
6.4 经典卷积网络模型 149
6.4.1 LeNet-5模型 150
6.4.2 Inception-v3模型 156
6.5 卷积神经网络迁移学习 160
6.5.1 迁移学习介绍 160
6.5.2 TensorFlow实现迁移学习 161
小结 169
第7章 图像数据处理 170
7.1 TFRecord输入数据格式 170
7.1.1 TFRecord格式介绍 171
7.1.2 TFRecord样例程序 171
7.2 图像数据处理 173
7.2.1 TensorFlow图像处理函数 174
7.2.2 图像预处理完整样例 183
7.3 多线程输入数据处理框架 185
7.3.1 队列与多线程 186
7.3.2 输入文件队列 190
7.3.3 组合训练数据（batching） 193
7.3.4 输入数据处理框架 196
小结 198
第8章 循环神经网络 200
8.1 循环神经网络简介 200
8.2 长短时记忆网络（LTSM）结构 206
8.3 循环神经网络的变种 212
8.3.1 双向循环神经网络和深层循环神经网络 212
8.3.2 循环神经网络的dropout 214
8.4 循环神经网络样例应用 215
8.4.1 自然语言建模 216
8.4.2 时间序列预测 225
小结 230
第9章 TensorBoard可视化 232
9.1 TensorBoard简介 232
9.2 TensorFlow计算图可视化 234
9.2.1 命名空间与TensorBoard图上节点 234
9.2.2 节点信息 241
9.3 监控指标可视化 246
小结 252
第10章 TensorFlow计算加速 253
10.1 TensorFlow使用GPU 253
10.2 深度学习训练并行模式 258
10.3 多GPU并行 261
10.4 分布式TensorFlow 268
10.4.1 分布式TensorFlow原理 269
10.4.2 分布式TensorFlow模型训练 272
10.4.3 使用Caicloud运行分布式TensorFlow 282
小结 287
・ ・ ・ ・ ・ ・ (收起)前言 .1
第一部分 生成式深度学习概述
第1 章 生成建模 11
1.1 什么是生成建模？ 11
1.1.1 生成建模与判别建模 13
1.1.2 机器学习的发展 . 14
1.1.3 生成建模的兴起 . 15
1.1.4 生成建模的框架 . 18
1.2 概率生成模型 21
1.2.1 你好，Wrodl ！ 24
1.2.2 你的第一个概率生成模型 . 25
1.2.3 朴素贝叶斯 28
1.2.4 你好，Wrodl ！续篇 . 31
1.3 生成建模的难题 33
表示学习 34
1.4 设置环境 37
1.5 小结 40
第2 章 深度学习 41
2.1 结构化与非结构化数据 41
2.2 深度神经网络 43
Keras 和TensorFlow 44
2.3 第一个深度神经网络 . 45
2.3.1 加载数据. 46
2.3.2 建立模型. 48
2.3.3 编译模型. 52
2.3.4 训练模型. 54
2.3.5 评估模型. 55
2.4 改进模型 58
2.4.1 卷积层 . 58
2.4.2 批标准化. 64
2.4.3 Dropout 层 . 66
2.4.4 结合所有层 68
2.5 小结 71
第3 章 变分自动编码器 73
3.1 画展 73
3.2 自动编码器 . 76
3.2.1 第一个自动编码器 . 77
3.2.2 编码器 . 78
3.2.3 解码器 . 80
3.2.4 连接编码器与解码器 82
3.2.5 分析自动编码器 . 84
3.3 变化后的画展 87
3.4 构建变分自动编码器 . 89
3.4.1 编码器 . 89
3.4.2 损失函数. 94
3.4.3 分析变分自动编码器 97
3.5 使用VAE 生成面部图像 98
3.5.1 训练VAE 99
3.5.2 分析VAE . 102
3.5.3 生成新面孔 . 103
3.5.4 隐空间的算术 104
3.5.5 面部变形 106
3.6 小结 . 107
第4 章 生成对抗网络 108
4.1 神秘兽 108
4.2 生成对抗网络简介 111
4.3 第一个生成对抗网络 112
4.3.1 判别器 113
4.3.2 生成器 115
4.3.3 训练GAN 119
4.4 GAN 面临的难题 125
4.4.1 损失震荡 125
4.4.2 模式收缩 126
4.4.3 不提供信息的损失函数 126
4.4.4 超参数 127
4.4.5 解决GAN 面临的难题 . 127
4.5 WGAN 127
4.5.1 Wasserstein 损失 128
4.5.2 利普希茨约束 130
4.5.3 权重裁剪 131
4.5.4 训练WGAN 132
4.5.5 分析WGAN 133
4.6 WGAN-GP 134
4.6.1 梯度惩罚损失 135
4.6.2 分析WGAN-GP 139
4.7 小结 . 140
第二部分 教机器绘画、写作、作曲和玩游戏
第5 章 绘画 145
5.1 苹果和橙子 146
5.2 CycleGAN 149
5.3 第一个CycleGAN 模型 . 151
5.3.1 简介 151
5.3.2 生成器（U-Net） 153
5.3.3 判别器 157
5.3.4 编译CycleGAN 158
5.3.5 训练CycleGAN 161
5.3.6 分析CycleGAN 162
5.4 创建一个模仿莫奈作品的CycleGAN . 164
5.4.1 生成器（ResNet） 165
5.4.2 分析CycleGAN 166
5.5 神经风格迁移 . 168
5.5.1 内容损失 169
5.5.2 风格损失 172
5.5.3 总方差损失 . 175
5.5.4 运行神经风格迁移 176
5.5.5 分析神经风格迁移模型 177
5.6 小结 . 178
第6 章 写作 179
6.1 坏家伙们的文学社 180
6.2 长短期记忆网络 181
6.3 第一个LSTM 网络 182
6.3.1 分词 183
6.3.2 建立数据集 . 185
6.3.3 LSTM 架构 . 187
6.3.4 嵌入层 187
6.3.5 LSTM 层 188
6.3.6 LSTM 元胞 . 190
6.4 生成新文本 192
6.5 RNN 扩展 . 196
6.5.1 堆叠式循环网络 196
6.5.2 门控制循环单元 198
6.5.3 双向元胞 200
6.6 编码器- 解码器模型 200
6.7 问答生成器 203
6.7.1 问答数据集 . 204
6.7.2 模型架构 205
6.7.3 推断 210
6.7.4 模型的结果 . 212
6.8 小结 . 214
第7 章 作曲 215
7.1 前提知识 216
音符 216
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习简介
1.1 人工智能、机器学习与深度学习
1.2 深度学习的发展历程
1.3 深度学习的应用
1.3.1 计算机视觉
1.3.2 语音识别
1.3.3 自然语言处理
1.3.4 人机博弈
1.4 深度学习工具介绍和对比
小结
第2章 TensorFlow环境搭建
2.1 TensorFlow的主要依赖包
2.1.1 Protocol Buffer
2.1.2 Bazel
2.2 TensorFlow安装
2.2.1 使用Docker安装
2.2.2 使用pip安装
2.2.3 从源代码编译安装
2.3 TensorFlow测试样例
小结
第3章 TensorFlow入门
3.1 TensorFlow计算模型――计算图
3.1.1 计算图的概念
3.1.2 计算图的使用
3.2 TensorFlow数据模型――张量
3.2.1 张量的概念
3.2.2 张量的使用
3.3 TensorFlow运行模型――会话
3.4 TensorFlow实现神经网络
3.4.1 TensorFlow游乐场及神经网络简介
3.4.2 前向传播算法简介
3.4.3 神经网络参数与TensorFlow变量
3.4.4 通过TensorFlow训练神经网络模型
3.4.5 完整神经网络样例程序
小结
第4章 深层神经网络
4.1 深度学习与深层神经网络
4.1.1 线性模型的局限性
4.1.2 激活函数实现去线性化
4.1.3 多层网络解决异或运算
4.2 损失函数定义
4.2.1 经典损失函数
4.2.2 自定义损失函数
4.3 神经网络优化算法
4.4 神经网络进一步优化
4.4.1 学习率的设置
4.4.2 过拟合问题
4.4.3 滑动平均模型
小结
第5章 MNIST数字识别问题
5.1 MNIST数据处理
5.2 神经网络模型训练及不同模型结果对比
5.2.1 TensorFlow训练神经网络
5.2.2 使用验证数据集判断模型效果
5.2.3 不同模型效果比较
5.3 变量管理
5.4 TensorFlow模型持久化
5.4.1 持久化代码实现
5.4.2 持久化原理及数据格式
5.5 TensorFlow最佳实践样例程序
小结
第6章 图像识别与卷积神经网络
6.1 图像识别问题简介及经典数据集
6.2 卷积神经网络简介
6.3 卷积神经网络常用结构
6.3.1 卷积层
6.3.2 池化层
6.4 经典卷积网络模型
6.4.1 LeNet-5模型
6.4.2 Inception-v3模型
6.5 卷积神经网络迁移学习
6.5.1 迁移学习介绍
6.5.2 TensorFlow实现迁移学习
小结
第7章 图像数据处理
7.1 TFRecord输入数据格式
7.1.1 TFRecord格式介绍
7.1.2 TFRecord样例程序
7.2 图像数据处理
7.2.1 TensorFlow图像处理函数
7.2.2 图像预处理完整样例
7.3 多线程输入数据处理框架
7.3.1 队列与多线程
7.3.2 输入文件队列
7.3.3 组合训练数据（batching）
7.3.4 输入数据处理框架
7.4 数据集（Dataset）
7.4.1 数据集的基本使用方法
7.4.2 数据集的高层操作
小结
第8章 循环神经网络
8.1 循环神经网络简介
8.2 长短时记忆网络（LSTM）结构
8.3 循环神经网络的变种
8.3.1 双向循环神经网络和深层循环神经网络
8.3.2 循环神经网络的dropout
8.4 循环神经网络样例应用
小结
第9章 自然语言处理
9.1 语言模型的背景知识
9.1.1 语言模型简介
9.1.2 语言模型的评价方法
9.2 神经语言模型
9.2.1 PTB数据集的预处理
9.2.2 PTB数据的batching方法
9.2.3 基于循环神经网络的神经语言模型
9.3 神经网络机器翻译
9.3.1 机器翻译背景与Seq2Seq模型介绍
9.3.2 机器翻译文本数据的预处理
9.3.3 Seq2Seq模型的代码实现
9.3.4 注意力机制
小结
第10章 TensorFlow高层封装
10.1 TensorFlow高层封装总览
10.2 Keras介绍
10.2.1 Keras基本用法
10.2.2 Keras高级用法
10.3 Estimator介绍
10.3.1 Estimator基本用法
10.3.2 Estimator自定义模型
10.3.3 使用数据集（Dataset）作为Estimator输入
小结
第11章 TensorBoard可视化
11.1 TensorBoard简介
11.2 TensorFlow计算图可视化
11.2.1 命名空间与TensorBoard图上节点
11.2.2 节点信息
11.3 监控指标可视化
11.4 高维向量可视化
小结
第12章 TensorFlow计算加速
12.1 TensorFlow使用GPU
12.2 深度学习训练并行模式
12.3 多GPU并行
12.4 分布式TensorFlow
12.4.1 分布式TensorFlow原理
12.4.2 分布式TensorFlow模型训练
小结
・ ・ ・ ・ ・ ・ (收起)第一部分　原理篇
第1章　机器学习与模型 2
1.1　模型 2
1.2　参数与训练 4
1.3　损失函数 9
1.4　计算图的训练 10
1.5　小结 12
第2章　计算图 13
2.1　什么是计算图 13
2.2　前向传播 14
2.3　函数优化与梯度下降法 18
2.4　链式法则与反向传播 29
2.5　在计算图上执行梯度下降法 36
2.6　节点类及其子类 36
2.7　用计算图搭建ADALINE并训练 44
2.8　小结 48
第3章　优化器 49
3.1　优化流程的抽象实现 49
3.2　BGD、SGD和MBGD 53
3.3　梯度下降优化器 58
3.4　朴素梯度下降法的局限 60
3.5　冲量优化器 61
3.6　AdaGrad优化器 62
3.7　RMSProp优化器 64
3.8　Adam优化器 65
3.9　小结 68
第二部分　模型篇
第4章　逻辑回归 70
4.1　对数损失函数 70
4.2　Logistic函数 73
4.3　二分类逻辑回归 75
4.4　多分类逻辑回归 78
4.5　交叉熵 81
4.6　实例：鸢尾花 85
4.7　小结 88
第5章　神经网络 90
5.1　神经元与激活函数 90
5.2　神经网络 95
5.3　多层全连接神经网络 99
5.4　多个全连接层的意义 101
5.5　实例：鸢尾花 108
5.6　实例：手写数字识别 110
5.7　小结 116
第6章　非全连接神经网络 117
6.1　带二次项的逻辑回归 117
6.2　因子分解机 124
6.3　Wide & Deep 132
6.4　DeepFM 137
6.5　实例：泰坦尼克号幸存者 141
6.6　小结 150
第7章　循环神经网络 151
7.1　RNN的结构 151
7.2　RNN的输出 152
7.3　实例：正弦波与方波 155
7.4　变长序列 159
7.5　实例：3D电磁发音仪单词识别 164
7.6　小结 167
第8章　卷积神经网络 168
8.1　蒙德里安与莫奈 168
8.2　滤波器 170
8.3　可训练的滤波器 176
8.4　卷积层 183
8.5　池化层 186
8.6　CNN的结构 189
8.7　实例：手写数字识别 190
8.8　小结 194
第三部分　工程篇
第9章　训练与评估 196
9.1　训练和Trainer训练器 196
9.2　评估和Metrics节点 202
9.3　混淆矩阵 204
9.4　正确率 204
9.5　查准率 206
9.6　查全率 206
9.7　ROC曲线和AUC 208
9.8　小结 211
第10章　模型保存、预测和服务 212
10.1　模型保存 213
10.2　模型加载和预测 216
10.3　模型服务 216
10.4　客户端 222
10.5　小结 223
第11章　分布式训练 224
11.1　分布式训练的原理 224
11.2　基于参数服务器的架构 230
11.3　Ring AllReduce原理 241
11.4　Ring AllReduce架构实现 248
11.5　分布式训练性能评测 257
11.6　小结 259
第12章　工业级深度学习框架 261
12.1　张量 262
12.2　计算加速 263
12.3　GPU 265
12.4　数据接口 266
12.5　模型并行 266
12.6　静态图和动态图 267
12.7　混合精度训练 268
12.8　图优化和编译优化 270
12.9　移动端和嵌入式端 270
12.10　小结 271
・ ・ ・ ・ ・ ・ (收起)第 1章　PyTorch与深度学习 1
1．1　人工智能　1
1．2　机器学习　3
1．3　深度学习　4
1．3．1　深度学习的应用　4
1．3．2　深度学习的浮夸宣传　6
1．3．3　深度学习发展史　6
1．3．4　为何是现在　7
1．3．5　硬件可用性　7
1．3．6　数据和算法　8
1．3．7　深度学习框架　9
1．4　小结　10
第　2章 神经网络的构成　11
2．1　安装PyTorch　11
2．2　实现第 一个神经网络　12
2．2．1　准备数据　13
2．2．2　为神经网络创建数据　20
2．2．3　加载数据　24
2．3　小结　25
第3章　深入了解神经网络　26
3．1　详解神经网络的组成部分　26
3．1．1　层―神经网络的基本组成　27
3．1．2　非线性激活函数　29
3．1．3　PyTorch中的非线性激活函数　32
3．1．4　使用深度学习进行图像分类　36
3．2　小结　46
第4章　机器学习基础　47
4．1　三类机器学习问题　47
4．1．1　有监督学习　48
4．1．2　无监督学习　48
4．1．3　强化学习　48
4．2　机器学习术语　49
4．3　评估机器学习模型　50
4．4　数据预处理与特征工程　54
4．4．1　向量化　54
4．4．2　值归一化　54
4．4．3　处理缺失值　55
4．4．4　特征工程　55
4．5　过拟合与欠拟合　56
4．5．1　获取更多数据　56
4．5．2　缩小网络规模　57
4．5．3　应用权重正则化　58
4．5．4　应用dropout　58
4．5．5　欠拟合　60
4．6　机器学习项目的工作流　60
4．6．1　问题定义与数据集创建　60
4．6．2　成功的衡量标准　61
4．6．3　评估协议　61
4．6．4　准备数据　62
4．6．5　模型基线　62
4．6．6　大到过拟合的模型　63
4．6．7　应用正则化　63
4．6．8　学习率选择策略　64
4．7　小结　65
第5章　深度学习之计算机视觉　66
5．1　神经网络简介　66
5．2　从零开始构建CNN模型　69
5．2．1　Conv2d　71
5．2．2　池化　74
5．2．3　非线性激活―ReLU　75
5．2．4　视图　76
5．2．5　训练模型　77
5．2．6　狗猫分类问题―从零开始构建CNN　80
5．2．7　利用迁移学习对狗猫分类　82
5．3　创建和探索VGG16模型　84
5．3．1　冻结层　85
5．3．2　微调VGG16模型　85
5．3．3　训练VGG16模型　86
5．4　计算预卷积特征　88
5．5　理解CNN模型如何学习　91
5．6　CNN层的可视化权重　94
5．7　小结　95
第6章　序列数据和文本的深度学习　96
6．1　使用文本数据　96
6．1．1　分词　98
6．1．2　向量化　100
6．2　通过构建情感分类器训练词向量　104
6．2．1　下载IMDB数据并对文本分词　104
6．2．2　构建词表　106
6．2．3　生成向量的批数据　107
6．2．4　使用词向量创建网络模型　108
6．2．5　训练模型　109
6．3　使用预训练的词向量　110
6．3．1　下载词向量　111
6．3．2　在模型中加载词向量　112
6．3．3　冻结embedding层权重　113
6．4　递归神经网络（RNN）　113
6．5　LSTM　117
6．5．1　长期依赖　117
6．5．2　LSTM网络　117
6．6　基于序列数据的卷积网络　123
6．7　小结　125
第7章　生成网络　126
7．1　神经风格迁移　126
7．1．1　加载数据　129
7．1．2　创建VGG模型　130
7．1．3　内容损失　131
7．1．4　风格损失　131
7．1．5　提取损失　133
7．1．6　为网络层创建损失函数　136
7．1．7　创建优化器　136
7．1．8　训练　137
7．2　生成对抗网络（GAN）　138
7．3　深度卷机生成对抗网络　139
7．3．1　定义生成网络　140
7．3．2　定义判别网络　144
7．3．3　定义损失函数和优化器　145
7．3．4　训练判别网络　145
7．3．5　训练生成网络　146
7．3．6　训练整个网络　147
7．3．7　检验生成的图片　148
7．4　语言建模　150
7．4．1　准备数据　151
7．4．2　生成批数据　152
7．4．3　定义基于LSTM的模型　153
7．4．4　定义训练和评估函数　155
7．4．5　训练模型　157
7．5　小结　159
第8章　现代网络架构　160
8．1　现代网络架构　160
8．1．1　ResNet　160
8．1．2　Inception　168
8．2　稠密连接卷积网络（DenseNet）　175
8．2．1　DenseBlock　175
8．2．2　DenseLayer　176
8．3　模型集成　180
8．3．1　创建模型　181
8．3．2　提取图片特征　182
8．3．3　创建自定义数据集和数据加载器　183
8．3．4　创建集成模型　184
8．3．5　训练和验证模型　185
8．4　encoder-decoder架构　186
8．4．1　编码器　188
8．4．2　解码器　188
8．5　小结　188
第9章　未来走向　189
9．1　未来走向　189
9．2　回顾　189
9．3　有趣的创意应用　190
9．3．1　对象检测　190
9．3．2　图像分割　191
9．3．3　PyTorch中的OpenNMT　192
9．3．4　Allen NLP　192
9．3．5　fast．ai―神经网络不再神秘　192
9．3．6　Open Neural Network Exchange　192
9．4　如何跟上前沿　193
9．5　小结　193
・ ・ ・ ・ ・ ・ (收起)第1章　深度学习入门　　1
1.1　机器学习简介　　1
1.1.1　监督学习　　2
1.1.2　无监督学习　　2
1.1.3　强化学习　　3
1.2　深度学习定义　　3
1.2.1　人脑的工作机制　　3
1.2.2　深度学习历史　　4
1.2.3　应用领域　　5
1.3　神经网络　　5
1.3.1　生物神经元　　5
1.3.2　人工神经元　　6
1.4　人工神经网络的学习方式　　8
1.4.1　反向传播算法　　8
1.4.2　权重优化　　8
1.4.3　随机梯度下降法　　9
1.5　神经网络架构　　10
1.5.1　多层感知器　　10
1.5.2　DNN架构　　11
1.5.3　卷积神经网络　　12
1.5.4　受限玻尔兹曼机　　12
1.6　自编码器　　13
1.7　循环神经网络　　14
1.8　几种深度学习框架对比　　14
1.9　小结　　16
第2章　TensorFlow初探　　17
2.1　总览　　17
2.1.1　TensorFlow 1.x版本特性　　18
2.1.2　使用上的改进　　18
2.1.3　TensorFlow安装与入门　　19
2.2　在Linux上安装TensorFlow　　19
2.3　为TensorFlow启用NVIDIA GPU　　20
2.3.1　第1步：安装NVIDIA CUDA　　20
2.3.2　　第2步：安装NVIDIA cuDNN v5.1+　　21
2.3.3　　第3步：确定GPU卡的CUDA计算能力为3.0+　　22
2.3.4　第4步：安装libcupti-dev库　　22
2.3.5　　第5步：安装Python
（或Python 3）　　22
2.3.6　第6步：安装并升级PIP
（或PIP3）　　22
2.3.7　第7步：安装TensorFlow　　23
2.4　如何安装TensorFlow　　23
2.4.1　直接使用pip安装　　23
2.4.2　使用virtualenv安装　　24
2.4.3　从源代码安装　　26
2.5　在Windows上安装TensorFlow　　27
2.5.1　在虚拟机上安装TensorFlow　　27
2.5.2　直接安装到Windows　　27
2.6　测试安装是否成功　　28
2.7　计算图　　28
2.8　为何采用计算图　　29
2.9　编程模型　　30
2.10　数据模型　　33
2.10.1　阶　　33
2.10.2　形状　　33
2.10.3　数据类型　　34
2.10.4　变量　　36
2.10.5　取回　　37
2.10.6　注入　　38
2.11　TensorBoard　　38
2.12　实现一个单输入神经元　　39
2.13　单输入神经元源代码　　43
2.14　迁移到TensorFlow 1.x版本　　43
2.14.1　如何用脚本升级　　44
2.14.2　局限　　47
2.14.3　手动升级代码　　47
2.14.4　变量　　47
2.14.5　汇总函数　　47
2.14.6　简化的数学操作　　48
2.14.7　其他事项　　49
2.15　小结　　49
第3章　用TensorFlow构建前馈
神经网络　　51
3.1　前馈神经网络介绍　　51
3.1.1　前馈和反向传播　　52
3.1.2　权重和偏差　　53
3.1.3　传递函数　　53
3.2　手写数字分类　　54
3.3　探究MNIST数据集　　55
3.4　softmax分类器　　57
3.5　TensorFlow模型的保存和还原　　63
3.5.1　保存模型　　63
3.5.2　还原模型　　63
3.5.3　softmax源代码　　65
3.5.4　softmax启动器源代码　　66
3.6　实现一个五层神经网络　　67
3.6.1　可视化　　69
3.6.2　五层神经网络源代码　　70
3.7　ReLU分类器　　72
3.8　可视化　　73
3.9　dropout优化　　76
3.10　可视化　　78
3.11　小结　　80
第4章　TensorFlow与卷积神经网络　　82
4.1　CNN简介　　82
4.2　CNN架构　　84
4.3　构建你的第一个CNN　　86
4.4　CNN表情识别　　95
4.4.1　表情分类器源代码　　104
4.4.2　使用自己的图像测试模型　　107
4.4.3　源代码　　109
4.5　小结　　111
第5章　优化TensorFlow自编码器　　112
5.1　自编码器简介　　112
5.2　实现一个自编码器　　113
5.3　增强自编码器的鲁棒性　　119
5.4　构建去噪自编码器　　120
5.5　卷积自编码器　　127
5.5.1　编码器　　127
5.5.2　解码器　　128
5.5.3　卷积自编码器源代码　　134
5.6　小结　　138
第6章　循环神经网络　　139
6.1　RNN的基本概念　　139
6.2　RNN的工作机制　　140
6.3　RNN的展开　　140
6.4　梯度消失问题　　141
6.5　LSTM网络　　142
6.6　RNN图像分类器　　143
6.7　双向RNN　　149
6.8　文本预测　　155
6.8.1　数据集　　156
6.8.2　困惑度　　156
6.8.3　PTB模型　　156
6.8.4　运行例程　　157
6.9　小结　　158
第7章　GPU计算　　160
7.1　GPGPU计算　　160
7.2　GPGPU的历史　　161
7.3　CUDA架构　　161
7.4　GPU编程模型　　162
7.5　TensorFlow中GPU的设置　　163
7.6　TensorFlow的GPU管理　　165
7.7　GPU内存管理　　168
7.8　在多GPU系统上分配单个GPU　　168
7.9　使用多个GPU　　170
7.10　小结　　171
第8章　TensorFlow高级编程　　172
8.1　Keras简介　　172
8.2　构建深度学习模型　　174
8.3　影评的情感分类　　175
8.4　添加一个卷积层　　179
8.5　Pretty Tensor　　181
8.6　数字分类器　　182
8.7　TFLearn　　187
8.8　泰坦尼克号幸存者预测器　　188
8.9　小结　　191
第9章　TensorFlow高级多媒体编程　　193
9.1　多媒体分析简介　　193
9.2　基于深度学习的大型对象检测　　193
9.2.1　瓶颈层　　195
9.2.2　使用重训练的模型　　195
9.3　加速线性代数　　197
9.3.1　TensorFlow的核心优势　　197
9.3.2　加速线性代数的准时编译　　197
9.4　TensorFlow和Keras　　202
9.4.1　Keras简介　　202
9.4.2　拥有Keras的好处　　203
9.4.3　视频问答系统　　203
9.5　Android上的深度学习　　209
9.5.1　TensorFlow演示程序　　209
9.5.2　Android入门　　211
9.6　小结　　214
第10章　强化学习　　215
10.1　强化学习基本概念　　216
10.2　Q-learning算法　　217
10.3　OpenAI Gym框架简介　　218
10.4　FrozenLake-v0实现问题　　220
10.5　使用TensorFlow实现Q-learning　　223
10.6　小结　　227
・ ・ ・ ・ ・ ・ (收起)上篇 初见
第1天 什么是深度学习 2
1.1 星星之火，可以燎原 3
1.2 师夷长技 4
1.2.1 谷歌与微软 4
1.2.2 Facebook、亚马逊与NVIDIA 5
1.3 中国崛起 6
1.3.1 BAT在路上 6
1.3.2 星光闪耀 7
1.3.3 企业热是风向标 8
1.4 练习题 9
第2天 深度学习的过往 10
2.1 传统机器学习的局限性 10
2.2 从表示学习到深度学习 11
2.3 监督学习 12
2.4 反向传播算法 13
2.5 卷积神经网络 15
2.6 深度学习反思 17
2.7 练习题 18
2.8 参考资料 18
第3天 深度学习工具汇总 19
3.1 Caffe 19
3.2 Torch & OverFeat 20
3.3 MxNet 22
3.4 TensorFlow 22
3.5 Theano 24
3.6 CNTK 24
3.7 练习题 25
3.8 参考资料 26
第4天 准备Caffe环境 27
4.1 Mac OS环境准备 27
4.2 Ubuntu环境准备 28
4.3 RHEL/Fedora/CentOS环境准备 29
4.4 Windows环境准备 29
4.5 常见问题 32
4.6 练习题 32
4.7 参考资料 33
第5天 Caffe依赖包解析 34
5.1 ProtoBuffer 34
5.2 Boost 38
5.3 GFLAGS 38
5.4 GLOG 39
5.5 BLAS 40
5.6 HDF5 41
5.7 OpenCV 42
5.8 LMDB和LEVELDB 42
5.9 Snappy 43
5.10 小结 43
5.11 练习题 49
5.12 参考资料 49
第6天 运行手写体数字识别例程 50
6.1 MNIST数据集 50
6.1.1 下载MNIST数据集 50
6.1.2 MNIST数据格式描述 51
6.1.3 转换格式 53
6.2 LeNet-5模型 60
6.2.1 LeNet-5模型描述 60
6.2.2 训练超参数 65
6.2.3 训练日志 66
6.2.4 用训练好的模型对数据进行预测 76
6.2.5 Windows下训练模型 76
6.3 回顾 78
6.4 练习题 79
6.5 参考资料 79
篇尾语 80
中篇 热恋
第7天 Caffe代码梳理 82
7.1 Caffe目录结构 82
7.2 如何有效阅读Caffe源码 84
7.3 Caffe支持哪些深度学习特性 86
7.3.1 卷积层 86
7.3.2 全连接层 89
7.3.3 激活函数 91
7.4 小结 99
7.5 练习题 99
7.6 参考资料 100
第8天 Caffe数据结构 101
8.1 Blob 101
8.1.1 Blob基本用法 102
8.1.2 数据结构描述 108
8.1.3 Blob是怎样炼成的 109
8.2 Layer 125
8.2.1 数据结构描述 126
8.2.2 Layer是怎样建成的 127
8.3 Net 136
8.3.1 Net基本用法 136
8.3.2 数据结构描述 139
8.3.3 Net是怎样绘成的 139
8.4 机制和策略 146
8.5 练习题 147
8.6 参考资料 148
第9天 Caffe I/O模块 149
9.1 数据读取层 149
9.1.1 数据结构描述 149
9.1.2 数据读取层实现 150
9.2 数据变换器 155
9.2.1 数据结构描述 155
9.2.2 数据变换器的实现 156
9.3 练习题 171
第10天 Caffe模型 172
10.1 prototxt表示 173
10.2 内存中的表示 176
10.3 磁盘上的表示 176
10.4 Caffe Model Zoo 178
10.5 练习题 180
10.6 参考资料 180
第11天 Caffe前向传播计算 181
11.1 前向传播的特点 181
11.2 前向传播的实现 182
11.2.1 DAG构造过程 182
11.2.2 Net Forward实现 190
11.3 练习题 192
第12天 Caffe反向传播计算 193
12.1 反向传播的特点 193
12.2 损失函数 193
12.2.1 算法描述 194
12.2.2 参数描述 195
12.2.3 源码分析 195
12.3 反向传播的实现 203
12.4 练习题 205
第13天 Caffe最优化求解过程 207
13.1 求解器是什么 207
13.2 求解器是如何实现的 208
13.2.1 算法描述 208
13.2.2 数据结构描述 210
13.2.3 CNN训练过程 218
13.2.4 CNN预测过程 225
13.2.5 Solver的快照和恢复功能 227
13.3 练习题 230
第14天 Caffe实用工具 231
14.1 训练和预测 231
14.2 特征提取 241
14.3 转换图像格式 247
14.4 计算图像均值 254
14.5 自己编写工具 257
14.6 练习题 257
篇尾语 258
下篇 升华
第15天 Caffe计算加速 260
15.1 Caffe计时功能 260
15.2 Caffe GPU加速模式 262
15.2.1 GPU是什么 262
15.2.2 CUDA是什么 263
15.2.3 GPU、CUDA和深度学习 263
15.2.4 Caffe GPU环境准备 264
15.2.5 切换到Caffe GPU加速模式 268
15.3 Caffe cuDNN加速模式 269
15.3.1 获取cuDNN 270
15.3.2 切换到Caffe cuDNN加速模式 270
15.3.3 Caffe不同硬件配置性能 272
15.4 练习题 273
15.5 参考资料 273
第16天 Caffe可视化方法 275
16.1 数据可视化 275
16.1.1 MNIST数据可视化 275
16.1.2 CIFAR10数据可视化 277
16.1.3 ImageNet数据可视化 278
16.2 模型可视化 279
16.2.1 网络结构可视化 279
16.2.2 网络权值可视化 281
16.3 特征图可视化 288
16.4 学习曲线 295
16.5 小结 298
16.6 练习题 298
16.7 参考资料 299
第17天 Caffe迁移和部署 300
17.1 从开发测试到生产部署 300
17.2 使用Docker 302
17.2.1 Docker基本概念 302
17.2.2 Docker安装 303
17.2.3 Docker入门 305
17.2.4 Docker使用进阶 312
17.3 练习题 317
17.4 参考资料 317
第18天 关于ILSVRC不得不说的一些事儿 318
18.1 ImageNet数据集 318
18.2 ILSVRC比赛项目 319
18.2.1 图像分类（CLS） 320
18.2.2 目标定位（LOC） 320
18.2.3 目标检测（DET） 321
18.2.4 视频目标检测（VID） 322
18.2.5 场景分类 322
18.3 Caffe ILSVRC实践 323
18.4 练习题 326
18.5 参考资料 326
第19天 放之四海而皆准 327
19.1 图像分类 327
19.1.1 问题描述 327
19.1.2 应用案例--商品分类 330
19.2 图像中的字符识别 332
19.2.1 问题描述 332
19.2.2 应用案例--身份证实名认证 333
19.3 目标检测 337
19.3.1 问题描述 337
19.3.2 最佳实践--运行R-CNN例程 337
19.4 人脸识别 340
19.4.1 问题描述 340
19.4.2 最佳实践--使用Face++ SDK实现人脸检测 342
19.5 自然语言处理 343
19.5.1 问题描述 343
19.5.2 最佳实践--NLP-Caffe 344
19.6 艺术风格 350
19.6.1 问题描述 350
19.6.2 最佳实践--style-transfer 352
19.7 小结 354
19.8 练习题 354
19.9 参考资料 355
第20天 继往开来的领路人 356
20.1 Caffe Traps and Pitfalls 356
20.1.1 不支持任意数据类型 356
20.1.2 不够灵活的高级接口 357
20.1.3 繁杂的依赖包 357
20.1.4 堪忧的卷积层实现 357
20.1.5 架构之殇 358
20.1.6 应用场景局限性 358
20.2 最佳实践--Caffe2 359
20.3 练习题 361
20.4 参考资料 362
第21天 新生 363
21.1 三人行，必有我师 363
21.2 路漫漫其修远兮，吾将上下而求索 364
篇尾语 366
结束语 367
附录A 其他深度学习工具
・ ・ ・ ・ ・ ・ (收起)第 1 章 深度学习介绍 1
1.1 人工智能 1
1.2 数据挖掘、机器学习与深度学习2
1.2.1 数据挖掘 3
1.2.2 机器学习 3
1.2.3 深度学习 4
1.3 学习资源与建议 8
第 2 章 深度学习框架 11
2.1 深度学习框架介绍 . 11
2.2 PyTorch 介绍. 13
2.2.1 什么是 PyTorch. 13
2.2.2 为何要使用 PyTorch 14
2.3 配置 PyTorch 深度学习环境 15
2.3.1 操作系统的选择. 15
2.3.2 Python 开发环境的安装 16
2.3.3 PyTorch 的安装. 18
第 3 章 多层全连接神经网络 24
3.1 热身：PyTorch 基础 24
3.1.1 Tensor（张量）. 24
3.1.2 Variable（变量）26
3.1.3 Dataset（数据集）28
3.1.4 nn.Module（模组） 29
3.1.5 torch.optim（优化） 30
3.1.6 模型的保存和加载 31
3.2 线性模型 32
3.2.1 问题介绍 32
3.2.2 一维线性回归33
3.2.3 多维线性回归34
3.2.4 一维线性回归的代码实现. 35
3.2.5 多项式回归 38
3.3 分类问题 42
3.3.1 问题介绍 42
3.3.2 Logistic 起源 42
3.3.3 Logistic 分布 42
3.3.4 二分类的 Logistic 回归 43
3.3.5 模型的参数估计. 44
3.3.6 Logistic 回归的代码实现45
3.4 简单的多层全连接前向网络 . 49
3.4.1 模拟神经元 49
3.4.2 单层神经网络的分类器 50
3.4.3 激活函数 51
3.4.4 神经网络的结构. 54
3.4.5 模型的表示能力与容量 55
3.5 深度学习的基石：反向传播算法57
3.5.1 链式法则 57
3.5.2 反向传播算法58
3.5.3 Sigmoid 函数举例58
3.6 各种优化算法的变式59
3.6.1 梯度下降法 59
3.6.2 梯度下降法的变式 62
3.7 处理数据和训练模型的技巧 . 64
3.7.1 数据预处理 64
3.7.2 权重初始化 66
3.7.3 防止过拟合 67
3.8 多层全连接神经网络实现 MNIST 手写数字分类 69
3.8.1 简单的三层全连接神经网络70
3.8.2 添加激活函数70
3.8.3 添加批标准化71
3.8.4 训练网络 71
第 4 章 卷积神经网络 76
4.1 主要任务及起源 76
4.2 卷积神经网络的原理和结构 . 77
4.2.1 卷积层80
4.2.2 池化层84
4.2.3 全连接层 85
4.2.4 卷积神经网络的基本形式. 85
4.3 PyTorch 卷积模块 . 87
4.3.1 卷积层87
4.3.2 池化层88
4.3.3 提取层结构 90
4.3.4 如何提取参数及自定义初始化 91
4.4 卷积神经网络案例分析. 92
4.4.1 LeNet. 93
4.4.2 AlexNet94
4.4.3 VGGNet 95
4.4.4 GoogLeNet . 98
4.4.5 ResNet100
4.5 再实现 MNIST 手写数字分类 . 103
4.6 图像增强的方法 105
4.7 实现 cifar10 分类 107
第 5 章 循环神经网络 111
5.1 循环神经网络111
5.1.1 问题介绍 112
5.1.2 循环神经网络的基本结构. 112
5.1.3 存在的问题 115
5.2 循环神经网络的变式：LSTM 与 GRU 116
5.2.1 LSTM. 116
5.2.2 GRU. 119
5.2.3 收敛性问题 120
5.3 循环神经网络的 PyTorch 实现 122
5.3.1 PyTorch 的循环网络模块122
5.3.2 实例介绍 127
5.4 自然语言处理的应用131
5.4.1 词嵌入131
5.4.2 词嵌入的 PyTorch 实现 133
5.4.3 N Gram 模型 133
5.4.4 单词预测的 PyTorch 实现134
5.4.5 词性判断 136
5.4.6 词性判断的 PyTorch 实现137
5.5 循环神经网络的更多应用140
5.5.1 Many to one 140
5.5.2 Many to Many（shorter）141
5.5.3 Seq2seq141
5.5.4 CNN+RNN . 142
第 6 章 生成对抗网络 144
6.1 生成模型 144
6.1.1 自动编码器 145
6.1.2 变分自动编码器. 150
6.2 生成对抗网络153
6.2.1 何为生成对抗网络 153
6.2.2 生成对抗网络的数学原理. 160
6.3 Improving GAN164
6.3.1 Wasserstein GAN. 164
6.3.2 Improving WGAN167
6.4 应用介绍 168
6.4.1 Conditional GAN. 168
6.4.2 Cycle GAN . 170
第 7 章 深度学习实战 173
7.1 实例一――猫狗大战：运用预训练卷积神经网络进行特征提取与预测 . 173
7.1.1 背景介绍 174
7.1.2 原理分析 174
7.1.3 代码实现 177
7.1.4 总结. 183
7.2 实例二――Deep Dream：探索卷积神经网络眼中的世界183
7.2.1 原理介绍 184
7.2.2 预备知识：backward . 185
7.2.3 代码实现 190
7.2.4 总结. 195
7.3 实例三――Neural-Style：使用 PyTorch 进行风格迁移196
7.3.1 背景介绍 196
7.3.2 原理分析 197
7.3.3 代码实现 199
7.3.4 总结. 205
7.4 实例四――Seq2seq：通过 RNN 实现简单的 Neural Machine Translation . 205
7.4.1 背景介绍 206
7.4.2 原理分析 206
7.4.3 代码实现 209
7.4.4 总结. 221
・ ・ ・ ・ ・ ・ (收起)第1章　深度学习介绍　　1
1.1　开始深度学习之旅　　5
1.1.1　深度前馈网络　　6
1.1.2　各种学习算法　　6
1.2　深度学习的相关术语　　10
1.3　深度学习――一场人工智能革命　　12
1.4　深度学习网络的分类　　18
1.4.1　深度生成或无监督模型　　19
1.4.2　深度判别模型　　20
1.5　小结　　22
第2章　大规模数据的分布式深度学习　　23
2.1　海量数据的深度学习　　24
2.2　大数据深度学习面临的挑战　　27
2.2.1　海量数据带来的挑战（第一个V）　　28
2.2.2　数据多样性带来的挑战（第二个V）　　28
2.2.3　数据快速处理带来的挑战（第三个V）　　29
2.2.4　数据真实性带来的挑战（第四个V）　　29
2.3　分布式深度学习和Hadoop　　29
2.3.1　Map-Reduce　　31
2.3.2　迭代Map-Reduce　　31
2.3.3　YARN　　32
2.3.4　分布式深度学习设计的重要特征　　32
2.4　深度学习的开源分布式框架Deeplearning4j　　34
2.4.1　Deeplearning4j的主要特性　　34
2.4.2　Deeplearning4j功能总结　　35
2.5　在Hadoop YARN上配置Deeplearning4j　　35
2.5.1　熟悉Deeplearning4j　　36
2.5.2　为进行分布式深度学习集成Hadoop YARN和Spark　　40
2.5.3　Spark在Hadoop YARN上的内存分配规则　　40
2.6　小结　　44
第3章　卷积神经网络　　45
3.1　卷积是什么　　46
3.2　卷积神经网络的背景　　47
3.3　卷积神经网络的基本层　　48
3.3.1　卷积神经网络深度的重要性　　49
3.3.2　卷积层　　49
3.3.3　为卷积层选择超参数　　52
3.3.4　ReLU层　　56
3.3.5　池化层　　57
3.3.6　全连接层　　58
3.4　分布式深度卷积神经网络　　58
3.4.1　最受欢迎的深度神经网络及其配置　　58
3.4.2　训练时间――深度神经网络面临的主要挑战　　59
3.4.3　将Hadoop应用于深度卷积神经网络　　59
3.5　使用Deeplearning4j构建卷积层　　61
3.5.1　加载数据　　61
3.5.2　模型配置　　62
3.5.3　训练与评估　　63
3.6　小结　　64
第4章　循环神经网络　　65
4.1　循环网络与众不同的原因　　66
4.2　循环神经网络　　67
4.2.1　展开循环计算　　68
4.2.2　循环神经网络的记忆　　69
4.2.3　架构　　70
4.3　随时间反向传播　　71
4.4　长短期记忆　　73
4.4.1　随时间深度反向传播的问题　　73
4.4.2　长短期记忆　　73
4.5　双向循环神经网络　　75
4.5.1　循环神经网络的不足　　75
4.5.2　解决方案　　76
4.6　分布式深度循环神经网络　　77
4.7　用Deeplearning4j训练循环神经网络　　77
4.8　小结　　80
第5章　受限玻尔兹曼机　　81
5.1　基于能量的模型　　82
5.2　玻尔兹曼机　　83
5.2.1　玻尔兹曼机如何学习　　84
5.2.2　玻尔兹曼机的不足　　85
5.3　受限玻尔兹曼机　　85
5.3.1　基础架构　　85
5.3.2　受限玻尔兹曼机的工作原理　　86
5.4　卷积受限玻尔兹曼机　　88
5.5　深度信念网络　　90
5.6　分布式深度信念网络　　91
5.6.1　受限玻尔兹曼机的分布式训练　　91
5.6.2　深度信念网络的分布式训练　　92
5.7　用Deeplearning4j实现受限玻尔兹曼机和深度信念网络　　94
5.7.1　受限玻尔兹曼机　　94
5.7.2　深度信念网络　　95
5.8　小结　　97
第6章　自动编码器　　98
6.1　自动编码器　　98
6.2　稀疏自动编码器　　101
6.2.1　稀疏编码　　101
6.2.2　稀疏自动编码器　　102
6.3　深度自动编码器　　104
6.3.1　训练深度自动编码器　　104
6.3.2　使用Deeplearning4j实现深度自动编码器　　107
6.4　降噪自动编码器　　108
6.4.1　降噪自动编码器的架构　　109
6.4.2　堆叠式降噪自动编码器　　109
6.4.3　使用Deeplearning4j实现堆叠式降噪自动编码器　　110
6.5　自动编码器的应用　　112
6.6　小结　　112
第7章　用Hadoop玩转深度学习　　113
7.1　Hadoop中的分布式视频解码　　114
7.2　使用Hadoop进行大规模图像处理　　116
7.3　使用Hadoop进行自然语言处理　　117
7.3.1　Web爬虫　　118
7.3.2　自然语言处理的关键词提取和模块　　118
7.3.3　从页面评估相关关键词　　118
7.4　小结　　119
参考文献　　120
・ ・ ・ ・ ・ ・ (收起)第1 章绪论.1
1.1 知识图谱简介2
1.2 深度学习的优势和挑战4
1.3 深度学习+ 知识图谱=1 .8
1.3.1 知识的表示学习9
1.3.2 知识的自动获取10
1.3.3 知识的计算应用13
1.4 本书结构14
1.5 本章总结14
第一篇世界知识图谱
第2 章世界知识的表示学习19
2.1 章节引言19
2.2 相关工作20
2.2.1 知识表示学习经典模型20
2.2.2 平移模型及其拓展模型22
2.3 基于复杂关系建模的知识表示学习25
2.3.1 算法模型.25
2.3.2 实验分析.26
2.3.3 小结32
2.4 基于关系路径建模的知识表示学习32
2.4.1 算法模型.32
2.4.2 实验分析.34
2.4.3 小结39
vi j 知识图谱与深度学习
2.5 基于属性关系建模的知识表示学习39
2.5.1 算法模型.40
2.5.2 实验分析.41
2.5.3 小结44
2.6 融合实体描述信息的知识表示学习44
2.6.1 算法模型.45
2.6.2 实验分析.47
2.6.3 小结54
2.7 融合层次类型信息的知识表示学习55
2.7.1 算法模型.55
2.7.2 实验分析.57
2.7.3 小结62
2.8 融合实体图像信息的知识表示学习62
2.8.1 算法模型.63
2.8.2 实验分析.64
2.8.3 小结68
2.9 本章总结68
第3 章世界知识的自动获取70
3.1 章节引言70
3.2 相关工作71
3.2.1 有监督的关系抽取模型71
3.2.2 远程监督的关系抽取模型.72
3.3 基于选择性注意力机制的关系抽取73
3.3.1 算法模型.74
3.3.2 实验分析.78
3.3.3 小结82
3.4 基于关系层次注意力机制的关系抽取83
3.4.1 算法模型.83
目录j vii
3.4.2 实验分析.86
3.4.3 小结89
3.5 基于选择性注意力机制的多语言关系抽取.89
3.5.1 算法模型.90
3.5.2 实验分析.93
3.5.3 小结98
3.6 引入对抗训练的多语言关系抽取98
3.6.1 算法模型.99
3.6.2 实验分析.103
3.6.3 小结106
3.7 基于知识图谱与文本互注意力机制的知识获取.106
3.7.1 算法模型.107
3.7.2 实验分析.112
3.7.3 小结117
3.8 本章总结118
第4 章世界知识的计算应用119
4.1 章节引言119
4.2 细粒度实体分类120
4.2.1 算法模型.120
4.2.2 实验分析.122
4.2.3 小结129
4.3 实体对齐129
4.3.1 算法模型.129
4.3.2 实验分析.132
4.3.3 小结135
4.4 融入知识的信息检索.136
4.4.1 算法模型.136
4.4.2 实验分析.138
4.4.3 小结143
viii j 知识图谱与深度学习
4.5 本章总结143
第二篇语言知识图谱
第5 章语言知识的表示学习147
5.1 章节引言147
5.2 相关工作148
5.2.1 词表示学习148
5.2.2 词义消歧.149
5.3 义原的表示学习149
5.3.1 算法模型.149
5.3.2 实验分析.152
5.3.3 小结155
5.4 基于义原的词表示学习156
5.4.1 算法模型.156
5.4.2 实验分析.159
5.4.3 小结164
5.5 本章总结164
第6 章语言知识的自动获取166
6.1 章节引言166
6.2 相关工作167
6.2.1 知识图谱及其构建167
6.2.2 子词和字级NLP 167
6.2.3 词表示学习及跨语言的词表示学习167
6.3 基于协同过滤和矩阵分解的义原预测168
6.3.1 算法模型.168
6.3.2 实验分析.171
6.3.3 小结175
6.4 融入中文字信息的义原预测175
6.4.1 算法模型.176
目录j ix
6.4.2 实验分析.179
6.4.3 小结183
6.5 跨语言词汇的义原预测183
6.5.1 算法模型.184
6.5.2 实验分析.188
6.5.3 小结194
6.6 本章总结194
第7 章语言知识的计算应用195
7.1 章节引言195
7.2 义原驱动的词典扩展.196
7.2.1 相关工作.196
7.2.2 任务设定.198
7.2.3 算法模型.199
7.2.4 实验分析.202
7.2.5 小结207
7.3 义原驱动的神经语言模型.207
7.3.1 相关工作.208
7.3.2 任务设定.209
7.3.3 算法模型.210
7.3.4 实验分析.213
7.3.5 小结219
7.4 本章总结219
第8 章总结与展望220
8.1 本书总结220
8.2 未来展望221
8.2.1 更全面的知识类型221
8.2.2 更复杂的知识结构222
8.2.3 更有效的知识获取223
8.2.4 更强大的知识指导223
x j 知识图谱与深度学习
8.2.5 更精深的知识推理224
8.3 结束语224
相关开源资源226
参考文献228
后记.243
・ ・ ・ ・ ・ ・ (收起)第1 章绪论.1
1.1 知识图谱简介2
1.2 深度学习的优势和挑战4
1.3 深度学习+ 知识图谱=1 .8
1.3.1 知识的表示学习9
1.3.2 知识的自动获取10
1.3.3 知识的计算应用13
1.4 本书结构14
1.5 本章总结14
第一篇世界知识图谱
第2 章世界知识的表示学习19
2.1 章节引言19
2.2 相关工作20
2.2.1 知识表示学习经典模型20
2.2.2 平移模型及其拓展模型22
2.3 基于复杂关系建模的知识表示学习25
2.3.1 算法模型.25
2.3.2 实验分析.26
2.3.3 小结32
2.4 基于关系路径建模的知识表示学习32
2.4.1 算法模型.32
2.4.2 实验分析.34
2.4.3 小结39
vi j 知识图谱与深度学习
2.5 基于属性关系建模的知识表示学习39
2.5.1 算法模型.40
2.5.2 实验分析.41
2.5.3 小结44
2.6 融合实体描述信息的知识表示学习44
2.6.1 算法模型.45
2.6.2 实验分析.47
2.6.3 小结54
2.7 融合层次类型信息的知识表示学习55
2.7.1 算法模型.55
2.7.2 实验分析.57
2.7.3 小结62
2.8 融合实体图像信息的知识表示学习62
2.8.1 算法模型.63
2.8.2 实验分析.64
2.8.3 小结68
2.9 本章总结68
第3 章世界知识的自动获取70
3.1 章节引言70
3.2 相关工作71
3.2.1 有监督的关系抽取模型71
3.2.2 远程监督的关系抽取模型.72
3.3 基于选择性注意力机制的关系抽取73
3.3.1 算法模型.74
3.3.2 实验分析.78
3.3.3 小结82
3.4 基于关系层次注意力机制的关系抽取83
3.4.1 算法模型.83
目录j vii
3.4.2 实验分析.86
3.4.3 小结89
3.5 基于选择性注意力机制的多语言关系抽取.89
3.5.1 算法模型.90
3.5.2 实验分析.93
3.5.3 小结98
3.6 引入对抗训练的多语言关系抽取98
3.6.1 算法模型.99
3.6.2 实验分析.103
3.6.3 小结106
3.7 基于知识图谱与文本互注意力机制的知识获取.106
3.7.1 算法模型.107
3.7.2 实验分析.112
3.7.3 小结117
3.8 本章总结118
第4 章世界知识的计算应用119
4.1 章节引言119
4.2 细粒度实体分类120
4.2.1 算法模型.120
4.2.2 实验分析.122
4.2.3 小结129
4.3 实体对齐129
4.3.1 算法模型.129
4.3.2 实验分析.132
4.3.3 小结135
4.4 融入知识的信息检索.136
4.4.1 算法模型.136
4.4.2 实验分析.138
4.4.3 小结143
viii j 知识图谱与深度学习
4.5 本章总结143
第二篇语言知识图谱
第5 章语言知识的表示学习147
5.1 章节引言147
5.2 相关工作148
5.2.1 词表示学习148
5.2.2 词义消歧.149
5.3 义原的表示学习149
5.3.1 算法模型.149
5.3.2 实验分析.152
5.3.3 小结155
5.4 基于义原的词表示学习156
5.4.1 算法模型.156
5.4.2 实验分析.159
5.4.3 小结164
5.5 本章总结164
第6 章语言知识的自动获取166
6.1 章节引言166
6.2 相关工作167
6.2.1 知识图谱及其构建167
6.2.2 子词和字级NLP 167
6.2.3 词表示学习及跨语言的词表示学习167
6.3 基于协同过滤和矩阵分解的义原预测168
6.3.1 算法模型.168
6.3.2 实验分析.171
6.3.3 小结175
6.4 融入中文字信息的义原预测175
6.4.1 算法模型.176
目录j ix
6.4.2 实验分析.179
6.4.3 小结183
6.5 跨语言词汇的义原预测183
6.5.1 算法模型.184
6.5.2 实验分析.188
6.5.3 小结194
6.6 本章总结194
第7 章语言知识的计算应用195
7.1 章节引言195
7.2 义原驱动的词典扩展.196
7.2.1 相关工作.196
7.2.2 任务设定.198
7.2.3 算法模型.199
7.2.4 实验分析.202
7.2.5 小结207
7.3 义原驱动的神经语言模型.207
7.3.1 相关工作.208
7.3.2 任务设定.209
7.3.3 算法模型.210
7.3.4 实验分析.213
7.3.5 小结219
7.4 本章总结219
第8 章总结与展望220
8.1 本书总结220
8.2 未来展望221
8.2.1 更全面的知识类型221
8.2.2 更复杂的知识结构222
8.2.3 更有效的知识获取223
8.2.4 更强大的知识指导223
x j 知识图谱与深度学习
8.2.5 更精深的知识推理224
8.3 结束语224
相关开源资源226
参考文献228
后记.243
・ ・ ・ ・ ・ ・ (收起)第1 章什么是推荐系统1
1.1 推荐系统的概念.1
1.1.1 推荐系统的基本概念1
1.1.2 深度学习与推荐系统4
第2 章深度神经网络.7
2.1 什么是深度学习.7
2.1.1 深度学习的三次兴起7
2.1.2 深度学习的优势9
2.2 神经网络基础11
2.2.1 神经元11
2.2.2 神经网络.12
2.2.3 反向传播.13
2.2.4 优化算法.14
2.3 卷积网络基础17
2.3.1 卷积层17
2.3.2 池化层19
2.3.3 常见的网络结构19
2.4 循环网络基础21
2.4.1 时序反向传播算法22
2.4.2 长短时记忆网络24
2.5 生成对抗基础25
2.5.1 对抗博弈.26
2.5.2 理论推导.27
2.5.3 常见的生成对抗网络29
iv j 推荐系统与深度学习
第3 章TensorFlow 平台31
3.1 什么是TensorFlow 31
3.2 TensorFlow 安装指南.33
3.2.1 Windows 环境安装.33
3.2.2 Linux 环境安装.34
3.3 TensorFlow 基础.36
3.3.1 数据流图.36
3.3.2 会话37
3.3.3 图可视化.37
3.3.4 变量37
3.3.5 占位符38
3.3.6 优化器38
3.3.7 一个简单的例子38
3.4 其他深度学习平台39
第4 章推荐系统的基础算法42
4.1 基于内容的推荐算法.42
4.1.1 基于内容的推荐算法基本流程42
4.1.2 基于内容推荐的特征提取.45
4.2 基于协同的推荐算法.47
4.2.1 基于物品的协同算法49
4.2.2 基于用户的协同算法57
4.2.3 基于用户协同和基于物品协同的区别59
4.2.4 基于矩阵分解的推荐方法.61
4.2.5 基于稀疏自编码的推荐方法.71
4.3 基于社交网络的推荐算法80
4.3.1 基于用户的推荐在社交网络中的应用81
4.3.2 node2vec 技术在社交网络推荐中的应用85
4.4 推荐系统的冷启动问题94
4.4.1 如何解决推荐系统冷启动问题94
4.4.2 深度学习技术在物品冷启动上的应用101
目录j v
第5 章混合推荐系统119
5.1 什么是混合推荐系统.119
5.1.1 混合推荐系统的意义120
5.1.2 混合推荐系统的算法分类.122
5.2 推荐系统特征处理方法125
5.2.1 特征处理方法126
5.2.2 特征选择方法134
5.3 常见的预测模型141
5.3.1 基于逻辑回归的模型141
5.3.2 基于支持向量机的模型.144
5.3.3 基于梯度提升树的模型.148
5.4 排序学习150
5.4.1 基于排序的指标来优化.150
5.4.2 L2R 算法的三种情形.152
第6 章基于深度学习的推荐模型156
6.1 基于DNN 的推荐算法156
6.2 基于DeepFM 的推荐算法163
6.3 基于矩阵分解和图像特征的推荐算法171
6.4 基于循环网络的推荐算法.174
6.5 基于生成对抗网络的推荐算法.176
6.5.1 IRGAN 的代码实现.179
第7 章推荐系统架构设计.183
7.1 推荐系统基本模型183
7.2 推荐系统常见架构185
7.2.1 基于离线训练的推荐系统架构设计185
7.2.2 面向深度学习的推荐系统架构设计191
7.2.3 基于在线训练的推荐系统架构设计194
7.2.4 面向内容的推荐系统架构设计197
7.3 推荐系统常用组件199
7.3.1 数据上报常用组件199
vi j 推荐系统与深度学习
7.3.2 离线存储常用组件200
7.3.3 离线计算常用组件200
7.3.4 在线存储常用组件201
7.3.5 模型服务常用组件201
7.3.6 实时计算常用组件201
7.4 推荐系统常见问题201
7.4.1 实时性.201
7.4.2 多样性.202
7.4.3 曝光打击和不良内容过滤.202
7.4.4 评估测试.202
后记.203
图1.1 淘宝猜你喜欢栏目2
图1.2 百度指数.4
图1.3 歌曲词嵌入模型空间向量.6
图2.1 神经网络的三次兴起8
图2.2 不同层数的神经网络拟合分界面的能力.10
图2.3 不同层数的神经网络表示能力10
图2.4 神经网络的基本结构11
图2.5 感知器算法12
图2.6 三层全连接神经网络13
图2.7 动量对比.16
图2.8 卷积运算.18
图2.9 池化层19
图2.10 LeNet 卷积结构.20
图2.11 Alex-Net 卷积结构20
图2.12 RNN 21
图2.13 LSTM 在t 时刻的内部结构24
图2.14 GAN 网络25
图3.1 TensorFlow 安装截图34
图3.2 TensorBoard 计算37
图4.1 腾讯视频APP 推荐页面.44
图4.2 截取自当当网.49
图4.3 截取自QQ 音乐APP.49
图4.4 用户购买物品记录50
图4.5 同时被购买次数矩阵C 51
图4.6 相似度计算结果1 52
图4.7 相似度计算结果2 54
viii j 推荐系统与深度学习
图4.8 相似度计算结果3 55
图4.9 截取自当当网.57
图4.10 物品的倒排索引57
图4.11 用户评分矩阵.63
图4.12 Sigma 值64
图4.13 NewData 值65
图4.14 Mydata 值65
图4.15 自编码神经网络模型72
图4.16 稀疏自编码第一个网络.73
图4.17 稀疏自编码第二个网络.74
图4.18 稀疏自编码第三个网络.75
图4.19 将三个网络组合起来75
图4.20 社交网络关系图示例81
图4.21 融入用户关系和物品关系82
图4.22 社交网络关系图示例86
图4.23 社交网络关系图示例86
图4.24 CBOW 和Skip-Gram 示例.88
图4.25 Skip-Gram 网络结构89
图4.26 CBOW 网络结构91
图4.27 word analogy 示例93
图4.28 某网站登录页面95
图4.29 QQ 互联开放注册平台1 96
图4.30 QQ 互联开放注册平台2 97
图4.31 QQ 互联应用管理页面1 97
图4.32 QQ 互联应用管理页面2 97
图4.33 QQ 互联QQ 登录功能获取97
图4.34 QQ 音乐APP 中的偏好选择98
图4.35 (a) 为每部电影被打分的分布，(b) 为每个用户打分的分布100
图4.36 (a) 为每部电影平均分分布，(b) 为每个用户平均分分布.100
图4.37 基于专家数据的CF 与基于用户数据CF 比较.101
图目录j ix
图4.38 音乐频谱示例102
图4.39 4 个流派的频谱图示例103
图4.40 CNN 音频分类结构.103
图4.41 CNN+LSTM 组合音频分类模型.104
图4.42 分类预测结果的混淆矩阵104
图4.43 模型倒数第二层128 维向量降维可视化104
图4.44 微软how-old.net 107
图4.45 SCUT-FBP 数据集示例图108
图4.46 脸部截取后的数据集示例图.108
图4.47 CNN 层数过多，误差反而较大113
图4.48 残差网络的基本结构113
图4.49 残差网络完整结构.114
图5.1 NetFlix 的实时推荐系统的架构图120
图5.2 整体式混合推荐系统125
图5.3 并行式混合推荐系统125
图5.4 流水线式混合推荐系统.125
图5.5 MDLP 特征离散化130
图5.6 ChiMerge 特征离散化.131
图5.7 层次化时间按序列特征.133
图5.8 Learn to rank 的局限153
图6.1 Wide & Deep 模型结构157
图6.2 推荐系统的召回和排序两个阶段158
图6.3 召回模型结构.159
图6.4 序列信息160
图6.5 排序模型结构.161
图6.6 不同NN 的效果162
图6.7 DeepFM 模型结构(网络左边为FM 层，右边为DNN 层).164
图6.8 FM 一阶部分165
图6.9 FM 二阶部分166
图6.10 FM/DNN/DeepFM 的比较171
x j 推荐系统与深度学习
图6.11 电影静止帧图片举例172
图6.12 Alex-Net 卷积网络.173
图6.13 左图：时间无关的推荐系统。右图：时间相关的推荐系统174
图6.14 基于循环神经网络的推荐系统175
图6.15 判别器177
图6.16 生成器178
图6.17 IRGAN 说明179
图7.1 监督学习基本模型.184
图7.2 基于离线训练的推荐系统架构设计186
图7.3 数据上报模块.187
图7.4 离线训练模块.187
图7.5 推荐系统中的存储分层.188
图7.6 在线预测的几个阶段189
图7.7 推荐系统通用性设计190
图7.8 面向深度学习的推荐系统架构设计191
图7.9 利用深度学习进行特征提取192
图7.10 参数服务器架构193
图7.11 基于在线训练的推荐系统架构设计195
图7.12 在线学习之实时特征处理196
图7.13 面向内容的推荐系统架构设计198
图7.14 用于推荐的内容池.198
图7.15 Apache Kafka 逻辑架构.200
表4.1 用户A 和B 的评分矩阵.43
表4.2 电影内容特征二进制表示45
表4.3 人脸魅力值打分不同模型的MAE 比较112
表4.4 人脸魅力值打分不同模型的MAE 比较117

表4.5 Keras 预训练好的图像分类模型118
・ ・ ・ ・ ・ ・ (收起)1 概述1
1.1 智能问答：让机器更好地服务于人 1
1.2 问答系统类型介绍 2
1.2.1 基于事实的问答系统 3
1.2.2 基于常见问题集的问答系统 3
1.2.3 开放域的问答系统 4
1.3 使用本书附带的源码程序 4
1.3.1 安装依赖软件 4
1.3.2 下载源码 5
1.3.3 执行示例程序 5
1.3.4 联系我们 6
1.4 全书结构 6
2 机器学习基础8
2.1 线性代数 8
2.1.1 标量、向量、矩阵和张量 8
2.1.2 矩阵运算 9
2.1.3 特殊类型的矩阵 10
2.1.4 线性相关 11
2.1.5 范数 12
2.2 概率论基础 12
2.2.1 随机变量 13
2.2.2 期望和方差 13
2.2.3 伯努利分布 14
2.2.4 二项分布 14
2.2.5 泊松分布 15
2.2.6 正态分布 15
2.2.7 条件概率、联合概率和全概率 17
2.2.8 先验概率与后验概率 18
2.2.9 边缘概率 18
2.2.10 贝叶斯公式 18
2.2.11 最大似然估计算法 19
2.2.12 线性回归模型 20
2.2.13 逻辑斯蒂回归模型 21
2.3 信息论基础 22
2.3.1 熵 23
2.3.2 联合熵和条件熵 23
2.3.3 相对熵与互信息 24
2.3.4 信道和信道容量 25
2.3.5 最大熵模型 26
2.3.6 信息论与机器学习 29
2.4 统计学习 29
2.4.1 输入空间、特征空间与输出空间 30
2.4.2 向量表示 30
2.4.3 数据集 31
2.4.4 从概率到函数 31
2.4.5 统计学习三要素 32
2.5 隐马尔可夫模型 33
2.5.1 随机过程和马尔可夫链 33
2.5.2 隐马尔可夫模型的定义 36
2.5.3 三个基本假设及适用场景 37
2.5.4 概率计算问题之直接计算 39
2.5.5 概率计算问题之前向算法 40
2.5.6 概率计算问题之后向算法 42
2.5.7 预测问题之维特比算法 45
2.5.8 学习问题之Baum-Welch 算法 48
2.6 条件随机场模型 52
2.6.1 超越HMM 52
2.6.2 项目实践 55
2.7 总结 59
3 自然语言处理基础60
3.1 中文自动分词 60
3.1.1 有向无环图 61
3.1.2 最大匹配算法 63
3.1.3 算法评测 69
3.1.4 由字构词的方法 72
3.2 词性标注 77
3.2.1 词性标注规范 77
3.2.2 隐马尔可夫模型词性标注 79
3.3 命名实体识别 81
3.4 上下文无关文法 82
3.4.1 原理介绍 83
3.4.2 算法浅析 83
3.5 依存关系分析 84
3.5.1 算法浅析 85
3.5.2 项目实践 92
3.5.3 小结 94
3.6 信息检索系统 95
3.6.1 什么是信息检索系统 95
3.6.2 衡量信息检索系统的关键指标 95
3.6.3 理解非结构化数据 97
3.6.4 倒排索引 98
3.6.5 处理查询 100
3.6.6 项目实践 102
3.6.7 Elasticsearch 103
3.6.8 小结 112
3.7 问答语料 113
3.7.1 WikiQA 113
3.7.2 中文版保险行业语料库InsuranceQA 113
3.8 总结 115
4 深度学习初步116
4.1 深度学习简史 116
4.1.1 感知机 116
4.1.2 寒冬和复苏 117
4.1.3 走出实验室 118
4.1.4 寒冬再临 119
4.1.5 走向大规模实际应用 119
4.2 基本架构 120
4.2.1 神经元 121
4.2.2 输入层、隐藏层和输出层 122
4.2.3 标准符号 123
4.3 神经网络是如何学习的 124
4.3.1 梯度下降 124
4.3.2 反向传播理论 127
4.3.3 神经网络全连接层的实现 130
4.3.4 使用简单神经网络实现问答任务 131
4.4 调整神经网络超参数 136
4.4.1 超参数 136
4.4.2 参考建议 137
4.5 卷积神经网络与池化 138
4.5.1 简介 138
4.5.2 卷积层的前向传播 139
4.5.3 池化层的前向传播 141
4.5.4 卷积层的实现 141
4.5.5 池化层的实现 145
4.5.6 使用卷积神经网络实现问答任务 148
4.6 循环神经网络及其变种 149
4.6.1 简介 149
4.6.2 循环神经网络 149
4.6.3 长短期记忆单元和门控循环单元 153
4.6.4 循环神经网络的实现 156
4.6.5 使用循环神经网络实现问答任务 159
4.7 简易神经网络工具包 160
5 词向量实现及应用161
5.1 语言模型 161
5.1.1 评测 162
5.1.2 ARPA 格式介绍 162
5.1.3 项目实践 163
5.2 One-hot 表示法 164
5.3 词袋模型 165
5.4 NNLM 和RNNLM 165
5.5 word2vec 168
5.5.1 C-BOW 的原理 169
5.5.2 Skip-gram 的原理 172
5.5.3 计算效率优化 174
5.5.4 项目实践 179
5.6 GloVe 189
5.6.1 GloVe 的原理 189
5.6.2 GloVe 与word2vec 的区别和联系 191
5.6.3 项目实践 193
5.7 fastText 198
5.7.1 fastText 的原理 198
5.7.2 fastText 与word2vec 的区别和联系 200
5.7.3 项目实践 201
5.8 中文近义词工具包 204
5.8.1 安装 205
5.8.2 接口 205
5.9 总结 205
6 社区问答中的QA 匹配206
6.1 社区问答任务简介 206
6.2 孪生网络模型 207
6.3 QACNN 模型 207
6.3.1 模型构建 207
6.3.2 实验结果 214
6.4 Decomposable Attention 模型 214
6.4.1 模型介绍 214
6.4.2 模型构建 216
6.5 多比较方式的比较C集成模型 216
6.5.1 模型介绍 216
6.5.2 模型构建 218
6.6 BiMPM 模型 219
6.6.1 模型介绍 219
6.6.2 模型构建 221
7 机器阅读理解222
7.1 完型填空型机器阅读理解任务 222
7.1.1 CNN/Daily Mail 数据集 222
7.1.2 Children’s Book Test（CBT）数据集 223
7.1.3 GA Reader 模型 226
7.1.4 SA Reader 模型 227
7.1.5 AoA Reader 模型 228
7.2 答案抽取型机器阅读理解任务 230
7.2.1 SQuAD 数据集 231
7.2.2 MS MARCO 数据集 232
7.2.3 TriviaQA 数据集 234
7.2.4 DuReader 数据集 235
7.2.5 BiDAF 模型 235
7.2.6 R-Net 模型 237
7.2.7 S-Net 模型 240
7.3 答案选择型机器阅读理解任务 243
7.4 展望 245
参考文献246
・ ・ ・ ・ ・ ・ (收起)第 1 章引言 1
1．1　本书面向的读者　7
1．2　深度学习的历史趋势　8
1．2．1　神经网络的众多名称和命运变迁　8
1．2．2　与日俱增的数据量　12
1．2．3　与日俱增的模型规模　13
1．2．4　与日俱增的精度、复杂度和对现实世界的冲击　15
第　1 部分应用数学与机器学习基础
第　2 章线性代数　19
2．1　标量、向量、矩阵和张量　19
2．2　矩阵和向量相乘　21
2．3　单位矩阵和逆矩阵　22
2．4　线性相关和生成子空间　23
2．5　范数　24
2．6　特殊类型的矩阵和向量　25
2．7　特征分解　26
2．8　奇异值分解　28
2．9　Moore-Penrose 伪逆　28
2．10　迹运算　29
2．11　行列式　30
2．12　实例：主成分分析　30
第3　章概率与信息论　34
3．1　为什么要使用概率　34
3．2　随机变量　35
3．3　概率分布　36
3．3．1　离散型变量和概率质量函数　36
3．3．2　连续型变量和概率密度函数　36
3．4　边缘概率　37
3．5　条件概率　37
3．6　条件概率的链式法则　38
3．7　独立性和条件独立性　38
3．8　期望、方差和协方差　38
3．9　常用概率分布　39
3．9．1　Bernoulli 分布　40
3．9．2　Multinoulli 分布　40
3．9．3　高斯分布　40
3．9．4　指数分布和Laplace 分布　41
3．9．5　Dirac 分布和经验分布　42
3．9．6　分布的混合　42
3．10　常用函数的有用性质　43
3．11　贝叶斯规则　45
3．12　连续型变量的技术细节　45
3．13　信息论　47
3．14　结构化概率模型　49
第4　章数值计算　52
4．1　上溢和下溢　52
4．2　病态条件　53
4．3　基于梯度的优化方法　53
4．4　约束优化　60
4．5　实例：线性最小二乘　61
第5　章机器学习基础　63
5．1　学习算法　63
5．1．1　任务T　63
5．1．2　性能度量P　66
5．1．3　经验E　66
5．1．4　示例：线性回归　68
5．2　容量、过拟合和欠拟合　70
5．2．1　没有免费午餐定理　73
5．2．2　正则化　74
5．3　超参数和验证集　76
5．4　估计、偏差和方差　77
5．4．1　点估计　77
5．4．2　偏差　78
5．4．3　方差和标准差　80
5．4．4　权衡偏差和方差以最小化均方误差　81
5．4．5　一致性　82
5．5　最大似然估计　82
5．5．1　条件对数似然和均方误差　84
5．5．2　最大似然的性质　84
5．6　贝叶斯统计　85
5．7　监督学习算法　88
5．7．1　概率监督学习　88
5．7．2　支持向量机　88
5．7．3　其他简单的监督学习算法　90
5．8　无监督学习算法　91
5．8．1　主成分分析　92
5．8．2　k- 均值聚类　94
5．9　随机梯度下降　94
5．10　构建机器学习算法　96
5．11　促使深度学习发展的挑战　96
5．11．1　维数灾难　97
5．11．2　局部不变性和平滑正则化　97
5．11．3　流形学习　99
第　2 部分深度网络：现代实践
第6　章深度前馈网络　105
6．1　实例：学习XOR　107
6．2　基于梯度的学习　110
6．2．1　代价函数　111
6．2．2　输出单元　113
6．3　隐藏单元　119
6．3．1　整流线性单元及其扩展　120
6．3．2　logistic sigmoid 与双曲正切函数　121
6．3．3　其他隐藏单元　122
6．4　架构设计　123
6．4．1　万能近似性质和深度　123
6．4．2　其他架构上的考虑　125
6．5　反向传播和其他的微分算法　126
6．5．1　计算图　127
6．5．2　微积分中的链式法则　127
6．5．3　递归地使用链式法则来实现反向传播　128
6．5．4　全连接MLP 中的反向传播计算　131
6．5．5　符号到符号的导数　131
6．5．6　一般化的反向传播　133
6．5．7　实例：用于MLP 训练的反向传播　135
6．5．8　复杂化　137
6．5．9　深度学习界以外的微分　137
6．5．10　高阶微分　138
6．6　历史小记　139
第7　章深度学习中的正则化　141
7．1　参数范数惩罚　142
7．1．1　L2 参数正则化　142
7．1．2　L1 正则化　144
7．2　作为约束的范数惩罚　146
7．3　正则化和欠约束问题　147
7．4　数据集增强　148
7．5　噪声鲁棒性　149
7．6　半监督学习　150
7．7　多任务学习　150
7．8　提前终止　151
7．9　参数绑定和参数共享　156
7．10　稀疏表示　157
7．11　Bagging 和其他集成方法　158
7．12　Dropout　159
7．13　对抗训练　165
7．14　切面距离、正切传播和流形正切分类器　167
第8　章深度模型中的优化　169
8．1　学习和纯优化有什么不同　169
8．1．1　经验风险最小化　169
8．1．2　代理损失函数和提前终止　170
8．1．3　批量算法和小批量算法　170
8．2　神经网络优化中的挑战　173
8．2．1　病态　173
8．2．2　局部极小值　174
8．2．3　高原、鞍点和其他平坦区域　175
8．2．4　悬崖和梯度爆炸　177
8．2．5　长期依赖　177
8．2．6　非精确梯度　178
8．2．7　局部和全局结构间的弱对应　178
8．2．8　优化的理论限制　179
8．3　基本算法　180
8．3．1　随机梯度下降　180
8．3．2　动量　181
8．3．3　Nesterov 动量　183
8．4　参数初始化策略　184
8．5　自适应学习率算法　187
8．5．1　AdaGrad　187
8．5．2　RMSProp　188
8．5．3　Adam　189
8．5．4　选择正确的优化算法　190
8．6　二阶近似方法　190
8．6．1　牛顿法　190
8．6．2　共轭梯度　191
8．6．3　BFGS　193
8．7　优化策略和元算法　194
8．7．1　批标准化　194
8．7．2　坐标下降　196
8．7．3　Polyak 平均　197
8．7．4　监督预训练　197
8．7．5　设计有助于优化的模型　199
8．7．6　延拓法和课程学习　199
第9　章卷积网络　201
9．1　卷积运算　201
9．2　动机　203
9．3　池化　207
9．4　卷积与池化作为一种无限强的先验　210
9．5　基本卷积函数的变体　211
9．6　结构化输出　218
9．7　数据类型　219
9．8　高效的卷积算法　220
9．9　随机或无监督的特征　220
9．10　卷积网络的神经科学基础　221
9．11　卷积网络与深度学习的历史　226
第　10 章序列建模：循环和递归网络　227
10．1　展开计算图　228
10．2　循环神经网络　230
10．2．1　导师驱动过程和输出循环网络　232
10．2．2　计算循环神经网络的梯度　233
10．2．3　作为有向图模型的循环网络　235
10．2．4　基于上下文的RNN 序列建模　237
10．3　双向RNN　239
10．4　基于编码{解码的序列到序列架构　240
10．5　深度循环网络　242
10．6　递归神经网络　243
10．7　长期依赖的挑战　244
10．8　回声状态网络　245
10．9　渗漏单元和其他多时间尺度的策略　247
10．9．1　时间维度的跳跃连接　247
10．9．2　渗漏单元和一系列不同时间尺度　247
10．9．3　删除连接　248
10．10　长短期记忆和其他门控RNN　248
10．10．1　LSTM　248
10．10．2　其他门控RNN　250
10．11　优化长期依赖　251
10．11．1　截断梯度　251
10．11．2　引导信息流的正则化　252
10．12　外显记忆　253
第　11 章实践方法论　256
11．1　性能度量　256
11．2　默认的基准模型　258
11．3　决定是否收集更多数据　259
11．4　选择超参数　259
11．4．1　手动调整超参数　259
11．4．2　自动超参数优化算法　262
11．4．3　网格搜索　262
11．4．4　随机搜索　263
11．4．5　基于模型的超参数优化　264
11．5　调试策略　264
11．6　示例：多位数字识别　267
第　12 章应用　269
12．1　大规模深度学习　269
12．1．1　快速的CPU 实现　269
12．1．2　GPU 实现　269
12．1．3　大规模的分布式实现　271
12．1．4　模型压缩　271
12．1．5　动态结构　272
12．1．6　深度网络的专用硬件实现　273
12．2　计算机视觉　274
12．2．1　预处理　275
12．2．2　数据集增强　277
12．3　语音识别　278
12．4　自然语言处理　279
12．4．1　n-gram　280
12．4．2　神经语言模型　281
12．4．3　高维输出　282
12．4．4　结合n-gram 和神经语言模型　286
12．4．5　神经机器翻译　287
12．4．6　历史展望　289
12．5　其他应用　290
12．5．1　推荐系统　290
12．5．2　知识表示、推理和回答　292
第3　部分深度学习研究
第　13 章线性因子模型　297
13．1　概率PCA 和因子分析　297
13．2　独立成分分析　298
13．3　慢特征分析　300
13．4　稀疏编码　301
13．5　PCA 的流形解释　304
第　14 章自编码器　306
14．1　欠完备自编码器　306
14．2　正则自编码器　307
14．2．1　稀疏自编码器　307
14．2．2　去噪自编码器　309
14．2．3　惩罚导数作为正则　309
14．3　表示能力、层的大小和深度　310
14．4　随机编码器和解码器　310
14．5　去噪自编码器详解　311
14．5．1　得分估计　312
14．5．2　历史展望　314
14．6　使用自编码器学习流形　314
14．7　收缩自编码器　317
14．8　预测稀疏分解　319
14．9　自编码器的应用　319
第　15 章表示学习　321
15．1　贪心逐层无监督预训练　322
15．2　迁移学习和领域自适应　326
15．3　半监督解释因果关系　329
15．4　分布式表示　332
15．5　得益于深度的指数增益　336
15．6　提供发现潜在原因的线索　337
第　16 章深度学习中的结构化概率模型　339
16．1　非结构化建模的挑战　339
16．2　使用图描述模型结构　342
16．2．1　有向模型　342
16．2．2　无向模型　344
16．2．3　配分函数　345
16．2．4　基于能量的模型　346
16．2．5　分离和d-分离　347
16．2．6　在有向模型和无向模型中转换　350
16．2．7　因子图　352
16．3　从图模型中采样　353
16．4　结构化建模的优势　353
16．5　学习依赖关系　354
16．6　推断和近似推断　354
16．7　结构化概率模型的深度学习方法 355第 17 章蒙特卡罗方法　359
17．1　采样和蒙特卡罗方法　359
17．1．1　为什么需要采样　359
17．1．2　蒙特卡罗采样的基础　359
17．2　重要采样　360
17．3　马尔可夫链蒙特卡罗方法　362
17．4　Gibbs 采样　365
17．5　不同的峰值之间的混合挑战　365
17．5．1　不同峰值之间通过回火来混合　367
17．5．2　深度也许会有助于混合　368
第　18 章直面配分函数　369
18．1　对数似然梯度　369
18．2　随机最大似然和对比散度　370
18．3　伪似然　375
18．4　得分匹配和比率匹配　376
18．5　去噪得分匹配　378
18．6　噪声对比估计　378
18．7　估计配分函数　380
18．7．1　退火重要采样　382
18．7．2　桥式采样　384
第　19 章近似推断　385
19．1　把推断视作优化问题　385
19．2　期望最大化　386
19．3　最大后验推断和稀疏编码　387
19．4　变分推断和变分学习　389
19．4．1　离散型潜变量　390
19．4．2　变分法　394
19．4．3　连续型潜变量　396
19．4．4　学习和推断之间的相互作用　397
19．5　学成近似推断　397
19．5．1　醒眠算法　398
19．5．2　学成推断的其他形式　398
第　20 章深度生成模型　399
20．1　玻尔兹曼机　399
20．2　受限玻尔兹曼机　400
20．2．1　条件分布　401
20．2．2　训练受限玻尔兹曼机　402
20．3　深度信念网络　402
20．4　深度玻尔兹曼机　404
20．4．1　有趣的性质　406
20．4．2　DBM 均匀场推断　406
20．4．3　DBM 的参数学习　408
20．4．4　逐层预训练　408
20．4．5　联合训练深度玻尔兹曼机　410
20．5　实值数据上的玻尔兹曼机　413
20．5．1　Gaussian-Bernoulli RBM　413
20．5．2　条件协方差的无向模型　414
20．6　卷积玻尔兹曼机　417
20．7　用于结构化或序列输出的玻尔兹曼机　418
20．8　其他玻尔兹曼机　419
20．9　通过随机操作的反向传播　419
20．10　有向生成网络　422
20．10．1　sigmoid 信念网络　422
20．10．2　可微生成器网络　423
20．10．3　变分自编码器　425
20．10．4　生成式对抗网络　426
20．10．5　生成矩匹配网络　429
20．10．6　卷积生成网络　430
20．10．7　自回归网络　430
20．10．8　线性自回归网络　430
20．10．9　神经自回归网络　431
20．10．10　NADE　432
20．11　从自编码器采样　433
20．11．1　与任意去噪自编码器相关的马尔可夫链 ．　434
20．11．2　夹合与条件采样　434
20．11．3　回退训练过程　435
20．12　生成随机网络　435
20．13　其他生成方案　436
20．14　评估生成模型　437
20．15　结论　438
参考文献　439
索引　486
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 深度学习的发展介绍 1
1.1 如何阅读本书 3
1.2 深度学习沉浮史 3
1.2.1 模拟生物大脑的疯狂远古时代 4
1.2.2 联结主义近代 5
1.2.3 百花齐放，层次结构主导，模型巨大的当代 6
1.3 Python简易教程 7
1.3.1 Anaconda搭建 7
1.3.2 IPython Notebook使用 7
1.3.3 Python基本用法 8
1.3.4 NumPy 15
1.3.5 Matplotlib 23
1.4 参考文献 25
第2章 机器学习快速入门 27
2.1 学习算法 28
2.1.1 学习任务 29
2.1.2 性能度量 30
2.1.3 学习经验 32
2.2 代价函数 33
2.2.1 均方误差函数 33
2.2.2 极大似然估计 34
2.3 梯度下降法 36
2.3.1 批量梯度下降法 38
2.3.2 随机梯度下降法 39
2.4 过拟合与欠拟合 40
2.4.1 没免费午餐理论 42
2.4.2 正则化 43
2.5 超参数与验证集 44
2.6 Softmax编码实战 46
2.6.1 编码说明 49
2.6.2 熟练使用CIFAR-10 数据集 50
2.6.3 显式循环计算损失函数及其梯度 53
2.6.4 向量化表达式计算损失函数及其梯度 56
2.6.5 最小批量梯度下降算法训练Softmax分类器 57
2.6.6 使用验证数据选择超参数 61
2.7 参考代码 68
2.8 参考文献 70
第3章 前馈神经网络 72
3.1 神经元 73
3.1.1 Sigmoid神经元 74
3.1.2 Tanh神经元 75
3.1.3 ReLU神经元 76
3.2 前馈神经网络 80
3.2.1 输出层单元 80
3.2.2 隐藏层单元 80
3.2.3 网络结构设计 81
3.3 BP算法 82
3.4 深度学习编码实战上 86
3.4.1 实现仿射传播 88
3.4.2 实现ReLU传播 91
3.4.3 组合单层神经元 93
3.4.4 实现浅层神经网络 96
3.4.5 实现深层全连接网络 101
3.5 参考代码 109
3.6 参考文献 113
第4章 深度学习正则化 115
4.1 参数范数惩罚 116
4.1.1 L2参数正则化 118
4.1.2 L1正则化 119
4.2 参数绑定与参数共享 120
4.3 噪声注入与数据扩充 120
4.4 稀疏表征 122
4.5 早停 123
4.6 Dropout 126
4.6.1 个体与集成 126
4.6.2 Dropout 127
4.7 深度学习编码实战中 129
4.7.1 Dropout传播 131
4.7.2 组合Dropout传播层 134
4.7.3 Dropout神经网络 136
4.7.4 解耦训练器trainer 138
4.7.5 解耦更新器updater 143
4.7.6 正则化实验 145
4.8 参考代码 148
4.9 参考文献 150
第5章 深度学习优化 152
5.1 神经网络优化困难 153
5.1.1 局部最优 153
5.1.2 鞍点 154
5.1.3 梯度悬崖 154
5.1.4 梯度消失或梯度爆炸 155
5.1.5 梯度不精确 156
5.1.6 优化理论的局限性 156
5.2 随机梯度下降 156
5.3 动量学习法 158
5.4 AdaGrad和RMSProp 159
5.5 Adam 160
5.6 参数初始化策略 161
5.7 批量归一化 163
5.7.1 BN算法详解 163
5.7.2 BN传播详解 165
5.8 深度学习编码实战下 166
5.8.1 Momentum 167
5.8.2 RMSProp 171
5.8.3 Adam 172
5.8.4 更新规则比较 174
5.8.5 BN前向传播 176
5.8.6 BN反向传播 180
5.8.7 使用BN的全连接网络 182
5.8.8 BN算法与权重标准差比较 188
5.9 参考代码 191
5.10 参考文献 195
第6章 卷积神经网络 196
6.1 卷积操作 197
6.2 卷积的意义 198
6.2.1 稀疏连接 199
6.2.2 参数共享 200
6.3 池化操作 201
6.4 设计卷积神经网络 204
6.4.1 跨步卷积 204
6.4.2 零填充 205
6.4.3 非共享卷积 206
6.4.4 平铺卷积 207
6.5 卷积网络编码练习 208
6.5.1 卷积前向传播 209
6.5.2 卷积反向传播 212
6.5.3 最大池化前向传播 215
6.5.4 最大池化反向传播 218
6.5.5 向量化执行 220
6.5.6 组合完整卷积层 223
6.5.7 浅层卷积网络 224
6.5.8 空间批量归一化 229
6.6 参考代码 233
6.7 参考文献 237
第7章 循环神经网络 238
7.1 循环神经网络 239
7.1.1 循环神经元展开 239
7.1.2 循环网络训练 240
7.2 循环神经网络设计 242
7.2.1 双向循环网络结构 242
7.2.2 编码-解码网络结构 243
7.2.3 深度循环网络结构 244
7.3 门控循环神经网络 245
7.3.1 LSTM 246
7.3.2 门控循环单元 249
7.4 RNN编程练习 250
7.4.1 RNN单步传播 252
7.4.2 RNN时序传播 255
7.4.3 词嵌入 258
7.4.4 RNN输出层 261
7.4.5 时序Softmax损失 262
7.4.6 RNN图片说明任务 264
7.5 LSTM编程练习 269
7.5.1 LSTM单步传播 269
7.5.2 LSTM时序传播 273
7.5.3 LSTM实现图片说明任务 276
7.6 参考代码 278
7.6.1 RNN参考代码 278
7.6.2 LSTM参考代码 282
7.7 参考文献 285
第8章 TensorFlow快速入门 287
8.1 TensorFlow介绍 288
8.2 TensorFlow 1.0安装指南 289
8.2.1 双版本切换Anaconda 289
8.2.2 安装CUDA 8.0 291
8.2.3 安装cuDNN 292
8.2.4 安装TensorFlow 293
8.2.5 验证安装 294
8.3 TensorFlow基础 295
8.3.1 Tensor 295
8.3.2 TensorFlow核心API教程 296
8.3.3 tf.train API 299
8.3.4 tf.contrib.learn 301
8.4 TensorFlow构造CNN 305
8.4.1 构建Softmax模型 305
8.4.2 使用TensorFlow训练模型 307
8.4.3 使用TensorFlow评估模型 308
8.4.4 使用TensorFlow构建卷积神经网络 308
8.5 TensorBoard快速入门 311
8.5.1 TensorBoard可视化学习 312
8.5.2 计算图可视化 316
・ ・ ・ ・ ・ ・ (收起)译者序 iv
序 vii
前言 ix
术语缩写 xxii
符号 xxvii
第 1 章 简介 1
1.1 自动语音识别：更好的沟通之桥 . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 人类之间的交流 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.2 人机交流 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 语音识别系统的基本结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 全书结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3.1 第一部分：传统声学模型 . . . . . . . . . . . . . . . . . . . . . . 6
1.3.2 第二部分：深度神经网络 . . . . . . . . . . . . . . . . . . . . . . 6
1.3.3 第三部分：语音识别中的 DNN-HMM 混合系统 . . . . . . . . . . 7
1.3.4 第四部分：深度神经网络中的表征学习 . . . . . . . . . . . . . . 7
1.3.5 第五部分：高级的深度模型 . . . . . . . . . . . . . . . . . . . . . 7
第一部分 传统声学模型 9
第 2 章 混合高斯模型 11
2.1 随机变量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 高斯分布和混合高斯随机变量 . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 参数估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4 采用混合高斯分布对语音特征建模 . . . . . . . . . . . . . . . . . . . . . 16
第 3 章 隐马尔可夫模型及其变体 19
3.1 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 马尔可夫链 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.3 序列与模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.1 隐马尔可夫模型的性质 . . . . . . . . . . . . . . . . . . . . . . . . 23
3.3.2 隐马尔可夫模型的仿真 . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.3 隐马尔可夫模型似然度的计算 . . . . . . . . . . . . . . . . . . . . 24
3.3.4 计算似然度的高效算法 . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3.5 前向与后向递归式的证明 . . . . . . . . . . . . . . . . . . . . . . 27
3.4 期望最大化算法及其在学习 HMM 参数中的应用 . . . . . . . . . . . . . 28
3.4.1 期望最大化算法介绍 . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.4.2 使用 EM 算法来学习 HMM 参数――Baum-Welch 算法 . . . . . . 30
3.5 用于解码 HMM 状态序列的维特比算法 . . . . . . . . . . . . . . . . . . . 34
3.5.1 动态规划和维特比算法 . . . . . . . . . . . . . . . . . . . . . . . . 34
3.5.2 用于解码 HMM 状态的动态规划算法 . . . . . . . . . . . . . . . . 35
3.6 隐马尔可夫模型和生成语音识别模型的变体 . . . . . . . . . . . . . . . . 37
3.6.1 用于语音识别的 GMM-HMM 模型 . . . . . . . . . . . . . . . . . 38
3.6.2 基于轨迹和隐藏动态模型的语音建模和识别 . . . . . . . . . . . . 39
3.6.3 使用生成模型 HMM 及其变体解决语音识别问题 . . . . . . . . . 40
第二部分 深度神经网络 43
第 4 章 深度神经网络 45
4.1 深度神经网络框架 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.2 使用误差反向传播来进行参数训练 . . . . . . . . . . . . . . . . . . . . . 48
4.2.1 训练准则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.2.2 训练算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.3 实际应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3.1 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.3.2 模型初始化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.3.3 权重衰减 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.3.4 丢弃法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.3.5 批量块大小的选择 . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.3.6 取样随机化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3.7 惯性系数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.3.8 学习率和停止准则 . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.3.9 网络结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.3.10 可复现性与可重启性 . . . . . . . . . . . . . . . . . . . . . . . . . 62
第 5 章 高级模型初始化技术 65
5.1 受限玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.1.1 受限玻尔兹曼机的属性 . . . . . . . . . . . . . . . . . . . . . . . . 67
5.1.2 受限玻尔兹曼机参数学习 . . . . . . . . . . . . . . . . . . . . . . 70
5.2 深度置信网络预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.3 降噪自动编码器预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.4 鉴别性预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.5 混合预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.6 采用丢弃法的预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
第三部分 语音识别中的深度神经网络C隐马尔可夫混合模型 81
第 6 章 深度神经网络C隐马尔可夫模型混合系统 83
6.1 DNN-HMM 混合系统 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.1.1 结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.1.2 用 CD-DNN-HMM 解码 . . . . . . . . . . . . . . . . . . . . . . . . 85
6.1.3 CD-DNN-HMM 训练过程 . . . . . . . . . . . . . . . . . . . . . . . 86
6.1.4 上下文窗口的影响 . . . . . . . . . . . . . . . . . . . . . . . . . . 88
6.2 CD-DNN-HMM 的关键模块及分析 . . . . . . . . . . . . . . . . . . . . . 90
6.2.1 进行比较和分析的数据集和实验 . . . . . . . . . . . . . . . . . . 90
6.2.2 对单音素或者三音素的状态进行建模 . . . . . . . . . . . . . . . . 92
6.2.3 越深越好 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.2.4 利用相邻的语音帧 . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.2.5 预训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.2.6 训练数据的标注质量的影响 . . . . . . . . . . . . . . . . . . . . . 95
6.2.7 调整转移概率 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.3 基于 KL 距离的隐马尔可夫模型 . . . . . . . . . . . . . . . . . . . . . . . 96
第 7 章 训练和解码的加速 99
7.1 训练加速 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
7.1.1 使用多 GPU 流水线反向传播 . . . . . . . . . . . . . . . . . . . . 100
7.1.2 异步随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.1.3 增广拉格朗日算法及乘子方向交替算法 . . . . . . . . . . . . . . 106
7.1.4 减小模型规模 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1.5 其他方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.2 加速解码 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.2.1 并行计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.2.2 稀疏网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2.3 低秩近似 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.2.4 用大尺寸 DNN 训练小尺寸 DNN . . . . . . . . . . . . . . . . . . 114
7.2.5 多帧 DNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
第 8 章 深度神经网络序列鉴别性训练 117
8.1 序列鉴别性训练准则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
8.1.1 最大相互信息 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
8.1.2 增强型 MMI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.1.3 最小音素错误/状态级最小贝叶斯风险 . . . . . . . . . . . . . . . 120
8.1.4 统一的公式 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
8.2 具体实现中的考量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.2.1 词图产生 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.2.2 词图补偿 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.2.3 帧平滑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.2.4 学习率调整 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.2.5 训练准则选择 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.2.6 其他考量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.3 噪声对比估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
8.3.1 将概率密度估计问题转换为二分类设计问题 . . . . . . . . . . . . 127
8.3.2 拓展到未归一化的模型 . . . . . . . . . . . . . . . . . . . . . . . . 129
8.3.3 在深度学习网络训练中应用噪声对比估计算法 . . . . . . . . . . 130
第四部分 深度神经网络中的特征表示学习 133
第 9 章 深度神经网络中的特征表示学习 135
9.1 特征和分类器的联合学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
9.2 特征层级 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
9.3 使用随意输入特征的灵活性 . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.4 特征的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4.1 对说话人变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4.2 对环境变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . 142
9.5 对环境的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
9.5.1 对噪声的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
9.5.2 对语速变化的鲁棒性 . . . . . . . . . . . . . . . . . . . . . . . . . 147
9.6 缺乏严重信号失真情况下的推广能力 . . . . . . . . . . . . . . . . . . . . 148
第 10 章 深度神经网络和混合高斯模型的融合 151
10.1 在 GMM-HMM 系统中使用由 DNN 衍生的特征 . . . . . . . . . . . . . . 151
10.1.1 使用 Tandem 和瓶颈特征的 GMM-HMM 模型 . . . . . . . . . . . 151
10.1.2 DNN-HMM 混合系统与采用深度特征的 GMM-HMM 系统的比较 154
10.2 识别结果融合技术 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.2.1 识别错误票选降低技术（ ROVER） . . . . . . . . . . . . . . . . . 157
10.2.2 分段条件随机场（ SCARF） . . . . . . . . . . . . . . . . . . . . . 159
10.2.3 最小贝叶斯风险词图融合 . . . . . . . . . . . . . . . . . . . . . . 160
10.3 帧级别的声学分数融合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
10.4 多流语音识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
第 11 章 深度神经网络的自适应技术 165
11.1 深度神经网络中的自适应问题 . . . . . . . . . . . . . . . . . . . . . . . . 165
11.2 线性变换 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.2.1 线性输入网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.2.2 线性输出网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.3 线性隐层网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
11.4 保守训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
11.4.1 L 2 正则项 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
11.4.2 KL 距离正则项 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
11.4.3 减少每个说话人的模型开销 . . . . . . . . . . . . . . . . . . . . . 173
11.5 子空间方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
11.5.1 通过主成分分析构建子空间 . . . . . . . . . . . . . . . . . . . . . 175
11.5.2 噪声感知、说话人感知及设备感知训练 . . . . . . . . . . . . . . 176
11.5.3 张量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
11.6 DNN 说话人自适应的效果 . . . . . . . . . . . . . . . . . . . . . . . . . . 181
11.6.1 基于 KL 距离的正则化方法 . . . . . . . . . . . . . . . . . . . . . 181
11.6.2 说话人感知训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
第五部分 先进的深度学习模型 185
第 12 章 深度神经网络中的表征共享和迁移 187
12.1 多任务和迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
12.1.1 多任务学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
12.1.2 迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.2 多语言和跨语言语音识别 . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.2.1 基于 Tandem 或瓶颈特征的跨语言语音识别 . . . . . . . . . . . . 190
12.2.2 共享隐层的多语言深度神经网络 . . . . . . . . . . . . . . . . . . 191
12.2.3 跨语言模型迁移 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
12.3 语音识别中深度神经网络的多目标学习 . . . . . . . . . . . . . . . . . . . 197
12.3.1 使用多任务学习的鲁棒语音识别 . . . . . . . . . . . . . . . . . . 197
12.3.2 使用多任务学习改善音素识别 . . . . . . . . . . . . . . . . . . . . 198
12.3.3 同时识别音素和字素（ graphemes） . . . . . . . . . . . . . . . . . 199
12.4 使用视听信息的鲁棒语音识别 . . . . . . . . . . . . . . . . . . . . . . . . 199
第 13 章 循环神经网络及相关模型 201
13.1 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
13.2 基本循环神经网络中的状态-空间公式 . . . . . . . . . . . . . . . . . . . . 203
13.3 沿时反向传播学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
13.3.1 最小化目标函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
13.3.2 误差项的递归计算 . . . . . . . . . . . . . . . . . . . . . . . . . . 205
13.3.3 循环神经网络权重的更新 . . . . . . . . . . . . . . . . . . . . . . 206
13.4 一种用于学习循环神经网络的原始对偶技术 . . . . . . . . . . . . . . . . 208
13.4.1 循环神经网络学习的难点 . . . . . . . . . . . . . . . . . . . . . . 208
13.4.2 回声状态（ Echo-State）性质及其充分条件 . . . . . . . . . . . . . 208
13.4.3 将循环神经网络的学习转化为带约束的优化问题 . . . . . . . . . 209
13.4.4 一种用于学习 RNN 的原始对偶方法 . . . . . . . . . . . . . . . . 210
13.5 结合长短时记忆单元（ LSTM）的循环神经网络 . . . . . . . . . . . . . . 212
13.5.1 动机与应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
13.5.2 长短时记忆单元的神经元架构 . . . . . . . . . . . . . . . . . . . . 213
13.5.3 LSTM-RNN 的训练 . . . . . . . . . . . . . . . . . . . . . . . . . . 214
13.6 循环神经网络的对比分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
13.6.1 信息流方向的对比：自上而下还是自下而上 . . . . . . . . . . . . 215
13.6.2 信息表征的对比：集中式还是分布式 . . . . . . . . . . . . . . . . 217
13.6.3 解释能力的对比：隐含层推断还是端到端学习 . . . . . . . . . . 218
13.6.4 参数化方式的对比：吝啬参数集合还是大规模参数矩阵 . . . . . 218
13.6.5 模型学习方法的对比：变分推理还是梯度下降 . . . . . . . . . . 219
13.6.6 识别正确率的比较 . . . . . . . . . . . . . . . . . . . . . . . . . . 220
13.7 讨论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
第 14 章 计算型网络 223
14.1 计算型网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
14.2 前向计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
14.3 模型训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
14.4 典型的计算节点 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
14.4.1 无操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . . . . 232
14.4.2 含一个操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . 232
14.4.3 含两个操作数的计算节点 . . . . . . . . . . . . . . . . . . . . . . 237
14.4.4 用来计算统计量的计算节点类型 . . . . . . . . . . . . . . . . . . 244
14.5 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
14.6 循环连接 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
14.6.1 只在循环中一个接一个地处理样本 . . . . . . . . . . . . . . . . . 249
14.6.2 同时处理多个句子 . . . . . . . . . . . . . . . . . . . . . . . . . . 251
14.6.3 创建任意的循环神经网络 . . . . . . . . . . . . . . . . . . . . . . 252
第 15 章 总结及未来研究方向 255
15.1 路线图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
15.1.1 语音识别中的深度神经网络启蒙 . . . . . . . . . . . . . . . . . . 255
15.1.2 深度神经网络训练和解码加速 . . . . . . . . . . . . . . . . . . . . 258
15.1.3 序列鉴别性训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
15.1.4 特征处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
15.1.5 自适应 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
15.1.6 多任务和迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . 261
15.1.7 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
15.1.8 循环神经网络和长短时记忆神经网络 . . . . . . . . . . . . . . . . 261
15.1.9 其他深度模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2 技术前沿和未来方向 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2.1 技术前沿简析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
15.2.2 未来方向 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
参考文献 267
・ ・ ・ ・ ・ ・ (收起)目录
前言1
第1章　工具与技术9
1.1 神经网络的类型9
1.2 数据获取19
1.3 数据预处理27
第2章　摆脱困境34
2.1 确定我们遇到的问题34
2.2 解决运行过程中的错误36
2.3 检查中间结果38
2.4 为最后一层选择正确的激活函数39
2.5 正则化和Dropout40
2.6 网络结构、批尺寸和学习率42
第3章　使用词嵌入计算文本相似性44
3.1 使用预训练的词嵌入发现词的相似性45
3.2 Word2vec数学特性47
3.3 可视化词嵌入49
3.4 在词嵌入中发现实体类51
3.5 计算类内部的语义距离55
3.6 在地图上可视化国家数据57
第4章　基于维基百科外部链接构建推荐系统58
4.1 收集数据58
4.2 训练电影嵌入62
4.3 构建电影推荐系统66
4.4 预测简单的电影属性67
第5章　按照示例文本的风格生成文本69
5.1 获取公开领域书籍文本69
5.2 生成类似莎士比亚的文本70
5.3 使用RNN编写代码74
5.4 控制输出温度76
5.5 可视化循环神经网络的活跃程度78
第6章　问题匹配80
6.1 从Stack Exchange网站获取数据80
6.2 使用Pandas探索数据82
6.3 使用Keras对文本进行特征化83
6.4 构建问答模型84
6.5 用Pandas训练模型86
6.6 检查相似性88
第7章　推荐表情符号90
7.1 构建一个简单的情感分类器90
7.2 检验一个简单的分类器93
7.3 使用卷积网络进行情感分析95
7.4 收集Twitter数据97
7.5 一个简单的表情符号预测器99
7.6 Dropout和多层窗口100
7.7 构建单词级模型102
7.8 构建你自己的嵌入104
7.9 使用循环神经网络进行分类106
7.10 可视化一致性/不一致性108
7.11 组合模型111
第8章　Sequence-to-Sequence映射113
8.1 训练一个简单的Sequence-to-Sequence模型113
8.2 从文本中提取对话115
8.3 处理开放词汇表117
8.4 训练seq2seq 聊天机器人119
第9章　复用预训练的图像识别网络123
9.1 加载预训练网络124
9.2 图像预处理124
9.3 推测图像内容126
9.4 使用Flickr API收集一组带标签的图像128
9.5 构建一个分辨猫狗的分类器129
9.6 改进搜索结果131
9.7 复训图像识别网络133
第10章　构建反向图像搜索服务137
10.1 从维基百科中获取图像137
10.2 向N维空间投影图像140
10.3 在高维空间中寻找最近邻141
10.4 探索嵌入中的局部邻域143
第11章　检测多幅图像145
11.1 使用预训练的分类器检测多个图像145
11.2 使用Faster RCNN进行目标检测149
11.3 在自己的图像上运行Faster RCNN152
第12章　图像风格155
12.1 可视化卷积神经网络激活值156
12.2 尺度和缩放159
12.3 可视化神经网络所见161
12.4 捕捉图像风格164
12.5 改进损失函数以提升图像相干性168
12.6 将风格迁移至不同图像169
12.7 风格内插171
第13章　用自编码器生成图像173
13.1 从Google Quick Draw中导入绘图174
13.2 为图像创建自编码器176
13.3 可视化自编码器结果178
13.4 从正确的分布中采样图像180
13.5 可视化变分自编码器空间183
13.6 条件变分编码器185
第14章　使用深度网络生成图标189
14.1 获得训练用的图标190
14.2 将图标转换为张量表示193
14.3 使用变分自编码器生成图标194
14.4 使用数据扩充提升自编码器的性能196
14.5 构建生成式对抗网络198
14.6 训练生成式对抗网络200
14.7 显示GAN生成的图标202
14.8 将图标编码成绘图指令204
14.9 训练RNN绘制图标205
14.10 使用RNN生成图标207
第15章　音乐与深度学习210
15.1 为音乐分类器创建训练数据集211
15.2 训练音乐风格检测器213
15.3 对混淆情况进行可视化215
15.4 为已有的音乐编制索引217
15.5 设置Spotify API219
15.6 从Spotify中收集播放列表和歌曲221
15.7 训练音乐推荐系统224
15.8 使用Word2vec模型推荐歌曲225
第16章　生产化部署机器学习系统228
16.1 使用scikit-learn最近邻计算嵌入229
16.2 使用Postgres存储嵌入230
16.3 填充和查询Postgres存储的嵌入231
16.4 在Postgres中存储高维模型233
16.5 使用Python编写微服务234
16.6 使用微服务部署Keras模型236
16.7 从Web框架中调用微服务237
16.8 Tensorflow seq2seq模型238
16.9 在浏览器中执行深度学习模型240
16.10 使用TensorFlow服务执行Keras模型243
16.11 在iOS中使用Keras模型245
・ ・ ・ ・ ・ ・ (收起)第1章 绪论 1
1.1 引言 1
1.2 本书内容 5
1.2.1 图像分类 7
1.2.2 动作识别 9
1.2.3 时序动作定位 12
1.2.4 视频 Embedding 14
1.3 本章小结 15
第2章 经典网络结构回顾 16
2.1 经典图像分类网络 16
2.1.1 LetNet-5 16
2.1.2 AlexNet 18
2.1.3 VGGNet 22
2.1.4 GoogLeNet 24
2.1.5 Inception V2/V3 27
2.1.6 ResNet 28
2.1.7 preResNet 31
2.1.8 WRN 32
2.1.9 随机深度网络 33
2.1.10 DenseNet 35
2.1.11 ResNeXt 36
2.1.12 SENet 39
2.1.13 MobileNet 41
2.1.14 MobileNet V2/V3 44
2.1.15 ShuffleNet 46
2.1.16 ShuffleNet V2 49
2.2 RNN、LSTM和GRU 51
2.2.1 RNN 51
2.2.2 梯度爆炸与梯度消失 52
2.2.3 LSTM 55
2.2.4 GRU 58
2.3 本章小结 60
第3章 基于2D卷积的动作识别 62
3.1 平均汇合 62
3.2 NetVLAD和NeXtVLAD 64
3.2.1 VLAD 65
3.2.2 NetVLAD 66
3.2.3 NeXtVLAD 71
3.2.4 NetFV和其他策略 75
3.3 利用RNN融合各帧特征 77
3.3.1 2D卷积 + RNN的基本结构 78
3.3.2 对RNN结构进行改造 80
3.4 利用3D卷积融合各帧特征 81
3.4.1 什么是3D卷积 82
3.4.2 ECO 85
3.5 双流法 87
3.5.1 什么是光流 87
3.5.2 双流法的基本网络结构 89
3.5.3 双流法的网络结构优化 91
3.6 时序稀疏采样 95
3.6.1 TSN 95
3.6.2 TSN的实现 98
3.6.3 ActionVLAD 99
3.6.4 StNet 100
3.6.5 TRN 102
3.7 利用iDT轨迹 104
3.7.1 DT和iDT 104
3.7.2 TDD 107
3.8 本章小结 108
第4章 基于3D卷积的动作识别 110
4.1 3D卷积基础网络结构 110
4.1.1 C3D 110
4.1.2 Res3D/3D ResNet 113
4.1.3 LTC 116
4.2 I3D 118
4.2.1 5类动作识别网络 118
4.2.2 2D卷积扩展为3D卷积 119
4.2.3 5类网络对比 121
4.3 3D卷积的低秩近似 123
4.3.1 低秩近似的基本原理 124
4.3.2 FSTCN 125
4.3.3 P3D 127
4.3.4 R(2+1)D 129
4.3.5 S3D 132
4.4 TSM 135
4.5 3D卷积 + RNN 137
4.6 ARTNet 139
4.7 Non-Local 141
4.7.1 Non-Local 操作 141
4.7.2 Non-Local 动作识别网络 144
4.8 SlowFast 148
4.8.1 Slow分支和Fast分支 149
4.8.2 网络结构设计 151
4.9 3D卷积神经网络超参数设计 152
4.9.1 多网格训练 152
4.9.2 X3D 154
4.10 本章小结 157
第5章 时序动作定位 159
5.1 基于滑动窗的算法 160
5.1.1 S-CNN 161
5.1.2 TURN 166
5.1.3 CBR 169
5.2 基于候选时序区间的算法 171
5.2.1 Faster R-CNN 回顾 172
5.2.2 R-C3D 175
5.2.3 TAL-Net 178
5.3 自底向上的时序动作定位算法 183
5.3.1 BSN 183
5.3.2 TSA-Net 187
5.3.3 BMN 191
5.4 对时序结构信息建模的算法 197
5.4.1 TAG 候选时序区间生成算法 198
5.4.2 SSN 网络结构 199
5.5 逐帧预测的算法 202
5.5.1 CDC层 203
5.5.2 CDC 网络结构 206
5.6 单阶段算法 208
5.6.1 SSAD 208
5.6.2 SS-TAD 212
5.6.3 GTAN 214
5.7 本章小结 217
第6章 视频Embedding 219
6.1 基于视频内容的无监督 Embedding 220
6.1.1 编码-解码网络 221
6.1.2 视频序列验证 222
6.1.3 视频和音频信息 224
6.1.4 视频和文本信息 225
6.2 Word2Vec 229
6.2.1 CBOW和Skip-Gram 229
6.2.2 分层 Softmax 234
6.2.3 负采样 239
6.3 Item2Vec 247
6.3.1 Item2Vec 基本形式 247
6.3.2 Item2Vec的改进 249
6.4 基于图的随机游走 252
6.4.1 DeepWalk 252
6.4.2 Node2Vec 254
6.5 结合一二阶相似度 257
6.5.1 LINE 258
6.5.2 SDNE 262
6.6 基于图的邻居结点 265
6.6.1 GCN 265
6.6.2 GraphSAGE 269
6.6.3 GAT 272
6.7 基于多种信息学习视频Embedding 274
6.7.1 召回模型 276
6.7.2 训练 278
6.8 本章小结 280
附录A 视频处理常用工具 281
A.1 FFmpeg 281
A.2 OpenCV 284
A.3 Decord 291
A.4 Lintel 294
参考文献 296
・ ・ ・ ・ ・ ・ (收起)第1 部分深度学习基础篇1
1 概述2
1.1 人工智能 3
1.1.1 人工智能的分类 3
1.1.2 人工智能发展史 3
1.2 机器学习 7
1.2.1 机器学习的由来 7
1.2.2 机器学习发展史 9
1.2.3 机器学习方法分类 10
1.2.4 机器学习中的基本概念 11
1.3 神经网络 12
1.3.1 神经网络发展史 13
参考文献 16
2 神经网络17
2.1 在神经科学中对生物神经元的研究 17
2.1.1 神经元激活机制 17
2.1.2 神经元的特点 18
2.2 神经元模型 19
2.2.1 线性神经元 19
2.2.2 线性阈值神经元 19
2.2.3 Sigmoid 神经元 21
2.2.4 Tanh 神经元 22
2.2.5 ReLU 22
2.2.6 Maxout 24
2.2.7 Softmax 24
2.2.8 小结 25
2.3 感知机 27
2.3.1 感知机的提出 27
2.3.2 感知机的困境 28
2.4 DNN 29
2.4.1 输入层、输出层及隐层 30
2.4.2 目标函数的选取 30
2.4.3 前向传播 32
2.4.4 后向传播 33
2.4.5 参数更新 35
2.4.6 神经网络的训练步骤 36
参考文献 36
3 初始化模型38
3.1 受限玻尔兹曼机 38
3.1.1 能量模型 39
3.1.2 带隐藏单元的能量模型 40
3.1.3 受限玻尔兹曼机基本原理 41
3.1.4 二值RBM 43
3.1.5 对比散度 45
3.2 自动编码器 47
3.2.1 稀疏自动编码器 48
3.2.2 降噪自动编码器 48
3.2.3 栈式自动编码器 49
3.3 深度信念网络 50
参考文献 52
4 卷积神经网络53
4.1 卷积算子 53
4.2 卷积的特征 56
4.3 卷积网络典型结构 59
4.3.1 基本网络结构 59
4.3.2 构成卷积神经网络的层 59
4.3.3 网络结构模式 60
4.4 卷积网络的层 61
4.4.1 卷积层 61
4.4.2 池化层 66
参考文献 67
5 循环神经网络68
5.1 循环神经网络简介 68
5.2 RNN、LSTM 和GRU 69
5.3 双向RNN 76
5.4 RNN 语言模型的简单实现 77
参考文献 80
6 深度学习优化算法81
6.1 SGD 81
6.2 Momentum 82
6.3 NAG 83
6.4 Adagrad 85
6.5 RMSProp 86
6.6 Adadelta 87
6.7 Adam 88
6.8 AdaMax 90
6.9 Nadam 90
6.10 关于优化算法的使用 92
参考文献 92
7 深度学习训练技巧94
7.1 数据预处理 94
7.2 权重初始化 95
7.3 正则化 96
7.3.1 提前终止 96
7.3.2 数据增强 96
7.3.3 L2/L1 参数正则化 98
7.3.4 集成 100
7.3.5 Dropout 101
参考文献 102
8 深度学习框架103
8.1 Theano 103
8.1.1 Theano 103
8.1.2 安装 104
8.1.3 计算图 104
8.2 Torch 105
8.2.1 概述 105
8.2.2 安装 106
8.2.3 核心结构 107
8.2.4 小试牛刀 110
8.3 PyTorch 113
8.3.1 概述 113
8.3.2 安装 113
8.3.3 核心结构 114
8.3.4 小试牛刀 114
8.4 Caffe 117
8.4.1 概述 117
8.4.2 安装 118
8.4.3 核心组件 119
8.4.4 小试牛刀 125
8.5 TensorFlow 125
8.5.1 概述 125
8.5.2 安装 126
8.5.3 核心结构 126
8.5.4 小试牛刀 127
8.6 MXNet 131
8.6.1 概述 131
8.6.2 安装 131
8.6.3 核心结构 132
8.6.4 小试牛刀 133
8.7 Keras 135
8.7.1 概述 135
8.7.2 安装 136
8.7.3 模块介绍 136
8.7.4 小试牛刀 136
参考文献 139
第2 部分计算机视觉篇140
9 计算机视觉背景141
9.1 传统计算机视觉 141
9.2 基于深度学习的计算机视觉 145
9.3 参考文献 146
10 图像分类模型147
10.1 LeNet-5 147
10.2 AlexNet 149
10.3 VGGNet 154
10.3.1 网络结构 155
10.3.2 配置 157
10.3.3 讨论 157
10.3.4 几组实验 158
10.4 GoogLeNet 159
10.4.1 NIN 161
10.4.2 GoogLeNet 的动机 161
10.4.3 网络结构细节 162
10.4.4 训练方法 164
10.4.5 后续改进版本 165
10.5 ResNet 165
10.5.1 基本思想 165
10.5.2 网络结构 167
10.6 DenseNet 169
10.7 DPN 170
参考文献 170
11 目标检测173
11.1 相关研究 175
11.1.1 选择性搜索 175
11.1.2 OverFeat 177
11.2 基于区域提名的方法 179
11.2.1 R-CNN 179
11.2.2 SPP-net 181
11.2.3 Fast R-CNN 182
11.2.4 Faster R-CNN 184
11.2.5 R-FCN 185
11.3 端到端的方法 186
11.3.1 YOLO 186
11.3.2 SSD 187
11.4 小结 188
参考文献 190
12 语义分割192
12.1 全卷积网络 193
12.1.1 FCN 193
12.1.2 DeconvNet 195
12.1.3 SegNet 197
12.1.4 DilatedConvNet 198
12.2 CRF/MRF 的使用 199
12.2.1 DeepLab 199
12.2.2 CRFasRNN 201
12.2.3 DPN 203
12.3 实例分割 205
12.3.1 Mask R-CNN 205
参考文献 206
13 图像检索的深度哈希编码208
13.1 传统哈希编码方法 208
13.2 CNNH 209
13.3 DSH 210
13.4 小结 212
参考文献 212
第3 部分语音识别篇214
14 传统语音识别基础215
14.1 语音识别简介 215
14.2 HMM 简介 216
14.2.1 HMM 是特殊的混合模型 218
14.2.2 转移概率矩阵 219
14.2.3 发射概率 220
14.2.4 Baum-Welch 算法 220
14.2.5 后验概率 224
14.2.6 前向-后向算法 224
14.3 HMM 梯度求解 227
14.3.1 梯度算法1 228
14.3.2 梯度算法2 230
14.3.3 梯度求解的重要性 234
14.4 孤立词识别 234
14.4.1 特征提取 234
14.4.2 孤立词建模 235
14.4.3 GMM-HMM 237
14.5 连续语音识别 240
14.6 Viterbi 解码 243
14.7 三音素状态聚类 245
14.8 判别式训练 248
参考文献 254
15 基于WFST 的语音解码256
15.1 有限状态机 257
15.2 WFST 及半环定义 257
15.2.1 WFST 257
15.2.2 半环（Semiring） 258
15.3 自动机操作 260
15.3.1 自动机基本操作 261
15.3.2 转换器基本操作 262
15.3.3 优化操作 265
15.4 基于WFST 的语音识别系统 277
15.4.1 声学模型WFST 279
15.4.2 三音素WFST 281
15.4.3 发音字典WFST 281
15.4.4 语言模型WFST 282
15.4.5 WFST 组合和优化 284
15.4.6 组合和优化实验 285
15.4.7 WFST 解码 286
参考文献 287
16 深度语音识别288
16.1 CD-DNN-HMM 288
16.2 TDNN 292
16.3 CTC 295
16.4 EESEN 299
16.5 Deep Speech 301
16.6 Chain 310
参考文献 313
17 CTC 解码315
17.1 序列标注 315
17.2 序列标注任务的解决办法 316
17.2.1 序列分类 316
17.2.2 分割分类 317
17.2.3 时序分类 318
17.3 隐马模型 318
17.4 CTC 基本定义 319
17.5 CTC 前向算法 321
17.6 CTC 后向算法 324
17.7 CTC 目标函数 325
17.8 CTC 解码基本原理 327
17.8.1 最大概率路径解码 327
17.8.2 前缀搜索解码 328
17.8.3 约束解码 329
参考文献 333
第4 部分自然语言处理篇334
18 自然语言处理简介335
18.1 NLP 的难点 335
18.2 NLP 的研究范围 336
19 词性标注338
19.1 传统词性标注模型 338
19.2 基于神经网络的词性标注模型 340
19.3 基于Bi-LSTM 的神经网络词性标注模型 342
参考文献 344
20 依存句法分析345
20.1 背景 346
20.2 SyntaxNet 技术要点 348
20.2.1 Transition-based 系统 349
20.2.2 “模板化” 技术 353
20.2.3 Beam Search 355
参考文献 357
21 word2vec 358
21.1 背景 359
21.1.1 词向量 359
21.1.2 统计语言模型 359
21.1.3 神经网络语言模型 362
21.1.4 Log-linear 模型 364
21.1.5 Log-bilinear 模型 365
21.1.6 层次化Log-bilinear 模型 365
21.2 CBOW 模型 366
21.3 Skip-gram 模型 369
21.4 Hierarchical Softmax 与Negative Sampling 371
21.5 fastText 372
21.6 GloVe 373
21.7 小结 374
参考文献 374
22 神经网络机器翻译376
22.1 机器翻译简介 376
22.2 神经网络机器翻译基本模型 377
22.3 基于Attention 的神经网络机器翻译 379
22.4 谷歌机器翻译系统GNMT 381
22.5 基于卷积的机器翻译 382
22.6 小结 383
参考文献 384
第5 部分深度学习研究篇385
23 Batch Normalization 386
23.1 前向与后向传播 387
23.1.1 前向传播 387
23.1.2 后向传播 390
23.2 有效性分析 393
23.2.1 内部协移 393
23.2.2 梯度流 393
23.3 使用与优化方法 395
23.4 小结 396
参考文献 396
24 Attention 397
24.1 从简单RNN 到RNN + Attention 398
24.2 Soft Attention 与Hard Attention 398
24.3 Attention 的应用 399
24.4 小结 401
参考文献 402
25 多任务学习403
25.1 背景 403
25.2 什么是多任务学习 404
25.3 多任务分类与其他分类概念的关系 406
25.3.1 二分类 406
25.3.2 多分类 407
25.3.3 多标签分类 407
25.3.4 相关关系 408
25.4 多任务学习如何发挥作用 409
25.4.1 提高泛化能力的潜在原因 410
25.4.2 多任务学习机制 410
25.4.3 后向传播多任务学习如何发现任务是相关的 412
25.5 多任务学习被广泛应用 413
25.5.1 使用未来预测现在 413
25.5.2 多种表示和度量 413
25.5.3 时间序列预测 413
25.5.4 使用不可操作特征 414
25.5.5 使用额外任务来聚焦 414
25.5.6 有序迁移 414
25.5.7 多个任务自然地出现 414
25.5.8 将输入变成输出 414
25.6 多任务深度学习应用 416
25.6.1 脸部特征点检测 416
25.6.2 DeepID2 418
25.6.3 Fast R-CNN 419
25.6.4 旋转人脸网络 420
25.6.5 实例感知语义分割的MNC 422
25.7 小结 423
参考文献 425
26 模型压缩426
26.1 模型压缩的必要性 426
26.2 较浅的网络 428
26.3 剪枝 428
26.4 参数共享 434
26.5 紧凑网络 437
26.6 二值网络 438
26.7 小结 442
参考文献 442
27 增强学习445
27.1 什么是增强学习 445
27.2 增强学习的数学表达形式 448
27.2.1 MDP 449
27.2.2 策略函数 450
27.2.3 奖励与回报 450
27.2.4 价值函数 452
27.2.5 贝尔曼方程 453
27.2.6 最优策略性质 453
27.3 用动态规划法求解增强学习问题 454
27.3.1 Agent 的目标 454
27.3.2 策略评估 455
27.3.3 策略改进 456
27.3.4 策略迭代 457
27.3.5 策略迭代的例子 458
27.3.6 价值迭代 459
27.3.7 价值迭代的例子 461
27.3.8 策略函数和价值函数的关系 462
27.4 无模型算法 462
27.4.1 蒙特卡罗法 463
27.4.2 时序差分法 465
27.4.3 Q-Learning 466
27.5 Q-Learning 的例子 467
27.6 AlphaGo 原理剖析 469
27.6.1 围棋与机器博弈 469
27.6.2 Alpha-Beta 树 472
27.6.3 MCTS 473
27.6.4 UCT 476
27.6.5 AlphaGo 的训练策略 478
27.6.6 AlphaGo 的招式搜索算法 482
27.6.7 围棋的对称性 484
27.7 AlphaGo Zero 484
参考文献 484
28 GAN 486
28.1 生成模型 486
28.2 生成对抗模型的概念 488
28.3 GAN 实战 492
28.4 InfoGAN――探寻隐变量的内涵 493
28.5 Image-Image Translation 496
28.6 WGAN（Wasserstein GAN） 499
28.6.1 GAN 目标函数的弱点 500
28.6.2 Wasserstein 度量的优势 501
28.6.3 WGAN 的目标函数 504
参考文献 505
A 本书涉及的开源资源列表 506
・ ・ ・ ・ ・ ・ (收起)1 深度学习简介1
1.1 人工智能、机器学习和深度学习 1
1.1.1 引言 1
1.1.2 人工智能、机器学习和深度学习三者的关系 2
1.2 神经网络 3
1.2.1 感知器 3
1.2.2 激活函数 5
1.2.3 损失函数 8
1.2.4 梯度下降和随机梯度下降 8
1.2.5 反向传播算法简述 11
1.2.6 其他神经网络 12
1.3 学习方法建议 13
1.3.1 网络资源 13
1.3.2 TensorFlow 官方深度学习教程 14
1.3.3 开源社区 15
1.4 TensorLayer 15
1.4.1 深度学习框架概况 15
1.4.2 TensorLayer 概括 16
1.4.3 实验环境配置 17
2 多层感知器19
2.1 McCulloch-Pitts 神经元模型 19
2.1.1 人工神经网络到底能干什么？到底在干什么 21
2.1.2 什么是激活函数？什么是偏值 22
2.2 感知器 23
2.2.1 什么是线性分类器 24
2.2.2 线性分类器有什么优缺点 26
2.2.3 感知器实例和异或问题（XOR 问题） 26
2.3 多层感知器 30
2.4 实现手写数字分类 32
2.5 过拟合 40
2.5.1 什么是过拟合 40
2.5.2 Dropout 41
2.5.3 批规范化 42
2.5.4 L1、L2 和其他正则化方法 42
2.5.5 Lp 正则化的图形化解释 44
2.6 再实现手写数字分类 46
2.6.1 数据迭代器 46
2.6.2 通过all_drop 启动与关闭Dropout 47
2.6.3 通过参数共享实现训练测试切换 50
3 自编码器54
3.1 稀疏性 54
3.2 稀疏自编码器 56
3.3 实现手写数字特征提取 59
3.4 降噪自编码器 65
3.5 再实现手写数字特征提取 68
3.6 堆栈式自编码器及其实现 72
4 卷积神经网络80
4.1 卷积原理 80
4.1.1 卷积操作 81
4.1.2 张量 84
4.1.3 卷积层 85
4.1.4 池化层 87
4.1.5 全连接层 89
4.2 经典任务 90
4.2.1 图像分类 90
4.2.2 目标检测 91
4.2.3 语义分割 94
4.2.4 实例分割 94
4.3 经典卷积网络 95
4.3.1 LeNet 95
4.3.2 AlexNet 96
4.3.3 VGGNet 96
4.3.4 GoogLeNet 98
4.3.5 ResNet 99
4.4 实现手写数字分类 100
4.5 数据增强与规范化 104
4.5.1 数据增强 104
4.5.2 批规范化 106
4.5.3 局部响应归一化 107
4.6 实现CIFAR10 分类 108
4.6.1 方法1：tl.prepro 做数据增强 108
4.6.2 方法2：TFRecord 做数据增强 114
4.7 反卷积神经网络 120
5 词的向量表达121
5.1 目的与原理 121
5.2 Word2Vec 124
5.2.1 简介 124
5.2.2 Continuous Bag-Of-Words（CBOW）模型 124
5.2.3 Skip Gram（SG）模型 129
5.2.4 Hierarchical Softmax 132
5.2.5 Negative Sampling 135
5.3 实现Word2Vec 136
5.3.1 简介 136
5.3.2 实现 136
5.4 重载预训练矩阵 144
6 递归神经网络148
6.1 为什么需要它 148
6.2 不同的RNNs 151
6.2.1 简单递归网络 151
6.2.2 回音网络 152
6.3 长短期记忆 153
6.3.1 LSTM 概括 153
6.3.2 LSTM 详解 157
6.3.3 LSTM 变种 159
6.4 实现生成句子 160
6.4.1 模型简介 160
6.4.2 数据迭代 163
6.4.3 损失函数和更新公式 164
6.4.4 生成句子及Top K 采样 167
6.4.5 接下来还可以做什么 169
7 深度增强学习171
7.1 增强学习 172
7.1.1 概述 172
7.1.2 基于价值的增强学习 173
7.1.3 基于策略的增强学习 176
7.1.4 基于模型的增强学习 177
7.2 深度增强学习 179
7.2.1 深度Q 学习 179
7.2.2 深度策略网络 181
7.3 更多参考资料 187
7.3.1 书籍 187
7.3.2 在线课程 187
8 生成对抗网络188
8.1 何为生成对抗网络 189
8.2 深度卷积对抗生成网络 190
8.3 实现人脸生成 191
8.4 还能做什么 198
9 高级实现技巧202
9.1 与其他框架对接 202
9.1.1 无参数层 203
9.1.2 有参数层 203
9.2 自定义层 204
9.2.1 无参数层 204
9.2.2 有参数层 205
9.3 建立词汇表 207
9.4 补零与序列长度 209
9.5 动态递归神经网络 210
9.6 实用小技巧 211
9.6.1 屏蔽显示 211
9.6.2 参数名字前缀 212
9.6.3 获取特定参数 213
9.6.4 获取特定层输出 213
10 实例一：使用预训练卷积网络214
10.1 高维特征表达 214
10.2 VGG 网络 215
10.3 连接TF-Slim 221
11 实例二：图像语义分割及其医学图像应用225
11.1 图像语义分割概述 225
11.1.1 传统图像分割算法简介 227
11.1.2 损失函数与评估指标 229
11.2 医学图像分割概述 230
11.3 全卷积神经网络和U-Net 网络结构 232
11.4 医学图像应用：实现脑部肿瘤分割 234
11.4.1 数据与数据增强 235
11.4.2 U-Net 网络 238
11.4.3 损失函数 239
11.4.4 开始训练 241
12 实例三：由文本生成图像244
12.1 条件生成对抗网络之GAN-CLS 245
12.2 实现句子生成花朵图片 246
13 实例四：超高分辨率复原260
13.1 什么是超高分辨率复原 260
13.2 网络结构 261
13.3 联合损失函数 264
13.4 训练网络 269
13.5 使用测试 277
14 实例五：文本反垃圾280
14.1 任务场景 280
14.2 网络结构 281
14.3 词的向量表示 282
14.4 Dynamic RNN 分类器 283
14.5 训练网络 284
14.5.1 训练词向量 284
14.5.2 文本的表示 290
14.5.3 训练分类器 291
14.5.4 模型导出 296
14.6 TensorFlow Serving 部署 299
14.7 客户端调用 301
14.8 其他常用方法 306
中英对照表及其缩写309
参考文献316
・ ・ ・ ・ ・ ・ (收起)目　　录
第　1章 深度学习简介　1
1．1　深度学习与人工智能　1
1．2　深度学习的历史渊源　2
1．2．1　从感知机到人工神经网络　3
1．2．2　深度学习时代　4
1．2．3　巨头之间的角逐　5
1．3　深度学习的影响因素　6
1．3．1　大数据　6
1．3．2　深度网络架构　7
1．3．3　GPU　11
1．4　深度学习为什么如此成功　11
1．4．1　特征学习（representation learning）　11
1．4．2　迁移学习（transfer learning）　12
1．5　小结　13
参考文献　14
第　2章 PyTorch简介　15
2．1　PyTorch安装　15
2．2　初识PyTorch　15
2．2．1　与Python的完美融合　16
2．2．2　张量计算　16
2．2．3　动态计算图　20
2．3　PyTorch实例：预测房价　27
2．3．1　准备数据　27
2．3．2　模型设计　28
2．3．3　训练　29
2．3．4　预测　31
2．3．5　术语汇总　32
2．4　小结　33
第3章　单车预测器：你的第 一个
神经网络　35
3．1　共享单车的烦恼　35
3．2　单车预测器1．0　37
3．2．1　神经网络简介　37
3．2．2　人工神经元　38
3．2．3　两个隐含层神经元　40
3．2．4　训练与运行　42
3．2．5　失败的神经预测器　43
3．2．6　过拟合　48
3．3　单车预测器2．0　49
3．3．1　数据的预处理过程　49
3．3．2　构建神经网络　52
3．3．3　测试神经网络　55
3．4　剖析神经网络Neu　57
3．5　小结　61
3．6　Q&A　61
第4章　机器也懂感情――中文情绪
分类器　63
4．1　神经网络分类器　64
4．1．1　如何用神经网络做分类　64
4．1．2　分类问题的损失函数　66
4．2　词袋模型分类器　67
4．2．1　词袋模型简介　68
4．2．2　搭建简单文本分类器　69
4．3　程序实现　70
4．3．1　数据获取　70
4．3．2　数据处理　74
4．3．3　文本数据向量化　75
4．3．4　划分数据集　76
4．3．5　建立神经网络　78
4．4　运行结果　80
4．5　剖析神经网络　81
4．6　小结　85
4．7　Q&A　85
第5章　手写数字识别器――认识卷积
神经网络　87
5．1　什么是卷积神经网络　88
5．1．1　手写数字识别任务的CNN
网络及运算过程　88
5．1．2　卷积运算操作　90
5．1．3　池化操作　96
5．1．4　立体卷积核　97
5．1．5　超参数与参数　98
5．1．6　其他说明　99
5．2　手写数字识别器　100
5．2．1　数据准备　100
5．2．2　构建网络　103
5．2．3　运行模型　105
5．2．4　测试模型　106
5．3　剖析卷积神经网络　107
5．3．1　第 一层卷积核与特征图　107
5．3．2　第二层卷积核与特征图　109
5．3．3　卷积神经网络的健壮性试验　110
5．4　小结　112
5．5　Q&A　112
5．6　扩展阅读　112
第6章　手写数字加法机――迁移学习　113
6．1　什么是迁移学习　114
6．1．1　迁移学习的由来　114
6．1．2　迁移学习的分类　115
6．1．3　迁移学习的意义　115
6．1．4　如何用神经网络实现迁移
学习　116
6．2　应用案例：迁移学习如何抗击贫困　118
6．2．1　背景介绍　118
6．2．2　方法探寻　119
6．2．3　迁移学习方法　120
6．3　蚂蚁还是蜜蜂：迁移大型卷积神经
网络　121
6．3．1　任务描述与初步尝试　121
6．3．2　ResNet与模型迁移　122
6．3．3　代码实现　123
6．3．4　结果分析　127
6．3．5　更多的模型与数据　128
6．4　手写数字加法机　128
6．4．1　网络架构　128
6．4．2　代码实现　129
6．4．3　训练与测试　136
6．4．4　结果　138
6．4．5　大规模实验　138
6．5　小结　143
6．6　实践项目：迁移与效率　143
第7章　你自己的Prisma――图像
风格迁移　145
7．1　什么是风格迁移　145
7．1．1　什么是风格　145
7．1．2　风格迁移的涵义　146
7．2　风格迁移技术发展简史　147
7．2．1　神经网络之前的风格迁移　147
7．2．2　特定风格的实现　148
7．3　神经网络风格迁移　149
7．3．1　神经网络风格迁移的优势　150
7．3．2　神经网络风格迁移的基本
思想　150
7．3．3　卷积神经网络的选取　151
7．3．4　内容损失　152
7．3．5　风格损失　152
7．3．6　风格损失原理分析　153
7．3．7　损失函数与优化　156
7．4　神经网络风格迁移实战　157
7．4．1　准备工作　157
7．4．2　建立风格迁移网络　159
7．4．3　风格迁移训练　162
7．5　小结　165
7．6　扩展阅读　165
第8章　人工智能造假术――图像生成
与对抗学习　166
8．1　反卷积与图像生成　169
8．1．1　CNN回顾　169
8．1．2　反卷积操作　171
8．1．3　反池化过程　173
8．1．4　反卷积与分数步伐　174
8．1．5　输出图像尺寸公式　175
8．1．6　批正则化技术　176
8．2　图像生成实验1――最小均方误差
模型　177
8．2．1　模型思路　177
8．2．2　代码实现　178
8．2．3　运行结果　182
8．3　图像生成实验2――生成器-识别器
模型　184
8．3．1　生成器-识别器模型的实现　184
8．3．2　对抗样本　187
8．4　图像生成实验3――生成对抗网络
GAN　190
8．4．1　GAN的总体架构　191
8．4．2　程序实现　192
8．4．3　结果展示　195
8．5　小结　197
8．6　Q&A　197
8．7　扩展阅读　198
第9章　词汇的星空――神经语言模型
与Word2Vec　199
9．1　词向量技术介绍　199
9．1．1　初识词向量　199
9．1．2　传统编码方式　200
9．2　NPLM：神经概率语言模型　201
9．2．1　NPLM的基本思想　202
9．2．2　NPLM的运作过程详解　202
9．2．3　读取NPLM中的词向量　205
9．2．4　NPLM的编码实现　206
9．2．5　运行结果　209
9．2．6　NPLM的总结与局限　211
9．3　Word2Vec　211
9．3．1　CBOW模型和Skip-gram模型的结构　211
9．3．2　层级软最大　213
9．3．3　负采样　213
9．3．4　总结及分析　214
9．4　Word2Vec的应用　214
9．4．1　在自己的语料库上训练Word2Vec词向量　214
9．4．2　调用现成的词向量　216
9．4．3　女人-男人＝皇后-国王　218
9．4．4　使用向量的空间位置进行词对词翻译　220
9．4．5　Word2Vec小结　221
9．5　小结　221
9．5　Q&A　222
第　10章 LSTM作曲机――序列生成
模型　224
10．1　序列生成问题　224
10．2　RNN与LSTM　225
10．2．1　RNN　226
10．2．2　LSTM　231
10．3　简单01序列的学习问题　235
10．3．1　RNN的序列学习　236
10．3．2　LSTM的序列学习　245
10．4　LSTM作曲机　248
10．4．1　MIDI文件　248
10．4．2　数据准备　249
10．4．3　模型结构　249
10．4．4　代码实现　250
10．5　小结　259
10．6　Q&A　259
10．7　扩展阅读　259
・ ・ ・ ・ ・ ・ (收起)第1章　编程和数学基础 1
1.1　Python快速入门 1
1.1.1　快速安装Python 1
1.1.2　Python基础 2
1.1.3　Python中的常见运算 5
1.1.4　Python控制语句 7
1.1.5　Python常用容器类型 10
1.1.6　Python常用函数 16
1.1.7　类和对象 22
1.1.8　Matplotlib入门 24
1.2　张量库NumPy 33
1.2.1　什么是张量 33
1.2.2　创建ndarray对象 37
1.2.3　ndarray数组的索引和切片 53
1.2.4　张量的计算 57
1.3　微积分 63
1.3.1　函数 64
1.3.2　四则运算和复合运算 66
1.3.3　极限和导数 69
1.3.4　导数的四则运算和链式法则 72
1.3.5　计算图、正向计算和反向传播求导 74
1.3.6　多变量函数的偏导数与梯度 75
1.3.7　向量值函数的导数与Jacobian矩阵 78
1.3.8　积分 83
1.4　概率基础 84
1.4.1　概率 84
1.4.2　条件概率、联合概率、全概率公式、贝叶斯公式 86
1.4.3　随机变量 88
1.4.4　离散型随机变量的概率分布 89
1.4.5　连续型随机变量的概率密度 91
1.4.6　随机变量的分布函数 93
1.4.7　期望、方差、协方差、协变矩阵 95
第2章　梯度下降法 99
2.1　函数极值的必要条件 99
2.2　梯度下降法基础 101
2.3　梯度下降法的参数优化策略 108
2.3.1　Momentum法 108
2.3.2　AdaGrad法 110
2.3.3　AdaDelta法 112
2.3.4　RMSprop法 114
2.3.5　Adam法 115
2.4　梯度验证 117
2.4.1　比较数值梯度和分析梯度 117
2.4.2　通用的数值梯度 118
2.5　分离梯度下降法与参数优化策略 119
2.5.1　参数优化器 119
2.5.2　接受参数优化器的梯度下降法 120
第3章　线性回归、逻辑回归和softmax回归 122
3.1　线性回归 122
3.1.1　餐车利润问题 122
3.1.2　机器学习与人工智能 123
3.1.3　什么是线性回归 126
3.1.4　用正规方程法求解线性回归问题 127
3.1.5　用梯度下降法求解线性回归问题 129
3.1.6　调试学习率 133
3.1.7　梯度验证 135
3.1.8　预测 135
3.1.9　多特征线性回归 136
3.2　数据的规范化 143
3.2.1　预测大坝出水量 143
3.2.2　数据的规范化过程 147
3.3　模型的评估 149
3.3.1　欠拟合和过拟合 149
3.3.2　验证集和测试集 153
3.3.3　学习曲线 155
3.3.4　偏差和方差 160
3.4　正则化 165
3.5　逻辑回归 168
3.5.1　逻辑回归基础 169
3.5.2　逻辑回归的NumPy实现 173
3.5.3　实战：鸢尾花分类的NumPy实现 178
3.6　softmax回归 180
3.6.1　spiral数据集 180
3.6.2　softmax函数 181
3.6.3　softmax回归模型 186
3.6.4　多分类交叉熵损失 188
3.6.5　通过加权和计算交叉熵损失 191
3.6.6　softmax回归的梯度计算 191
3.6.7　softmax回归的梯度下降法的实现 197
3.6.8　spiral数据集的softmax回归模型 197
3.7　批梯度下降法和随机梯度下降法 199
3.7.1　MNIST手写数字集 199
3.7.2　用部分训练样本训练逻辑回归模型 201
3.7.3　批梯度下降法 202
3.7.4　随机梯度下降法 207
第4章　神经网络 209
4.1　神经网络概述 209
4.1.1　感知机和神经元 209
4.1.2　激活函数 213
4.1.3　神经网络与深度学习 216
4.1.4　多个样本的正向计算 221
4.1.5　输出 224
4.1.6　损失函数 224
4.1.7　基于数值梯度的神经网络训练 229
4.2　反向求导 235
4.2.1　正向计算和反向求导 235
4.2.2　计算图 237
4.2.3　损失函数关于输出的梯度 239
4.2.4　2层神经网络的反向求导 242
4.2.5　2层神经网络的Python实现 247
4.2.6　任意层神经网络的反向求导 252
4.3　实现一个简单的深度学习框架 256
4.3.1　神经网络的训练过程 256
4.3.2　网络层的代码实现 257
4.3.3　网络层的梯度检验 260
4.3.4　神经网络的类 261
4.3.5　神经网络的梯度检验 263
4.3.6　基于深度学习框架的MNIST手写数字识别 266
4.3.7　改进的通用神经网络框架：分离加权和与激活函数 268
4.3.8　独立的参数优化器 276
4.3.9　fashion-mnist的分类训练 279
4.3.10　读写模型参数 282
第5章　改进神经网络性能的基本技巧 285
5.1　数据处理 285
5.1.1　数据增强 285
5.1.2　规范化 289
5.1.3　特征工程 289
5.2　参数调试 296
5.2.1　权重初始化 296
5.2.2　优化参数 301
5.3　批规范化 301
5.3.1　什么是批规范化 301
5.3.2　批规范化的反向求导 303
5.3.3　批规范化的代码实现 304
5.4　正则化 310
5.4.1　权重正则化 310
5.4.2　Dropout 312
5.4.3　早停法 316
5.5　梯度爆炸和梯度消失 317
第6章　卷积神经网络 318
6.1　卷积入门 319
6.1.1　什么是卷积 319
6.1.2　一维卷积 325
6.1.3　二维卷积 326
6.1.4　多通道输入和多通道输出 338
6.1.5　池化 341
6.2　卷积神经网络概述 344
6.2.1　全连接神经元和卷积神经元 345
6.2.2　卷积层和卷积神经网络 346
6.2.3　卷积层和池化层的反向求导及代码实现 349
6.2.4　卷积神经网络的代码实现 361
6.3　卷积的矩阵乘法 364
6.3.1　一维卷积的矩阵乘法 364
6.3.2　二维卷积的矩阵乘法 365
6.3.3　一维卷积反向求导的矩阵乘法 371
6.3.4　二维卷积反向求导的矩阵乘法 373
6.4　基于坐标索引的快速卷积 377
6.5　典型卷积神经网络结构 393
6.5.1　LeNet-5 393
6.5.2　AlexNet 394
6.5.3　VGG 395
6.5.4　残差网络 396
6.5.5　Inception网络 398
6.5.6　NiN 399
第7章　循环神经网络 403
7.1　序列问题和模型 403
7.1.1　股票价格预测问题 404
7.1.2　概率序列模型和语言模型 405
7.1.3　自回归模型 406
7.1.4　生成自回归数据 406
7.1.5　时间窗方法 408
7.1.6　时间窗采样 409
7.1.7　时间窗方法的建模和训练 409
7.1.8　长期预测和短期预测 410
7.1.9　股票价格预测的代码实现 412
7.1.10　k-gram语言模型 415
7.2　循环神经网络基础 416
7.2.1　无记忆功能的非循环神经网络 417
7.2.2　具有记忆功能的循环神经网络 418
7.3　穿过时间的反向传播 421
7.4　单层循环神经网络的实现 425
7.4.1　初始化模型参数 425
7.4.2　正向计算 425
7.4.3　损失函数 427
7.4.4　反向求导 427
7.4.5　梯度验证 429
7.4.6　梯度下降训练 432
7.4.7　序列数据的采样 433
7.4.8　序列数据的循环神经网络训练和预测 441
7.5　循环神经网络语言模型和文本的生成 448
7.5.1　字符表 448
7.5.2　字符序列样本的采样 450
7.5.3　模型的训练和预测 452
7.6　循环神经网络中的梯度爆炸和梯度消失 455
7.7　长短期记忆网络 456
7.7.1　LSTM的神经元 457
7.7.2　LSTM的反向求导 460
7.7.3　LSTM的代码实现 461
7.7.4　LSTM的变种 469
7.8　门控循环单元 470
7.8.1　门控循环单元的工作原理 470
7.8.2　门控循环单元的代码实现 472
7.9　循环神经网络的类及其实现 475
7.9.1　用类实现循环神经网络 475
7.9.2　循环神经网络单元的类实现 483
7.10　多层循环神经网络和双向循环神经网络 491
7.10.1　多层循环神经网络 491
7.10.2　多层循环神经网络的训练和预测 497
7.10.3　双向循环神经网络 500
7.11　Seq2Seq模型 506
7.11.1　机器翻译概述 507
7.11.2　Seq2Seq模型的实现 508
7.11.3　字符级的Seq2Seq模型 516
7.11.4　基于Word2Vec的Seq2Seq模型 522
7.11.5　基于词嵌入层的Seq2Seq模型 533
7.11.6　注意力机制 541
第8章　生成模型 552
8.1　生成模型概述 552
8.2　自动编码器 556
8.2.1　什么是自动编码器 557
8.2.2　稀疏编码器 559
8.2.3　自动编码器的代码实现 560
8.3　变分自动编码器 563
8.3.1　什么是变分自动编码器 563
8.3.2　变分自动编码器的损失函数 564
8.3.3　变分自动编码器的参数重采样 565
8.3.4　变分自动编码器的反向求导 565
8.3.5　变分自动编码器的代码实现 566
8.4　生成对抗网络 571
8.4.1　生成对抗网络的原理 573
8.4.2　生成对抗网络训练过程的代码实现 577
8.5　生成对抗网络建模实例 579
8.5.1　一组实数的生成对抗网络建模 579
8.5.2　二维坐标点的生成对抗网络建模 585
8.5.3　MNIST手写数字集的生成对抗网络建模 590
8.5.4　生成对抗网络的训练技巧 594
8.6　生成对抗网络的损失函数及其概率解释 594
8.6.1　生成对抗网络的损失函数的全局最优解 594
8.6.2　Kullback-Leibler散度和Jensen-Shannon散度 595
8.6.3　生成对抗网络的最大似然解释 598
8.7　改进的损失函数――Wasserstein GAN 599
8.7.1　Wasserstein GAN的原理 599
8.7.2　Wasserstein GAN的代码实现 603
8.8　深度卷积对抗网络 605
8.8.1　一维转置卷积 606
8.8.2　二维转置卷积 609
8.8.3　卷积对抗网络的代码实现 612
参考文献 617
・ ・ ・ ・ ・ ・ (收起)第一部分　元编程基础技术
第1章 基本技巧　3
1.1　元函数与type_traits　3
1.1.1　元函数介绍　3
1.1.2　类型元函数　4
1.1.3　各式各样的元函数　6
1.1.4　type_traits　7
1.1.5　元函数与宏　7
1.1.6　本书中元函数的命名方式　8
1.2　模板型模板参数与容器模板　8
1.2.1　模板作为元函数的输入　9
1.2.2　模板作为元函数的输出　9
1.2.3　容器模板　10
1.3　顺序、分支与循环代码的编写　12
1.3.1　顺序执行的代码　12
1.3.2　分支执行的代码　13
1.3.3　循环执行的代码　19
1.3.4　小心：实例化爆炸与编译崩溃　21
1.3.5　分支选择与短路逻辑　23
1.4　奇特的递归模板式　24
1.5　小结　25
1.6　练习　26
第2章 异类词典与policy模板　28
2.1　具名参数简介　28
2.2　异类词典　30
2.2.1　模块的使用方式　30
2.2.2　键的表示　32
2.2.3　异类词典的实现　34
2.2.4　VarTypeDict的性能简析　41
2.2.5　用std::tuple作为缓存　41
2.3　policy模板　42
2.3.1　policy介绍　42
2.3.2　定义policy与policy对象（模板）　45
2.3.3　使用policy　47
2.3.4　背景知识：支配与虚继承　49
2.3.5　policy对象与policy支配结构　50
2.3.6　policy选择元函数　52
2.3.7　使用宏简化policy对象的声明　57
2.4　小结　58
2.5　练习　58
第二部分 深度学习框架
第3章　深度学习概述　63
3.1　深度学习简介　63
3.1.1　从机器学习到深度学习　64
3.1.2　各式各样的人工神经网络　65
3.1.3　深度学习系统的组织与训练　68
3.2　本书所实现的框架：MetaNN　70
3.2.1　从矩阵计算工具到深度学习框架　70
3.2.2　MetaNN介绍　71
3.2.3　本书将要讨论的内容　72
3.2.4　本书不会涉及的主题　75
3.3　小结　75
第4章　类型体系与基本数据类型　76
4.1　类型体系　77
4.1.1　类型体系介绍　77
4.1.2　迭代器分类体系　78
4.1.3　将标签作为模板参数　80
4.1.4　MetaNN的类型体系　81
4.1.5　与类型体系相关的元函数　82
4.2　设计理念　84
4.2.1　支持不同的计算设备与计算单元　84
4.2.2　存储空间的分配与维护　85
4.2.3　浅拷贝与写操作检测　88
4.2.4　底层接口扩展　89
4.2.5　类型转换与求值　91
4.2.6　数据接口规范　92
4.3　标量　92
4.3.1　类模板的声明　93
4.3.2　基于CPU的特化版本　94
4.3.3　标量的主体类型　95
4.4　矩阵　96
4.4.1　Matrix类模板　96
4.4.2　特殊矩阵：平凡矩阵、全零矩阵与独热向量　101
4.4.3　引入新的矩阵类　104
4.5　列表　105
4.5.1　Batch模板　105
4.5.2　Array模板　108
4.5.3　重复与Duplicate模板　113
4.6　小结　116
4.7　练习　116
第5章　运算与表达式模板　119
5.1　表达式模板简介　119
5.2　MetaNN运算模板的设计思想　122
5.2.1　Add模板的问题　122
5.2.2　运算模板的行为分析　122
5.3　运算分类　124
5.4　辅助模板　125
5.4.1　辅助类模板OperElementType_/OperDeviceType_　125
5.4.2　辅助类模板OperXXX_　126
5.4.3　辅助类模板OperCateCal　126
5.4.4　辅助类模板OperOrganizer　128
5.4.5　辅助类模板OperSeq　130
5.5　运算模板的框架　131
5.5.1　运算模板的类别标签　131
5.5.2　UnaryOp的定义　132
5.6　运算实现示例　133
5.6.1　Sigmoid运算　133
5.6.2　Add运算　136
5.6.3　转置运算　139
5.6.4　折叠运算　141
5.7　MetaNN已支持的运算列表　141
5.7.1　一元运算　141
5.7.2　二元运算　142
5.7.3　三元运算　144
5.8　运算的折衷与局限性　144
5.8.1　运算的折衷　144
5.8.2　运算的局限性　145
5.9　小结　146
5.10　练习　146
第6章　基本层　148
6.1　层的设计理念　148
6.1.1　层的介绍　148
6.1.2　层对象的构造　150
6.1.3　参数矩阵的初始化与加载　151
6.1.4　正向传播　152
6.1.5　存储中间结果　154
6.1.6　反向传播　154
6.1.7　参数矩阵的更新　155
6.1.8　参数矩阵的获取　155
6.1.9　层的中性检测　156
6.2　层的辅助逻辑　156
6.2.1　初始化模块　156
6.2.2　DynamicData类模板　161
6.2.3　层的常用policy对象　166
6.2.4　InjectPolicy元函数　168
6.2.5　通用I/O结构　168
6.2.6　通用操作函数　169
6.3　层的具体实现　170
6.3.1　AddLayer　170
6.3.2　ElementMulLayer　172
6.3.3　BiasLayer　176
6.4　MetaNN已实现的基本层　181
6.5　小结　183
6.6　练习　184
第7章　复合层与循环层　185
7.1　复合层的接口与设计理念　186
7.1.1　基本结构　186
7.1.2　结构描述语法　187
7.1.3　policy的继承关系　188
7.1.4　policy的修正　189
7.1.5　复合层的构造函数　190
7.1.6　一个完整的复合层构造示例　190
7.2　policy继承与修正逻辑的实现　191
7.2.1　policy继承逻辑的实现　191
7.2.2　policy修正逻辑的实现　194
7.3　ComposeTopology的实现　195
7.3.1　功能介绍　195
7.3.2　拓扑排序算法介绍　195
7.3.3　ComposeTopology包含的主要步骤　196
7.3.4　结构描述子句与其划分　196
7.3.5　结构合法性检查　198
7.3.6　拓扑排序的实现　200
7.3.7　子层实例化元函数　203
7.4　ComposeKernel的实现　207
7.4.1　类模板的声明　208
7.4.2　子层对象管理　208
7.4.3　参数获取、梯度收集与中性检测　211
7.4.4　参数初始化与加载　212
7.4.5　正向传播　214
7.4.6　反向传播　221
7.5　复合层实现示例　221
7.6　循环层　222
7.6.1　GruStep　222
7.6.2　构建RecurrentLayer类模板　224
7.6.3　RecurrentLayer的使用　230
7.7　小结　230
7.8　练习　230
第8章　求值与优化　233
8.1　MetaNN的求值模型　234
8.1.1　运算的层次结构　234
8.1.2　求值子系统的模块划分　235
8.2　基本求值逻辑　242
8.2.1　主体类型的求值接口　242
8.2.2　非主体基本数据类型的求值　243
8.2.3　运算模板的求值　245
8.2.4　DyanmicData与求值　248
8.3　求值过程的优化　249
8.3.1　避免重复计算　249
8.3.2　同类计算合并　250
8.3.3　多运算协同优化　251
8.4　小结　258
8.5　练习　259
后记―方家休见笑，吾道本艰难　260
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序
前言
第1章　打造深度学习工具箱1
1.1　TensorFlow1
1.1.1　安装1
1.1.2　使用举例3
1.2　TFLearn3
1.3　PaddlePaddle4
1.3.1　安装5
1.3.2　使用举例6
1.4　Karas7
1.5　本章小结9
第2章　卷积神经网络10
2.1　传统的图像分类算法10
2.2　基于CNN的图像分类算法11
2.2.1　局部连接11
2.2.2　参数共享13
2.2.3　池化15
2.2.4　典型的CNN结构及实现16
2.2.5　AlexNet的结构及实现19
2.2.6　VGG的结构及实现24
2.3　基于CNN的文本处理29
2.3.1　典型的CNN结构30
2.3.2　典型的CNN代码实现30
2.4　本章小结32
第3章　循环神经网络33
3.1　循环神经算法概述34
3.2　单向循环神经网络结构与实现36
3.3　双向循环神经网络结构与实现38
3.4　循环神经网络在序列分类的应用41
3.5　循环神经网络在序列生成的应用42
3.6　循环神经网络在序列标记的应用43
3.7　循环神经网络在序列翻译的应用44
3.8　本章小结46
第4章　基于OpenSOC的机器学习框架47
4.1　OpenSOC框架47
4.2　数据源系统48
4.3　数据收集层53
4.4　消息系统层57
4.5　实时处理层60
4.6　存储层62
4.6.1　HDFS62
4.6.2　HBase64
4.6.3　Elasticsearch65
4.7　分析处理层66
4.8　计算系统67
4.9　实战演练72
4.10　本章小结77
第5章　验证码识别78
5.1　数据集79
5.2　特征提取80
5.3　模型训练与验证81
5.3.1　K近邻算法81
5.3.2　支持向量机算法81
5.3.3　深度学习算法之MLP82
5.3.4　深度学习算法之CNN83
5.4　本章小结87
第6章　垃圾邮件识别88
6.1　数据集89
6.2　特征提取90
6.2.1　词袋模型90
6.2.2　TF-IDF模型93
6.2.3　词汇表模型95
6.3　模型训练与验证97
6.3.1　朴素贝叶斯算法97
6.3.2　支持向量机算法100
6.3.3　深度学习算法之MLP101
6.3.4　深度学习算法之CNN102
6.3.5　深度学习算法之RNN106
6.4　本章小结108
第7章　负面评论识别109
7.1　数据集110
7.2　特征提取112
7.2.1　词袋和TF-IDF模型112
7.2.2　词汇表模型114
7.2.3　Word2Vec模型和Doc2Vec模型115
7.3　模型训练与验证119
7.3.1　朴素贝叶斯算法119
7.3.2　支持向量机算法122
7.3.3　深度学习算法之MLP123
7.3.4　深度学习算法之CNN124
7.4　本章小结127
第8章　骚扰短信识别128
8.1　数据集129
8.2　特征提取130
8.2.1　词袋和TF-IDF模型130
8.2.2　词汇表模型131
8.2.3　Word2Vec模型和Doc2Vec模型132
8.3　模型训练与验证134
8.3.1　朴素贝叶斯算法134
8.3.2　支持向量机算法136
8.3.3　XGBoost算法137
8.3.4　深度学习算法之MLP140
8.4　本章小结141
第9章　Linux后门检测142
9.1　数据集142
9.2　特征提取144
9.3　模型训练与验证145
9.3.1　朴素贝叶斯算法145
9.3.2　XGBoost算法146
9.3.3　深度学习算法之多层感知机148
9.4　本章小结149
第10章　用户行为分析与恶意行为检测150
10.1　数据集151
10.2　特征提取152
10.2.1　词袋和TF-IDF模型152
10.2.2　词袋和N-Gram模型154
10.2.3　词汇表模型155
10.3　模型训练与验证156
10.3.1　朴素贝叶斯算法156
10.3.2　XGBoost算法157
10.3.3　隐式马尔可夫算法159
10.3.4　深度学习算法之MLP164
10.4　本章小结166
第11章　WebShell检测167
11.1　数据集168
11.1.1　WordPress168
11.1.2　PHPCMS170
11.1.3　phpMyAdmin170
11.1.4　Smarty171
11.1.5　Yii171
11.2　特征提取172
11.2.1　词袋和TF-IDF模型172
11.2.2　opcode和N-Gram模型174
11.2.3　opcode调用序列模型180
11.3　模型训练与验证181
11.3.1　朴素贝叶斯算法181
11.3.2　深度学习算法之MLP182
11.3.3　深度学习算法之CNN184
11.4　本章小结188
第12章　智能扫描器189
12.1　自动生成XSS攻击载荷190
12.1.1　数据集190
12.1.2　特征提取194
12.1.3　模型训练与验证195
12.2　自动识别登录界面198
12.2.1　数据集198
12.2.2　特征提取199
12.2.3　模型训练与验证201
12.3　本章小结203
第13章　DGA域名识别204
13.1　数据集206
13.2　特征提取207
13.2.1　N-Gram模型207
13.2.2　统计特征模型208
13.2.3　字符序列模型210
13.3　模型训练与验证210
13.3.1　朴素贝叶斯算法210
13.3.2　XGBoost算法212
13.3.3　深度学习算法之多层感知机215
13.3.4　深度学习算法之RNN218
13.4　本章小结221
第14章　恶意程序分类识别222
14.1　数据集223
14.2　特征提取226
14.3　模型训练与验证228
14.3.1　支持向量机算法228
14.3.2　XGBoost算法229
14.3.3　深度学习算法之多层感知机230
14.4　本章小结231
第15章　反信用卡欺诈232
15.1　数据集232
15.2　特征提取234
15.2.1　标准化234
15.2.2　标准化和降采样234
15.2.3　标准化和过采样236
15.3　模型训练与验证239
15.3.1　朴素贝叶斯算法239
15.3.2　XGBoost算法243
15.3.3　深度学习算法之多层感知机247
15.4　本章小结251
・ ・ ・ ・ ・ ・ (收起)第1章 数据科学概述 1
1.1　挑战　2
1.1.1　工程实现的挑战　2
1.1.2　模型搭建的挑战　3
1.2　机器学习　5
1.2.1　机器学习与传统编程　5
1.2.2　监督式学习和非监督式学习　8
1.3　统计模型　8
1.4　关于本书　10
第2章 Python安装指南与简介：告别空谈　12
2.1　Python简介　13
2.1.1　什么是Python　15
2.1.2　Python在数据科学中的地位　16
2.1.3　不可能绕过的第三方库　17
2.2　Python安装　17
2.2.1　Windows下的安装　18
2.2.2　Mac下的安装　21
2.2.3　Linux下的安装　24
2.3　Python上手实践　26
2.3.1　Python shell　26
2.3.2　第 一个Python程序：Word Count　28
2.3.3　Python编程基础　30
2.3.4　Python的工程结构　34
2.4　本章小结　35
第3章　数学基础：恼人但又不可或缺的知识　36
3.1　矩阵和向量空间　37
3.1.1　标量、向量与矩阵　37
3.1.2　特殊矩阵　39
3.1.3　矩阵运算　39
3.1.4　代码实现　42
3.1.5　向量空间　44
3.2　概率：量化随机　46
3.2.1　定义概率：事件和概率空间　47
3.2.2　条件概率：信息的价值　48
3.2.3　随机变量：两种不同的随机　50
3.2.4　正态分布：殊途同归　52
3.2.5　P-value：自信的猜测　53
3.3　微积分　55
3.3.1　导数和积分：位置、速度　55
3.3.2　极限：变化的终点　57
3.3.3　复合函数：链式法则　58
3.3.4　多元函数：偏导数　59
3.3.5　极值与最值：最优选择　59
3.4　本章小结　61
第4章　线性回归：模型之母　62
4.1　一个简单的例子　64
4.1.1　从机器学习的角度看这个问题　66
4.1.2　从统计学的角度看这个问题　69
4.2　上手实践：模型实现　73
4.2.1　机器学习代码实现　74
4.2.2　统计方法代码实现　77
4.3　模型陷阱　82
4.3.1　过度拟合：模型越复杂越好吗　84
4.3.2　模型幻觉之统计学方案：假设检验　87
4.3.3　模型幻觉之机器学习方案：惩罚项　89
4.3.4　比较两种方案　92
4.4　模型持久化　92
4.4.1　模型的生命周期　93
4.4.2　保存模型　93
4.5　本章小结　96
第5章　逻辑回归：隐藏因子　97
5.1　二元分类问题：是与否　98
5.1.1　线性回归：为何失效　98
5.1.2　窗口效应：看不见的才是关键　100
5.1.3　逻辑分布：胜者生存　102
5.1.4　参数估计之似然函数：统计学角度　104
5.1.5　参数估计之损失函数：机器学习角度　104
5.1.6　参数估计之最终预测：从概率到选择　106
5.1.7　空间变换：非线性到线性　106
5.2　上手实践：模型实现　108
5.2.1　初步分析数据：直观印象　108
5.2.2　搭建模型　113
5.2.3　理解模型结果　116
5.3　评估模型效果：孰优孰劣　118
5.3.1　查准率与查全率　119
5.3.2　ROC曲线与AUC　123
5.4　多元分类问题：超越是与否　127
5.4.1　多元逻辑回归：逻辑分布的威力　128
5.4.2　One-vs.-all：从二元到多元　129
5.4.3　模型实现　130
5.5　非均衡数据集　132
5.5.1　准确度悖论　132
5.5.2　一个例子　133
5.5.3　解决方法　135
5.6　本章小结　136
第6章　工程实现：计算机是怎么算的　138
6.1　算法思路：模拟滚动　139
6.2　数值求解：梯度下降法　141
6.3　上手实践：代码实现　142
6.3.1　TensorFlow基础　143
6.3.2　定义模型　148
6.3.3　梯度下降　149
6.3.4　分析运行细节　150
6.4　更优化的算法：随机梯度下降法　153
6.4.1　算法细节　153
6.4.2　代码实现　154
6.4.3　两种算法比较　156
6.5　本章小结　158
第7章　计量经济学的启示：他山之石　159
7.1　定量与定性：变量的数学运算合理吗　161
7.2　定性变量的处理　162
7.2.1　虚拟变量　162
7.2.2　上手实践：代码实现　164
7.2.3　从定性变量到定量变量　168
7.3　定量变量的处理　170
7.3.1　定量变量转换为定性变量　171
7.3.2　上手实践：代码实现　171
7.3.3　基于卡方检验的方法　173
7.4　显著性　175
7.5　多重共线性：多变量的烦恼　176
7.5.1　多重共线性效应　176
7.5.2　检测多重共线性　180
7.5.3　解决方法　185
7.5.4　虚拟变量陷阱　188
7.6　内生性：变化来自何处　191
7.6.1　来源　192
7.6.2　内生性效应　193
7.6.3　工具变量　195
7.6.4　逻辑回归的内生性　198
7.6.5　模型的联结　200
7.7　本章小结　201
第8章　监督式学习： 目标明确　202
8.1　支持向量学习机　203
8.1.1　直观例子　204
8.1.2　用数学理解直观　205
8.1.3　从几何直观到最优化问题　207
8.1.4　损失项　209
8.1.5　损失函数与惩罚项　210
8.1.6　Hard margin 与soft margin比较　211
8.1.7　支持向量学习机与逻辑回归：隐藏的假设　213
8.2　核函数　216
8.2.1　空间变换：从非线性到线性　216
8.2.2　拉格朗日对偶　218
8.2.3　支持向量　220
8.2.4　核函数的定义：优化运算　221
8.2.5　常用的核函数　222
8.2.6　Scale variant　225
8.3　决策树　227
8.3.1　决策规则　227
8.3.2　评判标准　229
8.3.3　代码实现　231
8.3.4　决策树预测算法以及模型的联结　231
8.3.5　剪枝　235
8.4　树的集成　238
8.4.1　随机森林　238
8.4.2　Random forest embedding　239
8.4.3　GBTs之梯度提升　241
8.4.4　GBTs之算法细节　242
8.5　本章小结　244
第9章　生成式模型：量化信息的价值　246
9.1　贝叶斯框架　248
9.1.1　蒙提霍尔问题　248
9.1.2　条件概率　249
9.1.3　先验概率与后验概率　251
9.1.4　参数估计与预测公式　251
9.1.5　贝叶斯学派与频率学派　252
9.2　朴素贝叶斯　254
9.2.1　特征提取：文字到数字　254
9.2.2　伯努利模型　256
9.2.3　多项式模型　258
9.2.4　TF-IDF　259
9.2.5　文本分类的代码实现　260
9.2.6　模型的联结　265
9.3　判别分析　266
9.3.1　线性判别分析　267
9.3.2　线性判别分析与逻辑回归比较　269
9.3.3　数据降维　270
9.3.4　代码实现　273
9.3.5　二次判别分析　275
9.4　隐马尔可夫模型　276
9.4.1　一个简单的例子　276
9.4.2　马尔可夫链　278
9.4.3　模型架构　279
9.4.4　中文分词：监督式学习　280
9.4.5　中文分词之代码实现　282
9.4.6　股票市场：非监督式学习　284
9.4.7　股票市场之代码实现　286
9.5　本章小结　289
第10章 非监督式学习：聚类与降维　290
10.1　K-means　292
10.1.1　模型原理　292
10.1.2　收敛过程　293
10.1.3　如何选择聚类个数　295
10.1.4　应用示例　297
10.2　其他聚类模型　298
10.2.1　混合高斯之模型原理　299
10.2.2　混合高斯之模型实现　300
10.2.3　谱聚类之聚类结果　303
10.2.4　谱聚类之模型原理　304
10.2.5　谱聚类之图片分割　307
10.3　Pipeline　308
10.4　主成分分析　309
10.4.1　模型原理　310
10.4.2　模型实现　312
10.4.3　核函数　313
10.4.4　Kernel PCA的数学原理　315
10.4.5　应用示例　316
10.5　奇异值分解　317
10.5.1　定义　317
10.5.2　截断奇异值分解　317
10.5.3　潜在语义分析　318
10.5.4　大型推荐系统　320
10.6　本章小结　323
第11章 分布式机器学习：集体力量　325
11.1　Spark简介　327
11.1.1　Spark安装　328
11.1.2　从MapReduce到Spark　333
11.1.3　运行Spark　335
11.1.4　Spark DataFrame　336
11.1.5　Spark的运行架构　339
11.2　最优化问题的分布式解法　341
11.2.1　分布式机器学习的原理　341
11.2.2　一个简单的例子　342
11.3　大数据模型的两个维度　344
11.3.1　数据量维度　344
11.3.2　模型数量维度　346
11.4　开源工具的另一面　348
11.4.1　一个简单的例子　349
11.4.2　开源工具的阿喀琉斯之踵　351
11.5　本章小结　351
第12章 神经网络：模拟人的大脑　353
12.1　神经元　355
12.1.1　神经元模型　355
12.1.2　Sigmoid神经元与二元逻辑回归　356
12.1.3　Softmax函数与多元逻辑回归　358
12.2　神经网络　360
12.2.1　图形表示　360
12.2.2　数学基础　361
12.2.3　分类例子　363
12.2.4　代码实现　365
12.2.5　模型的联结　369
12.3　反向传播算法　370
12.3.1　随机梯度下降法回顾　370
12.3.2　数学推导　371
12.3.3　算法步骤　373
12.4　提高神经网络的学习效率　373
12.4.1　学习的原理　373
12.4.2　激活函数的改进　375
12.4.3　参数初始化　378
12.4.4　不稳定的梯度　380
12.5　本章小结　381
第13章 深度学习：继续探索　383
13.1　利用神经网络识别数字　384
13.1.1　搭建模型　384
13.1.2　防止过拟合之惩罚项　386
13.1.3　防止过拟合之dropout　387
13.1.4　代码实现　389
13.2　卷积神经网络　394
13.2.1　模型结构之卷积层　395
13.2.2　模型结构之池化层　397
13.2.3　模型结构之完整结构　399
13.2.4　代码实现　400
13.2.5　结构真的那么重要吗　405
13.3　其他深度学习模型　406
13.3.1　递归神经网络　406
13.3.2　长短期记忆　407
13.3.3　非监督式学习　409
13.4　本章小结　411
・ ・ ・ ・ ・ ・ (收起)基础篇
第1章 深度学习概述 2
1.1 深度学习发展简史 2
1.2 有监督学习 4
1.2.1 图像分类 4
1.2.2 目标检测 6
1.2.3 人脸识别 10
1.2.4 语音识别 13
1.3 无监督学习 18
1.3.1 无监督学习概述 18
1.3.2 生成对抗网络 18
1.4 强化学习 21
1.4.1 AlphaGo 21
1.4.2 AlphaGo Zero 23
1.5 小结 25
参考文献 25
第2章 深度神经网络 27
2.1 神经元 27
2.2 感知机 30
2.3 前向传递 32
2.3.1 前向传递的流程 32
2.3.2 激活函数 33
2.3.3 损失函数 37
2.4 后向传递 40
2.4.1 后向传递的流程 40
2.4.2 梯度下降 40
2.4.3 参数修正 42
2.5 防止过拟合 44
2.5.1 dropout 44
2.5.2 正则化 45
2.6 小结 46
第3章 卷积神经网络 47
3.1 卷积层 48
3.1.1 valid 卷积 48
3.1.2 full 卷积 50
3.1.3 same 卷积 51
3.2 池化层 52
3.3 反卷积 53
3.4 感受野 55
3.5 卷积网络实例 56
3.5.1 Lenet-5 56
3.5.2 AlexNet 59
3.5.3 VGGNet 62
3.5.4 GoogLeNet 64
3.5.5 ResNet 72
3.5.6 MobileNet 73
3.6 小结 76
进阶篇
第4章 两阶段目标检测方法 78
4.1 R-CNN 78
4.1.1 算法流程 79
4.1.2 训练过程 80
4.2 SPP-Net 83
4.2.1 网络结构 84
4.2.2 空间金字塔池化 84
4.3 Fast R-CNN 86
4.3.1 感兴趣区域池化层 86
4.3.2 网络结构 88
4.3.3 全连接层计算加速 89
4.3.4 目标分类 90
4.3.5 边界框回归 91
4.3.6 训练过程 93
4.4 Faster R-CNN 96
4.4.1 网络结构 97
4.4.2 RPN 98
4.4.3 训练过程 104
4.5 R-FCN 106
4.5.1 R-FCN 网络结构 107
4.5.2 位置敏感的分数图 108
4.5.3 位置敏感的RoI 池化 109
4.5.4 R-FCN 损失函数 110
4.5.5 Caffe 网络模型解析 111
4.6 Mask R-CNN 115
4.6.1 实例分割简介 115
4.6.2 COCO 数据集的像素级标注 116
4.6.3 网络结构 117
4.6.4 U-Net 121
4.6.5 SegNet 122
4.7 小结 123
第5章 单阶段目标检测方法 124
5.1 SSD 124
5.1.1 default box 125
5.1.2 网络结构 125
5.1.3 Caffe 网络模型解析 126
5.1.4 训练过程 134
5.2 RetinaNet 136
5.2.1 FPN 136
5.2.2 聚焦损失函数 138
5.3 RefineDet 139
5.3.1 网络模型 140
5.3.2 Caffe 网络模型解析 142
5.3.3 训练过程 151
5.4 YOLO 152
5.4.1 YOLO v1 152
5.4.2 YOLO v2 155
5.4.3 YOLO v3 157
5.5 目标检测算法应用 159
5.5.1 高速公路坑洞检测 159
5.5.2 息肉检测 160
5.6 小结 162
应用篇
第6章 肋骨骨折检测 164
6.1 国内外研究现状 165
6.2 解决方案 166
6.3 预处理 166
6.4 肋骨骨折检测 167
6.5 实验结果分析 168
6.6 小结 170
参考文献 171
第7章 肺结节检测 172
7.1 国内外研究现状 172
7.1.1 肺结节可疑位置推荐算法 173
7.1.2 假阳性肺结节抑制算法 173
7.2 总体框架 174
7.2.1 肺结节数据集 174
7.2.2 肺结节检测难点 175
7.2.3 算法框架 175
7.3 肺结节可疑位置推荐算法 176
7.3.1 CT图像的预处理 177
7.3.2 肺结节分割算法 178
7.3.3 优化方法 180
7.3.4 推断方法 182
7.4 可疑肺结节定位算法 183
7.5 实验结果与分析 184
7.5.1 实验结果 184
7.5.2 改进点效果分析 184
7.6 假阳性肺结节抑制算法 186
7.6.1 假阳性肺结节抑制网络 186
7.6.2 优化策略 190
7.6.3 推断策略 192
7.7 实验结果与分析 192
7.7.1 实验结果 193
7.7.2 改进点效果分析 193
7.7.3 可疑位置推荐与假阳抑制算法整合 194
7.8 小结 195
参考文献 195
第8章 车道线检测 198
8.1 国内外研究现状 198
8.2 主要研究内容 200
8.2.1 总体解决方案 200
8.2.2 各阶段概述 201
8.3 车道线检测系统的设计与实现 204
8.3.1 车道线图像数据标注与筛选 205
8.3.2 车道线图片预处理 206
8.3.3 车道线分割模型训练 211
8.3.4 车道线检测 220
8.3.5 车道线检测结果 224
8.4 车道线检测系统的性能测试 224
8.4.1 车道线检测质量测试 224
8.4.2 车道线检测时间测试 226
8.5 小结 227
参考文献 227
第9章 交通视频分析 229
9.1 国内外研究现状 230
9.2 主要研究内容 231
9.2.1 总体设计 231
9.2.2 精度和性能要求 232
9.3 交通视频分析 232
9.3.1 车辆检测和车牌检测 233
9.3.2 车牌识别功能设计详解 235
9.3.3 车辆品牌及颜色的识别 243
9.3.4 目标跟踪设计详解 244
9.4 系统测试 247
9.4.1 车辆检测 248
9.4.2 车牌检测 251
9.4.3 车牌识别 253
9.4.4 车辆品牌识别 256
9.4.5 目标跟踪 259
9.5 小结 259
参考文献 260
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序
前言
第1章　打造深度学习工具箱1
1.1　TensorFlow1
1.1.1　安装1
1.1.2　使用举例3
1.2　TFLearn3
1.3　PaddlePaddle4
1.3.1　安装5
1.3.2　使用举例6
1.4　Karas7
1.5　本章小结9
第2章　卷积神经网络10
2.1　传统的图像分类算法10
2.2　基于CNN的图像分类算法11
2.2.1　局部连接11
2.2.2　参数共享13
2.2.3　池化15
2.2.4　典型的CNN结构及实现16
2.2.5　AlexNet的结构及实现19
2.2.6　VGG的结构及实现24
2.3　基于CNN的文本处理29
2.3.1　典型的CNN结构30
2.3.2　典型的CNN代码实现30
2.4　本章小结32
第3章　循环神经网络33
3.1　循环神经算法概述34
3.2　单向循环神经网络结构与实现36
3.3　双向循环神经网络结构与实现38
3.4　循环神经网络在序列分类的应用41
3.5　循环神经网络在序列生成的应用42
3.6　循环神经网络在序列标记的应用43
3.7　循环神经网络在序列翻译的应用44
3.8　本章小结46
第4章　基于OpenSOC的机器学习框架47
4.1　OpenSOC框架47
4.2　数据源系统48
4.3　数据收集层53
4.4　消息系统层57
4.5　实时处理层60
4.6　存储层62
4.6.1　HDFS62
4.6.2　HBase64
4.6.3　Elasticsearch65
4.7　分析处理层66
4.8　计算系统67
4.9　实战演练72
4.10　本章小结77
第5章　验证码识别78
5.1　数据集79
5.2　特征提取80
5.3　模型训练与验证81
5.3.1　K近邻算法81
5.3.2　支持向量机算法81
5.3.3　深度学习算法之MLP82
5.3.4　深度学习算法之CNN83
5.4　本章小结87
第6章　垃圾邮件识别88
6.1　数据集89
6.2　特征提取90
6.2.1　词袋模型90
6.2.2　TF-IDF模型93
6.2.3　词汇表模型95
6.3　模型训练与验证97
6.3.1　朴素贝叶斯算法97
6.3.2　支持向量机算法100
6.3.3　深度学习算法之MLP101
6.3.4　深度学习算法之CNN102
6.3.5　深度学习算法之RNN106
6.4　本章小结108
第7章　负面评论识别109
7.1　数据集110
7.2　特征提取112
7.2.1　词袋和TF-IDF模型112
7.2.2　词汇表模型114
7.2.3　Word2Vec模型和Doc2Vec模型115
7.3　模型训练与验证119
7.3.1　朴素贝叶斯算法119
7.3.2　支持向量机算法122
7.3.3　深度学习算法之MLP123
7.3.4　深度学习算法之CNN124
7.4　本章小结127
第8章　骚扰短信识别128
8.1　数据集129
8.2　特征提取130
8.2.1　词袋和TF-IDF模型130
8.2.2　词汇表模型131
8.2.3　Word2Vec模型和Doc2Vec模型132
8.3　模型训练与验证134
8.3.1　朴素贝叶斯算法134
8.3.2　支持向量机算法136
8.3.3　XGBoost算法137
8.3.4　深度学习算法之MLP140
8.4　本章小结141
第9章　Linux后门检测142
9.1　数据集142
9.2　特征提取144
9.3　模型训练与验证145
9.3.1　朴素贝叶斯算法145
9.3.2　XGBoost算法146
9.3.3　深度学习算法之多层感知机148
9.4　本章小结149
第10章　用户行为分析与恶意行为检测150
10.1　数据集151
10.2　特征提取152
10.2.1　词袋和TF-IDF模型152
10.2.2　词袋和N-Gram模型154
10.2.3　词汇表模型155
10.3　模型训练与验证156
10.3.1　朴素贝叶斯算法156
10.3.2　XGBoost算法157
10.3.3　隐式马尔可夫算法159
10.3.4　深度学习算法之MLP164
10.4　本章小结166
第11章　WebShell检测167
11.1　数据集168
11.1.1　WordPress168
11.1.2　PHPCMS170
11.1.3　phpMyAdmin170
11.1.4　Smarty171
11.1.5　Yii171
11.2　特征提取172
11.2.1　词袋和TF-IDF模型172
11.2.2　opcode和N-Gram模型174
11.2.3　opcode调用序列模型180
11.3　模型训练与验证181
11.3.1　朴素贝叶斯算法181
11.3.2　深度学习算法之MLP182
11.3.3　深度学习算法之CNN184
11.4　本章小结188
第12章　智能扫描器189
12.1　自动生成XSS攻击载荷190
12.1.1　数据集190
12.1.2　特征提取194
12.1.3　模型训练与验证195
12.2　自动识别登录界面198
12.2.1　数据集198
12.2.2　特征提取199
12.2.3　模型训练与验证201
12.3　本章小结203
第13章　DGA域名识别204
13.1　数据集206
13.2　特征提取207
13.2.1　N-Gram模型207
13.2.2　统计特征模型208
13.2.3　字符序列模型210
13.3　模型训练与验证210
13.3.1　朴素贝叶斯算法210
13.3.2　XGBoost算法212
13.3.3　深度学习算法之多层感知机215
13.3.4　深度学习算法之RNN218
13.4　本章小结221
第14章　恶意程序分类识别222
14.1　数据集223
14.2　特征提取226
14.3　模型训练与验证228
14.3.1　支持向量机算法228
14.3.2　XGBoost算法229
14.3.3　深度学习算法之多层感知机230
14.4　本章小结231
第15章　反信用卡欺诈232
15.1　数据集232
15.2　特征提取234
15.2.1　标准化234
15.2.2　标准化和降采样234
15.2.3　标准化和过采样236
15.3　模型训练与验证239
15.3.1　朴素贝叶斯算法239
15.3.2　XGBoost算法243
15.3.3　深度学习算法之多层感知机247
15.4　本章小结251
・ ・ ・ ・ ・ ・ (收起)第1章　编程和数学基础 1
1.1　Python快速入门 1
1.1.1　快速安装Python 1
1.1.2　Python基础 2
1.1.3　Python中的常见运算 5
1.1.4　Python控制语句 7
1.1.5　Python常用容器类型 10
1.1.6　Python常用函数 16
1.1.7　类和对象 22
1.1.8　Matplotlib入门 24
1.2　张量库NumPy 33
1.2.1　什么是张量 33
1.2.2　创建ndarray对象 37
1.2.3　ndarray数组的索引和切片 53
1.2.4　张量的计算 57
1.3　微积分 63
1.3.1　函数 64
1.3.2　四则运算和复合运算 66
1.3.3　极限和导数 69
1.3.4　导数的四则运算和链式法则 72
1.3.5　计算图、正向计算和反向传播求导 74
1.3.6　多变量函数的偏导数与梯度 75
1.3.7　向量值函数的导数与Jacobian矩阵 78
1.3.8　积分 83
1.4　概率基础 84
1.4.1　概率 84
1.4.2　条件概率、联合概率、全概率公式、贝叶斯公式 86
1.4.3　随机变量 88
1.4.4　离散型随机变量的概率分布 89
1.4.5　连续型随机变量的概率密度 91
1.4.6　随机变量的分布函数 93
1.4.7　期望、方差、协方差、协变矩阵 95
第2章　梯度下降法 99
2.1　函数极值的必要条件 99
2.2　梯度下降法基础 101
2.3　梯度下降法的参数优化策略 108
2.3.1　Momentum法 108
2.3.2　AdaGrad法 110
2.3.3　AdaDelta法 112
2.3.4　RMSprop法 114
2.3.5　Adam法 115
2.4　梯度验证 117
2.4.1　比较数值梯度和分析梯度 117
2.4.2　通用的数值梯度 118
2.5　分离梯度下降法与参数优化策略 119
2.5.1　参数优化器 119
2.5.2　接受参数优化器的梯度下降法 120
第3章　线性回归、逻辑回归和softmax回归 122
3.1　线性回归 122
3.1.1　餐车利润问题 122
3.1.2　机器学习与人工智能 123
3.1.3　什么是线性回归 126
3.1.4　用正规方程法求解线性回归问题 127
3.1.5　用梯度下降法求解线性回归问题 129
3.1.6　调试学习率 133
3.1.7　梯度验证 135
3.1.8　预测 135
3.1.9　多特征线性回归 136
3.2　数据的规范化 143
3.2.1　预测大坝出水量 143
3.2.2　数据的规范化过程 147
3.3　模型的评估 149
3.3.1　欠拟合和过拟合 149
3.3.2　验证集和测试集 153
3.3.3　学习曲线 155
3.3.4　偏差和方差 160
3.4　正则化 165
3.5　逻辑回归 168
3.5.1　逻辑回归基础 169
3.5.2　逻辑回归的NumPy实现 173
3.5.3　实战：鸢尾花分类的NumPy实现 178
3.6　softmax回归 180
3.6.1　spiral数据集 180
3.6.2　softmax函数 181
3.6.3　softmax回归模型 186
3.6.4　多分类交叉熵损失 188
3.6.5　通过加权和计算交叉熵损失 191
3.6.6　softmax回归的梯度计算 191
3.6.7　softmax回归的梯度下降法的实现 197
3.6.8　spiral数据集的softmax回归模型 197
3.7　批梯度下降法和随机梯度下降法 199
3.7.1　MNIST手写数字集 199
3.7.2　用部分训练样本训练逻辑回归模型 201
3.7.3　批梯度下降法 202
3.7.4　随机梯度下降法 207
第4章　神经网络 209
4.1　神经网络概述 209
4.1.1　感知机和神经元 209
4.1.2　激活函数 213
4.1.3　神经网络与深度学习 216
4.1.4　多个样本的正向计算 221
4.1.5　输出 224
4.1.6　损失函数 224
4.1.7　基于数值梯度的神经网络训练 229
4.2　反向求导 235
4.2.1　正向计算和反向求导 235
4.2.2　计算图 237
4.2.3　损失函数关于输出的梯度 239
4.2.4　2层神经网络的反向求导 242
4.2.5　2层神经网络的Python实现 247
4.2.6　任意层神经网络的反向求导 252
4.3　实现一个简单的深度学习框架 256
4.3.1　神经网络的训练过程 256
4.3.2　网络层的代码实现 257
4.3.3　网络层的梯度检验 260
4.3.4　神经网络的类 261
4.3.5　神经网络的梯度检验 263
4.3.6　基于深度学习框架的MNIST手写数字识别 266
4.3.7　改进的通用神经网络框架：分离加权和与激活函数 268
4.3.8　独立的参数优化器 276
4.3.9　fashion-mnist的分类训练 279
4.3.10　读写模型参数 282
第5章　改进神经网络性能的基本技巧 285
5.1　数据处理 285
5.1.1　数据增强 285
5.1.2　规范化 289
5.1.3　特征工程 289
5.2　参数调试 296
5.2.1　权重初始化 296
5.2.2　优化参数 301
5.3　批规范化 301
5.3.1　什么是批规范化 301
5.3.2　批规范化的反向求导 303
5.3.3　批规范化的代码实现 304
5.4　正则化 310
5.4.1　权重正则化 310
5.4.2　Dropout 312
5.4.3　早停法 316
5.5　梯度爆炸和梯度消失 317
第6章　卷积神经网络 318
6.1　卷积入门 319
6.1.1　什么是卷积 319
6.1.2　一维卷积 325
6.1.3　二维卷积 326
6.1.4　多通道输入和多通道输出 338
6.1.5　池化 341
6.2　卷积神经网络概述 344
6.2.1　全连接神经元和卷积神经元 345
6.2.2　卷积层和卷积神经网络 346
6.2.3　卷积层和池化层的反向求导及代码实现 349
6.2.4　卷积神经网络的代码实现 361
6.3　卷积的矩阵乘法 364
6.3.1　一维卷积的矩阵乘法 364
6.3.2　二维卷积的矩阵乘法 365
6.3.3　一维卷积反向求导的矩阵乘法 371
6.3.4　二维卷积反向求导的矩阵乘法 373
6.4　基于坐标索引的快速卷积 377
6.5　典型卷积神经网络结构 393
6.5.1　LeNet-5 393
6.5.2　AlexNet 394
6.5.3　VGG 395
6.5.4　残差网络 396
6.5.5　Inception网络 398
6.5.6　NiN 399
第7章　循环神经网络 403
7.1　序列问题和模型 403
7.1.1　股票价格预测问题 404
7.1.2　概率序列模型和语言模型 405
7.1.3　自回归模型 406
7.1.4　生成自回归数据 406
7.1.5　时间窗方法 408
7.1.6　时间窗采样 409
7.1.7　时间窗方法的建模和训练 409
7.1.8　长期预测和短期预测 410
7.1.9　股票价格预测的代码实现 412
7.1.10　k-gram语言模型 415
7.2　循环神经网络基础 416
7.2.1　无记忆功能的非循环神经网络 417
7.2.2　具有记忆功能的循环神经网络 418
7.3　穿过时间的反向传播 421
7.4　单层循环神经网络的实现 425
7.4.1　初始化模型参数 425
7.4.2　正向计算 425
7.4.3　损失函数 427
7.4.4　反向求导 427
7.4.5　梯度验证 429
7.4.6　梯度下降训练 432
7.4.7　序列数据的采样 433
7.4.8　序列数据的循环神经网络训练和预测 441
7.5　循环神经网络语言模型和文本的生成 448
7.5.1　字符表 448
7.5.2　字符序列样本的采样 450
7.5.3　模型的训练和预测 452
7.6　循环神经网络中的梯度爆炸和梯度消失 455
7.7　长短期记忆网络 456
7.7.1　LSTM的神经元 457
7.7.2　LSTM的反向求导 460
7.7.3　LSTM的代码实现 461
7.7.4　LSTM的变种 469
7.8　门控循环单元 470
7.8.1　门控循环单元的工作原理 470
7.8.2　门控循环单元的代码实现 472
7.9　循环神经网络的类及其实现 475
7.9.1　用类实现循环神经网络 475
7.9.2　循环神经网络单元的类实现 483
7.10　多层循环神经网络和双向循环神经网络 491
7.10.1　多层循环神经网络 491
7.10.2　多层循环神经网络的训练和预测 497
7.10.3　双向循环神经网络 500
7.11　Seq2Seq模型 506
7.11.1　机器翻译概述 507
7.11.2　Seq2Seq模型的实现 508
7.11.3　字符级的Seq2Seq模型 516
7.11.4　基于Word2Vec的Seq2Seq模型 522
7.11.5　基于词嵌入层的Seq2Seq模型 533
7.11.6　注意力机制 541
第8章　生成模型 552
8.1　生成模型概述 552
8.2　自动编码器 556
8.2.1　什么是自动编码器 557
8.2.2　稀疏编码器 559
8.2.3　自动编码器的代码实现 560
8.3　变分自动编码器 563
8.3.1　什么是变分自动编码器 563
8.3.2　变分自动编码器的损失函数 564
8.3.3　变分自动编码器的参数重采样 565
8.3.4　变分自动编码器的反向求导 565
8.3.5　变分自动编码器的代码实现 566
8.4　生成对抗网络 571
8.4.1　生成对抗网络的原理 573
8.4.2　生成对抗网络训练过程的代码实现 577
8.5　生成对抗网络建模实例 579
8.5.1　一组实数的生成对抗网络建模 579
8.5.2　二维坐标点的生成对抗网络建模 585
8.5.3　MNIST手写数字集的生成对抗网络建模 590
8.5.4　生成对抗网络的训练技巧 594
8.6　生成对抗网络的损失函数及其概率解释 594
8.6.1　生成对抗网络的损失函数的全局最优解 594
8.6.2　Kullback-Leibler散度和Jensen-Shannon散度 595
8.6.3　生成对抗网络的最大似然解释 598
8.7　改进的损失函数――Wasserstein GAN 599
8.7.1　Wasserstein GAN的原理 599
8.7.2　Wasserstein GAN的代码实现 603
8.8　深度卷积对抗网络 605
8.8.1　一维转置卷积 606
8.8.2　二维转置卷积 609
8.8.3　卷积对抗网络的代码实现 612
参考文献 617
・ ・ ・ ・ ・ ・ (收起)基础篇
第1章 深度学习概述 2
1.1 深度学习发展简史 2
1.2 有监督学习 4
1.2.1 图像分类 4
1.2.2 目标检测 6
1.2.3 人脸识别 10
1.2.4 语音识别 13
1.3 无监督学习 18
1.3.1 无监督学习概述 18
1.3.2 生成对抗网络 18
1.4 强化学习 21
1.4.1 AlphaGo 21
1.4.2 AlphaGo Zero 23
1.5 小结 25
参考文献 25
第2章 深度神经网络 27
2.1 神经元 27
2.2 感知机 30
2.3 前向传递 32
2.3.1 前向传递的流程 32
2.3.2 激活函数 33
2.3.3 损失函数 37
2.4 后向传递 40
2.4.1 后向传递的流程 40
2.4.2 梯度下降 40
2.4.3 参数修正 42
2.5 防止过拟合 44
2.5.1 dropout 44
2.5.2 正则化 45
2.6 小结 46
第3章 卷积神经网络 47
3.1 卷积层 48
3.1.1 valid 卷积 48
3.1.2 full 卷积 50
3.1.3 same 卷积 51
3.2 池化层 52
3.3 反卷积 53
3.4 感受野 55
3.5 卷积网络实例 56
3.5.1 Lenet-5 56
3.5.2 AlexNet 59
3.5.3 VGGNet 62
3.5.4 GoogLeNet 64
3.5.5 ResNet 72
3.5.6 MobileNet 73
3.6 小结 76
进阶篇
第4章 两阶段目标检测方法 78
4.1 R-CNN 78
4.1.1 算法流程 79
4.1.2 训练过程 80
4.2 SPP-Net 83
4.2.1 网络结构 84
4.2.2 空间金字塔池化 84
4.3 Fast R-CNN 86
4.3.1 感兴趣区域池化层 86
4.3.2 网络结构 88
4.3.3 全连接层计算加速 89
4.3.4 目标分类 90
4.3.5 边界框回归 91
4.3.6 训练过程 93
4.4 Faster R-CNN 96
4.4.1 网络结构 97
4.4.2 RPN 98
4.4.3 训练过程 104
4.5 R-FCN 106
4.5.1 R-FCN 网络结构 107
4.5.2 位置敏感的分数图 108
4.5.3 位置敏感的RoI 池化 109
4.5.4 R-FCN 损失函数 110
4.5.5 Caffe 网络模型解析 111
4.6 Mask R-CNN 115
4.6.1 实例分割简介 115
4.6.2 COCO 数据集的像素级标注 116
4.6.3 网络结构 117
4.6.4 U-Net 121
4.6.5 SegNet 122
4.7 小结 123
第5章 单阶段目标检测方法 124
5.1 SSD 124
5.1.1 default box 125
5.1.2 网络结构 125
5.1.3 Caffe 网络模型解析 126
5.1.4 训练过程 134
5.2 RetinaNet 136
5.2.1 FPN 136
5.2.2 聚焦损失函数 138
5.3 RefineDet 139
5.3.1 网络模型 140
5.3.2 Caffe 网络模型解析 142
5.3.3 训练过程 151
5.4 YOLO 152
5.4.1 YOLO v1 152
5.4.2 YOLO v2 155
5.4.3 YOLO v3 157
5.5 目标检测算法应用 159
5.5.1 高速公路坑洞检测 159
5.5.2 息肉检测 160
5.6 小结 162
应用篇
第6章 肋骨骨折检测 164
6.1 国内外研究现状 165
6.2 解决方案 166
6.3 预处理 166
6.4 肋骨骨折检测 167
6.5 实验结果分析 168
6.6 小结 170
参考文献 171
第7章 肺结节检测 172
7.1 国内外研究现状 172
7.1.1 肺结节可疑位置推荐算法 173
7.1.2 假阳性肺结节抑制算法 173
7.2 总体框架 174
7.2.1 肺结节数据集 174
7.2.2 肺结节检测难点 175
7.2.3 算法框架 175
7.3 肺结节可疑位置推荐算法 176
7.3.1 CT图像的预处理 177
7.3.2 肺结节分割算法 178
7.3.3 优化方法 180
7.3.4 推断方法 182
7.4 可疑肺结节定位算法 183
7.5 实验结果与分析 184
7.5.1 实验结果 184
7.5.2 改进点效果分析 184
7.6 假阳性肺结节抑制算法 186
7.6.1 假阳性肺结节抑制网络 186
7.6.2 优化策略 190
7.6.3 推断策略 192
7.7 实验结果与分析 192
7.7.1 实验结果 193
7.7.2 改进点效果分析 193
7.7.3 可疑位置推荐与假阳抑制算法整合 194
7.8 小结 195
参考文献 195
第8章 车道线检测 198
8.1 国内外研究现状 198
8.2 主要研究内容 200
8.2.1 总体解决方案 200
8.2.2 各阶段概述 201
8.3 车道线检测系统的设计与实现 204
8.3.1 车道线图像数据标注与筛选 205
8.3.2 车道线图片预处理 206
8.3.3 车道线分割模型训练 211
8.3.4 车道线检测 220
8.3.5 车道线检测结果 224
8.4 车道线检测系统的性能测试 224
8.4.1 车道线检测质量测试 224
8.4.2 车道线检测时间测试 226
8.5 小结 227
参考文献 227
第9章 交通视频分析 229
9.1 国内外研究现状 230
9.2 主要研究内容 231
9.2.1 总体设计 231
9.2.2 精度和性能要求 232
9.3 交通视频分析 232
9.3.1 车辆检测和车牌检测 233
9.3.2 车牌识别功能设计详解 235
9.3.3 车辆品牌及颜色的识别 243
9.3.4 目标跟踪设计详解 244
9.4 系统测试 247
9.4.1 车辆检测 248
9.4.2 车牌检测 251
9.4.3 车牌识别 253
9.4.4 车辆品牌识别 256
9.4.5 目标跟踪 259
9.5 小结 259
参考文献 260
・ ・ ・ ・ ・ ・ (收起)序言
前言
如何使用本书 ――写在第十次印刷之际
主要符号表
第1章 绪论
1.1 引言
1.2 基本术语
1.3 假设空间
1.4 归纳偏好
1.5 发展历程
1.6 应用现状
1.7 阅读材料
习题
参考文献
休息一会儿
第2章 模型评估与选择
2.1 经验误差与过拟合
2.2 评估方法
2.3 性能度量
2.4 比较检验
2.5 偏差与方差
2.6 阅读材料
习题
参考文献
休息一会儿
第3章 线性模型
3.1 基本形式
3.2 线性回归
3.3 对数几率回归
3.4 线性判别分析
3.5 多分类学习
3.6 类别不平衡问题
3.7 阅读材料
习题
参考文献
休息一会儿
第4章 决策树
4.1 基本流程
4.2 划分选择
4.3 剪枝处理
4.4 连续与缺失值
4.5 多变量决策树
4.6 阅读材料
习题
参考文献
休息一会儿
第5章 神经网络
5.1 神经元模型
5.2 感知机与多层网络
5.3 误差逆传播算法
5.4 全局最小与局部极小
5.5 其他常见神经网络
5.6 深度学习
5.7 阅读材料
习题
参考文献
休息一会儿
第6章 支持向量机
6.1 间隔与支持向量
6.2 对偶问题
6.3 核函数
6.4 软间隔与正则化
6.5 支持向量回归
6.6 核方法
6.7 阅读材料
习题
参考文献
休息一会儿
第7章 贝叶斯分类器
7.1 贝叶斯决策论
7.2 极大似然估计
7.3 朴素贝叶斯分类器
7.4 半朴素贝叶斯分类器
7.5 贝叶斯网
7.6 EM算法
7.7 阅读材料
习题
参考文献
休息一会儿
第8章 集成学习
8.1 个体与集成
8.2 Boosting
8.3 Bagging与随机森林
8.4 结合策略
8.5 多样性
8.6 阅读材料
习题
参考文献
休息一会儿
第9章 聚类
9.1 聚类任务
9.2 性能度量
9.3 距离计算
9.4 原型聚类
9.5 密度聚类
9.6 层次聚类
9.7 阅读材料
习题
参考文献
休息一会儿
第10章 降维与度量学习
10.1 k近邻学习
10.2 低维嵌入
10.3 主成分分析
10.4 归纳偏好
10.5 流形学习
10.6 度量学习
10.7 阅读材料
习题
参考文献
休息一会儿
第11章 特征选择与稀疏学习
11.1 子集搜索与评价
11.2 过滤式选择
11.3包裹式选择
11.4 嵌入式选择与L1正则化
11.5 稀疏表示与字典学习
11.6 压缩感知
11.7 阅读材料
习题
参考文献
休息一会儿
第12章 计算学习理论
12.1 基础知识
12.2 PAC学习
12.3 有限假设空间
12.4 VC维
12.5 Rademacher复杂度
12.6 稳定性
12.7 阅读材料
习题
参考文献
休息一会儿
第13章 半监督学习
13.1 未标记样本
13.2 生成式方法
13.3 半监督SVM
13.4 图半监督学习
13.5 基于分歧的方法
13.6 半监督聚类
13.7 阅读材料
习题
参考文献
休息一会儿
第14章 概率图模型
14.1 隐马尔可夫模型
14.2 马尔可夫随机场
14.3 条件随机场
14.4学习与推断
14.5 近似推断
14.6 话题模型
14.7 阅读材料
习题
参考文献
休息一会儿
第15章 规则学习
15.1 基本概念
15.2 序贯覆盖
15.3 剪枝优化
15.4 一阶规则学习
15.5 归纳逻辑程序设计
15.6 阅读材料
习题
参考文献
休息一会儿
第16章 强化学习
16.1 任务与奖赏
16.2 K-摇臂赌博机
16.3 有模型学习
16.4 免模型学习
16.5 值函数近似
16.6 模仿学习
16.7 阅读材料
习题
参考文献
休息一会儿
附录
A 矩阵
B 优化
C 概率分布
后记
・ ・ ・ ・ ・ ・ (收起)前言 1
第一部分 机器学习的基础知识 11
第 1 章 机器学习概览
13 1.1 什么是机器学习 14
1.2 为什么使用机器学习 14
1.3 机器学习的应用示例 16
1.4 机器学习系统的类型 18
1.5 机器学习的主要挑战 32
1.6 测试与验证 38
1.7 练习题 40
第 2 章 端到端的机器学习项目 42
2.1 使用真实数据
42 2.2 观察大局 44
2.3 获取数据 48
2.4 从数据探索和可视化中获得洞见 60
2.5 机器学习算法的数据准备 66
2.6 选择和训练模型 74
2.7 微调模型 77
2.8 启动、监控和维护你的系统 .82
2.9 试试看 84
2.10 练习题 84
第 3 章 分类 86
3.1 MNIST 86
3.2 训练二元分类器 88
3.3 性能测量 89
3.4 多类分类器 99
3.5 误差分析 101
3.6 多标签分类 104
3.7 多输出分类 105
3.8 练习题 107
第 4 章 训练模型 108
4.1 线性回归 109
4.2 梯度下降 113
4.3 多项式回归 122
4.4 学习曲线 124
4.5 正则化线性模型 127
4.6 逻辑回归 134
4.7 练习题 141
第 5 章 支持向量机 143
5.1 线性 SVM 分类 143
5.2 非线性 SVM 分类 146
5.3 SVM 回归 151
5.4 工作原理 152
5.5 练习题 160
第 6 章 决策树 162
6.1 训练和可视化决策树 162
6.2 做出预测 163
6.3 估计类概率 165
6.4 CART 训练算法 166
6.5 计算复杂度 166
6.6 基尼不纯度或熵 167
6.7 正则化超参数 167
6.8 回归 168
6.9 不稳定性 170
6.10 练习题 172
第 7 章 集成学习和随机森林 173
7.1 投票分类器 173
7.2 bagging 和 pasting 176
7.3 随机补丁和随机子空间 179
7.4 随机森林 180
7.5 提升法 182
7.6 堆叠法 190
7.7 练习题 192
第 8 章 降维 193
8.1 维度的诅咒 194
8.2 降维的主要方法 195
8.3 PCA 198
8.4 内核 PCA . 204
8.5 LLE 206
8.6 其他降维技术 208
8.7 练习题 209
第 9 章 无监督学习技术 211
9.1 聚类 212
9.2 高斯混合模型 232
9.3 练习题 245
第二部分 神经网络与深度学习 247
第 10 章 Keras 人工神经网络简介 249
10.1 从生物神经元到人工神经元 250
10.2 使用 Keras 实现 MLP 262
10.3 微调神经网络超参数 284
10.4 练习题 290
第 11 章 训练深度神经网络 293
11.1 梯度消失与梯度爆炸问题 293
11.2 重用预训练层 305
11.3 更快的优化器 310
11.4 通过正则化避免过拟合 321
11.5 总结和实用指南 327
11.6 练习题 329
第 12 章 使用 TensorFlow 自定义模型和训练 330
12.1 TensorFlow 快速浏览 330
12.2 像 NumPy 一样使用 TensorFlow 333
12.3 定制模型和训练算法 338
12.4 TensorFlow 函数和图 356
12.5 练习题 360
第 13 章 使用 TensorFlow 加载和预处理数据 362
13.1 数据 API 363
13.2 TFRecord 格式 372
13.3 预处理输入特征 377
13.4 TF Transform 385
13.5 TensorFlow 数据集项目 386
13.6 练习题 388
第 14 章 使用卷积神经网络的深度计算机视觉 390
14.1 视觉皮层的架构 390
14.2 卷积层 392
14.3 池化层 399
14.4 CNN 架构 402
14.5 使用 Keras 实现 ResNet-34 CNN 416
14.6 使用 Keras 的预训练模型 417
14.7 迁移学习的预训练模型 418
14.8 分类和定位 421
14.9 物体检测 422
14.10 语义分割 428
14.11 练习题 431
第 15 章 使用 RNN 和 CNN 处理序列 432
15.1 循环神经元和层 432
15.2 训练 RNN 436
15.3 预测时间序列 437
15.4 处理长序列 444
15.5 练习题 453
第 16 章 使用 RNN 和注意力机制进行自然语言处理 455
16.1 使用字符 RNN 生成莎士比亚文本 456
16.2 情感分析 464
16.3 神经机器翻译的编码器 - 解码器网络 470
16.4 注意力机制 476
16.5 最近语言模型的创新 486
16.6 练习题 ... 488
第 17 章 使用自动编码器和 GAN 的表征学习和生成学习 489
17.1 有效的数据表征 490
17.2 使用不完整的线性自动编码器执行 PCA 491
17.3 堆叠式自动编码器 493
17.4 卷积自动编码器 499
17.5 循环自动编码器 500
17.6 去噪自动编码器 501
17.7 稀疏自动编码器 502
17.8 变分自动编码器 505
17.9 生成式对抗网络 510
17.10 练习题 522
第 18 章 强化学习 523
18.1 学习优化奖励 524
18.2 策略搜索 525
18.3 OpenAI Gym 介绍 526
18.4 神经网络策略 529
18.5 评估动作：信用分配问题 531
18.6 策略梯度 532
18.7 马尔可夫决策过程 536
18.8 时序差分学习 540
18.9 Q 学习 540
18.10 实现深度 Q 学习 544
18.11 深度 Q 学习的变体 547
18.12 TF-Agents 库 550
18.13 一些流行的 RL 算法概述 568
18.14 练习题 569
第 19 章 大规模训练和部署TensorFlow 模型 571
19.1 为 TensorFlow 模型提供服务 572
19.2 将模型部署到移动端或嵌入式设备 586
19.3 使用 GPU 加速计算 589
19.4 跨多个设备的训练模型 600
19.5 练习题 613
19.6 致谢 613
附录 A 课后练习题解答 .....614
附录 B 机器学习项目清单 ..642
附录 C SVM 对偶问题 647
附录 D 自动微分 .650
附录 E 其他流行的人工神经网络架构 ...656
附录 F 特殊数据结构..663
附录 G TensorFlow 图 ......669
・ ・ ・ ・ ・ ・ (收起)引子：AI 菜鸟的挑战―100 天上线智能预警系统
第壹 课 机器学习快速上手路径―唯有实战
1．1 机器学习的家族谱
1．1．1 新手入门机器学习的3 个好消息
1．1．2 机器学习*是从数据中发现规律
1．1．3 机器学习的类别―监督学习及其他
1．1．4 机器学习的重要分支―深度学习
1．1．5 机器学习新热点―强化学习
1．1．6 机器学习的两大应用场景―回归与分类
1．1．7 机器学习的其他应用场景
1．2 快捷的云实战学习模式
1．2．1 在线学习平台上的机器学习课程
1．2．2 用Jupyter Notebook 直接实战
1．2．3 用Google Colab 开发第壹个机器学习程序
1．2．4 在Kaggle 上参与机器学习竞赛
1．2．5 在本机上“玩”机器学习
1．3 基本机器学习术语
1．3．1 特征
1．3．2 标签
1．3．3 模型
1．4 Python 和机器学习框架
1．4．1 为什么选择用Python
1．4．2 机器学习和深度学习框架
1．5 机器学习项目实战架构
1．5．1 第壹 个环节：问题定义
1．5．2 第2 个环节：数据的收集和预处理
1．5．3 第3 个环节：选择机器学习模型
1．5．4 第4 个环节：训练机器，确定参数
1．5．5 第5 个环节：超参数调试和性能优化
1．6 本课内容小结
1．7 课后练习
第2 课 数学和Python 基础知识―*天搞定
2．1 函数描述了事物间的关系
2．1．1 什么是函数
2．1．2 机器学习中的函数
2．2 捕捉函数的变化趋势
2．2．1 连续性是求导的前提条件
2．2．2 通过求导发现y 如何随x 而变
2．2．3 凸函数有一个全局*低点
2．3 梯度下降是机器学习的动力之源
2．3．1 什么是梯度
2．3．2 梯度下降：下山的隐喻
2．3．3 梯度下降有什么用
2．4 机器学习的数据结构―张量
2．4．1 张量的轴、阶和形状
2．4．2 标量―0D（阶）张量
2．4．3 向量―1D（阶）张量
2．4．4 矩阵―2D（阶）张量
2．4．5 序列数据 ―3D（阶）张量
2．4．6 图像数据 ―4D（阶）张量
2．4．7 视频数据―5D（阶）张量
2．4．8 数据的维度和空间的维度
2．5 Python 的张量运算
2．5．1 机器学习中张量的创建
2．5．2 通过索引和切片访问张量中的数据
2．5．3 张量的整体操作和逐元素运算
2．5．4 张量的变形和转置
2．5．5 Python 中的广播
2．5．6 向量和矩阵的点积运算
2．6 机器学习的几何意义
2．6．1 机器学习的向量空间
2．6．2 深度学习和数据流形
2．7 概率与统计研究了随机事件的规律
2．7．1 什么是概率
2．7．2 正态分布
2．7．3 标准差和方差
2．8 本课内容小结
2．9 课后练习
第3 课 线性回归―预测网店的销售额
3．1 问题定义：小冰的网店广告该如何投放
3．2 数据的收集和预处理
3．2．1 收集网店销售额数据
3．2．2 数据读取和可视化
3．2．3 数据的相关分析
3．2．4 数据的散点图
3．2．5 数据集清洗和规范化
3．2．6 拆分数据集为训练集和测试集
3．2．7 把数据归一化
3．3 选择机器学习模型
3．3．1 确定线性回归模型
3．3．2 假设（预测）函数―h （x ）
3．3．3 损失（误差）函数―L （w ，b ）
3．4 通过梯度下降找到*佳参数
3．4．1 训练机器要有正确的方向
3．4．2 凸函数确保有*小损失点
3．4．3 梯度下降的实现
3．4．4 学习速率也很重要
3．5 实现一元线性回归模型并调试超参数
3．5．1 权重和偏置的初始值
3．5．2 进行梯度下降
3．5．3 调试学习速率
3．5．4 调试迭代次数
3．5．5 在测试集上进行预测
3．5．6 用轮廓图描绘L 、w 和b 的关系
3．6 实现多元线性回归模型
3．6．1 向量化的点积运算
3．6．2 多变量的损失函数和梯度下降
3．6．3 构建一个线性回归函数模型
3．6．4 初始化权重并训练机器
3．7 本课内容小结
3．8 课后练习
第4 课 逻辑回归―给病患和鸢尾花分类
4．1 问题定义：判断客户是否患病
4．2 从回归问题到分类问题
4．2．1 机器学习中的分类问题
4．2．2 用线性回归+ 阶跃函数完成分类
4．2．3 通过Sigmiod 函数进行转换
4．2．4 逻辑回归的假设函数
4．2．5 逻辑回归的损失函数
4．2．6 逻辑回归的梯度下降
4．3 通过逻辑回归解决二元分类问题
4．3．1 数据的准备与分析
4．3．2 建立逻辑回归模型
4．3．3 开始训练机器
4．3．4 测试分类结果
4．3．5 绘制损失曲线
4．3．6 直接调用Sklearn 库
4．3．7 哑特征的使用
4．4 问题定义：确定鸢尾花的种类
4．5 从二元分类到多元分类
4．5．1 以一对多
4．5．2 多元分类的损失函数
4．6 正则化、欠拟合和过拟合
4．6．1 正则化
4．6．2 欠拟合和过拟合
4．6．3 正则化参数
4．7 通过逻辑回归解决多元分类问题
4．7．1 数据的准备与分析
4．7．2 通过Sklearn 实现逻辑回归的多元分类
4．7．3 正则化参数―C 值的选择
4．8 本课内容小结
4．9 课后练习
第5 课 深度神经网络―找出可能流失的客户
5．1 问题定义：咖哥接手的金融项目
5．2 神经网络的原理
5．2．1 神经网络极简史
5．2．2 传统机器学习算法的局限性
5．2．3 神经网络的优势
5．3 从感知器到单隐层网络
5．3．1 感知器是*基本的神经元
5．3．2 假设空间要能覆盖特征空间
5．3．3 单神经元特征空间的局限性
5．3．4 分层：加入一个网络隐层
5．4 用Keras 单隐层网络预测客户流失率
5．4．1 数据的准备与分析
5．4．2 先尝试逻辑回归算法
5．4．3 单隐层神经网络的Keras 实现
5．4．4 训练单隐层神经网络
5．4．5 训练过程的图形化显示
5．5 分类数据不平衡问题：只看准确率够用吗
5．5．1 混淆矩阵、精que率、召回率和F1 分数
5．5．2 使用分类报告和混淆矩阵
5．5．3 特征缩放的魔力
5．5．4 阈值调整、欠采样和过采样
5．6 从单隐层神经网络到深度神经网络
5．6．1 梯度下降：正向传播和反向传播
5．6．2 深度神经网络中的一些可调超参数
5．6．3 梯度下降优化器
5．6．4 激活函数：从Sigmoid 到ReLU
5．6．5 损失函数的选择
5．6．6 评估指标的选择
5．7 用Keras 深度神经网络预测客户流失率
5．7．1 构建深度神经网络
5．7．2 换一换优化器试试
5．7．3 神经网络正则化：添加Dropout 层
5．8 深度神经网络的调试及性能优化
5．8．1 使用回调功能
5．8．2 使用TensorBoard
5．8．3 神经网络中的过拟合
5．8．4 梯度消失和梯度bao炸
5．9 本课内容小结
5．10 课后练习
第6课 卷积神经网络―识别狗狗的图像
6．1 问题定义：有趣的狗狗图像识别
6．2 卷积网络的结构
6．3 卷积层的原理
6．3．1 机器通过“模式”进行图像识别
6．3．2 平移不变的模式识别
6．3．3 用滑动窗口抽取局部特征
6．3．4 过滤器和响应通道
6．3．5 对特征图进行卷积运算
6．3．6 模式层级结构的形成
6．3．7 卷积过程中的填充和步幅
6．4 池化层的功能
6．5 用卷积网络给狗狗图像分类
6．5．1 图像数据的读入
6．5．2 构建简单的卷积网络
6．5．3 训练网络并显示误差和准确率
6．6 卷积网络性能优化
6．6．1 第壹招：更新优化器并设置学习速率
6．6．2 第2招：添加Dropout 层
6．6．3 “大杀器”：进行数据增强
6．7 卷积网络中特征通道的可视化
6．8 各种大型卷积网络模型
6．8．1 经典的VGGNet
6．8．2 采用Inception 结构的GoogLeNet
6．8．3 残差网络ResNet
6．9 本课内容小结
6．10 课后练习
第7 课 循环神经网络―鉴定留言及探索系外行星
7．1 问题定义：鉴定评论文本的情感属性
7．2 循环神经网络的原理和结构
7．2．1 什么是序列数据
7．2．2 前馈神经网络处理序列数据的局限性
7．2．3 循环神经网络处理序列问题的策略
7．2．4 循环神经网络的结构
7．3 原始文本如何转化成向量数据
7．3．1 文本的向量化：分词
7．3．2 通过One-hot 编码分词
7．3．3 词嵌入
7．4 用SimpleRNN 鉴定评论文本
7．4．1 用Tokenizer 给文本分词
7．4．2 构建包含词嵌入的SimpleRNN
7．4．3 训练网络并查看验证准确率
7．5 从SimpleRNN 到LSTM
7．5．1 SimpleRNN 的局限性
7．5．2 LSTM 网络的记忆传送带
7．6 用LSTM 鉴定评论文本
7．7 问题定义：太阳系外哪些恒星有行星环绕
7．8 用循环神经网络处理时序问题
7．8．1 时序数据的导入与处理
7．8．2 建模：CNN 和RNN 的组合
7．8．3 输出阈值的调整
7．8．4 使用函数式API
7．9 本课内容小结
7．10 课后练习
第8 课 经典算法“宝刀未老”
8．1 K *近邻
8．2 支持向量机
8．3 朴素贝叶斯
8．4 决策树
8．4．1 熵和特征节点的选择
8．4．2 决策树的深度和剪枝
8．5 随机森林
8．6 如何选择*佳机器学习算法
8．7 用网格搜索超参数调优
8．8 本课内容小结
8．9 课后练习
第9 课 集成学习“笑傲江湖”
9．1 偏差和方差―机器学习性能优化的风向标
9．1．1 目标：降低偏差与方差
9．1．2 数据集大小对偏差和方差的影响
9．1．3 预测空间的变化带来偏差和方差的变化
9．2 Bagging 算法―多个基模型的聚合
9．2．1 决策树的聚合
9．2．2 从树的聚合到随机森林
9．2．3 从随机森林到ji端随机森林
9．2．4 比较决策树、树的聚合、随机森林、ji端随机森林的效率
9．3 Boosting 算法―锻炼弱模型的“肌肉”
9．3．1 AdaBoost 算法
9．3．2 梯度提升算法
9．3．3 XGBoost 算法
9．3．4 Bagging 算法与Boosting 算法的不同之处
9．4 Stacking/Blending 算法―以预测结果作为新特征
9．4．1 Stacking 算法
9．4．2 Blending 算法
9．5 Voting/Averaging 算法―集成基模型的预测结果
9．5．1 通过Voting 进行不同算法的集成
9．5．2 通过Averaging 集成不同算法的结果
9．6 本课内容小结
9．7 课后练习
第壹0 课 监督学习之外―其他类型的机器学习
10．1 无监督学习―聚类
10．1．1 K 均值算法
10．1．2 K 值的选取：手肘法
10．1．3 用聚类辅助理解营销数据
10．2 无监督学习―降维
10．2．1 PCA 算法
10．2．2 通过PCA 算法进行图像特征采样
10．3 半监督学习
10．3．1 自我训练
10．3．2 合作训练
10．3．3 半监督聚类
10．4 自监督学习
10．4．1 潜隐空间
10．4．2 自编码器
10．4．3 变分自编码器
10．5 生成式学习
10．5．1 机器学习的生成式
10．5．2 生成式对抗网络
10．6 本课内容小结
10．7 课后练习
第壹1 课 强化学习实战―咖哥的冰湖挑战
11．1 问题定义：帮助智能体完成冰湖挑战
11．2 强化学习基础知识
11．2．1 延迟满足
11．2．2 更复杂的环境
11．2．3 强化学习中的元素
11．2．4 智能体的视角
11．3 强化学习基础算法Q-Learning 详解
11．3．1 迷宫游戏的示例
11．3．2 强化学习中的局部*优
11．3．3 ε -Greedy 策略
11．3．4 Q-Learning 算法的伪代码
11．4 用Q-Learning 算法来解决冰湖挑战问题
11．4．1 环境的初始化
11．4．2 Q-Learning 算法的实现
11．4．3 Q-Table 的更新过程
11．5 从Q-Learning 算法到SARSA算法
11．5．1 异策略和同策略
11．5．2 SARSA 算法的实现
11．6 用SARSA 算法来解决冰湖挑战问题
11．7 Deep Q Network 算法：用深度网络实现Q-Learning
11．8 本课内容小结
11．9 课后练习
尾声：如何实现机器学习中的知识迁移及持续性的学习
练习答案
・ ・ ・ ・ ・ ・ (收起)目 录
草 1 蒲 血 督 学 习
第 1 章 机器学习及监督学习概论 3
1.1 机器学习 3
1.2 机器学习的分类 5
1.2.1 基本分类 5
1.2.2 按模型分类 10
1.2.3 按算法分类 11
1.2.4 按技巧分类 12
1.3 机器学习方法主要素 13
1.3.1 模型 13
1.3.2 策略 14
1.3.3 算法 16
1.4 模型评估与模型选择 17
1.4.1 训练误差与测试误差 17
1.4.2 过拟合与模型选择 18
1.5 正则化与交叉验证 20
1.5.1 正则化 20
1.5.2 交叉验证 20
1.6 泛化能力 21
1.6.1 泛化误差 21
1.6.2 泛化误差上界 22
1.7 生成模型与判别模型 24
1.8 监督学习应用 24
1.8.1 分类问题 24
1.8.2 标注问题 26
1.8.3 回归问题 27
本章概要 28
继续阅读 29
习题 29
参考文献 29
VIII 机器学习方法
第 2 章 感知机 30
2.1 感知机模型 30
2.2 感知机学习策略 31
2.2.1 数据集的线性可分性 31
2.2.2 感知机学习策略 31
2.3 感知机学习算法 32
2.3.1 感知机学习算法的原始形式 33
2.3.2 算法的收敛性 35
2.3.3 感知机学习算法的对偶形式 37
本章概要 39
继续阅读 40
习题 40
参考文献 40
第 3 章 k 近邻法 41
3.1 k 近邻算法 41
3.2 k 近邻模型 42
3.2.1 模型 42
3.2.2 距离度量 42
3.2.3 k 值的选择 43
3.2.4 分类决策规则 44
3.3 k 近邻法的实现：kd 树 44
3.3.1 构造 kd 树 45
3.3.2 搜索 kd 树 46
本章概要 48
继续阅读 48
习题 48
参考文献 49
第 4 章 朴素贝叶斯法 50
4.1 朴素贝叶斯法的学习与分类 50
4.1.1 基本方法 50
4.1.2 后验概率最大化的含义 51
4.2 朴素贝叶斯法的参数估计 52
4.2.1 极大似然估计 52
4.2.2 学习与分类算法 53
4.2.3 贝叶斯估计 54
本章概要 55
继续阅读 56
目录 IX
习题 56
参考文献 56
第 5 章 决策树 57
5.1 决策树模型与学习 57
5.1.1 决策树模型 57
5.1.2 决策树与 if-then 规则 58
5.1.3 决策树与条件概率分布 58
5.1.4 决策树学习 58
5.2 特征选择 60
5.2.1 特征选择问题 60
5.2.2 信息增益 61
5.2.3 信息增益比 64
5.3 决策树的生成 64
5.3.1 ID3 算法 65
5.3.2 C4.5 的生成算法 66
5.4 决策树的剪枝 66
5.5 CART 算法 68
5.5.1 CART 生成 69
5.5.2 CART 剪枝 72
本章概要 74
继续阅读 75
习题 75
参考文献 75
第 6 章 逻辑斯谛回归与最大烟模型 77
6.1 逻辑斯谛回归模型 77
6.1.1 逻辑斯谛分布 77
6.1.2 二项逻辑斯谛回归模型 78
6.1.3 模型参数估计 79
6.1.4 多项逻辑斯谛回归 79
6.2 最大煽模型 80
6.2.1 最大煽原理 80
6.2.2 最大煽模型的定义 82
6.2.3 最大煽模型的学习 83
6.2.4 极大似然估计 86
6.3 模型学习的最优化算法 87
6.3.1 改进的迭代尺度法 87
6.3.2 拟牛顿法 90
X 机器学习方法
本章概要 91
继续阅读 92
习题 92
参考文献 93
第 7 章 支持向量机 94
7.1 线性可分支持向量机与硬间隔最大化 94
7.1.1 线性可分支持向量机 94
7.1.2 函数间隔和儿何间隔 96
7.1.3 间隔最大化 97
7.1.4 学习的对偶算法 101
7.2 线性支持向量机与软间隔最大化 106
7.2.1 线性支持向量机 106
7.2.2 学习的对偶算法 107
7.2.3 支持向量 110
7.2.4 合页损失函数 111
7.3 非线性支持向量机与核函数 112
7.3.1 核技巧 112
7.3.2 正定核 115
7.3.3 常用核函数 118
7.3.4 非线性支持向量分类机 120
7.4 序列最小最优化算法 121
7.4.1 两个变量二次规划的求解方法 122
7.4.2 变量的选择方法 124
7.4.3 SMO 算法 126
本章概要 127
继续阅读 129
习题 129
参考文献 129
第 8 章 Boosting 131
AdaBoost 算法 131
8.1.1 Boosting 的基本思路 131
AdaBoost 算法 132
AdaBoost 的例子 134
8.2 AdaBoost 算法的训练误差分析 135
8.3 AdaBoost 算法的解释 137
8.3.1 前向分步算法 137
8.3.2 前向分步算法与 AdaBoost 138
目录 XI
8.4 提升树 140
8.4.1 提升树模型 140
8.4.2 提升树算法 140
8.4.3 梯度提升 144
本章概要 145
继续阅读 146
习题 146
参考文献 146
第 9 章 EM 算法及其推广 148
9.1 EM 算法的引入 148
9.1.1 EM 算法 148
9.1.2 EM 算法的导出 151
9.1.3 EM 算法在无监督学习中的应用 153
9.2 EM 算法的收敛性 153
9.3 EM 算法在高斯混合模型学习中的应用 154
9.3.1 高斯混合模型 155
9.3.2 高斯混合模型参数估计的 EM 算法 155
9.4 EM 算法的推广 158
9.4.1 F 函数的极大-极大算法 158
9.4.2 GEM 算法 160
本章概要 161
继续阅读 162
习题 162
参考文献 162
第 10 章 隐马尔可夫模型 163
10.1 隐马尔可夫模型的基本概念 163
10.1.1 隐马尔可夫模型的定义 163
10.1.2 观测序列的生成过程 166
10.1.3 隐马尔可夫模型的 3 个基本问题 166
10.2 概率计算算法 166
10.2.1 直接计算法 166
10.2.2 前向算法 167
10.2.3 后向算法 169
10.2.4 一些概率与期望值的计算 170
10.3 学习算法 172
10.3.1 监督学习方法 172
10.3.2 Baum-Welch 算法 172
XII 机器学习方法
10.3.3 Baum-Welch 模型参数估计公式 174
10.4 预测算法 175
10.4.1 近似算法 175
10.4.2 维特比算法 176
本章概要 179
继续阅读 179
习题 180
参考文献 180
第 11 章 条件随机场 181
11.1 概率无向图模型 181
11.1.1 模型定义 181
11.1.2 概率无向图模型的因子分解 183
11.2 条件随机场的定义与形式 184
11.2.1 条件随机场的定义 184
11.2.2 条件随机场的参数化形式 185
11.2.3 条件随机场的简化形式 186
11.2.4 条件随机场的矩阵形式 187
11.3 条件随机场的概率计算问题 189
11.3.1 前向-后向算法 189
11.3.2 概率计算 189
11.3.3 期望值的计算 190
11.4 条件随机场的学习算法 191
11.4.1 改进的迭代尺度法 191
11.4.2 拟牛顿法 194
11.5 条件随机场的预测算法 195
本章概要 197
继续阅读 198
习题 198
参考文献 199
第 12 章 监督学习方法总结 200
草 2 蒲 元元血血督学学习习
第 13 章 无监督学习概论 207
13.1 无监督学习基本原理 207
13.2 基本问题 208
13.3 机器学习主要素 210
13.4 无监督学习方法 210
目录 XIII
本章概要 214
继续阅读 215
参考文献 215
第 14 章 聚类方法 216
14.1 聚类的基本概念 216
14.1.1 相似度或距离 216
14.1.2 类或簇 219
14.1.3 类与类之间的距离 220
14.2 层次聚类 220
14.3 k 均值聚类 222
14.3.1 模型 222
14.3.2 策略 223
14.3.3 算法 224
14.3.4 算法特性 225
本章概要 226
继续阅读 227
习题 227
参考文献 227
第 15 章 奇异值分解 229
15.1 奇异值分解的定义与性质 229
15.1.1 定义与定理 229
15.1.2 紧奇异值分解与截断奇异值分解 233
15.1.3 儿何解释 235
15.1.4 主要性质 237
15.2 奇异值分解的计算 238
15.3 奇异值分解与矩阵近似 241
15.3.1 弗罗贝尼乌斯范数 241
15.3.2 矩阵的最优近似 242
15.3.3 矩阵的外积展开式 245
本章概要 247
继续阅读 248
习题 248
参考文献 249
第 16 章 主成分分析 250
16.1 总体主成分分析 250
16.1.1 基本想法 250
XIV 机器学习方法
16.1.2 定义和导出 252
16.1.3 主要性质 253
16.1.4 主成分的个数 257
16.1.5 规范化变量的总体主成分 260
16.2 样本主成分分析 260
16.2.1 样本主成分的定义和性质 261
16.2.2 相关矩阵的特征值分解算法 263
16.2.3 数据矩阵的奇异值分解算法 265
本章概要 267
继续阅读 269
习题 269
参考文献 269
第 17 章 潜在语义分析 271
17.1 单词向量空间与话题向量空间 271
17.1.1 单词向量空间 271
17.1.2 话题向量空间 273
17.2 潜在语义分析算法 276
17.2.1 矩阵奇异值分解算法 276
17.2.2 例子 278
17.3 非负矩阵分解算法 279
17.3.1 非负矩阵分解 279
17.3.2 潜在语义分析模型 280
17.3.3 非负矩阵分解的形式化 280
17.3.4 算法 281
本章概要 283
继续阅读 284
习题 284
参考文献 285
第 18 章 概率潜在语义分析 286
18.1 概率潜在语义分析模型 286
18.1.1 基本想法 286
18.1.2 生成模型 287
18.1.3 共现模型 288
18.1.4 模型性质 289
18.2 概率潜在语义分析的算法 291
本章概要 293
继续阅读 294
目录 XV
习题 294
参考文献 295
第 19 章 马尔可夫链蒙特卡罗法 296
19.1 蒙特卡罗法 296
19.1.1 随机抽样 296
19.1.2 数学期望估计 297
19.1.3 积分计算 298
19.2 马尔可夫链 299
19.2.1 基本定义 299
19.2.2 离散状态马尔可夫链 300
19.2.3 连续状态马尔可夫链 305
19.2.4 马尔可夫链的性质 306
19.3 马尔可夫链蒙特卡罗法 310
19.3.1 基本想法 310
19.3.2 基本步骤 311
19.3.3 马尔可夫链蒙特卡罗法与统计学习 311
Metropolis-Hastings 算法 312
19.4.1 基本原理 312
Metropolis-Hastings 算法 315
单分量 Metropolis-Hastings 算法 315
19.5 吉布斯抽样 316
19.5.1 基本原理 316
19.5.2 吉布斯抽样算法 318
19.5.3 抽样计算 319
本章概要 320
继续阅读 321
习题 321
参考文献 322
第 20 章 潜在狄利克雷分配 324
20.1 狄利克雷分布 324
20.1.1 分布定义 324
20.1.2 共辄先验 327
20.2 潜在狄利克雷分自模型 328
20.2.1 基本想法 328
20.2.2 模型定义 329
20.2.3 概率图模型 331
20.2.4 随机变量序列的可交换性 332
XVI 机器学习方法
20.2.5 概率公式 332
20.3 LDA 的吉布斯抽样算法 333
20.3.1 基本想法 333
20.3.2 算法的主要部分 334
20.3.3 算法的后处理 336
20.3.4 算法 337
20.4 LDA 的变分 EM 算法 338
20.4.1 变分推理 338
20.4.2 变分 EM 算法 339
20.4.3 算法推导 340
20.4.4 算法总结 346
本章概要 346
继续阅读 348
习题 348
参考文献 348
第 21 章 PageRank 算法 349
PageRank 的定义 349
21.1.1 基本想法 349
21.1.2 有向图和随机游走模型 350
21.1.3 PageRank 的基本定义 352
21.1.4 PageRank 的一般定义 354
PageRank 的计算 355
21.2.1 迭代算法 355
21.2.2 幕法 357
21.2.3 代数算法 361
本章概要 362
继续阅读 363
习题 363
参考文献 364
第 22 章 无监督学习方法总结 365
22.1 无监督学习方法的关系和特点 365
22.1.1 各种方法之间的关系 365
22.1.2 无监督学习方法 366
22.1.3 基础机器学习方法 366
22.2 话题模型之间的关系和特点 367
参考文献 368
目录 XVII
草 3 蒲 深 反 学 习
第 23 章 前馈神经网络 371
23.1 前馈神经网络的模型 371
23.1.1 前馈神经网络定义 372
23.1.2 前馈神经网络的例子 381
23.1.3 前馈神经网络的表示能力 386
23.2 前馈神经网络的学习算法 389
23.2.1 前馈神经网络学习 389
23.2.2 前馈神经网络学习的优化算法 391
23.2.3 反向传播算法 393
23.2.4 在计算图上的实现 397
23.2.5 算法的实现技巧 401
23.3 前馈神经网络学习的正则化 406
23.3.1 深度学习中的正则化 406
23.3.2 早停法 406
23.3.3 暂返法 408
本章概要 410
继续阅读 413
习题 413
参考文献 414
第 24 章 卷积神经网络 415
24.1 卷积神经网络的模型 415
24.1.1 背景 415
24.1.2 卷积 416
24.1.3 汇聚 424
24.1.4 卷积神经网络 427
24.1.5 卷积神经网络性质 430
24.2 卷积神经网络的学习算法 432
24.2.1 卷积导数 432
24.2.2 反向传播算法 433
24.3 图像分类中的应用 436
24.3.1 AlexNet 436
24.3.2 残差网络 437
本章概要 441
继续阅读 443
习题 443
参考文献 445
XVIII 机器学习方法
第 25 章 循环神经网络 447
25.1 简单循环神经网络 447
25.1.1 模型 447
25.1.2 学习算法 450
25.2 常用循环神经网络 454
25.2.1 长短期记忆网络 454
25.2.2 门控循环单元网络 457
25.2.3 深度循环神经网络 458
25.2.4 双向循环神经网络 459
25.3 自然语言生成中的应用 460
25.3.1 词向量 460
25.3.2 语言模型与语言生成 463
本章概要 465
继续阅读 467
习题 467
参考文献 468
第 26 章 序列到序列模型 469
26.1 序列到序列基本模型 469
26.1.1 序列到序列学习 469
26.1.2 基本模型 471
RNN Search 模型 472
26.2.1 注意力 472
26.2.2 模型定义 474
26.2.3 模型特点 475
Transformer 模型 475
26.3.1 模型架构 476
26.3.2 模型特点 482
本章概要 483
继续阅读 486
习题 486
参考文献 486
第 27 章 预训练语言模型 488
27.1 GPT 模型 488
27.1.1 预训练语言模型 488
27.1.2 模型和学习 490
27.2 BERT 模型 493
27.2.1 去噪自动编码器 493
27.2.2 模型和学习 495
目录 XIX
27.2.3 模型特点 499
本章概要 500
继续阅读 502
习题 502
参考文献 502
第 28 章 生成对抗网络 504
28.1 GAN 基本模型 504
28.1.1 模型 504
28.1.2 学习算法 506
28.1.3 理论分析 507
28.2 图像生成中的应用 508
28.2.1 转置卷积 509
28.2.2 DCGAN 511
本章概要 513
继续阅读 514
习题 514
参考文献 515
第 29 章 深度学习方法总结 516
29.1 深度学习的模型 516
29.2 深度学习的方法 518
29.3 深度学习的优化算法 520
29.4 深度学习的优缺点 522
参考文献 523
附录 A 梯度下降法 524
附录 B 牛顿法和拟牛顿法 526
附录 C 拉格朗日对偶性 531
附录 D 矩阵的基本子空间 534
附录 E KL 散度的定义和狄利克雷分布的性质 537
附录 F 软最大化函数的偏导数和交叉烟损失函数的偏导数 539
索引 541
・ ・ ・ ・ ・ ・ (收起)目录
前言 ix
第 1 章　引言 1
1.1　为何选择机器学习 1
1.1.1　机器学习能够解决的问题 2
1.1.2　熟悉任务和数据 4
1.2　为何选择Python 4
1.3　scikit-learn 4
1.4　必要的库和工具 5
1.4.1　Jupyter Notebook 6
1.4.2　NumPy 6
1.4.3　SciPy 6
1.4.4　matplotlib 7
1.4.5　pandas 8
1.4.6　mglearn 9
1.5　Python 2 与Python 3 的对比 9
1.6　本书用到的版本 10
1.7　第 一个应用：鸢尾花分类 11
1.7.1　初识数据 12
1.7.2　衡量模型是否成功：训练数据与测试数据 14
1.7.3　要事第 一：观察数据 15
1.7.4　构建第 一个模型：k 近邻算法 16
1.7.5　做出预测 17
1.7.6　评估模型 18
1.8　小结与展望 19
第 2 章　监督学习 21
2.1　分类与回归 21
2.2　泛化、过拟合与欠拟合 22
2.3　监督学习算法 24
2.3.1　一些样本数据集 25
2.3.2　k 近邻 28
2.3.3　线性模型 35
2.3.4　朴素贝叶斯分类器 53
2.3.5　决策树 54
2.3.6　决策树集成 64
2.3.7　核支持向量机 71
2.3.8　神经网络（深度学习） 80
2.4　分类器的不确定度估计 91
2.4.1　决策函数 91
2.4.2　预测概率 94
2.4.3　多分类问题的不确定度 96
2.5　小结与展望 98
第3 章　无监督学习与预处理 100
3.1　无监督学习的类型 100
3.2　无监督学习的挑战 101
3.3　预处理与缩放 101
3.3.1　不同类型的预处理 102
3.3.2　应用数据变换 102
3.3.3　对训练数据和测试数据进行相同的缩放 104
3.3.4　预处理对监督学习的作用 106
3.4　降维、特征提取与流形学习 107
3.4.1　主成分分析 107
3.4.2　非负矩阵分解 120
3.4.3　用t-SNE 进行流形学习 126
3.5　聚类 130
3.5.1　k 均值聚类 130
3.5.2　凝聚聚类 140
3.5.3　DBSCAN 143
3.5.4　聚类算法的对比与评估 147
3.5.5　聚类方法小结 159
3.6　小结与展望 159
第4 章　数据表示与特征工程 161
4.1　分类变量 161
4.1.1　One-Hot 编码（虚拟变量） 162
4.1.2　数字可以编码分类变量 166
4.2　分箱、离散化、线性模型与树 168
4.3　交互特征与多项式特征 171
4.4　单变量非线性变换 178
4.5　自动化特征选择 181
4.5.1　单变量统计 181
4.5.2　基于模型的特征选择 183
4.5.3　迭代特征选择 184
4.6　利用专家知识 185
4.7　小结与展望 192
第5 章　模型评估与改进 193
5.1　交叉验证 194
5.1.1　scikit-learn 中的交叉验证 194
5.1.2　交叉验证的优点 195
5.1.3　分层k 折交叉验证和其他策略 196
5.2　网格搜索 200
5.2.1　简单网格搜索 201
5.2.2　参数过拟合的风险与验证集 202
5.2.3　带交叉验证的网格搜索 203
5.3　评估指标与评分 213
5.3.1　牢记目标 213
5.3.2　二分类指标 214
5.3.3　多分类指标 230
5.3.4　回归指标 232
5.3.5　在模型选择中使用评估指标 232
5.4　小结与展望 234
第6 章　算法链与管道 236
6.1　用预处理进行参数选择 237
6.2　构建管道 238
6.3　在网格搜索中使用管道 239
6.4　通用的管道接口 242
6.4.1　用make_pipeline 方便地创建管道 243
6.4.2　访问步骤属性 244
6.4.3　访问网格搜索管道中的属性 244
6.5　网格搜索预处理步骤与模型参数 246
6.6　网格搜索选择使用哪个模型 248
6.7　小结与展望 249
第7 章　处理文本数据 250
7.1　用字符串表示的数据类型 250
7.2　示例应用：电影评论的情感分析 252
7.3　将文本数据表示为词袋 254
7.3.1　将词袋应用于玩具数据集 255
7.3.2　将词袋应用于电影评论 256
7.4　停用词 259
7.5　用tf-idf 缩放数据 260
7.6　研究模型系数 263
7.7　多个单词的词袋（n 元分词） 263
7.8　分词、词干提取与词形还原 267
7.9　主题建模与文档聚类 270
7.10　小结与展望 277
第8 章　全书总结 278
8.1　处理机器学习问题 278
8.2　从原型到生产 279
8.3　测试生产系统 280
8.4　构建你自己的估计器 280
8.5　下一步怎么走 281
8.5.1　理论 281
8.5.2　其他机器学习框架和包 281
8.5.3　排序、推荐系统与其他学习类型 282
8.5.4　概率建模、推断与概率编程 282
8.5.5　神经网络 283
8.5.6　推广到更大的数据集 283
8.5.7　磨练你的技术 284
8.6　总结 284
关于作者 285
关于封面 285
・ ・ ・ ・ ・ ・ (收起)序（王斌 小米AI 实验室主任、NLP 首席科学家）
前言
主要符号表
第1章 绪论
式(1.1)
式(1.2)
第2章 模型评估与选择
式(2.20)
式(2.21)
式(2.27)
式(2.41)
附注
参考文献
第3章 线性模型
式(3.5)
式(3.6)
式(3.7)
式(3.10)
式(3.27)
式(3.30)
式(3.32)
式(3.37)
式(3.38)
式(3.39)
式(3.43)
式(3.44)
式(3.45)
第4章 决策树
式(4.1)
式(4.2)
式(4.6)
式(4.7)
式(4.8)
附注
参考文献
第5章 神经网络
式(5.2)
式(5.10)
式(5.12)
式(5.13)
式(5.14)
式(5.15)
ii j 目录
式(5.20)
式(5.22)
式(5.23)
式(5.24)
附注
参考文献
第6章 支持向量机
式(6.9)
式(6.10)
式(6.11)
式(6.13)
式(6.35)
式(6.37)
式(6.38)
式(6.39)
式(6.40)
式(6.41)
式(6.52)
式(6.60)
式(6.62)
式(6.63)
式(6.65)
式(6.66)
式(6.67)
式(6.70)
附注
参考文献
第7章 贝叶斯分类器
式(7.5)
式(7.6)
式(7.12)
式(7.13)
式(7.19)
式(7.20)
式(7.24)
式(7.25)
式(7.27)
式(7.34)
附注
参考文献
第8章 集成学习
式(8.1)
式(8.2)
式(8.3)
式(8.4)
式(8.5)
式(8.6)
式(8.7)
式(8.8)
式(8.9)
式(8.10)
式(8.11)
式(8.12)
式(8.13)
式(8.14)
式(8.16)
式(8.17)
式(8.18)
式(8.19)
式(8.20)
式(8.21)
式(8.22)
式(8.23)
式(8.24)
式(8.25)
式(8.26)
式(8.27)
式(8.28)
式(8.29)
式(8.30)
式(8.31)
式(8.32)
式(8.33)
式(8.34)
式(8.35)
式(8.36)
第9章 聚类
式(9.5)
式(9.6)
式(9.7)
式(9.8)
式(9.33)
式(9.34)
式(9.35)
式(9.38)
第10章 降维与度量学习
式(10.1)
式(10.2)
式(10.3)
式(10.4)
式(10.5)
式(10.6)
式(10.10)
式(10.14)
式(10.17)
式(10.24)
式(10.28)
式(10.31)
第11章 特征选择与稀疏学习
式(11.1)
式(11.2)
式(11.5)
式(11.6)
式(11.7)
式(11.10)
式(11.11)
式(11.12)
式(11.13)
式(11.14)
式(11.15)
式(11.16)
式(11.17)
式(11.18)
第12章 计算学习理论
式(12.1)
式(12.2)
式(12.3)
式(12.4)
式(12.5)
式(12.7)
式(12.9)
式(12.10)
式(12.11)
式(12.12)
式(12.13)
式(12.14)
式(12.15)
式(12.16)
式(12.17)
式(12.18)
式(12.19)
式(12.20)
式(12.21)
式(12.22)
式(12.23)
式(12.24)
式(12.25)
式(12.26)
式(12.27)
式(12.28)
式(12.29)
式(12.30)
式(12.31)
式(12.32)
式(12.34)
式(12.36)
式(12.37)
式(12.38)
式(12.39)
式(12.40)
式(12.41)
式(12.42)
式(12.43)
式(12.44)
式(12.45)
式(12.46)
式(12.52)
式(12.53)
式(12.57)
式(12.58)
式(12.59)
式(12.60)
参考文献
第13章 半监督学习
式(13.1)
式(13.2)
式(13.3)
式(13.4)
式(13.5)
式(13.6)
式(13.7)
式(13.8)
式(13.9)
式(13.12)
式(13.13)
式(13.14)
式(13.15)
式(13.16)
式(13.17)
式(13.20)
第14章 概率图模型
式(14.1)
式(14.2)
式(14.3)
式(14.4)
式(14.7)
式(14.8)
式(14.9)
式(14.10)
式(14.14)
式(14.15)
式(14.16)
式(14.17)
式(14.18)
式(14.19)
式(14.20)
式(14.22)
式(14.26)
式(14.27)
式(14.28)
式(14.29)
式(14.30)
式(14.31)
式(14.32)
式(14.33)
式(14.34)
式(14.35)
式(14.36)
式(14.37)
式(14.38)
式(14.39)
式(14.40)
式(14.41)
式(14.42)
式(14.43)
式(14.44)
第15章 规则学习
式(15.2)
式(15.3)
式(15.6)
式(15.7)
式(15.9)
式(15.10)
式(15.11)
式(15.12)
式(15.13)
式(15.14)
式(15.16)
第16章 强化学习
式(16.2)
式(16.3)
式(16.4)
式(16.7)
式(16.8)
式(16.10)
式(16.14)
式(16.16)
式(16.31)
・ ・ ・ ・ ・ ・ (收起)序
前言
第1章 概率思想：构建理论基础 1
1.1 理论基石：条件概率、独立性与贝叶斯 1
1.1.1 从概率到条件概率 1
1.1.2 条件概率的具体描述 2
1.1.3 条件概率的表达式分析 3
1.1.4 两个事件的独立性 4
1.1.5 从条件概率到全概率公式 5
1.1.6 聚焦贝叶斯公式 6
1.1.7 本质内涵：由因到果，由果推因 7
1.2 事件的关系：深入理解独立性 8
1.2.1 重新梳理两个事件的独立性 8
1.2.2 不相容与独立性 8
1.2.3 条件独立 9
1.2.4 独立与条件独立 11
1.2.5 独立重复实验 11
第2章 变量分布：描述随机世界 13
2.1 离散型随机变量：分布与数字特征 13
2.1.1 从事件到随机变量 13
2.1.2 离散型随机变量及其要素 14
2.1.3 离散型随机变量的分布列 15
2.1.4 分布列和概率质量函数 16
2.1.5 二项分布及二项随机变量 17
2.1.6 几何分布及几何随机变量 21
2.1.7 泊松分布及泊松随机变量 24
2.2 连续型随机变量：分布与数字特征 27
2.2.1 概率密度函数 27
2.2.2 连续型随机变量区间概率的计算 29
2.2.3 连续型随机变量的期望与方差 29
2.2.4 正态分布及正态随机变量 30
2.2.5 指数分布及指数随机变量 33
2.2.6 均匀分布及其随机变量 35
2.3 多元随机变量（上）：联合、边缘与条件 38
2.3.1 实验中引入多个随机变量 38
2.3.2 联合分布列 38
2.3.3 边缘分布列 39
2.3.4 条件分布列 40
2.3.5 集中梳理核心的概率理论 44
2.4 多元随机变量（下）：独立与相关 46
2.4.1 随机变量与事件的独立性 46
2.4.2 随机变量之间的独立性 47
2.4.3 独立性示例 48
2.4.4 条件独立的概念 48
2.4.5 独立随机变量的期望和方差 50
2.4.6 随机变量的相关性分析及量化方法 52
2.4.7 协方差及协方差矩阵 52
2.4.8 相关系数的概念 54
2.5 多元随机变量实践：聚焦多元正态分布 55
2.5.1 再谈相关性：基于二元标准正态分布 55
2.5.2 二元一般正态分布 57
2.5.3 聚焦相关系数 60
2.5.4 独立和相关性的关系 64
2.6 多元高斯分布：参数特征和几何意义 66
2.6.1 从一元分布到多元分布 66
2.6.2 多元高斯分布的参数形式 67
2.6.3 二元高斯分布的具体示例 68
2.6.4 多元高斯分布的几何特征 71
2.6.5 二元高斯分布几何特征实例分析 74
第3章 参数估计：探寻最大可能 77
3.1 极限思维：大数定律与中心极限定理 77
3.1.1 一个背景话题 77
3.1.2 大数定律 78
3.1.3 大数定律的模拟 80
3.1.4 中心极限定理 83
3.1.5 中心极限定理的工程意义 84
3.1.6 中心极限定理的模拟 85
3.1.7 大数定律的应用：蒙特卡罗方法 86
3.2 推断未知：统计推断的基本框架 89
3.2.1 进入统计学 89
3.2.2 统计推断的例子 90
3.2.3 统计推断中的一些重要概念 91
3.2.4 估计量的偏差与无偏估计 92
3.2.5 总体均值的估计 93
3.2.6 总体方差的估计 95
3.3 极大似然估计 100
3.3.1 极大似然估计法的引例 100
3.3.2 似然函数的由来 102
3.3.3 极大似然估计的思想 103
3.3.4 极大似然估计值的计算 105
3.3.5 简单极大似然估计案例 106
3.3.6 高斯分布参数的极大似然估计 107
3.4 含有隐变量的参数估计问题 110
3.4.1 参数估计问题的回顾 110
3.4.2 新情况：场景中含有隐变量 111
3.4.3 迭代法：解决含有隐变量情形的抛硬币问题 112
3.4.4 代码实验 115
3.5 概率渐增：EM算法的合理性 118
3.5.1 EM算法的背景介绍 119
3.5.2 先抛出EM算法的迭代公式 119
3.5.3 EM算法为什么是有效的 120
3.6 探索EM公式的底层逻辑与由来 123
3.6.1 EM公式中的E步和M步 124
3.6.2 剖析EM算法的由来 124
3.7 探索高斯混合模型：EM 迭代实践 127
3.7.1 高斯混合模型的引入 128
3.7.2 从混合模型的角度看内部机理 129
3.7.3 高斯混合模型的参数估计 131
3.8 高斯混合模型的参数求解 132
3.8.1 利用 EM 迭代模型参数的思路 132
3.8.2 参数估计示例 136
3.8.3 高斯混合模型的应用场景 139
第4章 随机过程：聚焦动态特征 145
4.1 由静向动：随机过程导引 145
4.1.1 随机过程场景举例1：博彩 146
4.1.2 随机过程场景举例2：股价的变化 150
4.1.3 随机过程场景举例3：股价变化过程的展现 152
4.1.4 两类重要的随机过程概述 154
4.2 状态转移：初识马尔可夫链 155
4.2.1 马尔可夫链三要素 155
4.2.2 马尔可夫性：灵魂特征 156
4.2.3 转移概率和状态转移矩阵 157
4.2.4 马尔可夫链性质的总结 158
4.2.5 一步到达与多步转移的含义 159
4.2.6 多步转移与矩阵乘法 160
4.2.7 路径概率问题 163
4.3 变与不变：马尔可夫链的极限与稳态 164
4.3.1 极限与初始状态无关的情况 164
4.3.2 极限依赖于初始状态的情况 165
4.3.3 吸收态与收敛分析 167
4.3.4 可达与常返 168
4.3.5 周期性问题 171
4.3.6 马尔可夫链的稳态分析和判定 172
4.3.7 稳态的求法 174
4.4 隐马尔可夫模型：明暗两条线 176
4.4.1 从马尔可夫链到隐马尔可夫模型 176
4.4.2 典型实例1：盒子摸球实验 177
4.4.3 典型实例2：小宝宝的日常生活 180
4.4.4 隐马尔可夫模型的外在表征 181
4.4.5 推动模型运行的内核三要素 182
4.4.6 关键性质：齐次马尔可夫性和观测独立性 183
4.5 概率估计：隐马尔可夫模型观测序列描述 183
4.5.1 隐马尔可夫模型的研究内容 183
4.5.2 模型研究问题的描述 185
4.5.3 一个直观的思路 186
4.5.4 更优的方法：前向概率算法 187
4.5.5 概率估计实践 190
4.5.6 代码实践 192
4.6 状态解码：隐马尔可夫模型隐状态揭秘 194
4.6.1 隐状态解码问题的描述 194
4.6.2 最大路径概率与维特比算法 195
4.6.3 应用维特比算法进行解码 197
4.6.4 维特比算法的案例实践 199
4.6.5 代码实践 202
4.7 连续域上的无限维：高斯过程 204
4.7.1 高斯过程的一个实际例子 205
4.7.2 高斯过程的核心要素和严谨描述 206
4.7.3 径向基函数的代码演示 207
4.7.4 高斯过程回归原理详解 208
4.7.5 高斯过程回归代码演示 210
第5章 统计推断：贯穿近似策略 215
5.1 统计推断的基本思想和分类 215
5.1.1 统计推断的根源和场景 215
5.1.2 后验分布：推断过程的关注重点 216
5.1.3 精确推断和近似推断 216
5.1.4 确定性近似：变分推断概述 217
5.2 随机近似方法 219
5.2.1 蒙特卡罗方法的理论支撑 219
5.2.2 随机近似的核心：蒙特卡罗 220
5.2.3 接受-拒绝采样的问题背景 221
5.2.4 接受-拒绝采样的方法和步骤 221
5.2.5 接受-拒绝采样的实践 222
5.2.6 接受-拒绝采样方法背后的内涵挖掘 225
5.2.7 重要性采样 226
5.2.8 两种采样方法的问题及思考 227
5.3 采样绝佳途径：借助马尔可夫链的稳态性质 228
5.3.1 马尔可夫链回顾 228
5.3.2 核心：马尔可夫链的平稳分布 229
5.3.3 马尔可夫链进入稳态的转移过程 231
5.3.4 稳态及转移过程演示 231
5.3.5 马尔可夫链稳态的价值和意义 235
5.3.6 基于马尔可夫链进行采样的原理分析 236
5.3.7 采样过程实践与分析 238
5.3.8 一个显而易见的问题和难点 242
5.4 马尔可夫链-蒙特卡罗方法详解 242
5.4.1 稳态判定：细致平稳条件 243
5.4.2 Metropolis-Hastings采样方法的原理 244
5.4.3 如何理解随机游走叠加接受概率 245
5.4.4 如何实现随机游走叠加接受概率 247
5.4.5 建议转移概率矩阵Q的设计 247
5.4.6 Metropolis-Hastings方法的步骤和代码演示 251
5.5 Gibbs采样方法简介 253
5.5.1 Gibbs方法核心流程 253
5.5.2 Gibbs采样的合理性 255
5.5.3 Gibbs采样代码实验 256
・ ・ ・ ・ ・ ・ (收起)第1章　初识Python与Jupyter 1
1.1　Python概要 2
1.1.1　为什么要学习Python 2
1.1.2　Python中常用的库 2
1.2　Python的版本之争 4
1.3　安装Anaconda 5
1.3.1　Linux环境下的Anaconda安装 5
1.3.2　conda命令的使用 6
1.3.3　Windows环境下的Anaconda安装 7
1.4　运行Python 11
1.4.1　验证Python 11
1.4.2　Python版本的Hello World 12
1.4.3　Python的脚本文件 13
1.4.4　代码缩进 15
1.4.5　代码注释 17
1.5　Python中的内置函数 17
1.6　文学化编程―Jupyter 20
1.6.1　Jupyter的由来 20
1.6.2　Jupyter的安装 21
1.6.3　Jupyter的使用 23
1.6.4　Markdown编辑器 26
1.7　Jupyter中的魔法函数 31
1.7.1　%lsmagic函数 31
1.7.2　%matplotlib inline函数 32
1.7.3　%timeit函数 32
1.7.4　%%writefile函数 33
1.7.5　其他常用的魔法函数 34
1.7.6　在Jupyter中执行shell命令 35
1.8　本章小结 35
1.9　思考与提高 36
第2章　数据类型与程序控制结构 40
2.1　为什么需要不同的数据类型 41
2.2　Python中的基本数据类型 42
2.2.1　数值型（Number） 42
2.2.2　布尔类型（Boolean） 45
2.2.3　字符串型（String） 45
2.2.4　列表（List） 49
2.2.5　元组（Tuple） 59
2.2.6　字典（Dictionary） 62
2.2.7　集合（Set） 65
2.3　程序控制结构 67
2.3.1　回顾那段难忘的历史 67
2.3.2 顺序结构 69
2.3.3 选择结构 70
2.3.4 循环结构 74
2.4　高效的推导式 80
2.4.1　列表推导式 80
2.4.2　字典推导式 83
2.4.3　集合推导式 83
2.5　本章小结 84
2.6　思考与提高 84
第3章　自建Python模块与第三方模块 90
3.1　导入Python标准库 91
3.2　编写自己的模块 93
3.3　模块的搜索路径 97
3.4　创建模块包 100
3.5　常用的内建模块 103
3.5.1 collection模块 103
3.5.2　datetime模块 110
3.5.3　json模块 115
3.5.4　random模块 118
3.6　本章小结 121
3.7　思考与提高 122
第4章　Python函数 124
4.1　Python中的函数 125
4.1.1　函数的定义 125
4.1.2　函数返回多个值 127
4.1.3 函数文档的构建 128
4.2　函数参数的“花式”传递 132
4.2.1　关键字参数 132
4.2.2　可变参数 133
4.2.3　默认参数 136
4.2.4　参数序列的打包与解包 138
4.2.5　传值还是传引用 142
4.3　函数的递归 146
4.3.1 感性认识递归 146
4.3.2 思维与递归思维 148
4.3.3 递归调用的函数 149
4.4　函数式编程的高阶函数 151
4.4.1　lambda表达式 152
4.4.2 filter()函数 153
4.4.3 map()函数 155
4.4.4 reduce()函数 157
4.4.5 sorted()函数 158
4.5 本章小结 159
4.6 思考与提高 160
第5章　Python高级特性 165
5.1　面向对象程序设计 166
5.1.1　面向过程与面向对象之辩 166
5.1.2 类的定义与使用 169
5.1.3 类的继承 173
5.2　生成器与迭代器 176
5.2.1 生成器 176
5.2.2 迭代器 183
5.3 文件操作 187
5.3.1 打开文件 187
5.3.2 读取一行与读取全部行 191
5.3.3 写入文件 193
5.4　异常处理 193
5.4.1 感性认识程序中的异常 194
5.4.2 异常处理的三步走 195
5.5 错误调试 197
5.5.1 利用print()输出观察变量 197
5.5.2 assert断言 198
5.6　本章小结 201
5.7　思考与提高 202
第6章　NumPy向量计算 204
6.1　为何需要NumPy 205
6.2　如何导入NumPy 205
6.3 生成NumPy数组 206
6.3.1　利用序列生成 206
6.3.2　利用特定函数生成 207
6.3.3 Numpy数组的其他常用函数 209
6.4　N维数组的属性 212
6.5 NumPy数组中的运算 215
6.5.1 向量运算 216
6.5.2 算术运算 216
6.5.3 逐元素运算与张量点乘运算 218
6.6　爱因斯坦求和约定 222
6.6.1　不一样的标记法 222
6.6.2　NumPy中的einsum()方法 224
6.7 NumPy中的“轴”方向 231
6.8 操作数组元素 234
6.8.1 通过索引访问数组元素 234
6.8.2 NumPy中的切片访问 236
6.8.3 二维数组的转置与展平 238
6.9 NumPy中的广播 239
6.10 NumPy数组的高级索引 242
6.10.1 “花式”索引 242
6.10.2 布尔索引 247
6.11 数组的堆叠操作 249
6.11.1 水平方向堆叠hstack() 250
6.11.2 垂直方向堆叠vstack() 251
6.11.3 深度方向堆叠hstack() 252
6.11.4 列堆叠与行堆叠 255
6.11.5 数组的分割操作 257
6.12 NumPy中的随机数模块 264
6.13 本章小结 266
6.14 思考与提高 267
第7章　Pandas数据分析 271
7.1 Pandas简介 272
7.2 Pandas的安装 272
7.3 Series类型数据 273
7.3.1 Series的创建 273
7.3.2 Series中的数据访问 277
7.3.3 Series中的向量化操作与布尔索引 280
7.3.4 Series中的切片操作 283
7.3.5 Series中的缺失值 284
7.3.6 Series中的删除与添加操作 286
7.3.7 Series中的name属性 288
7.4 DataFrame 类型数据 289
7.4.1 构建DataFrame 289
7.4.2 访问DataFrame中的列与行 293
7.4.3 DataFrame中的删除操作 298
7.4.4 DataFrame中的“轴”方向 301
7.4.5 DataFrame中的添加操作 303
7.5 基于Pandas的文件读取与分析 310
7.5.1 利用Pandas读取文件 311
7.5.2 DataFrame中的常用属性 312
7.5.3 DataFrame中的常用方法 314
7.5.4 DataFrame的条件过滤 318
7.5.5 DataFrame的切片操作 320
7.5.6 DataFrame的排序操作 323
7.5.7 Pandas的聚合和分组运算 325
7.5.8 DataFrame的透视表 334
7.5.9 DataFrame的类SQL操作 339
7.5.10 DataFrame中的数据清洗方法 341
7.6 泰坦尼克幸存者数据预处理 342
7.6.1 数据集简介 342
7.6.2 数据集的拼接 344
7.6.3 缺失值的处理 350
7.7 本章小结 353
7.8 思考与提高 353
第8章　Matplotlib与Seaborn可视化分析 365
8.1 Matplotlib与图形绘制 366
8.2 绘制简单图形 366
8.3 pyplot的高级功能 371
8.3.1 添加图例与注释 371
8.3.2 设置图形标题及坐标轴 374
8.3.3 添加网格线 378
8.3.4 绘制多个子图 380
8.3.5 Axes与Subplot的区别 382
8.4 散点图 388
8.5 条形图与直方图 392
8.5.1 垂直条形图 392
8.5.2 水平条形图 394
8.5.3 并列条形图 395
8.5.4 叠加条形图 400
8.5.5 直方图 402
8.6 饼图 407
8.7 箱形图 409
8.8 误差条 411
8.9 绘制三维图形 413
8.10 与Pandas协作绘图―以谷歌流感趋势数据为例 416
8.10.1 谷歌流感趋势数据描述 416
8.10.2 导入数据与数据预处理 417
8.10.3 绘制时序曲线图 421
8.10.4 选择合适的数据可视化表达 423
8.10.5 基于条件判断的图形绘制 427
8.10.6 绘制多个子图 430
8.11 惊艳的Seaborn 431
8.11.1 pairplot（对图） 432
8.11.2 heatmap（热力图） 434
8.11.3 boxplot（箱形图） 436
8.11.4 violin plot（小提琴图） 442
8.11.5 Density Plot（密度图） 446
8.12 本章小结 450
8.13 思考与提高 450
第9章　机器学习初步 459
9.1 机器学习定义 460
9.1.1　什么是机器学习 460
9.1.2 机器学习的三个步骤 461
9.1.3 传统编程与机器学习的差别 464
9.1.4 为什么机器学习不容易 465
9.2　监督学习 467
9.2.1　感性认识监督学习 467
9.2.2　监督学习的形式化描述 468
9.2.3 损失函数 470
9.3　非监督学习 471
9.4　半监督学习 473
9.5 机器学习的哲学视角 474
9.6 模型性能评估 476
9.6.1 经验误差与测试误差 476
9.6.2 过拟合与欠拟合 477
9.6.3 模型选择与数据拟合 479
9.7 性能度量 480
9.7.1　二分类的混淆矩阵 480
9.7.2 查全率、查准率与F1分数 481
9.7.3 P-R曲线 484
9.7.4 ROC曲线 485
9.7.5 AUC 489
9.8　本章小结 489
9.9　思考与提高 490
第10章　sklearn与经典机器学习算法 492
10.1 机器学习的利器―sklearn 493
10.1.1 sklearn简介 494
10.1.2 sklearn的安装 496
10.2 线性回归 497
10.2.1　线性回归的概念 497
10.2.2 使用sklearn实现波士顿房价预测 499
10.3　k-近邻算法 516
10.3.1 算法简介 516
10.3.2　k值的选取 518
10.3.3　特征数据的归一化 519
10.3.4　邻居距离的度量 521
10.3.5 分类原则的制定 522
10.3.6　基于sklearn的k-近邻算法实战 522
10.4 Logistic回归 527
10.4.1 为什么需要Logistic回归 527
10.4.2 Logistic源头初探 529
10.4.3 Logistic回归实战 532
10.5 神经网络学习算法 536
10.5.1 人工神经网络的定义 537
10.5.2 神经网络中的“学习”本质 537
10.5.3 神经网络结构的设计 540
10.5.4 利用sklearn搭建多层神经网络 541
10.6 非监督学习的代表―k均值聚类 550
10.6.1 聚类的基本概念 551
10.6.2 簇的划分 552
10.6.3 k均值聚类算法核心 552
10.6.4 k均值聚类算法优缺点 554
10.6.5 基于sklearn的k均值聚类算法实战 555
10.7 本章小结 561
10.8 思考与提高 562
・ ・ ・ ・ ・ ・ (收起)赛题一 工业蒸汽量预测
1 赛题理解 2
1.1 赛题背景 2
1.2 赛题目标 2
1.3 数据概览 2
1.4 评估指标 3
1.5 赛题模型 4
2 数据探索 6
2.1 理论知识 6
2.1.1 变量识别 6
2.1.2 变量分析 6
2.1.3 缺失值处理 10
2.1.4 异常值处理 11
2.1.5 变量转换 14
2.1.6 新变量生成 15
2.2 赛题数据探索 16
2.2.1 导入工具包 16
2.2.2 读取数据 16
2.2.3 查看数据 16
2.2.4 可视化数据分布 18
2.2.5 查看特征变量的相关性 26
3 特征工程 33
3.1 特征工程的重要性和处理 33
3.2 数据预处理和特征处理 33
3.2.1 数据预处理 33
3.2.2 特征处理 34
3.3 特征降维 38
3.3.1 特征选择 39
3.3.2 线性降维 44
3.4 赛题特征工程 45
3.4.1 异常值分析 45
3.4.2 最大值和最小值的
归一化 46
3.4.3 查看数据分布 47
3.4.4 特征相关性 48
3.4.5 特征降维 48
3.4.6 多重共线性分析 49
3.4.7 PCA处理 50
4 模型训练 52
4.1 回归及相关模型 52
4.1.1 回归的概念 52
4.1.2 回归模型训练和预测 52
4.1.3 线性回归模型 52
4.1.4 K近邻回归模型 54
4.1.5 决策树回归模型 55
4.1.6 集成学习回归模型 58
4.2 赛题模型训练 61
4.2.1 导入相关库 61
4.2.2 切分数据 62
4.2.3 多元线性回归 62
4.2.4 K近邻回归 62
4.2.5 随机森林回归 63
4.2.6 LGB模型回归 63
5 模型验证 64
5.1 模型评估的概念和方法 64
5.1.1 欠拟合与过拟合 64
5.1.2 模型的泛化与正则化 68
5.1.3 回归模型的评估指标和
调用方法 70
5.1.4 交叉验证 72
5.2 模型调参 75
5.2.1 调参 75
5.2.2 网格搜索 76
5.2.3 学习曲线 77
5.2.4 验证曲线 78
5.3 赛题模型验证和调参 78
5.3.1 模型过拟合与欠拟合 78
5.3.2 模型正则化 81
5.3.3 模型交叉验证 82
5.3.4 模型超参空间及调参 85
5.3.5 学习曲线和验证曲线 89
6 特征优化 93
6.1 特征优化的方法 93
6.1.1 合成特征 93
6.1.2 特征的简单变换 93
6.1.3 用决策树创造新特征 94
6.1.4 特征组合 94
6.2 赛题特征优化 96
6.2.1 导入数据 96
6.2.2 特征构造方法 96
6.2.3 特征构造函数 96
6.2.4 特征降维处理 96
6.2.5 模型训练和评估 97
7 模型融合 100
7.1 模型优化 100
7.1.1 模型学习曲线 100
7.1.2 模型融合提升技术 100
7.1.3 预测结果融合策略 102
7.1.4 其他提升方法 105
7.2 赛题模型融合 106
7.2.1 导入工具包 106
7.2.2 获取训练数据和测试
数据 106
7.2.3 模型评价函数 107
7.2.4 采用网格搜索训练
模型 107
7.2.5 单一模型预测效果 109
7.2.6 模型融合Boosting方法 115
7.2.7 多模型预测Bagging
方法 118
7.2.8 多模型融合Stacking
方法 119
7.2.9 模型验证 127
7.2.10 使用lr_reg和lgb_reg
进行融合预测 127

赛题二 天猫用户重复购买预测
1 赛题理解 130
1.1 赛题背景 130
1.2 数据介绍 131
1.3 评估指标 133
1.4 赛题分析 134
2 数据探索 137
2.1 理论知识 137
2.1.1 缺失数据处理 137
2.1.2 不均衡样本 138
2.1.3 常见的数据分布 141
2.2 赛题数据探索 144
2.2.1 导入工具包 145
2.2.2 读取数据 145
2.2.3 数据集样例查看 145
2.2.4 查看数据类型和数据
大小 146
2.2.5 查看缺失值 147
2.2.6 观察数据分布 148
2.2.7 探查影响复购的各种
因素 150
3 特征工程 155
3.1 特征工程介绍 155
3.1.1 特征工程的概念 155
3.1.2 特征归一化 155
3.1.3 类别型特征的转换 156
3.1.4 高维组合特征的处理 156
3.1.5 组合特征 157
3.1.6 文本表示模型 157
3.2 赛题特征工程思路 158
3.3 赛题特征工程构造 160
3.3.1 工具导入 160
3.3.2 数据读取 160
3.3.3 对数据进行内存压缩 161
3.3.4 数据处理 163
3.3.5 定义特征统计函数 164
3.3.6 提取统计特征 166
3.3.7 利用Countvector和
TF-IDF提取特征 170
3.3.8 嵌入特征 170

3.3.9 Stacking分类特征 171
4 模型训练 179
4.1 分类的概念 179
4.2 分类相关模型 179
4.2.1 逻辑回归分类模型 179
4.2.2 K近邻分类模型 180
4.2.3 高斯贝叶斯分类模型 182
4.2.4 决策树分类模型 182
4.2.5 集成学习分类模型 183
5 模型验证 186
5.1 模型验证指标 186
5.1.1 准确度 186
5.1.2 查准率和查全率 188
5.1.3 F1值 189
5.1.4 分类报告 189
5.1.5 混淆矩阵 189
5.1.6 ROC 190
5.1.7 AUC曲线 190
5.2 赛题模型验证和评估 190
5.2.1 基础代码 190
5.2.2 简单验证 191
5.2.3 设置交叉验证方式 192
5.2.4 模型调参 194
5.2.5 混淆矩阵 195
5.2.6 不同的分类模型 198
5.2.7 自己封装模型 205
6 特征优化 211
6.1 特征选择技巧 211
6.2 赛题特征优化 213
6.2.1 基础代码 213
6.2.2 缺失值补全 213
6.2.3 特征选择 213
赛题三 O2O优惠券预测
1 赛题理解 222
1.1 赛题介绍 222
1.2 赛题分析 223
2 数据探索 225
2.1 理论知识 225
2.1.1 数据探索的定义 225
2.1.2 数据探索的目的 226
2.1.3 相关Python包 226
2.2 初步的数据探索 226
2.2.1 数据读取 226
2.2.2 数据查看 227
2.2.3 数据边界探索 231
2.2.4 训练集与测试集的
相关性 232
2.2.5 数据统计 236
2.3 数据分布 238
2.3.1 对文本数据的数值化
处理 238
2.3.2 数据分布可视化 242
3 特征工程 246
3.1 赛题特征工程思路 246
3.2 赛题特征构建 248
3.2.1 工具函数 248
3.2.2 特征群生成函数 250
3.2.3 特征集成函数 256
3.2.4 特征输出 257
3.3 对特征进行探索 260
3.3.1 特征读取函数 260
3.3.2 特征总览 261
3.3.3 查看特征的分布 262
3.3.4 特征相关性分析 265
4 模型训练 266
4.1 模型训练与评估 266
4.2 不同算法模型的性能对比 271
4.2.1 朴素贝叶斯 271
4.2.2 逻辑回归 271
4.2.3 决策树 272
4.2.4 随机森林 272
4.2.5 XGBoost 273
4.2.6 LightGBM 274
4.2.7 不同特征效果对比 274
4.3 结果输出 274
5 模型验证 276
5.1 评估指标 276
5.2 交叉验证 276
5.3 模型比较 279
5.4 验证结果可视化 282
5.5 结果分析 289
5.6 模型调参 290
5.7 实际方案 292
6 提交结果 299
6.1 整合及输出结果 299
6.2 结果提交及线上验证 302
赛题四 阿里云安全恶意程序检测
1 赛题理解 306
1.1 赛题介绍 306
1.2 赛题分析 307
2 数据探索 310
2.1 训练集数据探索 310
2.1.1 数据特征类型 310
2.1.2 数据分布 311
2.1.3 缺失值 312
2.1.4 异常值 312
2.1.5 标签分布 313
2.2 测试集数据探索 314
2.2.1 数据信息 314
2.2.2 缺失值 315
2.2.3 数据分布 315
2.2.4 异常值 315
2.3 数据集联合分析 316
2.3.1 file_id分析 316
2.3.2 API分析 317
3 特征工程与基线模型 318
3.1 特征工程概述 318
3.1.1 特征工程介绍 318
3.1.2 构造特征 318
3.1.3 特征选择 319
3.2 构造线下验证集 319
3.2.1 评估穿越 319
3.2.2 训练集和测试集的特征
差异性 320
3.2.3 训练集和测试集的分布
差异性 320
3.3 基线模型 320
3.3.1 数据读取 320
3.3.2 特征工程 321

3.3.3 基线构建 322
3.3.4 特征重要性分析 324
3.3.5 模型测试 325
4 高阶数据探索 326
4.1 变量分析 326
4.2 高阶数据探索实战 329
4.2.1 数据读取 329
4.2.2 多变量交叉探索 329
5 特征工程进阶与方案优化 343
5.1 pivot特征构建 343
5.1.1 pivot特征 343
5.1.2 pivot特征构建时间 343
5.1.3 pivot特征构建细节和
特点 343
5.2 业务理解和结果分析 344
5.2.1 结合模型理解业务 344
5.2.2 多分类问题预测结果
分析 344
5.3 特征工程进阶实践 344
5.3.1 特征工程基础部分 344
5.3.2 特征工程进阶部分 348
5.3.3 基于LightGBM的模型
验证 349
5.3.4 模型结果分析 351
5.3.5 模型测试 354
6 优化技巧与解决方案升级 355
6.1 优化技巧：Python处理大数据
的技巧 355
6.1.1 内存管理控制 355
6.1.2 加速数据处理的技巧 356
6.1.3 其他开源工具包 356
6.2 深度学习解决方案：TextCNN
建模 358
6.2.1 问题转化 358
6.2.2 TextCNN建模 358
6.2.3 数据预处理 360
6.2.4 TextCNN网络结构 361
6.2.5 TextCNN训练和测试 362
6.2.6 结果提交 364
7 开源方案学习 365
・ ・ ・ ・ ・ ・ (收起)第一部分 通用流程
第　1章 问题建模　2
1.1　评估指标　3
1.1.1　分类指标　4
1.1.2　回归指标　7
1.1.3　排序指标　9
1.2　样本选择　10
1.2.1　数据去噪　11
1.2.2　采样　12
1.2.3　原型选择和训练集选择　13
1.3　交叉验证　14
1.3.1　留出法　14
1.3.2　K折交叉验证　15
1.3.3　自助法　16
参考文献　17
第　2章 特征工程　18
2.1　特征提取　18
2.1.1　探索性数据分析　19
2.1.2　数值特征　20
2.1.3　类别特征　22
2.1.4　时间特征　24
2.1.5　空间特征　25
2.1.6　文本特征　25
2.2　特征选择　27
2.2.1　过滤方法　28
2.2.2　封装方法　31
2.2.3　嵌入方法　31
2.2.4　小结　32
2.2.5　工具介绍　33
参考文献　33
第3章　常用模型　35
3.1　逻辑回归　35
3.1.1　逻辑回归原理　35
3.1.2　逻辑回归应用　38
3.2　场感知因子分解机　39
3.2.1　因子分解机原理　39
3.2.2　场感知因子分解机原理　40
3.2.3　场感知因子分解机的应用　41
3.3　梯度提升树　42
3.3.1　梯度提升树原理　42
3.3.2　梯度提升树的应用　44
参考文献　44
第4章　模型融合　45
4.1　理论分析　46
4.1.1　融合收益　46
4.1.2　模型误差 分歧分解　46
4.1.3　模型多样性度量　48
4.1.4　多样性增强　49
4.2　融合方法　50
4.2.1　平均法　50
4.2.2　投票法　52
4.2.3　Bagging　54
4.2.4　Stacking　55
4.2.5　小结　56
参考文献　57
第二部分　数据挖掘
第5章　用户画像　60
5.1　什么是用户画像　60
5.2　用户画像数据挖掘　63
5.2.1　画像数据挖掘整体架构　63
5.2.2　用户标识　65
5.2.3　特征数据　67
5.2.4　样本数据　68
5.2.5　标签建模　69
5.3　用户画像应用　83
5.3.1　用户画像实时查询系统　83
5.3.2　人群画像分析系统　87
5.3.3　其他系统　90
5.3.4　线上应用效果　91
5.4　小结　91
参考文献　91
第6章　POI实体链接　92
6.1　问题的背景与难点　92
6.2　国内酒店POI实体链接解决方案　94
6.2.1　酒店POI实体链接　94
6.2.2　数据清洗　96
6.2.3　特征生成　97
6.2.4　模型选择与效果评估　100
6.2.5　索引粒度的配置　101
6.3　其他场景的策略调整　101
6.4　小结　103
第7章　评论挖掘　104
7.1　评论挖掘的背景　104
7.1.1　评论挖掘的粒度　105
7.1.2　评论挖掘的维度　105
7.1.3　评论挖掘的整合思考　106
7.2　评论标签提取　106
7.2.1　数据的获取及预处理　107
7.2.2　无监督的标签提取方法　109
7.2.3　基于深度学习的标签提取方法　111
7.3　标签情感分析　113
7.3.1　评论标签情感分析的特殊性　113
7.3.2　基于深度学习的情感分析方法　115
7.3.3　评论标签情感分析的后续优 化与思考　118
7.4　评论挖掘的未来应用及实践　119
7.5　小结　119
参考文献　119
第三部分　搜索和推荐
第8章　O2O场景下的查询理解与 用户引导　122
8.1　现代搜索引擎原理　123
8.2　精确理解查询　124
8.2.1　用户查询意图的定义与识别　125
8.2.2　查询实体识别与结构化　129
8.2.3　召回策略的变迁　130
8.2.4　查询改写　131
8.2.5　词权重与相关性计算　134
8.2.6　类目相关性与人工标注　135
8.2.7　查询理解小结　136
8.3　引导用户完成搜索　137
8.3.1　用户引导的产品定义与衡量 标准　137
8.3.2　搜索前的引导――查询词 推荐　140
8.3.3　搜索中的引导――查询补全　143
8.3.4　搜索后的引导――相关搜索　145
8.3.5　效率提升与效果提升　145
8.3.6　用户引导小结　149
8.4　小结　149
参考文献　150
第9章　O2O场景下排序的特点　152
9.1　系统概述　154
9.2　在线排序服务　154
9.3　多层正交A/B测试　155
9.4　特征获取　155
9.5　离线调研系统　156
9.6　特征工程　156
9.7　排序模型　157
9.8　场景化排序　160
9.9　小结　165
第　10章 推荐在O2O场景的应用　166
10.1　典型的O2O推荐场景　166
10.2　O2O推荐场景特点　167
10.2.1　O2O场景的地理位置因素　168
10.2.2　O2O场景的用户历史行为　168
10.2.3　O2O场景的实时推荐　169
10.3　美团推荐实践――推荐框架　169
10.4　美团推荐实践――推荐召回　170
10.4.1　基于协同过滤的召回　171
10.4.2　基于位置的召回　171
10.4.3　基于搜索查询的召回　172
10.4.4　基于图的召回　172
10.4.5　基于实时用户行为的召回　172
10.4.6　替补策略　172
10.5　美团推荐实践――推荐排序　173
10.5.1　排序特征　173
10.5.2　排序样本　174
10.5.3　排序模型　175
10.6　推荐评价指标　176
参考文献　176
第四部分　计算广告
第　11章 O2O场景下的广告营销　178
11.1　O2O场景下的广告业务特点　178
11.2　商户、用户和平台三者利益平衡　180
11.2.1　商户效果感知　180
11.2.2　用户体验　181
11.2.3　平台收益　182
11.3　O2O广告机制设计　183
11.3.1　广告位设定　183
11.3.2　广告召回机制　183
11.3.3　广告排序机制　184
11.4　O2O推送广告　187
11.5　O2O广告系统工具　190
11.5.1　面向开发人员的系统工具　190
11.5.2　面向广告主和运营人员的 工具　192
11.6　小结　194
参考文献　194
第　12章 用户偏好和损失建模　196
12.1　如何定义用户偏好　196
12.1.1　什么是用户偏好　196
12.1.2　如何衡量用户偏好　196
12.1.3　对不同POI 的偏好　197
12.1.4　用户对 POI 偏好的衡量　197
12.2　广告价值与偏好损失的兑换　198
12.2.1　优化目标　199
12.2.2　模型建模　199
12.3　Pairwise 模型学习　201
12.3.1　GBRank　202
12.3.2　RankNet　204
参考文献　205
第五部分　深度学习
第　13章 深度学习概述　208
13.1　深度学习技术发展历程　209
13.2　深度学习基础结构　211
13.3　深度学习研究热点　216
13.3.1　基于深度学习的生成式模型　216
13.3.2　深度强化学习　218
参考文献　219
第　14章 深度学习在文本领域的应用　220
14.1　基于深度学习的文本匹配　221
14.2　基于深度学习的排序模型　231
14.2.1　排序模型简介　231
14.2.2　深度学习排序模型的演进　232
14.2.3　美团的深度学习排序模型 尝试　235
14.3　小结　237
参考文献　237
第　15章 深度学习在计算机视觉中的 应用　238
15.1　基于深度学习的OCR　238
15.1.1　OCR技术发展历程　239
15.1.2　基于深度学习的文字检测　244
15.1.3　基于序列学习的文字识别　248
15.1.4　小结　251
15.2　基于深度学习的图像智能审核　251
15.2.1　基于深度学习的水印检测　252
15.2.2　明星脸识别　254
15.2.3　色情图片检测　257
15.2.4　场景分类　257
15.3　基于深度学习的图像质量排序　259
15.3.1　图像美学质量评价　260
15.3.2　面向点击预测的图像质量 评价　260
15.4　小结　263
参考文献　264
第六部分　算法工程
第　16章 大规模机器学习　268
16.1　并行计算编程技术　268
16.1.1　向量化　269
16.1.2　多核并行OpenMP　270
16.1.3　GPU编程　272
16.1.4　多机并行MPI　273
16.1.5　并行编程技术小结　276
16.2　并行计算模型　276
16.2.1　BSP　277
16.2.2　SSP　279
16.2.3　ASP　280
16.2.4　参数服务器　281
16.3　并行计算案例　284
16.3.1　XGBoost并行库Rabit　284
16.3.2　MXNet并行库PS-Lite　286
16.4　美团并行计算机器学习平台　287
参考文献　289
第　17章 特征工程和实验平台　290
17.1　特征平台　290
17.1.1　特征生产　290
17.1.2　特征上线　293
17.1.3　在线特征监控　301
17.2　实验管理平台　302
17.2.1　实验平台概述　302
17.2.2　美团实验平台――Gemini　304
・ ・ ・ ・ ・ ・ (收起)第1章 向量和向量空间 1
1.1 向量 2
1.1.1 描述向量 3
1.1.2 向量的加法 10
1.1.3 向量的数量乘法 12
1.2 向量空间 14
1.2.1 什么是向量空间 14
1.2.2 线性组合 16
1.2.3 线性无关 17
1.2.4 子空间 23
1.3 基和维数 25
1.3.1 极大线性无关组 25
1.3.2 基 26
1.3.3 维数 32
1.4 内积空间 34
1.4.1 什么是内积空间 34
1.4.2 点积和欧几里得空间 36
1.5 距离和角度 38
1.5.1 距离 38
1.5.2 基于距离的分类 43
1.5.3 范数和正则化 46
1.5.4 角度 49
1.6 非欧几何 51
第2章 矩阵 54
2.1 基础知识 55
2.1.1 什么是矩阵 55
2.1.2 初等变换 59
2.1.3 矩阵加法 62
2.1.4 数量乘法 63
2.1.5 矩阵乘法 65
2.2 线性映射 70
2.2.1 理解什么是线性 70
2.2.2 线性映射 72
2.2.3 矩阵与线性映射 76
2.2.4 齐次坐标系 79
2.3 矩阵的逆和转置 85
2.3.1 逆矩阵 85
2.3.2 转置矩阵 89
2.3.3 矩阵LU分解 91
2.4 行列式 94
2.4.1 计算方法和意义 94
2.4.2 线性方程组 98
2.5 矩阵的秩 102
2.6 稀疏矩阵 107
2.6.1 生成稀疏矩阵 107
2.6.2 稀疏矩阵压缩 108
2.7 图与矩阵 112
2.7.1 图的基本概念 112
2.7.2 邻接矩阵 114
2.7.3 关联矩阵 119
2.7.4 拉普拉斯矩阵 120
第3章 特征值和特征向量 122
3.1 基本概念 123
3.1.1 定义 123
3.1.2 矩阵的迹 127
3.1.3 一般性质 128
3.2 应用示例 129
3.2.1 动力系统微分方程 129
3.2.2 马尔科夫矩阵 131
3.3 相似矩阵 135
3.3.1 相似变换 137
3.3.2 几何理解 141
3.3.3 对角化 144
3.4 正交和投影 150
3.4.1 正交集和标准正交基 150
3.4.2 正交矩阵 154
3.4.3 再探对称矩阵 156
3.4.4 投影 159
3.5 矩阵分解 163
3.5.1 QR分解 163
3.5.2 特征分解 167
3.5.3 奇异值分解 172
3.5.4 数据压缩 178
3.5.5 降噪 182
3.6 最小二乘法（1） 184
3.6.1 正规方程 184
3.6.2 线性回归（1） 186
第4章 向量分析 191
4.1 向量的代数运算 192
4.1.1 叉积 192
4.1.2 张量和外积 196
4.2 向量微分 199
4.2.1 函数及其导数 199
4.2.2 偏导数 201
4.2.3 梯度 206
4.2.4 矩阵导数 211
4.3 最优化方法 215
4.3.1 简单的线性规划 215
4.3.2 最小二乘法（2） 218
4.3.3 梯度下降法 221
4.3.4 线性回归（2） 226
4.3.5 牛顿法 228
4.4 反向传播算法 229
4.4.1 神经网络 230
4.4.2 参数学习 234
4.4.3 损失函数 248
4.4.4 激活函数 253
4.4.5 理论推导 258
第5章 概率 263
5.1 基本概念 264
5.1.1 试验和事件 264
5.1.2 理解概率 266
5.1.3 条件概率 269
5.2 贝叶斯定理 272
5.2.1 事件的独立性 273
5.2.2 全概率公式 274
5.2.3 理解贝叶斯定理 276
5.3 随机变量和概率分布 279
5.3.1 随机变量 279
5.3.2 离散型随机变量的分布 281
5.3.3 连续型随机变量的分布 295
5.3.4 多维随机变量及分布 307
5.3.5 条件概率分布 312
5.4 随机变量的和 317
5.4.1 离散型随机变量的和 317
5.4.2 连续型随机变量的和 318
5.5 随机变量的数字特征 321
5.5.1 数学期望 321
5.5.2 方差和协方差 326
5.5.3 计算相似度 337
5.5.4 协方差矩阵 343
第6章 数理统计 346
6.1 样本和抽样 347
6.1.1 总体和样本 347
6.1.2 统计量 348
6.2 点估计 353
6.2.1 最大似然估计 354
6.2.2 线性回归（3） 358
6.2.3 最大后验估计 362
6.2.4 估计的选择标准 365
6.3 区间估计 368
6.4 参数检验 373
6.4.1 基本概念 374
6.4.2 正态总体均值的假设检验 378
6.4.3 正态总体方差的假设检验 384
6.4.4 p值检验 385
6.4.5 用假设检验比较模型 388
6.5 非参数检验 391
6.5.1 拟合优度检验 391
6.5.2 列联表检验 394
第7章 信息与熵 399
7.1 度量信息 399
7.2 信息熵 402
7.3 联合熵和条件熵 406
7.4 相对熵和交叉熵 409
7.5 互信息 414
7.6 连续分布 416
附录 419
后记 436
・ ・ ・ ・ ・ ・ (收起)目　录

第一部分　分类
第1章　机器学习基础　　2
1.1 　何谓机器学习　　3
1.1.1 　传感器和海量数据　　4
1.1.2 　机器学习非常重要　　5
1.2 　关键术语　　5
1.3 　机器学习的主要任务　　7
1.4 　如何选择合适的算法　　8
1.5 　开发机器学习应用程序的步骤　　9
1.6 　Python语言的优势　　10
1.6.1 　可执行伪代码　　10
1.6.2 　Python比较流行　　10
1.6.3 　Python语言的特色　　11
1.6.4 　Python语言的缺点　　11
1.7 　NumPy函数库基础　　12
1.8 　本章小结　　13
第2章　k-近邻算法 　　15
2.1 　k-近邻算法概述　　15
2.1.1 　准备：使用Python导入数据　　17
2.1.2 　从文本文件中解析数据　　19
2.1.3 　如何测试分类器　　20
2.2 　示例：使用k-近邻算法改进约会网站的配对效果　　20
2.2.1 　准备数据：从文本文件中解析数据　　21
2.2.2 　分析数据：使用Matplotlib创建散点图　　23
2.2.3 　准备数据：归一化数值　　25
2.2.4 　测试算法：作为完整程序验证分类器　　26
2.2.5 　使用算法：构建完整可用系统　　27
2.3 　示例：手写识别系统　　28
2.3.1 　准备数据：将图像转换为测试向量　　29
2.3.2 　测试算法：使用k-近邻算法识别手写数字　　30
2.4 　本章小结　　31
第3章　决策树 　　32
3.1 　决策树的构造　　33
3.1.1 　信息增益　　35
3.1.2 　划分数据集　　37
3.1.3 　递归构建决策树　　39
3.2 　在Python中使用Matplotlib注解绘制树形图　　42
3.2.1 　Matplotlib注解　　43
3.2.2 　构造注解树　　44
3.3 　测试和存储分类器　　48
3.3.1 　测试算法：使用决策树执行分类　　49
3.3.2 　使用算法：决策树的存储　　50
3.4 　示例：使用决策树预测隐形眼镜类型　　50
3.5 　本章小结　　52
第4章　基于概率论的分类方法：朴素贝叶斯 　　53
4.1 　基于贝叶斯决策理论的分类方法　　53
4.2 　条件概率　　55
4.3 　使用条件概率来分类　　56
4.4 　使用朴素贝叶斯进行文档分类　　57
4.5 　使用Python进行文本分类　　58
4.5.1 　准备数据：从文本中构建词向量　　58
4.5.2 　训练算法：从词向量计算概率　　60
4.5.3 　测试算法：根据现实情况修改分类器　　62
4.5.4 　准备数据：文档词袋模型　　64
4.6 　示例：使用朴素贝叶斯过滤垃圾邮件　　64
4.6.1 　准备数据：切分文本　　65
4.6.2 　测试算法：使用朴素贝叶斯进行交叉验证　　66
4.7 　示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向　　68
4.7.1 　收集数据：导入RSS源　　68
4.7.2 　分析数据：显示地域相关的用词　　71
4.8 　本章小结　　72
第5章　Logistic回归 　　73
5.1 　基于Logistic回归和Sigmoid函数的分类　　74
5.2 　基于最优化方法的最佳回归系数确定　　75
5.2.1 　梯度上升法　　75
5.2.2 　训练算法：使用梯度上升找到最佳参数　　77
5.2.3 　分析数据：画出决策边界　　79
5.2.4 　训练算法：随机梯度上升　　80
5.3 　示例：从疝气病症预测病马的死亡率　　85
5.3.1 　准备数据：处理数据中的缺失值　　85
5.3.2 　测试算法：用Logistic回归进行分类　　86
5.4 　本章小结　　88
第6章　支持向量机　　89
6.1 　基于最大间隔分隔数据　　89
6.2 　寻找最大间隔　　91
6.2.1 　分类器求解的优化问题　　92
6.2.2 　SVM应用的一般框架　　93
6.3 　SMO高效优化算法　　94
6.3.1 　Platt的SMO算法　　94
6.3.2 　应用简化版SMO算法处理小规模数据集　　94
6.4 　利用完整Platt SMO算法加速优化　　99
6.5 　在复杂数据上应用核函数　　105
6.5.1 　利用核函数将数据映射到高维空间　　106
6.5.2 　径向基核函数　　106
6.5.3 　在测试中使用核函数　　108
6.6 　示例：手写识别问题回顾　　111
6.7 　本章小结　　113
第7章　利用AdaBoost元算法提高分类
性能 　　115
7.1 　基于数据集多重抽样的分类器　　115
7.1.1 　bagging：基于数据随机重抽样的分类器构建方法　　116
7.1.2 　boosting　　116
7.2 　训练算法：基于错误提升分类器的性能　　117
7.3 　基于单层决策树构建弱分类器　　118
7.4 　完整AdaBoost算法的实现　　122
7.5 　测试算法：基于AdaBoost的分类　　124
7.6 　示例：在一个难数据集上应用AdaBoost　　125
7.7 　非均衡分类问题　　127
7.7.1 　其他分类性能度量指标：正确率、召回率及ROC曲线　　128
7.7.2 　基于代价函数的分类器决策控制　　131
7.7.3 　处理非均衡问题的数据抽样方法　　132
7.8 　本章小结　　132
第二部分　利用回归预测数值型数据
第8章　预测数值型数据：回归 　　136
8.1 　用线性回归找到最佳拟合直线　　136
8.2 　局部加权线性回归　　141
8.3 　示例：预测鲍鱼的年龄　　145
8.4 　缩减系数来“理解”数据　　146
8.4.1 　岭回归　　146
8.4.2 　lasso　　148
8.4.3 　前向逐步回归　　149
8.5 　权衡偏差与方差　　152
8.6 　示例：预测乐高玩具套装的价格　　153
8.6.1 　收集数据：使用Google购物的API　　153
8.6.2 　训练算法：建立模型　　155
8.7 　本章小结　　158
第9章　树回归　　159
9.1 　复杂数据的局部性建模　　159
9.2 　连续和离散型特征的树的构建　　160
9.3 　将CART算法用于回归　　163
9.3.1 　构建树　　163
9.3.2 　运行代码　　165
9.4 　树剪枝　　167
9.4.1 　预剪枝　　167
9.4.2 　后剪枝　　168
9.5 　模型树　　170
9.6 　示例：树回归与标准回归的比较　　173
9.7 　使用Python的Tkinter库创建GUI　　176
9.7.1 　用Tkinter创建GUI　　177
9.7.2 　集成Matplotlib和Tkinter　　179
9.8 　本章小结　　182
第三部分　无监督学习
第10章　利用K-均值聚类算法对未标注数据分组　　184
10.1 　K-均值聚类算法　　185
10.2 　使用后处理来提高聚类性能　　189
10.3 　二分K-均值算法　　190
10.4 　示例：对地图上的点进行聚类　　193
10.4.1 　Yahoo! PlaceFinder API　　194
10.4.2 　对地理坐标进行聚类　　196
10.5 　本章小结　　198
第11章　使用Apriori算法进行关联分析　　200
11.1 　关联分析　　201
11.2 　Apriori原理　　202
11.3 　使用Apriori算法来发现频繁集　　204
11.3.1 　生成候选项集　　204
11.3.2 　组织完整的Apriori算法　　207
11.4 　从频繁项集中挖掘关联规则　　209
11.5 　示例：发现国会投票中的模式　　212
11.5.1 　收集数据：构建美国国会投票记录的事务数据集　　213
11.5.2 　测试算法：基于美国国会投票记录挖掘关联规则　　219
11.6 　示例：发现毒蘑菇的相似特征　　220
11.7 　本章小结　　221
第12章　使用FP-growth算法来高效发现频繁项集　　223
12.1 　FP树：用于编码数据集的有效方式　　224
12.2 　构建FP树　　225
12.2.1 　创建FP树的数据结构　　226
12.2.2 　构建FP树　　227
12.3 　从一棵FP树中挖掘频繁项集　　231
12.3.1 　抽取条件模式基　　231
12.3.2 　创建条件FP树　　232
12.4 　示例：在Twitter源中发现一些共现词　　235
12.5 　示例：从新闻网站点击流中挖掘　　238
12.6 　本章小结　　239
第四部分　其他工具
第13章　利用PCA来简化数据　　242
13.1 　降维技术　　242
13.2 　PCA　　243
13.2.1 　移动坐标轴　　243
13.2.2 　在NumPy中实现PCA　　246
13.3 　示例：利用PCA对半导体制造数据降维　　248
13.4 　本章小结　　251
第14章　利用SVD简化数据　　252
14.1 　SVD的应用　　252
14.1.1 　隐性语义索引　　253
14.1.2 　推荐系统　　253
14.2 　矩阵分解　　254
14.3 　利用Python实现SVD　　255
14.4 　基于协同过滤的推荐引擎　　257
14.4.1 　相似度计算　　257
14.4.2 　基于物品的相似度还是基于用户的相似度？　　260
14.4.3 　推荐引擎的评价　　260
14.5 　示例：餐馆菜肴推荐引擎　　260
14.5.1 　推荐未尝过的菜肴　　261
14.5.2 　利用SVD提高推荐的效果　　263
14.5.3 　构建推荐引擎面临的挑战　　265
14.6 　基于SVD的图像压缩　　266
14.7 　本章小结　　268
第15章　大数据与MapReduce　　270
15.1 　MapReduce：分布式计算的框架　　271
15.2 　Hadoop流　　273
15.2.1 　分布式计算均值和方差的mapper　　273
15.2.2 　分布式计算均值和方差的reducer　　274
15.3 　在Amazon网络服务上运行Hadoop程序　　275
15.3.1 　AWS上的可用服务　　276
15.3.2 　开启Amazon网络服务之旅　　276
15.3.3 　在EMR上运行Hadoop作业　　278
15.4 　MapReduce上的机器学习　　282
15.5 　在Python中使用mrjob来自动化MapReduce　　283
15.5.1 　mrjob与EMR的无缝集成　　283
15.5.2 　mrjob的一个MapReduce脚本剖析　　284
15.6 　示例：分布式SVM的Pegasos算法　　286
15.6.1 　Pegasos算法　　287
15.6.2 　训练算法：用mrjob实现MapReduce版本的SVM　　288
15.7 　你真的需要MapReduce吗？　　292
15.8 　本章小结　　292
附录A 　Python入门　　294
附录B 　线性代数　　303
附录C 　概率论复习　　309
附录D 　资源　　312
索引　　313
版权声明　　316
・ ・ ・ ・ ・ ・ (收起)第1章　Spark的环境搭建与运行　　1
1.1　Spark的本地安装与配置　　2
1.2　Spark集群　　3
1.3　Spark编程模型　　4
1.3.1　SparkContext类与SparkConf 类　　4
1.3.2　Spark shell　　5
1.3.3　弹性分布式数据集　　6
1.3.4　广播变量和累加器　　10
1.4　Spark Scala编程入门　　11
1.5　Spark Java编程入门　　14
1.6　Spark Python编程入门　　17
1.7　在Amazon EC2上运行Spark　　18
1.8　小结　　23
第2章　设计机器学习系统　　24
2.1　MovieStream介绍　　24
2.2　机器学习系统商业用例　　25
2.2.1　个性化　　26
2.2.2　目标营销和客户细分　　26
2.2.3　预测建模与分析　　26
2.3　机器学习模型的种类　　27
2.4　数据驱动的机器学习系统的组成　　27
2.4.1　数据获取与存储　　28
2.4.2　数据清理与转换　　28
2.4.3　模型训练与测试回路　　29
2.4.4　模型部署与整合　　30
2.4.5　模型监控与反馈　　30
2.4.6　批处理或实时方案的选择　　31
2.5　机器学习系统架构　　31
2.6　小结　　33
第3章　Spark上数据的获取、处理与准备　　34
3.1　获取公开数据集　　35
3.2　探索与可视化数据　　37
3.2.1　探索用户数据　　38
3.2.2　探索电影数据　　41
3.2.3　探索评级数据　　43
3.3　处理与转换数据　　46
3.4　从数据中提取有用特征　　48
3.4.1　数值特征　　48
3.4.2　类别特征　　49
3.4.3　派生特征　　50
3.4.4　文本特征　　51
3.4.5　正则化特征　　55
3.4.6　用软件包提取特征　　56
3.5　小结　　57
第4章　构建基于Spark的推荐引擎　　58
4.1　推荐模型的分类　　59
4.1.1　基于内容的过滤　　59
4.1.2　协同过滤　　59
4.1.3　矩阵分解　　60
4.2　提取有效特征　　64
4.3　训练推荐模型　　67
4.3.1　使用MovieLens 100k数据集训练模型　　67
4.3.2　使用隐式反馈数据训练模型　　68
4.4　使用推荐模型　　69
4.4.1　用户推荐　　69
4.4.2　物品推荐　　72
4.5　推荐模型效果的评估　　75
4.5.1　均方差　　75
4.5.2　K值平均准确率　　77
4.5.3　使用MLlib内置的评估函数　　81
4.6　小结　　82
第5章　Spark构建分类模型　　83
5.1　分类模型的种类　　85
5.1.1　线性模型　　85
5.1.2　朴素贝叶斯模型　　89
5.1.3　决策树　　90
5.2　从数据中抽取合适的特征　　91
5.3　训练分类模型　　93
5.4　使用分类模型　　95
5.5　评估分类模型的性能　　96
5.5.1　预测的正确率和错误率　　96
5.5.2　准确率和召回率　　97
5.5.3　ROC曲线和AUC　　99
5.6　改进模型性能以及参数调优　　101
5.6.1　特征标准化　　101
5.6.2　其他特征　　104
5.6.3　使用正确的数据格式　　106
5.6.4　模型参数调优　　107
5.7　小结　　115
第6章　Spark构建回归模型　　116
6.1　回归模型的种类　　116
6.1.1　最小二乘回归　　117
6.1.2　决策树回归　　117
6.2　从数据中抽取合适的特征　　118
6.3　回归模型的训练和应用　　123
6.4　评估回归模型的性能　　125
6.4.1　均方误差和均方根误差　　125
6.4.2　平均绝对误差　　126
6.4.3　均方根对数误差　　126
6.4.4　R-平方系数　　126
6.4.5　计算不同度量下的性能　　126
6.5　改进模型性能和参数调优　　127
6.5.1　变换目标变量　　128
6.5.2　模型参数调优　　132
6.6　小结　　140
第7章　Spark构建聚类模型　　141
7.1　聚类模型的类型　　142
7.1.1　K-均值聚类　　142
7.1.2　混合模型　　146
7.1.3　层次聚类　　146
7.2　从数据中提取正确的特征　　146
7.3　训练聚类模型　　150
7.4　使用聚类模型进行预测　　151
7.5　评估聚类模型的性能　　155
7.5.1　内部评价指标　　155
7.5.2　外部评价指标　　156
7.5.3　在MovieLens数据集计算性能　　156
7.6　聚类模型参数调优　　156
7.7　小结　　158
第8章　Spark应用于数据降维　　159
8.1　降维方法的种类　　160
8.1.1　主成分分析　　160
8.1.2　奇异值分解　　160
8.1.3　和矩阵分解的关系　　161
8.1.4　聚类作为降维的方法　　161
8.2　从数据中抽取合适的特征　　162
8.3　训练降维模型　　169
8.4　使用降维模型　　172
8.4.1　在LFW数据集上使用PCA投影数据　　172
8.4.2　PCA和SVD模型的关系　　173
8.5　评价降维模型　　174
8.6　小结　　176
第9章　Spark高级文本处理技术　　177
9.1　处理文本数据有什么特别之处　　177
9.2　从数据中抽取合适的特征　　177
9.2.1　短语加权表示　　178
9.2.2　特征哈希　　179
9.2.3　从20新闻组数据集中提取TF-IDF特征　　180
9.3　使用TF-IDF模型　　192
9.3.1　20 Newsgroups数据集的文本相似度和TF-IDF特征　　192
9.3.2　基于20 Newsgroups数据集使用TF-IDF训练文本分类器　　194
9.4　评估文本处理技术的作用　　196
9.5　Word2Vec 模型　　197
9.6　小结　　200
第10章　Spark Streaming在实时机器学习上的应用　　201
10.1　在线学习　　201
10.2　流处理　　202
10.2.1　Spark Streaming介绍　　202
10.2.2　使用Spark Streaming缓存和容错　　205
10.3　创建Spark Streaming应用　　206
10.3.1　消息生成端　　207
10.3.2　创建简单的流处理程序　　209
10.3.3　流式分析　　211
10.3.4　有状态的流计算　　213
10.4　使用Spark Streaming进行在线学习　　215
10.4.1　流回归　　215
10.4.2　一个简单的流回归程序　　216
10.4.3　流K-均值　　220
10.5　在线模型评估　　221
10.6　小结　　224
・ ・ ・ ・ ・ ・ (收起)第1 章一元函数微积分1
1.1 极限与连续. . . . . . . . . . . . . . 1
1.1.1 可数集与不可数集. . . . . . . . 1
1.1.2 数列的极限. . . . . . . . . . . . 3
1.1.3 函数的极限. . . . . . . . . . . . 7
1.1.4 函数的连续性与间断点. . . . . 9
1.1.5 上确界与下确界. . . . . . . . . 11
1.1.6 李普希茨连续性. . . . . . . . . 12
1.1.7 无穷小量. . . . . . . . . . . . . 13
1.2 导数与微分. . . . . . . . . . . . . . 14
1.2.1 一阶导数. . . . . . . . . . . . . 14
1.2.2 机器学习中的常用函数. . . . . 20
1.2.3 高阶导数. . . . . . . . . . . . . 22
1.2.4 微分. . . . . . . . . . . . . . . . 24
1.2.5 导数与函数的单调性. . . . . . 25
1.2.6 极值判别法则. . . . . . . . . . 26
1.2.7 导数与函数的凹凸性. . . . . . 28
1.3 微分中值定理. . . . . . . . . . . . . 29
1.3.1 罗尔中值定理. . . . . . . . . . 29
1.3.2 拉格朗日中值定理. . . . . . . . 29
1.3.3 柯西中值定理. . . . . . . . . . 31
1.4 泰勒公式. . . . . . . . . . . . . . . . 31
1.5 不定积分. . . . . . . . . . . . . . . . 33
1.5.1 不定积分的定义与性质. . . . . 33
1.5.2 换元积分法. . . . . . . . . . . . 35
1.5.3 分部积分法. . . . . . . . . . . . 36
1.6 定积分. . . . . . . . . . . . . . . . . 37
1.6.1 定积分的定义与性质. . . . . . 38
1.6.2 牛顿-莱布尼茨公式. . . . . . . 39
1.6.3 定积分的计算. . . . . . . . . . 40
1.6.4 变上限积分. . . . . . . . . . . . 41
1.6.5 定积分的应用. . . . . . . . . . 42
1.6.6 广义积分. . . . . . . . . . . . . 44
1.7 常微分方程. . . . . . . . . . . . . . 45
1.7.1 基本概念. . . . . . . . . . . . . 45
1.7.2 一阶线性微分方程. . . . . . . . 46
第2 章线性代数与矩阵论49
2.1 向量及其运算. . . . . . . . . . . . . 49
2.1.1 基本概念. . . . . . . . . . . . . 49
2.1.2 基本运算. . . . . . . . . . . . . 51
2.1.3 向量的范数. . . . . . . . . . . . 53
2.1.4 解析几何. . . . . . . . . . . . . 55
2.1.5 线性相关性. . . . . . . . . . . . 57
2.1.6 向量空间. . . . . . . . . . . . . 58
2.1.7 应用――线性回归. . . . . . . . 61
2.1.8 应用――线性分类器与支持
向量机. . . . . . . . . . . . . . 62
2.2 矩阵及其运算. . . . . . . . . . . . . 65
2.2.1 基本概念. . . . . . . . . . . . . 65
2.2.2 基本运算. . . . . . . . . . . . . 67
2.2.3 逆矩阵. . . . . . . . . . . . . . 72
2.2.4 矩阵的范数. . . . . . . . . . . . 78
2.2.5 应用――人工神经网络. . . . . 78
2.2.6 线性变换. . . . . . . . . . . . . 81
2.3 行列式. . . . . . . . . . . . . . . . . 82
2.3.1 行列式的定义与性质. . . . . . 83
2.3.2 计算方法. . . . . . . . . . . . . 91
2.4 线性方程组. . . . . . . . . . . . . . 92
2.4.1 高斯消元法. . . . . . . . . . . . 92
2.4.2 齐次方程组. . . . . . . . . . . . 93
2.4.3 非齐次方程组. . . . . . . . . . 95
2.5 特征值与特征向量. . . . . . . . . . 97
2.5.1 特征值与特征向量. . . . . . . . 97
2.5.2 相似变换. . . . . . . . . . . . . 105
2.5.3 正交变换. . . . . . . . . . . . . 106
2.5.4 QR 算法. . . . . . . . . . . . . . 110
2.5.5 广义特征值. . . . . . . . . . . . 112
2.5.6 瑞利商. . . . . . . . . . . . . . 112
2.5.7 谱范数与特征值的关系. . . . . 114
2.5.8 条件数. . . . . . . . . . . . . . 114
2.5.9 应用――谱归一化与谱正则化. . . . . . . . . . . . . . . . . 115
2.6 二次型. . . . . . . . . . . . . . . . . 116
2.6.1 基本概念. . . . . . . . . . . . . 116
2.6.2 正定二次型与正定矩阵. . . . . 116
2.6.3 标准型. . . . . . . . . . . . . . 119
2.7 矩阵分解. . . . . . . . . . . . . . . . 121
2.7.1 楚列斯基分解. . . . . . . . . . 121
2.7.2 QR 分解. . . . . . . . . . . . . . 123
2.7.3 特征值分解. . . . . . . . . . . . 127
2.7.4 奇异值分解. . . . . . . . . . . . 128
第3 章多元函数微积分133
3.1 偏导数. . . . . . . . . . . . . . . . . 133
3.1.1 一阶偏导数. . . . . . . . . . . . 133
3.1.2 高阶偏导数. . . . . . . . . . . . 134
3.1.3 全微分. . . . . . . . . . . . . . 136
3.1.4 链式法则. . . . . . . . . . . . . 136
3.2 梯度与方向导数. . . . . . . . . . . . 138
3.2.1 梯度. . . . . . . . . . . . . . . . 138
3.2.2 方向导数. . . . . . . . . . . . . 139
3.2.3 应用――边缘检测与HOG
特征. . . . . . . . . . . . . . . . 139
3.3 黑塞矩阵. . . . . . . . . . . . . . . . 140
3.3.1 黑塞矩阵的定义与性质. . . . . 141
3.3.2 凹凸性. . . . . . . . . . . . . . 141
3.3.3 极值判别法则. . . . . . . . . . 143
3.3.4 应用――最小二乘法. . . . . . . 145
3.4 雅可比矩阵. . . . . . . . . . . . . . 146
3.4.1 雅可比矩阵的定义和性质. . . . 146
3.4.2 链式法则的矩阵形式. . . . . . 148
3.5 向量与矩阵求导. . . . . . . . . . . . 150
3.5.1 常用求导公式. . . . . . . . . . 150
3.5.2 应用――反向传播算法. . . . . 154
3.6 微分算法. . . . . . . . . . . . . . . . 156
3.6.1 符号微分. . . . . . . . . . . . . 156
3.6.2 数值微分. . . . . . . . . . . . . 157
3.6.3 自动微分. . . . . . . . . . . . . 158
3.7 泰勒公式. . . . . . . . . . . . . . . . 159
3.8 多重积分. . . . . . . . . . . . . . . . 161
3.8.1 二重积分. . . . . . . . . . . . . 161
3.8.2 三重积分. . . . . . . . . . . . . 164
3.8.3 n 重积分. . . . . . . . . . . . . 167
3.9 无穷级数. . . . . . . . . . . . . . . . 170
3.9.1 常数项级数. . . . . . . . . . . . 170
3.9.2 函数项级数. . . . . . . . . . . . 173
第4 章最优化方法176
4.1 基本概念. . . . . . . . . . . . . . . . 176
4.1.1 问题定义. . . . . . . . . . . . . 177
4.1.2 迭代法的基本思想. . . . . . . . 179
4.2 一阶优化算法. . . . . . . . . . . . . 180
4.2.1 梯度下降法. . . . . . . . . . . . 180
4.2.2 最速下降法. . . . . . . . . . . . 183
4.2.3 梯度下降法的改进. . . . . . . . 184
4.2.4 随机梯度下降法. . . . . . . . . 186
4.2.5 应用――人工神经网络. . . . . 187
4.3 二阶优化算法. . . . . . . . . . . . . 188
4.3.1 牛顿法. . . . . . . . . . . . . . 188
4.3.2 拟牛顿法. . . . . . . . . . . . . 189
4.4 分治法. . . . . . . . . . . . . . . . . 193
4.4.1 坐标下降法. . . . . . . . . . . . 193
4.4.2 SMO 算法. . . . . . . . . . . . . 194
4.4.3 分阶段优化. . . . . . . . . . . . 195
4.4.4 应用――logistic 回归. . . . . . 196
4.5 凸优化问题. . . . . . . . . . . . . . 198
4.5.1 数值优化算法面临的问题. . . . 198
4.5.2 凸集. . . . . . . . . . . . . . . . 199
4.5.3 凸优化问题及其性质. . . . . . 200
4.5.4 机器学习中的凸优化问题. . . . 201
4.6 带约束的优化问题. . . . . . . . . . 202
4.6.1 拉格朗日乘数法. . . . . . . . . 202
4.6.2 应用――线性判别分析. . . . . 204
4.6.3 拉格朗日对偶. . . . . . . . . . 205
4.6.4 KKT 条件. . . . . . . . . . . . . 208
4.6.5 应用――支持向量机. . . . . . . 209
4.7 多目标优化问题. . . . . . . . . . . . 213
4.7.1 基本概念. . . . . . . . . . . . . 213
4.7.2 求解算法. . . . . . . . . . . . . 215
4.7.3 应用――多目标神经结构搜
索. . . . . . . . . . . . . . . . . 215
4.8 泛函极值与变分法. . . . . . . . . . 216
4.8.1 泛函与变分. . . . . . . . . . . . 217
4.8.2 欧拉―拉格朗日方程. . . . . . 218
4.8.3 应用――证明两点之间直线
最短. . . . . . . . . . . . . . . . 220
4.9 目标函数的构造. . . . . . . . . . . . 221
4.9.1 有监督学习. . . . . . . . . . . . 221
4.9.2 无监督学习. . . . . . . . . . . . 224
4.9.3 强化学习. . . . . . . . . . . . . 225
第5 章概率论228
5.1 随机事件与概率. . . . . . . . . . . . 229
5.1.1 随机事件概率. . . . . . . . . . 229
5.1.2 条件概率. . . . . . . . . . . . . 233
5.1.3 全概率公式. . . . . . . . . . . . 234
5.1.4 贝叶斯公式. . . . . . . . . . . . 235
5.1.5 条件独立. . . . . . . . . . . . . 236
5.2 随机变量. . . . . . . . . . . . . . . . 236
5.2.1 离散型随机变量. . . . . . . . . 236
5.2.2 连续型随机变量. . . . . . . . . 237
5.2.3 数学期望. . . . . . . . . . . . . 240
5.2.4 方差与标准差. . . . . . . . . . 242
5.2.5 Jensen 不等式. . . . . . . . . . . 243
5.3 常用概率分布. . . . . . . . . . . . . 244
5.3.1 均匀分布. . . . . . . . . . . . . 244
5.3.2 伯努利分布. . . . . . . . . . . . 246
5.3.3 二项分布. . . . . . . . . . . . . 247
5.3.4 多项分布. . . . . . . . . . . . . 248
5.3.5 几何分布. . . . . . . . . . . . . 249
5.3.6 正态分布. . . . . . . . . . . . . 250
5.3.7 t 分布. . . . . . . . . . . . . . . 252
5.3.8 应用――颜色直方图. . . . . . . 253
5.3.9 应用――贝叶斯分类器. . . . . 254
5.4 分布变换. . . . . . . . . . . . . . . . 254
5.4.1 随机变量函数. . . . . . . . . . 254
5.4.2 逆变换采样算法. . . . . . . . . 256
5.5 随机向量. . . . . . . . . . . . . . . . 258
5.5.1 离散型随机向量. . . . . . . . . 258
5.5.2 连续型随机向量. . . . . . . . . 260
5.5.3 数学期望. . . . . . . . . . . . . 261
5.5.4 协方差. . . . . . . . . . . . . . 262
5.5.5 常用概率分布. . . . . . . . . . 265
5.5.6 分布变换. . . . . . . . . . . . . 268
5.5.7 应用――高斯混合模型. . . . . 269
5.6 极限定理. . . . . . . . . . . . . . . . 271
5.6.1 切比雪夫不等式. . . . . . . . . 271
5.6.2 大数定律. . . . . . . . . . . . . 271
5.6.3 中心极限定理. . . . . . . . . . 273
5.7 参数估计. . . . . . . . . . . . . . . . 273
5.7.1 最大似然估计. . . . . . . . . . 274
5.7.2 最大后验概率估计. . . . . . . . 276
5.7.3 贝叶斯估计. . . . . . . . . . . . 278
5.7.4 核密度估计. . . . . . . . . . . . 278
5.7.5 应用――logistic 回归. . . . . . 280
5.7.6 应用――EM 算法. . . . . . . . 282
5.7.7 应用――Mean Shift 算法. . . . 286
5.8 随机算法. . . . . . . . . . . . . . . . 288
5.8.1 基本随机数生成算法. . . . . . 288
5.8.2 遗传算法. . . . . . . . . . . . . 290
5.8.3 蒙特卡洛算法. . . . . . . . . . 293
5.9 采样算法. . . . . . . . . . . . . . . . 295
5.9.1 拒绝采样. . . . . . . . . . . . . 296
5.9.2 重要性采样. . . . . . . . . . . . 297
第6 章信息论298
6.1 熵与联合熵. . . . . . . . . . . . . . 298
6.1.1 信息量与熵. . . . . . . . . . . . 298
6.1.2 熵的性质. . . . . . . . . . . . . 300
6.1.3 应用――决策树. . . . . . . . . 302
6.1.4 联合熵. . . . . . . . . . . . . . 303
6.2 交叉熵. . . . . . . . . . . . . . . . . 305
6.2.1 交叉熵的定义. . . . . . . . . . 306
6.2.2 交叉熵的性质. . . . . . . . . . 306
6.2.3 应用――softmax 回归. . . . . . 307
6.3 Kullback-Leibler 散度. . . . . . . . . 309
6.3.1 KL 散度的定义. . . . . . . . . . 309
6.3.2 KL 散度的性质. . . . . . . . . . 311
6.3.3 与交叉熵的关系. . . . . . . . . 312
6.3.4 应用――流形降维. . . . . . . . 312
6.3.5 应用――变分推断. . . . . . . . 313
6.4 Jensen-Shannon 散度. . . . . . . . . 316
6.4.1 JS 散度的定义. . . . . . . . . . 316
6.4.2 JS 散度的性质. . . . . . . . . . 316
6.4.3 应用――生成对抗网络. . . . . 317
6.5 互信息. . . . . . . . . . . . . . . . . 320
6.5.1 互信息的定义. . . . . . . . . . 320
6.5.2 互信息的性质. . . . . . . . . . 321
6.5.3 与熵的关系. . . . . . . . . . . . 322
6.5.4 应用――特征选择. . . . . . . . 323
6.6 条件熵. . . . . . . . . . . . . . . . . 324
6.6.1 条件熵定义. . . . . . . . . . . . 324
6.6.2 条件熵的性质. . . . . . . . . . 325
6.6.3 与熵以及互信息的关系. . . . . 325
6.7 总结. . . . . . . . . . . . . . . . . . 326
第7 章随机过程328
7.1 马尔可夫过程. . . . . . . . . . . . . 328
7.1.1 马尔可夫性. . . . . . . . . . . . 329
7.1.2 马尔可夫链的基本概念. . . . . 330
7.1.3 状态的性质与分类. . . . . . . . 333
7.1.4 平稳分布与极限分布. . . . . . 337
7.1.5 细致平衡条件. . . . . . . . . . 342
7.1.6 应用――隐马尔可夫模型. . . . 343
7.1.7 应用――强化学习. . . . . . . . 345
7.2 马尔可夫链采样算法. . . . . . . . . 348
7.2.1 基本马尔可夫链采样. . . . . . 349
7.2.2 MCMC 采样算法. . . . . . . . . 349
7.2.3 Metropolis-Hastings 算法. . . . . 351
7.2.4 Gibbs 算法. . . . . . . . . . . . 353
7.3 高斯过程. . . . . . . . . . . . . . . . 355
7.3.1 高斯过程性质. . . . . . . . . . 355
7.3.2 高斯过程回归. . . . . . . . . . 355
7.3.3 应用――贝叶斯优化. . . . . . . 358
第8 章图论363
8.1 图的基本概念. . . . . . . . . . . . . 363
8.1.1 基本概念. . . . . . . . . . . . . 363
8.1.2 应用――计算图与自动微分. . . 365
8.1.3 应用――概率图模型. . . . . . . 370
8.1.4 邻接矩阵与加权度矩阵. . . . . 371
8.1.5 应用――样本集的相似度图. . . 372
8.2 若干特殊的图. . . . . . . . . . . . . 373
8.2.1 联通图. . . . . . . . . . . . . . 373
8.2.2 二部图. . . . . . . . . . . . . . 374
8.2.3 应用――受限玻尔兹曼机. . . . 374
8.2.4 有向无环图. . . . . . . . . . . . 376
8.2.5 应用――神经结构搜索. . . . . 376
8.3 重要的算法. . . . . . . . . . . . . . 380
8.3.1 遍历算法. . . . . . . . . . . . . 380
8.3.2 最短路径算法. . . . . . . . . . 381
8.3.3 拓扑排序算法. . . . . . . . . . 382
8.4 谱图理论. . . . . . . . . . . . . . . . 384
8.4.1 拉普拉斯矩阵. . . . . . . . . . 385
8.4.2 归一化拉普拉斯矩阵. . . . . . 388
8.4.3 应用――流形降维. . . . . . . . 390
・ ・ ・ ・ ・ ・ (收起)绪论　机器学习概述　　1
第1章　机器学习的构成要素　　9
1.1　任务：可通过机器学习解决的问题　　9
1.1.1　探寻结构　　11
1.1.2　性能评价　　13
1.2　模型：机器学习的输出　　14
1.2.1　几何模型　　14
1.2.2　概率模型　　17
1.2.3　逻辑模型　　22
1.2.4　分组模型与评分模型　　26
1.3　特征：机器学习的马达　　26
1.3.1　特征的两种用法　　28
1.3.2　特征的构造与变换　　29
1.3.3　特征之间的交互　　32
1.4　总结与展望　　33
第2章　两类分类及相关任务　　37
2.1　分类　　39
2.1.1　分类性能的评价　　40
2.1.2　分类性能的可视化　　43
2.2　评分与排序　　46
2.2.1　排序性能的评价及可视化　　48
2.2.2　将排序器转化为分类器　　52
2.3　类概率估计　　54
2.3.1　类概率估计量　　55
2.3.2　将排序器转化为概率估计子　　57
2.4　小结与延伸阅读　　59
第3章　超越两类分类　　61
3.1　处理多类问题　　61
3.1.1　多类分类　　61
3.1.2　多类得分及概率　　65
3.2　回归　　68
3.3　无监督学习及描述性学习　　70
3.3.1　预测性聚类与描述性聚类　　71
3.2.2　其他描述性模型　　74
3.4　小结与延伸阅读　　76
第4章　概念学习　　77
4.1　假设空间　　78
4.1.1　最小一般性　　79
4.1.2　内部析取　　82
4.2　通过假设空间的路径　　84
4.2.1　最一般相容假设　　86
4.2.2　封闭概念　　87
4.3　超越合取概念　　88
4.4　可学习性　　92
4.5　小结与延伸阅读　　94
第5章　树模型　　97
5.1　决策树　　100
5.2　排序与概率估计树　　103
5.3　作为减小方差的树学习方法　　110
5.3.1　回归树　　110
5.3.2　聚类树　　113
5.4　小结与延伸阅读　　115
第6章　规则模型　　117
6.1　学习有序规则列表　　117
6.2　学习无序规则集　　124
6.2.1　用于排序和概率估计的规则集　　128
6.2.2　深入探究规则重叠　　130
6.3　描述性规则学习　　131
6.3.1　用于子群发现的规则学习　　131
6.3.2　关联规则挖掘　　135
6.4　一阶规则学习　　139
6.5　小结与延伸阅读　　143
第7章　线性模型　　145
7.1　最小二乘法　　146
7.1.1　多元线性回归　　150
7.1.2　正则化回归　　153
7.1.3　利用最小二乘回归实现分类　　153
7.2　感知机　　155
7.3　支持向量机　　158
7.4　从线性分类器导出概率　　164
7.5　超越线性的核方法　　168
7.6　小结与延伸阅读　　170
第8章　基于距离的模型　　173
8.1　距离测度的多样性　　173
8.2　近邻与范例　　178
8.3　最近邻分类器　　182
8.4　基于距离的聚类　　184
8.4.1　K均值算法　　186
8.4.2　K中心点聚类　　187
8.4.3　silhouette　　188
8.5　层次聚类　　190
8.6　从核函数到距离　　194
8.7　小结与延伸阅读　　195
第9章　概率模型　　197
9.1　正态分布及其几何意义　　200
9.2　属性数据的概率模型　　205
9.2.1　利用朴素贝叶斯模型实现分类　　206
9.2.2　训练朴素贝叶斯模型　　209
9.3　通过优化条件似然实现鉴别式学习　　211
9.4　含隐变量的概率模型　　214
9.4.1　期望最大化算法　　215
9.4.2　高斯混合模型　　216
9.5　基于压缩的模型　　218
9.6　小结与延伸阅读　　220
第10章　特征　　223
10.1　特征的类型　　223
10.1.1　特征上的计算　　223
10.1.2　属性特征、有序特征及数量特征　　227
10.1.3　结构化特征　　228
10.2　特征变换　　229
10.2.1　阈值化与离散化　　229
10.2.2　归一化与标定　　234
10.2.3　特征缺失　　239
10.3　特征的构造与选择　　240
10.4　小结与延伸阅读　　243
第11章　模型的集成　　245
11.1　Bagging与随机森林　　246
11.2　Boosting　　247
11.3　集成学习进阶　　250
11.3.1　偏差、方差及裕量　　250
11.3.2　其他集成方法　　251
11.3.3　元学习　　252
11.4　小结与延伸阅读　　252
第12章　机器学习的实验　　255
12.1　度量指标的选择　　256
12.2　量指标的获取　　258
12.3　如何解释度量指标　　260
12.4　小结与延伸阅读　　264
后记　路在何方　　267
记忆要点　　269
参考文献　　271
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
缩写和符号
术语
第0章 导言1
0.1 什么是神经网络1
0.2 人类大脑4
0.3 神经元模型7
0.4 被看作有向图的神经网络10
0.5 反馈11
0.6 网络结构13
0.7 知识表示14
0.8 学习过程20
0.9 学习任务22
0.10 结束语27
注释和参考文献27
第1章 Rosenblatt感知器28
1.1 引言28
1.2 感知器28
1.3 感知器收敛定理29
1.4 高斯环境下感知器与贝叶斯分类器的关系33
1.5 计算机实验：模式分类36
1.6 批量感知器算法38
1.7 小结和讨论39
注释和参考文献39
习题40
第2章 通过回归建立模型28
2.1 引言41
2.2 线性回归模型：初步考虑41
2.3 参数向量的最大后验估计42
2.4 正则最小二乘估计和MAP估计之间的关系46
2.5 计算机实验：模式分类47
2.6 最小描述长度原则48
2.7 固定样本大小考虑50
2.8 工具变量方法53
2.9 小结和讨论54
注释和参考文献54
习题55
第3章 最小均方算法56
3.1 引言56
3.2 LMS算法的滤波结构56
3.3 无约束最优化：回顾58
3.4 维纳滤波器61
3.5 最小均方算法63
3.6 用马尔可夫模型来描画LMS算法和维纳滤波器的偏差64
3.7 朗之万方程：布朗运动的特点65
3.8 Kushner直接平均法66
3.9 小学习率参数下统计LMS学习理论67
3.10 计算机实验Ⅰ：线性预测68
3.11 计算机实验Ⅱ：模式分类69
3.12 LMS算法的优点和局限71
3.13 学习率退火方案72
3.14 小结和讨论73
注释和参考文献74
习题74
第4章 多层感知器77
4.1 引言77
4.2 一些预备知识78
4.3 批量学习和在线学习79
4.4 反向传播算法81
4.5 异或问题89
4.6 改善反向传播算法性能的试探法90
4.7 计算机实验：模式分类94
4.8 反向传播和微分95
4.9 Hessian矩阵及其在在线学习中的规则96
4.10 学习率的最优退火和自适应控制98
4.11 泛化102
4.12 函数逼近104
4.13 交叉验证107
4.14 复杂度正则化和网络修剪109
4.15 反向传播学习的优点和局限113
4.16 作为最优化问题看待的监督学习117
4.17 卷积网络126
4.18 非线性滤波127
4.19 小规模和大规模学习问题131
4.20 小结和讨论136
注释和参考文献137
习题138
第5章 核方法和径向基函数网络144
5.1 引言144
5.2 模式可分性的Cover定理144
5.3 插值问题148
5.4 径向基函数网络150
5.5 K-均值聚类152
5.6 权向量的递归最小二乘估计153
5.7 RBF网络的混合学习过程156
5.8 计算机实验：模式分类157
5.9 高斯隐藏单元的解释158
5.10 核回归及其与RBF网络的关系160
5.11 小结和讨论162
注释和参考文献164
习题165
第6章 支持向量机168
6.1 引言168
6.2 线性可分模式的最优超平面168
6.3 不可分模式的最优超平面173
6.4 使用核方法的支持向量机176
6.5 支持向量机的设计178
6.6 XOR问题179
6.7 计算机实验:模式分类181
6.8 回归：鲁棒性考虑184
6.9 线性回归问题的最优化解184
6.10 表示定理和相关问题187
6.11 小结和讨论191
注释和参考文献192
习题193
第7章 正则化理论197
7.1 引言197
7.2 良态问题的Hadamard条件198
7.3 Tikhonov正则化理论198
7.4 正则化网络205
7.5 广义径向基函数网络206
7.6 再论正则化最小二乘估计209
7.7 对正则化的附加要点211
7.8 正则化参数估计212
7.9 半监督学习215
7.10 流形正则化：初步的考虑216
7.11 可微流形217
7.12 广义正则化理论220
7.13 光谱图理论221
7.14 广义表示定理222
7.15 拉普拉斯正则化最小二乘算法223
7.16 用半监督学习对模式分类的实验225
7.17 小结和讨论227
注释和参考文献228
习题229
第8章 主分量分析232
8.1 引言232
8.2 自组织原则232
8.3 自组织的特征分析235
8.4 主分量分析：扰动理论235
8.5 基于Hebb的最大特征滤波器241
8.6 基于Hebb的主分量分析247
8.7 计算机实验：图像编码251
8.8 核主分量分析252
8.9 自然图像编码中的基本问题256
8.10 核Hebb算法257
8.11 小结和讨论260
注释和参考文献262
习题264
第9章 自组织映射268
9.1 引言268
9.2 两个基本的特征映射模型269
9.3 自组织映射270
9.4 特征映射的性质275
9.5 计算机实验Ⅰ：利用SOM解网格动力学问题280
9.6 上下文映射281
9.7 分层向量量化283
9.8 核自组织映射285
9.9 计算机实验Ⅱ：利用核SOM解点阵动力学问题290
9.10 核SOM和相对熵之间的关系291
9.11 小结和讨论293
注释和参考文献294
习题295
第10章 信息论学习模型299
10.1 引言299
10.2 熵300
10.3 最大熵原则302
10.4 互信息304
10.5 相对熵306
10.6 系词308
10.7 互信息作为最优化的目标函数310
10.8 最大互信息原则311
10.9 最大互信息和冗余减少314
10.10 空间相干特征316
10.11 空间非相干特征318
10.12 独立分量分析320
10.13 自然图像的稀疏编码以及与ICA编码的比较324
10.14 独立分量分析的自然梯度学习326
10.15 独立分量分析的最大似然估计332
10.16 盲源分离的最大熵学习334
10.17 独立分量分析的负熵最大化337
10.18 相关独立分量分析342
10.19 速率失真理论和信息瓶颈347
10.20 数据的最优流形表达350
10.21 计算机实验：模式分类354
10.22 小结和讨论354
注释和参考文献356
习题361
第11章 植根于统计力学的随机方法366
11.1 引言366
11.2 统计力学367
11.3 马尔可夫链368
11.4 Metropolis算法374
11.5 模拟退火375
11.6 Gibbs抽样377
11.7 Boltzmann机378
11.8 logistic信度网络382
11.9 深度信度网络383
11.10 确定性退火385
11.11 和EM算法的类比389
11.12 小结和讨论390
注释和参考文献390
习题392
第12章 动态规划396
12.1 引言396
12.2 马尔可夫决策过程397
12.3 Bellman最优准则399
12.4 策略迭代401
12.5 值迭代402
12.6 逼近动态规划：直接法406
12.7 时序差分学习406
12.8 Q学习410
12.9 逼近动态规划：非直接法412
12.10 最小二乘策略评估414
12.11 逼近策略迭代417
12.12 小结和讨论419
注释和参考文献421
习题422
第13章 神经动力学425
13.1 引言425
13.2 动态系统426
13.3 平衡状态的稳定性428
13.4 吸引子432
13.5 神经动态模型433
13.6 作为递归网络范例的吸引子操作435
13.7 Hopfield模型435
13.8 Cohen-Grossberg定理443
13.9 盒中脑状态模型445
13.10 奇异吸引子和混沌448
13.11 混沌过程的动态重构452
13.12 小结和讨论455
注释和参考文献457
习题458
第14章 动态系统状态估计的贝叶斯滤波461
14.1 引言461
14.2 状态空间模型462
14.3 卡尔曼滤波器464
14.4 发散现象及平方根滤波469
14.5 扩展的卡尔曼滤波器474
14.6 贝叶斯滤波器477
14.7 数值积分卡尔曼滤波器:基于卡尔曼滤波器480
14.8 粒子滤波器484
14.9 计算机实验：扩展的卡尔曼滤波器和粒子滤波器对比评价490
14.10 大脑功能建模中的
卡尔曼滤波493
14.11 小结和讨论494
注释和参考文献496
习题497
第15章 动态驱动递归网络501
15.1 引言501
15.2 递归网络体系结构502
15.3 通用逼近定理505
15.4 可控性和可观测性507
15.5 递归网络的计算能力510
15.6 学习算法511
15.7 通过时间的反向传播512
15.8 实时递归学习515
15.9 递归网络的消失梯度519
15.10 利用非线性逐次状态估计的递归网络监督学习框架521
15.11 计算机实验：Mackay-Glass吸引子的动态重构526
15.12 自适应考虑527
15.13 实例学习：应用于神经控制的模型参考529
15.14 小结和讨论530
注释和参考文献533
习题534
参考文献538
・ ・ ・ ・ ・ ・ (收起)推荐序
译者序
前言
致谢
关于技术评审人
第1章　机器学习简介 1
1.1　机器学习的起源 2
1.2　机器学习的使用与滥用 3
1.3　机器如何学习 5
1.3.1　抽象化和知识表达 6
1.3.2　一般化 7
1.3.3　评估学习的成功性 9
1.4　将机器学习应用于数据中的步骤 9
1.5　选择机器学习算法 10
1.5.1　考虑输入的数据 10
1.5.2　考虑机器学习算法的类型 11
1.5.3　为数据匹配合适的算法 13
1.6　使用R进行机器学习 13
1.7　总结 17
第2章　数据的管理和理解 18
2.1　R数据结构 18
2.2　向量 19
2.3　因子 20
2.3.1　列表 21
2.3.2　数据框 22
2.3.3　矩阵和数组 24
2.4　用R管理数据 25
2.4.1　保存和加载R数据结构 25
2.4.2　用CSV文件导入和保存数据 26
2.4.3　从SQL数据库导入数据 27
2.5　探索和理解数据 28
2.5.1　探索数据的结构 29
2.5.2　探索数值型变量 29
2.5.3　探索分类变量 37
2.5.4　探索变量之间的关系 39
2.6　总结 42
第3章　懒惰学习――使用近邻分类 44
3.1　理解使用近邻进行分类 45
3.1.1　kNN算法 45
3.1.2　为什么kNN算法是懒惰的 51
3.2　用kNN算法诊断乳腺癌 51
3.2.1　第1步――收集数据 51
3.2.2　第2步――探索和准备数据 52
3.2.3　第3步――基于数据训练模型 55
3.2.4　第4步――评估模型的性能 57
3.2.5　第5步――提高模型的性能 58
3.3　总结 60
第4章　概率学习――朴素贝叶斯分类 61
4.1　理解朴素贝叶斯 61
4.1.1　贝叶斯方法的基本概念 62
4.1.2　朴素贝叶斯算法 65
4.2　例子――基于贝叶斯算法的手机垃圾短信过滤 70
4.2.1　第1步――收集数据 70
4.2.2　第2步――探索和准备数据 71
4.2.3　数据准备――处理和分析文本数据 72
4.2.4　第3步――基于数据训练模型 78
4.2.5　第4步――评估模型的性能 79
4.2.6　第5步――提升模型的性能 80
4.3　总结 81
第5章　分而治之――应用决策树和规则进行分类 82
5.1　理解决策树 82
5.1.1　分而治之 83
5.1.2　C5.0决策树算法 86
5.2　例子――使用C5.0决策树识别高风险银行贷款 89
5.2.1　第1步――收集数据 89
5.2.2　第2步――探索和准备数据 89
5.2.3　第3步――基于数据训练模型 92
5.2.4　第4步――评估模型的性能 95
5.2.5　第5步――提高模型的性能 95
5.3　理解分类规则 98
5.3.1　独立而治之 99
5.3.2　单规则（1R）算法 101
5.3.3　RIPPER算法 103
5.3.4　来自决策树的规则 105
5.4　例子――应用规则学习识别有毒的蘑菇 105
5.4.1　第1步――收集数据 106
5.4.2　第2步――探索和准备数据 106
5.4.3　第3步――基于数据训练模型 107
5.4.4　第4步――评估模型的性能 109
5.4.5　第5步――提高模型的性能 109
5.5　总结 111
第6章　预测数值型数据――回归方法 113
6.1　理解回归 113
6.1.1　简单线性回归 115
6.1.2　普通最小二乘估计 117
6.1.3　相关系数 118
6.1.4　多元线性回归 120
6.2　例子――应用线性回归预测医疗费用 122
6.2.1　第1步――收集数据 122
6.2.2　第2步――探索和准备数据 123
6.2.3　第3步――基于数据训练模型 127
6.2.4　第4步――评估模型的性能 129
6.2.5　第5步――提高模型的性能 130
6.3　理解回归树和模型树 133
6.4　例子――用回归树和模型树估计葡萄酒的质量 135
6.4.1　第1步――收集数据 135
6.4.2　第2步――探索和准备数据 136
6.4.3　第3步――基于数据训练模型 137
6.4.4　第4步――评估模型的性能 140
6.4.5　第5步――提高模型的性能 142
6.5　总结 144
第7章　黑箱方法――神经网络和支持向量机 146
7.1　理解神经网络 146
7.1.1　从生物神经元到人工神经元 148
7.1.2　激活函数 148
7.1.3　网络拓扑 151
7.1.4　用后向传播训练神经网络 153
7.2　用人工神经网络对混凝土的强度进行建模 154
7.2.1　第1步――收集数据 154
7.2.2　第2步――探索和准备数据 155
7.2.3　第3步――基于数据训练模型 156
7.2.4　第4步――评估模型的性能 158
7.2.5　第5步――提高模型的性能 159
7.3　理解支持向量机 160
7.3.1　用超平面分类 161
7.3.2　寻找最大间隔 161
7.3.3　对非线性空间使用核函数 164
7.4　用支持向量机进行光学字符识别 165
7.4.1　第1步――收集数据 166
7.4.2　第2步――探索和准备数据 166
7.4.3　第3步――基于数据训练模型 167
7.4.4　第4步――评估模型的性能 169
7.4.5　第5步――提高模型的性能 170
7.5　总结 171
第8章　探寻模式――基于关联规则的购物篮分析 172
8.1　理解关联规则 172
8.2　例子――用关联规则确定经常一起购买的食品杂货 176
8.2.1　第1步――收集数据 176
8.2.2　第2步――探索和准备数据 177
8.2.3　第3步――基于数据训练模型 183
8.2.4　第4步――评估模型的性能 184
8.2.5　第5步――提高模型的性能 187
8.3　总结 189
第9章　寻找数据的分组――k均值聚类 191
9.1　理解聚类 191
9.1.1　聚类――一种机器学习任务 192
9.1.2　k均值聚类算法 193
9.1.3　用k均值聚类探寻青少年市场细分 198
9.1.4　第1步――收集数据 198
9.1.5　第2步――探索和准备数据 199
9.1.6　第3步――基于数据训练模型 202
9.1.7　第4步――评估模型的性能 204
9.1.8　第5步――提高模型的性能 206
9.2　总结 207
第10章　模型性能的评价 208
10.1　度量分类方法的性能 208
10.1.1　在R中处理分类预测数据 209
10.1.2　深入探讨混淆矩阵 211
10.1.3　使用混淆矩阵度量性能 212
10.1.4　准确度之外的其他性能评价指标 214
10.1.5　性能权衡的可视化 221
10.2　评估未来的性能 224
10.2.1　保持法 225
10.2.2　交叉验证 226
10.2.3　自助法抽样 229
10.3　总结 229
第11章　提高模型的性能 231
11.1　调整多个模型来提高性能 231
11.2　使用元学习来提高模型的性能 239
11.2.1　理解集成学习 239
11.2.2　bagging 241
11.2.3　boosting 243
11.2.4　随机森林 244
11.3　总结 248
第12章　其他机器学习主题 249
12.1　分析专用数据 250
12.1.1　用RCurl添加包从网上获取数据 250
12.1.2　用XML添加包读/写XML格式数据 250
12.1.3　用rjson添加包读/写JSON 251
12.1.4　用xlsx添加包读/写Microsoft Excel电子表格 251
12.1.5　生物信息学数据 251
12.1.6　社交网络数据和图数据 252
12.2　提高R语言的性能 252
12.2.1　处理非常大的数据集 253
12.2.2　使用并行处理来加快学习过程 254
12.2.3　GPU计算 257
12.2.4　部署最优的学习算法 257
12.3　总结 258
・ ・ ・ ・ ・ ・ (收起)第1章　Python机器学习入门　　1
1.1 　梦之队：机器学习与Python　　1
1.2 　这本书将教给你什么（以及不会教什么）　　2
1.3 　遇到困难的时候怎么办　　3
1.4 　开始　　4
1.4.1 　NumPy、SciPy和Matplotlib简介　　4
1.4.2 　安装Python　　5
1.4.3 　使用NumPy和SciPy智能高效地处理数据　　5
1.4.4 　学习NumPy　　5
1.4.5 　学习SciPy　　9
1.5 　我们第一个（极小的）机器学习应用　　10
1.5.1 　读取数据　　10
1.5.2 　预处理和清洗数据　　11
1.5.3 　选择正确的模型和学习算法　　12
1.6 　小结　　20
第2章　如何对真实样本分类　　22
2.1 　Iris数据集　　22
2.1.1 　第一步是可视化　　23
2.1.2 　构建第一个分类模型　　24
2.2 　构建更复杂的分类器　　28
2.3 　更复杂的数据集和更复杂的分类器　　29
2.3.1 　从Seeds数据集中学习　　29
2.3.2 　特征和特征工程　　30
2.3.3 　最邻近分类　　30
2.4 　二分类和多分类　　33
2.5 　小结　　34
第3章　聚类：寻找相关的帖子　　35
3.1 　评估帖子的关联性　　35
3.1.1 　不应该怎样　　36
3.1.2 　应该怎样　　36
3.2 　预处理：用相近的公共词语个数来衡量相似性　　37
3.2.1 　将原始文本转化为词袋　　37
3.2.2 　统计词语　　38
3.2.3 　词语频次向量的归一化　　40
3.2.4 　删除不重要的词语　　41
3.2.5 　词干处理　　42
3.2.6 　停用词兴奋剂　　44
3.2.7 　我们的成果和目标　　45
3.3 　聚类　　46
3.3.1 　K均值　　46
3.3.2 　让测试数据评估我们的想法　　49
3.3.3 　对帖子聚类　　50
3.4 　解决我们最初的难题　　51
3.5 　调整参数　　54
3.6 　小结　　54
第4章　主题模型　　55
4.1 　潜在狄利克雷分配（LDA）　　55
4.2 　在主题空间比较相似度　　59
4.3 　选择主题个数　　64
4.4 　小结　　65
第5章　分类：检测劣质答案　　67
5.1 　路线图概述　　67
5.2 　学习如何区分出优秀的答案　　68
5.2.1 　调整样本　　68
5.2.2 　调整分类器　　68
5.3 　获取数据　　68
5.3.1 　将数据消减到可处理的程度　　69
5.3.2 　对属性进行预选择和处理　　70
5.3.3 　定义什么是优质答案　　71
5.4 　创建第一个分类器　　71
5.4.1 　从k邻近（kNN）算法开始　　71
5.4.2 　特征工程　　72
5.4.3 　训练分类器　　73
5.4.4 　评估分类器的性能　　74
5.4.5 　设计更多的特征　　74
5.5 　决定怎样提升效果　　77
5.5.1 　偏差?方差及其折中　　77
5.5.2 　解决高偏差　　78
5.5.3 　解决高方差　　78
5.5.4 　高偏差或低偏差　　78
5.6 　采用逻辑回归　　81
5.6.1 　一点数学和一个小例子　　81
5.6.2 　在帖子分类问题上应用逻辑回归　　83
5.7 　观察正确率的背后：准确率和召回率　　84
5.8 　为分类器瘦身　　87
5.9 　出货　　88
5.10 　小结　　88
第6章　分类II：情感分析　　89
6.1 　路线图概述　　89
6.2 　获取推特（Twitter）数据　　89
6.3 　朴素贝叶斯分类器介绍　　90
6.3.1 　了解贝叶斯定理　　90
6.3.2 　朴素　　91
6.3.3 　使用朴素贝叶斯进行分类　　92
6.3.4 　考虑未出现的词语和其他古怪情况　　94
6.3.5 　考虑算术下溢　　95
6.4 　创建第一个分类器并调优　　97
6.4.1 　先解决一个简单问题　　97
6.4.2 　使用所有的类　　99
6.4.3 　对分类器的参数进行调优　　101
6.5 　清洗推文　　104
6.6 　将词语类型考虑进去　　106
6.6.1 　确定词语的类型　　106
6.6.2 　用SentiWordNet成功地作弊　　108
6.6.3 　我们第一个估算器　　110
6.6.4 　把所有东西融合在一起　　111
6.7 　小结　　112
第7章　回归：推荐　　113
7.1 　用回归预测房价　　113
7.1.1 　多维回归　　116
7.1.2 　回归里的交叉验证　　116
7.2 　惩罚式回归　　117
7.2.1 　L1和L2惩罚　　117
7.2.2 　在Scikit-learn中使用Lasso或弹性网　　118
7.3 　P大于N的情形　　119
7.3.1 　基于文本的例子　　120
7.3.2 　巧妙地设置超参数（hyperparameter）　　121
7.3.3 　评分预测和推荐　　122
7.4 　小结　　126
第8章　回归：改进的推荐　　127
8.1 　改进的推荐　　127
8.1.1 　使用二值推荐矩阵　　127
8.1.2 　审视电影的近邻　　129
8.1.3 　组合多种方法　　130
8.2 　购物篮分析　　132
8.2.1 　获取有用的预测　　133
8.2.2 　分析超市购物篮　　134
8.2.3 　关联规则挖掘　　136
8.2.4 　更多购物篮分析的高级话题　　137
8.3 　小结　　138
第9章　分类III：音乐体裁分类　　139
9.1 　路线图概述　　139
9.2 　获取音乐数据　　139
9.3 　观察音乐　　140
9.4 　用FFT构建第一个分类器　　143
9.4.1 　增加实验敏捷性　　143
9.4.2 　训练分类器　　144
9.4.3 　在多分类问题中用混淆矩阵评估正确率　　144
9.4.4 　另一种方式评估分类器效果：受试者工作特征曲线（ROC）　　146
9.5 　用梅尔倒频谱系数（MFCC）提升分类效果　　148
9.6 　小结　　152
第10章　计算机视觉：模式识别　　154
10.1 　图像处理简介　　154
10.2 　读取和显示图像　　155
10.2.1 　图像处理基础　　156
10.2.2 　加入椒盐噪声　　161
10.2.3 　模式识别　　163
10.2.4 　计算图像特征　　163
10.2.5 　设计你自己的特征　　164
10.3 　在更难的数据集上分类　　166
10.4 　局部特征表示　　167
10.5 　小结　　170
第11章　降维　　171
11.1 　路线图　　171
11.2 　选择特征　　172
11.2.1 　用筛选器检测冗余特征　　172
11.2.2 　用封装器让模型选择特征　　178
11.3 　其他特征选择方法　　180
11.4 　特征抽取　　181
11.4.1 　主成分分析（PCA）　　181
11.4.2 　PCA的局限性以及LDA会有什么帮助　　183
11.5 　多维标度法（MDS）　　184
11.6 　小结　　187
第12章　大数据　　188
12.1 　了解大数据　　188
12.2 　用Jug程序包把你的处理流程分解成几个任务　　189
12.2.1 　关于任务　　189
12.2.2 　复用部分结果　　191
12.2.3 　幕后的工作原理　　192
12.2.4 　用Jug分析数据　　192
12.3 　使用亚马逊Web服务（AWS）　　194
12.3.1 　构建你的第一台机器　　195
12.3.2 　用starcluster自动创建集群　　199
12.4 　小结　　202
附录A 　更多机器学习知识　　203
A.1 　在线资源　　203
A.2 　参考书　　203
A.2.1 　问答网站　　203
A.2.2 　博客　　204
A.2.3 　数据资源　　205
A.2.4 　竞争日益加剧　　205
A.3 　还剩下什么　　205
A.4 　小结　　206
索引　　207
・ ・ ・ ・ ・ ・ (收起)第1章 引言 1
1.1 学习与智能优化：燎原之火 1
1.2 寻找黄金和寻找伴侣 3
1.3 需要的只是数据 5
1.4 超越传统的商业智能 5
1.5 LION方法的实施 6
1.6 “动手”的方法 6
第2章 懒惰学习：最近邻方法 9
第3章 学习需要方法 14
3.1 从已标记的案例中学习：最小化和泛化 16
3.2 学习、验证、测试 18
3.3 不同类型的误差 21
第一部分 监督学习
第4章 线性模型 26
4.1 线性回归 27
4.2 处理非线性函数关系的技巧 28
4.3 用于分类的线性模型 29
4.4 大脑是如何工作的 30
4.5 线性模型为何普遍，为何成功 31
4.6 最小化平方误差和 32
4.7 数值不稳定性和岭回归 34
第5章 广义线性最小二乘法 37
5.1 拟合的优劣和卡方分布 38
5.2 最小二乘法与最大似然估计 42
5.2.1 假设检验 42
5.2.2 交叉验证 44
5.3 置信度的自助法 44
第6章 规则、决策树和森林 50
6.1 构造决策树 52
6.2 民主与决策森林 56
第7章 特征排序及选择 59
7.1 特征选择：情境 60
7.2 相关系数 62
7.3 相关比 63
7.4 卡方检验拒绝统计独立性 64
7.5 熵和互信息 64
第8章 特定非线性模型 67
8.1 logistic 回归 67
8.2 局部加权回归 69
8.3 用LASSO来缩小系数和选择输入值 72
第9章 神经网络：多层感知器 76
9.1 多层感知器 78
9.2 通过反向传播法学习 80
9.2.1 批量和bold driver反向传播法 81
9.2.2 在线或随机反向传播 82
9.2.3 训练多层感知器的高级优化 83
第10章 深度和卷积网络 84
10.1 深度神经网络 85
10.1.1 自动编码器 86
10.1.2 随机噪声、屏蔽和课程 88
10.2 局部感受野和卷积网络 89
第11章 统计学习理论和支持向量机 94
11.1 经验风险最小化 96
11.1.1 线性可分问题 98
11.1.2 不可分问题 100
11.1.3 非线性假设 100
11.1.4 用于回归的支持向量 101
第12章 最小二乘法和健壮内核机器 103
12.1 最小二乘支持向量机分类器 104
12.2 健壮加权最小二乘支持向量机 106
12.3 通过修剪恢复稀疏 107
12.4 算法改进：调谐QP、原始版本、无补偿 108
第13章 机器学习中的民主 110
13.1 堆叠和融合 111
13.2 实例操作带来的多样性：装袋法和提升法 113
13.3 特征操作带来的多样性 114
13.4 输出值操作带来的多样性：纠错码 115
13.5 训练阶段随机性带来的多样性 115
13.6 加性logistic回归 115
13.7 民主有助于准确率－拒绝的折中 118
第14章 递归神经网络和储备池计算 121
14.1 递归神经网络 122
14.2 能量极小化霍普菲尔德网络 124
14.3 递归神经网络和时序反向传播 126
14.4 递归神经网络储备池学习 127
14.5 超限学习机 128
第二部分 无监督学习和聚类
第15章 自顶向下的聚类：K均值 132
15.1 无监督学习的方法 134
15.2 聚类：表示与度量 135
15.3 硬聚类或软聚类的K均值方法 137
第16章 自底向上（凝聚）聚类 142
16.1 合并标准以及树状图 142
16.2 适应点的分布距离：马氏距离 144
16.3 附录：聚类的可视化 146
第17章 自组织映射 149
17.1 将实体映射到原型的人工皮层 150
17.2 使用成熟的自组织映射进行分类 153
第18章 通过线性变换降维（投影） 155
18.1 线性投影 156
18.2 主成分分析 158
18.3 加权主成分分析：结合坐标和关系 160
18.4 通过比值优化进行线性判别 161
18.5 费希尔线性判别分析 163
第19章 通过非线性映射可视化图与网络 165
19.1 最小应力可视化 166
19.2 一维情况：谱图绘制 168
19.3 复杂图形分布标准 170
第20章 半监督学习 174
20.1 用部分无监督数据进行学习 175
20.1.1 低密度区域中的分离 177
20.1.2 基于图的算法 177
20.1.3 学习度量 179
20.1.4 集成约束和度量学习 179
第三部分 优化：力量之源
第21章 自动改进的局部方法 184
21.1 优化和学习 185
21.2 基于导数技术的一维情况 186
21.2.1 导数可以由割线近似 190
21.2.2 一维最小化 191
21.3 求解高维模型（二次正定型） 191
21.3.1 梯度与最速下降法 194
21.3.2 共轭梯度法 196
21.4 高维中的非线性优化 196
21.4.1 通过线性查找的全局收敛 197
21.4.2 解决不定黑塞矩阵 198
21.4.3 与模型信赖域方法的关系 199
21.4.4 割线法 200
21.4.5 缩小差距：二阶方法与线性复杂度 201
21.5 不涉及导数的技术：反馈仿射振荡器 202
21.5.1 RAS：抽样区域的适应性 203
21.5.2 为健壮性和多样化所做的重复 205
第22章 局部搜索和反馈搜索优化 211
22.1 基于扰动的局部搜索 212
22.2 反馈搜索优化：搜索时学习 215
22.3 基于禁忌的反馈搜索优化 217
第23章 合作反馈搜索优化 222
23.1 局部搜索过程的智能协作 223
23.2 CoRSO：一个政治上的类比 224
23.3 CoRSO的例子：RSO与RAS合作 226
第24章 多目标反馈搜索优化 232
24.1 多目标优化和帕累托最优 233
24.2 脑－计算机优化：循环中的用户 235
第四部分 应用精选
第25章 文本和网页挖掘 240
25.1 网页信息检索与组织 241
25.1.1 爬虫 241
25.1.2 索引 242
25.2 信息检索与排名 244
25.2.1 从文档到向量：向量－空间模型 245
25.2.2 相关反馈 247
25.2.3 更复杂的相似性度量 248
25.3 使用超链接来进行网页排名 250
25.4 确定中心和权威：HITS 254
25.5 聚类 256
第26章 协同过滤和推荐 257
26.1 通过相似用户结合评分 258
26.2 基于矩阵分解的模型 260
参考文献 263
索引 269
・ ・ ・ ・ ・ ・ (收起)序言一
序言二
前　言
作者介绍
第1章　绪论/ 1
1.1　人工智能及其飞速发展/ 2
1.2　大规模、分布式机器学习/ 4
1.3　本书的安排/ 6
参考文献/ 7
第2章　机器学习基础/ 9
2.1　机器学习的基本概念/ 10
2.2　机器学习的基本流程/ 13
2.3　常用的损失函数/ 16
2.3.1　Hinge损失函数/ 16
2.3.2　指数损失函数/ 16
2.3.3　交叉熵损失函数/ 17
2.4　常用的机器学习模型/ 18
2.4.1　线性模型/ 18
2.4.2　核方法与支持向量机/ 18
2.4.3　决策树与Boosting/ 21
2.4.4　神经网络/ 23
2.5　常用的优化方法/ 32
2.6　机器学习理论/ 33
2.6.1　机器学习算法的泛化误差/ 34
2.6.2　泛化误差的分解/ 34
2.6.3　基于容度的估计误差的上界/ 35
2.7　总结/ 36
参考文献/ 36
第3章　分布式机器学习框架/ 41
3.1　大数据与大模型的挑战/ 42
3.2　分布式机器学习的基本流程/ 44
3.3　数据与模型划分模块/ 46
3.4　单机优化模块/ 48
3.5　通信模块/ 48
3.5.1　通信的内容/ 48
3.5.2　通信的拓扑结构/ 49
3.5.3　通信的步调/ 51
3.5.4　通信的频率/ 52
3.6　数据与模型聚合模块/ 53
3.7　分布式机器学习理论/ 54
3.8　分布式机器学习系统/ 55
3.9　总结/ 56
参考文献/ 57
第4章　单机优化之确定性算法/ 61
4.1　基本概述/ 62
4.1.1　机器学习的优化框架/ 62
4.1.2　优化算法的分类和发展历史/ 65
4.2　一阶确定性算法/ 67
4.2.1　梯度下降法/ 67
4.2.2　投影次梯度下降法/ 69
4.2.3　近端梯度下降法/ 70
4.2.4　Frank-Wolfe算法/ 71
4.2.5　Nesterov加速法/ 72
4.2.6　坐标下降法/ 75
4.3　二阶确定性算法/ 75
4.3.1　牛顿法/ 76
4.3.2　拟牛顿法/ 77
4.4　对偶方法/ 78
4.5　总结/ 81
参考文献/ 8
第5章　单机优化之随机算法/ 85
5.1　基本随机优化算法/ 86
5.1.1　随机梯度下降法/ 86
5.1.2　随机坐标下降法/ 88
5.1.3　随机拟牛顿法/ 91
5.1.4　随机对偶坐标上升法/ 93
5.1.5　小结/ 95
5.2　随机优化算法的改进/ 96
5.2.1　方差缩减方法/ 96
5.2.2　算法组合方法/ 100
5.3　非凸随机优化算法/ 101
5.3.1　Ada系列算法/ 102
5.3.2　非凸理论分析/ 104
5.3.3　逃离鞍点问题/ 106
5.3.4　等级优化算法/ 107
5.4　总结/ 109
参考文献/ 109
第6章　数据与模型并行/ 113
6.1　基本概述/ 114
6.2　计算并行模式/ 117
6.3　数据并行模式/ 119
6.3.1　数据样本划分/ 120
6.3.2　数据维度划分/ 123
6.4　模型并行模式/ 123
6.4.1　线性模型/ 123
6.4.2　神经网络/ 127
6.5　总结/ 133
参考文献/ 133
第7章　通信机制/ 135
7.1　基本概述/ 136
7.2　通信的内容/ 137
7.2.1　参数或参数的更新/ 137
7.2.2　计算的中间结果/ 137
7.2.3　讨论/ 138
7.3　通信的拓扑结构/ 139
7.3.1　基于迭代式MapReduce/AllReduce的通信拓扑/ 140
7.3.2　基于参数服务器的通信拓扑/ 142
7.3.3　基于数据流的通信拓扑/ 143
7.3.4　讨论/ 145
7.4　通信的步调/ 145
7.4.1　同步通信/ 146
7.4.2　异步通信/ 147
7.4.3　同步和异步的平衡/ 148
7.4.4　讨论/ 150
7.5　通信的频率/ 150
7.5.1　时域滤波/ 150
7.5.2　空域滤波/ 153
7.5.3　讨论/ 155
7.6　总结/ 156
参考文献/ 156
第8章　数据与模型聚合/ 159
8.1　基本概述/ 160
8.2　基于模型加和的聚合方法/ 160
8.2.1　基于全部模型加和的聚合/ 160
8.2.2　基于部分模型加和的聚合/ 162
8.3　基于模型集成的聚合方法/ 167
8.3.1　基于输出加和的聚合/ 168
8.3.2　基于投票的聚合/ 171
8.4　总结/ 174
参考文献/ 174
第9章　分布式机器学习算法/ 177
9.1　基本概述/ 178
9.2　同步算法/ 179
9.2.1　同步SGD方法/ 179
9.2.2　模型平均方法及其改进/ 182
9.2.3　ADMM算法/ 183
9.2.4　弹性平均SGD算法/ 185
9.2.5　讨论/ 186
9.3　异步算法/ 187
9.3.1　异步SGD/ 187
9.3.2　Hogwild!算法/ 189
9.3.3　Cyclades算法/ 190
9.3.4　带延迟处理的异步算法/ 192
9.3.5　异步方法的进一步加速/ 199
9.3.6　讨论/ 199
9.4　同步和异步的对比与融合/ 199
9.4.1　同步和异步算法的实验对比/ 199
9.4.2　同步和异步的融合/ 201
9.5　模型并行算法/ 203
9.5.1　DistBelief/ 203
9.5.2　AlexNet/ 204
9.6　总结/ 205
参考文献/ 205
第10章　分布式机器学习理论/ 209
10.1　基本概述/ 210
10.2　收敛性分析/ 210
10.2.1　优化目标和算法/ 211
10.2.2　数据和模型并行/ 213
10.2.3　同步和异步/ 215
10.3　加速比分析/ 217
10.3.1　从收敛速率到加速比/ 218
10.3.2　通信量的下界/ 219
10.4　泛化分析/ 221
10.4.1　优化的局限性/ 222
10.4.2　具有更好泛化能力的非凸优化算法/ 224
10.5　总结/ 226
参考文献/ 226
第11章　分布式机器学习系统/ 229
11.1　基本概述/ 230
11.2　基于IMR的分布式机器学习系统/ 231
11.2.1　IMR和Spark/ 231
11.2.2　Spark MLlib/ 234
11.3　基于参数服务器的分布式机器学习系统/ 236
11.3.1　参数服务器/ 236
11.3.2　Multiverso参数服务器/ 237
11.4　基于数据流的分布式机器学习系统/ 241
11.4.1　数据流/ 241
11.4.2　TensorFlow数据流系统/ 243
11.5　实战比较/ 248
11.6　总结/ 252
参考文献/ 252
第12章　结语/ 255
12.1　全书总结/ 256
12.2　未来展望/ 257
索引/ 260
・ ・ ・ ・ ・ ・ (收起)前言1
第一部分 机器学习基础
第1章 机器学习概览11
什么是机器学习12
为什么要使用机器学习12
机器学习系统的种类15
监督式/无监督式学习16
批量学习和在线学习21
基于实例与基于模型的学习24
机器学习的主要挑战29
训练数据的数量不足29
训练数据不具代表性30
质量差的数据32
无关特征32
训练数据过度拟合33
训练数据拟合不足34
退后一步35
测试与验证35
练习37
第2章 端到端的机器学习项目39
使用真实数据39
观察大局40
框架问题41
选择性能指标42
检查假设45
获取数据45
创建工作区45
下载数据48
快速查看数据结构49
创建测试集52
从数据探索和可视化中获得洞见56
将地理数据可视化57
寻找相关性59
试验不同属性的组合61
机器学习算法的数据准备62
数据清理63
处理文本和分类属性65
自定义转换器67
特征缩放68
转换流水线68
选择和训练模型70
培训和评估训练集70
使用交叉验证来更好地进行评估72
微调模型74
网格搜索74
随机搜索76
集成方法76
分析最佳模型及其错误76
通过测试集评估系统77
启动、监控和维护系统78
试试看79
练习79
第3章 分类80
MNIST80
训练一个二元分类器82
性能考核83
使用交叉验证测量精度83
混淆矩阵84
精度和召回率86
精度/召回率权衡87
ROC曲线90
多类别分类器93
错误分析95
多标签分类98
多输出分类99
练习100
第4章 训练模型102
线性回归103
标准方程104
计算复杂度106
梯度下降107
批量梯度下降110
随机梯度下降112
小批量梯度下降114
多项式回归115
学习曲线117
正则线性模型121
岭回归121
套索回归123
弹性网络125
早期停止法126
逻辑回归127
概率估算127
训练和成本函数128
决策边界129
Softmax回归131
练习134
第5章 支持向量机136
线性SVM分类136
软间隔分类137
非线性SVM分类139
多项式核140
添加相似特征141
高斯RBF核函数142
计算复杂度143
SVM回归144
工作原理145
决策函数和预测146
训练目标146
二次规划148
对偶问题149
核化SVM149
在线SVM151
练习152
第6章 决策树154
决策树训练和可视化154
做出预测155
估算类别概率157
CART训练算法158
计算复杂度158
基尼不纯度还是信息熵159
正则化超参数159
回归161
不稳定性162
练习163
第7章 集成学习和随机森林165
投票分类器165
bagging和pasting168
Scikit-Learn的bagging和pasting169
包外评估170
Random Patches和随机子空间171
随机森林172
极端随机树173
特征重要性173
提升法174
AdaBoost175
梯度提升177
堆叠法181
练习184
第8章 降维185
维度的诅咒186
数据降维的主要方法187
投影187
流形学习189
PCA190
保留差异性190
主成分191
低维度投影192
使用Scikit-Learn192
方差解释率193
选择正确数量的维度193
PCA压缩194
增量PCA195
随机PCA195
核主成分分析196
选择核函数和调整超参数197
局部线性嵌入199
其他降维技巧200
练习201
第二部分 神经网络和深度学习
第9章 运行TensorFlow205
安装207
创建一个计算图并在会话中执行208
管理图209
节点值的生命周期210
TensorFlow中的线性回归211
实现梯度下降211
手工计算梯度212
使用自动微分212
使用优化器214
给训练算法提供数据214
保存和恢复模型215
用TensorBoard来可视化图和训练曲线216
命名作用域219
模块化220
共享变量222
练习225
第10章 人工神经网络简介227
从生物神经元到人工神经元227
生物神经元228
具有神经元的逻辑计算229
感知器230
多层感知器和反向传播233
用TensorFlow的高级API来训练MLP236
使用纯TensorFlow训练DNN237
构建阶段237
执行阶段240
使用神经网络241
微调神经网络的超参数242
隐藏层的个数242
每个隐藏层中的神经元数243
激活函数243
练习244
第11章 训练深度神经网络245
梯度消失/爆炸问题245
Xavier初始化和He初始化246
非饱和激活函数248
批量归一化250
梯度剪裁254
重用预训练图层255
重用TensorFlow模型255
重用其他框架的模型256
冻结低层257
缓存冻结层257
调整、丢弃或替换高层258
模型动物园258
无监督的预训练259
辅助任务中的预训练260
快速优化器261
Momentum优化261
Nesterov梯度加速262
AdaGrad263
RMSProp265
Adam优化265
学习速率调度267
通过正则化避免过度拟合269
提前停止269
1和2正则化269
dropout270
最大范数正则化273
数据扩充274
实用指南275
练习276
第12章 跨设备和服务器的分布式TensorFlow279
一台机器上的多个运算资源280
安装280
管理GPU RAM282
在设备上操作284
并行执行287
控制依赖288
多设备跨多服务器288
开启一个会话290
master和worker服务290
分配跨任务操作291
跨多参数服务器分片变量291
用资源容器跨会话共享状态292
使用TensorFlow队列进行异步通信294
直接从图中加载数据299
在TensorFlow集群上并行化神经网络305
一台设备一个神经网络305
图内与图间复制306
模型并行化308
数据并行化309
练习314
第13章 卷积神经网络31
・ ・ ・ ・ ・ ・ (收起)译者序
前言
第1章　引言1
1.1　应用与问题1
1.2　定义与术语2
1.3　交叉验证4
1.4　学习情境5
1.5　本书概览6
第2章　PAC学习框架8
2.1　PAC学习模型8
2.2　对有限假设集的学习保证――一致的情况12
2.3　对有限假设集的学习保证――不一致的情况16
2.4　泛化性18
2.4.1　确定性与随机性情境18
2.4.2　贝叶斯误差与噪声19
2.4.3　估计误差与近似误差19
2.4.4　模型选择20
2.5　文献评注21
2.6　习题22
第3章　Rademacher复杂度和VC-维25
3.1　Rademacher复杂度25
3.2　生长函数29
3.3　VC-维31
3.4　下界36
3.5　文献评注41
3.6　习题42
第4章　支持向量机47
4.1　线性分类47
4.2　可分情况下的支持向量机48
4.2.1　原始优化问题48
4.2.2　支持向量49
4.2.3　对偶优化问题50
4.2.4　留一法51
4.3　不可分情况下的支持向量机52
4.3.1　原始优化问题53
4.3.2　支持向量54
4.3.3　对偶优化问题55
4.4　间隔理论56
4.5　文献评注62
4.6　习题62
第5章　核方法65
5.1　引言65
5.2　正定对称核67
5.2.1　定义67
5.2.2　再生核希尔伯特空间69
5.2.3　性质70
5.3　基于核的算法73
5.3.1　具有PDS核的SVM73
5.3.2　表示定理74
5.3.3　学习保证75
5.4　负定对称核76
5.5　序列核78
5.5.1　加权转换器79
5.5.2　有理核82
5.6　文献评注85
5.7　习题85
第6章　boosting89
6.1　引言89
6.2　AdaBoost算法90
6.2.1　经验误差的界92
6.2.2　与坐标下降的关系93
6.2.3　与逻辑回归的关系94
6.2.4　实践中的标准使用方式95
6.3　理论结果95
6.3.1　基于VC-维的分析96
6.3.2　基于间隔的分析96
6.3.3　间隔最大化100
6.3.4　博弈论解释101
6.4　讨论103
6.5　文献评注104
6.6　习题105
第7章　在线学习108
7.1　引言108
7.2　有专家建议的预测109
7.2.1　错误界和折半算法109
7.2.2　加权多数算法110
7.2.3　随机加权多数算法111
7.2.4　指数加权平均算法114
7.3　线性分类117
7.3.1　感知机算法117
7.3.2　Winnow算法122
7.4　在线到批处理的转换124
7.5　与博弈论的联系127
7.6　文献评注127
7.7　习题128
第8章　多分类133
8.1　多分类问题133
8.2　泛化界134
8.3　直接型多分类算法139
8.3.1　多分类SVM139
8.3.2　多分类boosting算法140
8.3.3　决策树141
8.4　类别分解型多分类算法144
8.4.1　一对多144
8.4.2　一对一145
8.4.3　纠错编码146
8.5　结构化预测算法148
8.6　文献评注149
8.7　习题150
第9章　排序152
9.1　排序问题152
9.2　泛化界153
9.3　使用SVM进行排序155
9.4　RankBoost156
9.4.1　经验误差界158
9.4.2　与坐标下降的关系159
9.4.3　排序问题集成算法的间隔界160
9.5　二部排序161
9.5.1　二部排序中的boosting算法162
9.5.2　ROC曲线下面积164
9.6　基于偏好的情境165
9.6.1　两阶段排序问题166
9.6.2　确定性算法167
9.6.3　随机性算法168
9.6.4　关于其他损失函数的扩展168
9.7　讨论169
9.8　文献评注170
9.9　习题171
第10章　回归172
10.1　回归问题172
10.2　泛化界173
10.2.1　有限假设集173
10.2.2　Rademacher复杂度界174
10.2.3　伪维度界175
10.3　回归算法177
10.3.1　线性回归178
10.3.2　核岭回归179
10.3.3　支持向量回归182
10.3.4　Lasso186
10.3.5　组范数回归算法188
10.3.6　在线回归算法189
10.4　文献评注190
10.5　习题190
第11章　算法稳定性193
11.1　定义193
11.2　基于稳定性的泛化保证194
11.3　基于核的正则化算法的稳定性196
11.3.1　应用于回归算法：SVR和KRR198
11.3.2　应用于分类算法：SVM200
11.3.3　讨论200
11.4　文献评述201
11.5　习题201
第12章　降维203
12.1　主成分分析204
12.2　核主成分分析205
12.3　KPCA和流形学习206
12.3.1　等距映射206
12.3.2　拉普拉斯特征映射207
12.3.3　局部线性嵌入207
12.4　Johnson-Lindenstrauss引理208
12.5　文献评注210
12.6　习题210
第13章　学习自动机和语言212
13.1　引言212
13.2　有限自动机213
13.3　高效精确学习214
13.3.1　被动学习214
13.3.2　通过查询学习215
13.3.3　通过查询学习自动机216
13.4　极限下的识别220
13.5　文献评注224
13.6　习题225
第14章　强化学习227
14.1　学习情境227
14.2　马尔可夫决策过程模型228
14.3　策略229
14.3.1　定义229
14.3.2　策略值229
14.3.3　策略评估230
14.3.4　最优策略230
14.4　规划算法231
14.4.1　值迭代231
14.4.2　策略迭代233
14.4.3　线性规划235
14.5　学习算法235
14.5.1　随机逼近236
14.5.2　TD（0）算法239
14.5.3　Q-学习算法240
14.5.4　SARSA242
14.5.5　TD（λ）算法242
14.5.6　大状态空间243
14.6　文献评注244
结束语245
附录A　线性代数回顾246
附录B　凸优化251
附录C　概率论回顾257
附录D　集中不等式264
附录E　符号273
索引274
参考文献
・ ・ ・ ・ ・ ・ (收起)序言
第 一章 机器学习概述 1
1．1 机器学习简介 1
1．1．1 机器学习简史 1
1．1．2 机器学习主要流派 2
1．2 机器学习、人工智能和数据挖掘 4
1．2．1 什么是人工智能 4
1．2．2 机器学习、人工智能与数据挖掘 5
1．3 典型机器学习应用领域 5
1．4 机器学习算法 12
1．5 机器学习的一般流程 20
第 二章 机器学习基本方法 23
2．1 统计分析 23
2．1．1 统计基础 23
2．1．2 常见概率分布 29
2．1．3 参数估计 31
2．1．4 假设检验 33
2．1．5 线性回归 33
2．1．6 Logistics回归 37
2．1．7 判别分析 38
2．1．8 非线性模型 39
2．2 高维数据降维 40
2．2．1 主成分分析 40
2．2．2 线性判别分析 43
2．2．3 局部线性嵌入 47
2．3 特征工程 48
2．3．1 特征构造 48
2．3．2 特征选择 49
2．3．3 特征提取 50
2．4 模型训练 50
2．4．1 模型训练常见术语 50
2．4．2 训练数据收集 51
2．5 可视化分析 52
2．5．1 可视化分析的作用 52
2．5．2 可视化分析方法 53
2．5．3 可视化分析常用工具 54
2．5．4 常见的可视化图表 56
2．5．5 可视化分析面临的挑战 62
第三章 决策树与分类算法 64
3．1 决策树算法 64
3．1．1 分支处理 66
3．1．2 连续属性离散化 72
3．1．3 过拟合问题 74
3．1．4 分类效果评价 78
3．2 集成学习 83
3．2．1 装袋法 83
3．2．2 提升法 84
3．2．3 GBDT 86
3．2．4 随机森林 87
3．3 决策树应用 89
第四章 聚类分析 95
4．1 聚类分析概念 95
4．1．1 聚类方法分类 95
4．1．2 良好聚类算法的特征 97
4．2 聚类分析的度量 97
4．2．1 外部指标 98
4．2．2 内部指标 99
4．3 基于划分的方法 101
4．3．1 k-均值算法 101
4．3．2 k-medoids算法 106
4．3．3 k-prototype算法 107
4．4 基于密度聚类 107
4．4．1 DBSCAN算法 108
4．4．2 OPTICS算法 110
4．4．3 DENCLUE算法 111
4．5 基于层次的聚类 116
4．5．1 BIRCH聚类 117
4．5．2 CURE算法 120
4．6 基于网格的聚类 122
4．7 基于模型的聚类 123
4．7．1 概率模型聚类 123
4．7．2 模糊聚类 129
4．7．3 Kohonen神经网络聚类 129
第五章 文本分析 137
5．1 文本分析介绍 137
5．2 文本特征提取及表示 138
5．2．1 TF-IDF 138
5．2．2 信息增益 139
5．2．3 互信息 139
5．2．4 卡方统计量 140
5．2．5 词嵌入 141
5．2．6 语言模型 142
5．2．7 向量空间模型 144
5．3 知识图谱 146
5．3．1 知识图谱相关概念 147
5．3．2 知识图谱的存储 147
5．3．3 知识图谱挖掘与计算 148
5．3．4 知识图谱的构建过程 150
5．4 词法分析 155
5．4．1 文本分词 156
5．4．2 命名实体识别 159
5．4．3 词义消歧 160
5．5 句法分析 161
5．6 语义分析 163
5．7 文本分析应用 164
5．7．1 文本分类 164
5．7．2 信息抽取 167
5．7．3 问答系统 168
5．7．4 情感分析 169
5．7．5 自动摘要 171
第六章 神经网络 173
6．1 神经网络介绍 173
6．1．1 前馈神经网络 173
6．1．2 反馈神经网络 176
6．1．3 自组织神经网络 179
6．2 神经网络相关概念 180
6．2．1 激活函数 180
6．2．2 损失函数 184
6．2．3 学习率 185
6．2．4 过拟合 188
6．2．5 模型训练中的问题 189
6．2．6 神经网络效果评价 192
6．3 神经网络应用 192
第七章 贝叶斯网络 197
7．1 贝叶斯理论概述 197
7．1．1 贝叶斯方法的基本观点 197
7．1．2 贝叶斯网络的应用 198
7．2 贝叶斯概率基础 198
7．2．1 概率论 198
7．2．2 贝叶斯概率 199
7．3 朴素贝叶斯分类模型 200
7．4 贝叶斯网络 203
7．5 贝叶斯网络的应用 209
7．5．1 中文分词 210
7．5．2 机器翻译 210
7．5．3 故障诊断 211
7．5．4 疾病诊断 211
第八章 支持向量机 215
8．1 支持向量机模型 215
8．1．1 核函数 215
8．1．2 模型原理分析 216
8．2 支持向量机应用 219
第九章 进化计算 226
9．1 遗传算法的基础 226
9．1．1 基因重组（交叉）与基因突变 227
9．1．2 遗传算法实现技术 228
9．1．3 遗传算法案例 234
9．2 蚁群算法 237
9．2．1 蚁群算法应用案例 238
9．3 蜂群算法简介 239
9．3．1 蜂群算法应用案例 241
第十章 分布式机器学习 245
10．1 分布式机器学习基础 245
10．1．1 参数服务器 245
10．1．2 分布式并行计算类型 246
10．2 分布式机器学习框架 247
10．3 并行决策树 254
10．4 并行k-均值算法 255
第十一章 深度学习 258
11．1 卷积神经网络 258
11．1．1 卷积神经网络的整体结构 259
11．1．2 常见卷积神经网络 262
11．2 循环神经网络 271
11．2．1 RNN基本原理 271
11．2．2 长短期记忆网络 274
11．2．3 门限循环单元 277
11．3 深度学习流行框架 278
第十二章 高等级深度学习 281
12．1 高等级卷积神经网络 281
12．1．1 目标检测与追踪 281
12．1．2 目标分割 295
12．2 高等级循环神经网络应用 301
12．2．1 Encoder-Decoder模型 301
12．2．2 注意力模型 301
12．2．3 LSTM高等级应用 302
12．3 无监督式深度学习 307
12．3．1 深度信念网络 307
12．3．2 自动编码器网络 309
12．3．3 生成对抗网络模型 312
12．4 强化学习 316
12．4．1 增强学习基础 316
12．4．2 深度增强学习 318
12．5 迁移学习 321
12．6 对偶学习 324
第十三章 推荐系统 327
13．1 推荐系统介绍 327
13．1．1 推荐系统的应用场景 327
13．2 推荐系统通用模型 329
13．2．1 推荐系统结构 329
13．2．2 基于内容的推荐 330
13．2．3 基于协同过滤的推荐算法 331
13．2．4 基于图的模型 334
13．2．5 基于关联规则的推荐 335
13．2．6 基于知识的推荐 341
13．2．7 基于标签的推荐 342
13．3 推荐系统评测 343
13．3．1 评测方法 343
13．3．2 评测指标 345
13．4 推荐系统常见问题 349
13．4．1 冷启动问题 349
13．4．2 推荐系统注意事项 351
13．5 推荐系统实例 352
第十四章 实验 364
14．1 华为FusionInsight产品平台介绍 364
14．2 银行定期存款业务预测 365
14．2．1 上传银行客户及存贷款数据 366
14．2．2 准备存款业务分析工作区 367
14．2．3 创建数据挖掘流程 368
14．2．4 定期存款业务模型保存和应用 375
14．3 客户分群 378
14．3．1 分析业务需求 379
14．3．2 上传客户信息数据 381
14．3．3 准备客户分群工作区 382
14．3．4 创建数据挖掘流程 383
14．3．5 客户分群模型保存和应用 392
・ ・ ・ ・ ・ ・ (收起)第1章 基础知识 1
1.1　准备数据 1
1.1.1　数据格式 1
1.1.2　变量类型 2
1.1.3　变量选择 3
1.1.4　特征工程 3
1.1.5　缺失数据 4
1.2　选择算法 4
1.2.1　无监督学习 5
1.2.2　监督学习 6
1.2.3　强化学习 7
1.2.4　注意事项 7
1.3　参数调优 7
1.4　评价模型 9
1.4.1　分类指标 9
1.4.2　回归指标 10
1.4.3　验证 10
1.5　小结 11
第2章　k均值聚类 13
2.1　找出顾客群 13
2.2　示例：影迷的性格特征 13
2.3　定义群组 16
2.3.1　有多少个群组 16
2.3.2　每个群组中有谁 17
2.4　局限性 18
2.5　小结 19
第3章　主成分分析 21
3.1　食物的营养成分 21
3.2　主成分 22
3.3　示例：分析食物种类 24
3.4　局限性 27
3.5　小结 29
第4章　关联规则 31
4.1　发现购买模式 31
4.2　支持度、置信度和提升度 31
4.3　示例：分析杂货店的销售数据 33
4.4　先验原则 35
4.4.1　寻找具有高支持度的项集 36
4.4.2　寻找具有高置信度或高提升度的关联规则 37
4.5　局限性 37
4.6　小结 37
第5章　社会网络分析 39
5.1　展现人际关系 39
5.2　示例：国际贸易 40
5.3　Louvain方法 42
5.4　PageRank算法 43
5.5　局限性 46
5.6　小结 47
第6章　回归分析 49
6.1　趋势线 49
6.2　示例：预测房价 49
6.3　梯度下降法 52
6.4　回归系数 54
6.5　相关系数 55
6.6　局限性 56
6.7　小结 57
第7章　k最近邻算法和异常检测 59
7.1　食品检测 59
7.2　物以类聚，人以群分 60
7.3　示例：区分红白葡萄酒 61
7.4　异常检测 62
7.5　局限性 63
7.6　小结 63
第8章　支持向量机 65
8.1　医学诊断 65
8.2　示例：预测心脏病 65
8.3　勾画最佳分界线 66
8.4　局限性 69
8.5　小结 69
第9章　决策树 71
9.1　预测灾难幸存者 71
9.2　示例：逃离泰坦尼克号 72
9.3　生成决策树 73
9.4　局限性 74
9.5　小结 75
第10章　随机森林 77
10.1　集体智慧 77
10.2　示例：预测犯罪行为 77
10.3　集成模型 81
10.4　自助聚集法 82
10.5　局限性 83
10.6　小结 84
第11章　神经网络 85
11.1　建造人工智能大脑 85
11.2　示例：识别手写数字 86
11.3　神经网络的构成 89
11.4　激活规则 91
11.5　局限性 92
11.6　小结 94
第12章　A/B测试和多臂老虎机 95
12.1　初识A/B测试 95
12.2　A/B测试的局限性 95
12.3　epsilon递减策略 96
12.4　示例：多臂老虎机 97
12.5　胜者为先 99
12.6　epsilon递减策略的局限性 99
12.7　小结 100
附录A　无监督学习算法概览 101
附录B　监督学习算法概览 102
附录C　调节参数列表 103
附录D　更多评价指标 104
术语表 107
关于作者 114
・ ・ ・ ・ ・ ・ (收起)推荐序
作者序
致谢
译者序
关于本书
作者简介
关于封面插图
第1部分机器学习工作流程
第1章什么是机器学习
1.1理解机器学习
1.2使用数据进行决策
1.2.1传统方法
1.2.2机器学习方法
1.2.3机器学习的五大优势
1.2.4面临的挑战
1.3跟踪机器学习流程：从数据到部署
1.3.1数据集合和预处理
1.3.2数据构建模型
1.3.3模型性能评估
1.3.4模型性能优化
1.4提高模型性能的高级技巧
1.4.1数据预处理和特征工程
1.4.2用在线算法持续改进模型
1.4.3具有数据量和速度的规模化模型
1.5总结
1.6本章术语
第2章实用数据处理
2.1起步：数据收集
2.1.1应包含哪些特征
2.1.2如何获得目标变量的真实值
2.1.3需要多少训练数据
2.1.4训练集是否有足够的代表性
2.2数据预处理
2.2.1分类特征
2.2.2缺失数据处理
2.2.3简单特征工程
2.2.4数据规范化
2.3数据可视化
2.3.1马赛克图
2.3.2盒图
2.3.3密度图
2.3.4散点图
2.4总结
2.5本章术语
第3章建模和预测
3.1基础机器学习建模
3.1.1寻找输入和目标间的关系
3.1.2寻求好模型的目的
3.1.3建模方法类型
3.1.4有监督和无监督学习
3.2分类：把数据预测到桶中
3.2.1构建分类器并预测
3.2.2非线性数据与复杂分类
3.2.3多类别分类
3.3回归：预测数值型数据
3.3.1构建回归器并预测
3.3.2对复杂的非线性数据进行回归
3.4总结
3.5本章术语
第4章模型评估与优化
4.1模型泛化：评估新数据的预测准确性
4.1.1问题：过度拟合与乐观模型
4.1.2解决方案：交叉验证
4.1.3交叉验证的注意事项
4.2分类模型评估
4.2.1分类精度和混淆矩阵
4.2.2准确度权衡与ROC曲线
4.2.3多类别分类
4.3回归模型评估
4.3.1使用简单回归性能指标
4.3.2检验残差
4.4参数调整优化模型
4.4.1机器学习算法和它们的调整参数
4.4.2网格搜索
4.5总结
4.6本章术语
第5章基础特征工程
5.1动机：为什么特征工程很有用
5.1.1什么是特征工程
5.1.2使用特征工程的5个原因
5.1.3特征工程与领域专业知识
5.2基本特征工程过程
5.2.1实例：事件推荐
5.2.2处理日期和时间特征
5.2.3处理简单文本特征
5.3特征选择
5.3.1前向选择和反向消除
5.3.2数据探索的特征选择
5.3.3实用特征选择实例
5.4总结
5.5本章术语
第2部分实 际 应 用
第6章案例：NYC出租车数据
6.1数据：NYC出租车旅程和收费信息
6.1.1数据可视化
6.1.2定义问题并准备数据
6.2建模
6.2.1基本线性模型
6.2.2非线性分类器
6.2.3包含分类特征
6.2.4包含日期-时间特征
6.2.5模型的启示
6.3总结
6.4本章术语
第7章高级特征工程
7.1高级文本特征
7.1.1词袋模型
7.1.2主题建模
7.1.3内容拓展
7.2图像特征
7.2.1简单图像特征
7.2.2提取物体和形状
7.3时间序列特征
7.3.1时间序列数据的类型
7.3.2时间序列数据的预测
7.3.3经典时间序列特征
7.3.4事件流的特征工程
7.4总结
7.5本章术语
第8章NLP高级案例：电影评论情感预测
8.1研究数据和应用场景
8.1.1数据集初探
8.1.2检查数据
8.1.3应用场景有哪些
8.2提取基本NLP特征并构建初始模型
8.2.1词袋特征
8.2.2用朴素贝叶斯算法构建模型
8.2.3tf-idf算法规范词袋特征
8.2.4优化模型参数
8.3高级算法和模型部署的考虑
8.3.1word2vec特征
8.3.2随机森林模型
8.4总结
8.5本章术语
第9章扩展机器学习流程
9.1扩展前需考虑的问题
9.1.1识别关键点
9.1.2选取训练数据子样本代替扩展性
9.1.3可扩展的数据管理系统
9.2机器学习建模流程扩展
9.3预测扩展
9.3.1预测容量扩展
9.3.2预测速度扩展
9.4总结
9.5本章术语
第10章案例：数字显示广告
10.1显示广告
10.2数字广告数据
10.3特征工程和建模策略
10.4数据大小和形状
10.5奇异值分解
10.6资源估计和优化
10.7建模
10.8K近邻算法
10.9随机森林算法
10.10其他实用考虑
10.11总结
10.12本章术语
10.13摘要和结论
附录常用机器学习算法
名词术语中英文对照
・ ・ ・ ・ ・ ・ (收起)第 1 章 机器学习是什么――机器学习定义 .. 1
引言.. 2
1.1 数据. 5
1.1.1 结构型与非结构型数据 5
1.1.2 原始数据与加工.. 7
1.1.3 样本内数据与样本外数据 . 9
1.2 机器学习类别 9
1.2.1 有监督学习 10
1.2.2 无监督学习 10
1.2.3 半监督学习. 11
1.2.4 增强学习 11
1.2.5 深度学习 11
1.2.6 迁移学习.. 12
1.3 性能度量. 12
1.3.1 误差函数.. 13
1.3.2 回归度量.. 14
1.3.3 分类度量.. 15
1.4 总结.. 19
参考资料.. 20
第 2 章 机器学习可行吗――计算学习理论 22
引言 23
2.1 基础知识. 25
2.1.1 二分类. 25
2.1.2 对分 26
2.1.3 增长函数.. 29
2.1.4 突破点. 30
2.2 核心推导. 31
2.2.1 机器学习可行条件 . 31
2.2.2 从已知推未知. 33
2.2.3 从民意调查到机器学习 35
2.2.4 从单一到有限. 36
2.2.5 从有限到无限. 37
2.2.6 从无限到有限. 38
2.3 结论应用. 39
2.3.1 VC 不等式 39
2.3.2 VC 维度 .. 40
2.3.3 模型复杂度 40
2.3.4 样本复杂度 41
2.4 总结.. 42
参考资料.. 43
技术附录.. 43
第 3 章 机器学习怎么学――模型评估选择 47
引言 48
3.1 模型评估. 52
3.2 训练误差和测试误差. 52
3.2.1 训练误差.. 52
3.2.2 真实误差.. 54
3.2.3 测试误差.. 57
3.2.4 学习理论.. 57
3.3 验证误差和交叉验证误差. 60
3.3.1 验证误差.. 60
3.3.2 交叉验证误差. 61
3.3.3 学习理论.. 62
3.4 误差剖析. 64
3.4.1 误差来源.. 64
3.4.2 偏差―方差权衡 66
3.5 模型选择. 67
3.6 总结.. 70
参考资料.. 71
技术附录.. 71
第 4 章 线性回归 73
引言 74
4.1 基础知识. 75
4.1.1 标量微积分 75
4.1.2 向量微积分 76
4.2 模型介绍. 77
4.2.1 核心问题.. 77
4.2.2 通用线性回归模型 . 83
4.2.3 特征缩放.. 84
4.2.4 学习率设定 86
4.2.5 数值算法比较. 87
4.2.6 代码实现.. 89
4.3 总结.. 90
参考资料.. 90
第 5 章 对率回归 92
引言 93
5.1 基础内容. 94
5.1.1 联系函数.. 94
5.1.2 函数绘图.. 95
5.2 模型介绍. 96
5.2.1 核心问题.. 96
5.2.2 查准和查全. 102
5.2.3 类别不平衡. 104
5.2.4 线性不可分. 105
5.2.5 多分类问题. 106
5.2.6 代码实现 109
5.3 总结. 110
参考资料. 111
第 6 章 正则化回归 . 112
引言. 113
6.1 基础知识 114
6.1.1 等值线图. 114
6.1.2 坐标下降. 116
6.2 模型介绍 116
6.2.1 核心问题. 116
6.2.2 模型对比 122
6.2.3 最佳模型 125
6.2.4 代码实现 126
6.3 总结 126
参考资料 127
第 7 章 支持向量机 . 128
引言 129
7.1 基础知识.. 133
7.1.1 向量初体验. 133
7.1.2 拉格朗日量. 136
7.1.3 原始和对偶. 137
7.2 模型介绍.. 138
7.2.1 硬间隔 SVM 原始问题. 138
7.2.2 硬间隔 SVM 对偶问题. 144
7.2.3 软间隔 SVM 原始问题. 148
7.2.4 软间隔 SVM 对偶问题. 150
7.2.5 空间转换 151
7.2.6 核技巧. 155
7.2.7 核 SVM . 158
7.2.8 SMO 算法 .. 159
7.2.9 模型选择 161
7.3 总结 162
参考资料 164
技术附录 164
第 8 章 朴素贝叶斯 . 170
引言 171
8.1 基础知识.. 174
8.1.1 两种概率学派.. 174
8.1.2 两种独立类别.. 174
8.1.3 两种学习算法.. 175
8.1.4 两种估计方法.. 176
8.1.5 两类概率分布.. 177
8.2 模型介绍.. 179
8.2.1 问题剖析 179
8.2.2 朴素贝叶斯算法 182
8.2.3 多元伯努利模型 183
8.2.4 多项事件模型.. 184
8.2.5 高斯判别分析模型 . 184
8.2.6 多分类问题. 186
8.2.7 拉普拉斯校正.. 187
8.2.8 最大似然估计和最大后验估计 . 188
8.3 总结 190
参考资料 191
技术附录 191
第 9 章 决策树 . 195
引言 196
9.1 基础知识.. 198
9.1.1 多数规则 198
9.1.2 熵和条件熵. 198
9.1.3 信息增益和信息增益比 . 200
9.1.4 基尼指数 201
9.2 模型介绍.. 201
9.2.1 二分类决策树.. 201
9.2.2 多分类决策树.. 209
9.2.3 连续值分裂. 210
9.2.4 欠拟合和过拟合. 211
9.2.5 预修剪和后修剪 212
9.2.6 数据缺失 215
9.2.7 代码实现 218
9.3 总结 219
参考资料 219
第 10 章 人工神经网络 220
引言 221
10.1 基本知识 223
10.1.1 转换函数 223
10.1.2 单输入单层单输出神经网络 . 224
10.1.3 多输入单层单输出神经网络 . 224
10.1.4 多输入单层多输出神经网络 . 225
10.1.5 多输入多层多输出神经网络 . 225
10.2 模型应用 227
10.2.1 创建神经网络模型 .. 227
10.2.2 回归应用 230
10.2.3 分类应用 238
第 11 章 正向/反向传播 246
引言 247
11.1 基础知识 250
11.1.1 神经网络元素 250
11.1.2 链式法则 254
11.2 算法介绍 254
11.2.1 正向传播 254
11.2.2 梯度下降 257
11.2.3 反向传播 258
11.2.4 代码实现 262
11.3 总结 268
参考资料 268
技术附录 269
第 12 章 集成学习. 272
引言 273
12.1 结合假设 277
12.1.1 语文和数学. 277
12.1.2 准确和多样. 278
12.1.3 独裁和民主. 279
12.1.4 学习并结合. 279
12.2 装袋法. 280
12.2.1 基本概念 280
12.2.2 自助采样 280
12.2.3 结合假设 281
12.3 提升法. 282
12.3.1 基本概念 282
12.3.2 最优加权 283
12.3.3 结合假设 285
12.4 集成方式 286
12.4.1 同质学习器. 286
12.4.2 异质学习器. 286
12.5 总结 288
参考资料 288
第 13 章 随机森林和提升树 . 289
引言 290
13.1 基础知识 293
13.1.1 分类回归树. 293
13.1.2 前向分布算法 294
13.1.3 置换检验 295
13.2 模型介绍 296
13.2.1 随机森林 296
13.2.2 提升树.. 302
13.2.3 代码实现 306
13.3 总结 307
参考资料 307
第 14 章 极度梯度提升 309
引言 310
14.1 基础知识. 311
14.1.1 树的重定义 311
14.1.2 树的复杂度. 313
14.2 模型介绍 313
14.2.1 XGB 简介. 313
14.2.2 XGB 的泛化度 314
14.2.3 XGB 的精确度 315
14.2.4 XGB 的速度.. 318
14.2.5 代码实现 324
14.3 总结 325
参考资料 326
第 15 章 本书总结. 327
15.1 正交策略 328
15.2 单值评估指标.. 330
15.3 偏差和方差. 332
15.3.1 理论定义 332
15.3.2 实用定义 334
15.3.3 最优误差 335
15.3.4 两者权衡 336
15.3.5 学习曲线 336
结语 339
・ ・ ・ ・ ・ ・ (收起)前言
第1章　机器学习概述 1
1.1　什么是机器学习 1
1.2　机器学习的几个需求层次 3
1.3　机器学习的基本原理 5
1.4　机器学习的基本概念 7
1.4.1　书中用到的术语介绍 7
1.4.2　机器学习的基本模式 11
1.4.3　优化方法 12
1.5　机器学习问题分类 14
1.6　常用的机器学习算法 15
1.7　机器学习算法的性能衡量指标 16
1.8　数据对算法结果的影响 18
第2章　机器学习所需的环境 20
2.1　常用环境 20
2.2　Python简介 21
2.2.1　Python的安装 23
2.2.2　Python的基本用法 24
2.3　Numpy简介 25
2.3.1　Numpy的安装 26
2.3.2　Numpy的基本用法 26
2.4　Scikit-Learn简介 27
2.4.1　Scikit-Learn的安装 28
2.4.2　Scikit-Learn的基本用法 28
2.5　Pandas简介 29
2.5.1　Pandas的安装 30
2.5.2　Pandas的基本用法 31
第3章　线性回归算法 33
3.1　线性回归：“钢铁直男”解决回归问题的正确方法 33
3.1.1　用于预测未来的回归问题 35
3.1.2　怎样预测未来 38
3.1.3　线性方程的“直男”本性 40
3.1.4　最简单的回归问题―线性回归问题 44
3.2　线性回归的算法原理 46
3.2.1　线性回归算法的基本思路 46
3.2.2　线性回归算法的数学解析 48
3.2.3　线性回归算法的具体步骤 53
3.3　在Python中使用线性回归算法 54
3.4　线性回归算法的使用场景 60
第4章　Logistic回归分类算法 61
4.1　Logistic回归：换上“S型曲线马甲”的线性回归 61
4.1.1　分类问题：选择困难症患者的自我救赎 63
4.1.2　Logistic函数介绍 66
4.1.3　此回归非彼回归：“LR”辨析 70
4.2　Logistic回归的算法原理 71
4.2.1　Logistic回归算法的基本思路 71
4.2.2　Logistic回归算法的数学解析 74
4.2.3　Logistic回归算法的具体步骤 78
4.3　在Python中使用Logistic回归算法 78
4.4　Logistic回归算法的使用场景 81
第5章　KNN分类算法 82
5.1　KNN分类算法：用多数表决进行分类 82
5.1.1　用“同类相吸”的办法解决分类问题 84
5.1.2　KNN分类算法的基本方法：多数表决 86
5.1.3　表决权问题 89
5.1.4　KNN的具体含义 89
5.2　KNN分类的算法原理 90
5.2.1　KNN分类算法的基本思路 90
5.2.2　KNN分类算法的数学解析 93
5.2.3　KNN分类算法的具体步骤 94
5.3　在Python中使用KNN分类算法 95
5.4　KNN分类算法的使用场景 96
第6章　朴素贝叶斯分类算法 98
6.1　朴素贝叶斯：用骰子选择 98
6.1.1　从统计角度看分类问题 99
6.1.2　贝叶斯公式的基本思想 102
6.1.3　用贝叶斯公式进行选择 104
6.2　朴素贝叶斯分类的算法原理 106
6.2.1　朴素贝叶斯分类算法的基本思路 106
6.2.2　朴素贝叶斯分类算法的数学解析 108
6.2.3　朴素贝叶斯分类算法的具体步骤 111
6.3　在Python中使用朴素贝叶斯分类算法 111
6.4　朴素贝叶斯分类算法的使用场景 112
第7章　决策树分类算法 114
7.1　决策树分类：用“老朋友”if-else进行选择 114
7.1.1　程序员的选择观：if-else 116
7.1.2　如何种植一棵有灵魂的“树” 118
7.1.3　决策条件的选择艺术 119
7.1.4　决策树的剪枝问题 122
7.2　决策树分类的算法原理 125
7.2.1　决策树分类算法的基本思路 125
7.2.2　决策树分类算法的数学解析 127
7.2.3　决策树分类算法的具体步骤 133
7.3　在Python中使用决策树分类算法 134
7.4　决策树分类算法的使用场景 135
第8章　支持向量机分类算法 137
8.1　支持向量机：线性分类器的“王者” 137
8.1.1　距离是不同类别的天然间隔 139
8.1.2　何为“支持向量” 140
8.1.3　从更高维度看“线性不可分” 142
8.2　支持向量机分类的算法原理 146
8.2.1　支持向量机分类算法的基本思路 146
8.2.2　支持向量机分类算法的数学解析 150
8.2.3　支持向量机分类算法的具体步骤 153
8.3　在Python中使用支持向量机分类算法 154
8.4　支持向量机分类算法的使用场景 156
第9章　K-means聚类算法 157
9.1　用投票表决实现“物以类聚” 157
9.1.1　聚类问题就是“物以类聚”的实施问题 159
9.1.2　用“K”来决定归属类别 162
9.1.3　度量“相似”的距离 164
9.1.4　聚类问题中的多数表决 165
9.2　K-means聚类的算法原理 168
9.2.1　K-means聚类算法的基本思路 168
9.2.2　K-means聚类算法的数学解析 169
9.2.3　K-means聚类算法的具体步骤 170
9.3　在Python中使用K-means聚类算法 171
9.4　K-means聚类算法的使用场景 172
第10章　神经网络分类算法 174
10.1　用神经网络解决分类问题 174
10.1.1　神经元的“内心世界” 177
10.1.2　从神经元看分类问题 180
10.1.3　神经网络的“细胞”：人工神经元 181
10.1.4　构成网络的魔力 184
10.1.5　神经网络与深度学习 188
10.2　神经网络分类的算法原理 188
10.2.1　神经网络分类算法的基本思路 188
10.2.2　神经网络分类算法的数学解析 190
10.2.3　神经网络分类算法的具体步骤 193
10.3　在Python中使用神经网络分类算法 194
10.4　神经网络分类算法的使用场景 195
第11章　集成学习方法 197
11.1　集成学习方法：三个臭皮匠赛过诸葛亮 197
11.1.1　集成学习方法与经典机器学习算法的关系 198
11.1.2　集成学习的主要思想 199
11.1.3　几种集成结构 200
11.2　集成学习方法的具体实现方式 202
11.2.1　Bagging算法 202
11.2.2　Boosting算法 202
11.2.3　Stacking算法 202
11.3　在Python中使用集成学习方法 203
11.4　集成学习方法的使用场景 205
・ ・ ・ ・ ・ ・ (收起)第1章　机器学习应用快速入门　　1
1.1　机器学习与数据科学　　1
1.1.1　机器学习能够解决的问题　　2
1.1.2　机器学习应用流程　　3
1.2　数据与问题定义　　4
1.3　数据收集　　5
1.3.1　发现或观察数据　　5
1.3.2　生成数据　　6
1.3.3　采样陷阱　　7
1.4　数据预处理　　7
1.4.1　数据清洗　　8
1.4.2　填充缺失值　　8
1.4.3　剔除异常值　　8
1.4.4　数据转换　　9
1.4.5　数据归约　　10
1.5　无监督学习　　10
1.5.1　查找相似项目　　10
1.5.2　聚类　　12
1.6　监督学习　　13
1.6.1　分类　　14
1.6.2　回归　　16
1.7　泛化与评估　　18
1.8　小结　　21
第2章　面向机器学习的Java库与平台　　22
2.1　Java环境　　22
2.2　机器学习库　　23
2.2.1　Weka　　23
2.2.2　Java机器学习　　25
2.2.3　Apache Mahout　　26
2.2.4　Apache Spark　　27
2.2.5　Deeplearning4j　　28
2.2.6　MALLET　　29
2.2.7　比较各个库　　30
2.3　创建机器学习应用　　31
2.4　处理大数据　　31
2.5　小结　　33
第3章　基本算法――分类、回归和聚类　　34
3.1　开始之前　　34
3.2　分类　　35
3.2.1　数据　　35
3.2.2　加载数据　　36
3.2.3　特征选择　　37
3.2.4　学习算法　　38
3.2.5　对新数据分类　　40
3.2.6　评估与预测误差度量　　41
3.2.7　混淆矩阵　　41
3.2.8　选择分类算法　　42
3.3　回归　　43
3.3.1　加载数据　　43
3.3.2　分析属性　　44
3.3.3　创建与评估回归模型　　45
3.3.4　避免常见回归问题的小技巧　　48
3.4　聚类　　49
3.4.1　聚类算法　　49
3.4.2　评估　　50
3.5　小结　　51
第4章　利用集成方法预测客户关系　　52
4.1　客户关系数据库　　52
4.1.1　挑战　　53
4.1.2　数据集　　53
4.1.3　评估　　54
4.2　最基本的朴素贝叶斯分类器基准　　55
4.2.1　获取数据　　55
4.2.2　加载数据　　56
4.3　基准模型　　58
4.3.1　评估模型　　58
4.3.2　实现朴素贝叶斯基准线　　59
4.4　使用集成方法进行高级建模　　60
4.4.1　开始之前　　60
4.4.2　数据预处理　　61
4.4.3　属性选择　　62
4.4.4　模型选择　　63
4.4.5　性能评估　　66
4.5　小结　　66
第5章　关联分析　　67
5.1　购物篮分析　　67
5.2　关联规则学习　　69
5.2.1　基本概念　　69
5.2.2　Apriori算法　　71
5.2.3　FP-增长算法　　71
5.2.4　超市数据集　　72
5.3　发现模式　　73
5.3.1　Apriori算法　　73
5.3.2　FP-增长算法　　74
5.4　在其他领域中的应用　　75
5.4.1　医疗诊断　　75
5.4.2　蛋白质序列　　75
5.4.3　人口普查数据　　76
5.4.4　客户关系管理　　76
5.4.5　IT运营分析　　76
5.5　小结　　77
第6章　使用Apache Mahout制作推荐引擎　　78
6.1　基本概念　　78
6.1.1　关键概念　　79
6.1.2　基于用户与基于项目的分析　　79
6.1.3　计算相似度的方法　　80
6.1.4　利用与探索　　81
6.2　获取Apache Mahout　　81
6.3　创建一个推荐引擎　　84
6.3.1　图书评分数据集　　84
6.3.2　加载数据　　84
6.3.3　协同过滤　　89
6.4　基于内容的过滤　　97
6.5　小结　　97
第7章　欺诈与异常检测　　98
7.1　可疑与异常行为检测　　98
7.2　可疑模式检测　　99
7.3　异常模式检测　　100
7.3.1　分析类型　　100
7.3.2　事务分析　　101
7.3.3　规划识别　　101
7.4　保险理赔欺诈检测　　101
7.4.1　数据集　　102
7.4.2　为可疑模式建模　　103
7.5　网站流量异常检测　　107
7.5.1　数据集　　107
7.5.2　时序数据中的异常检测　　108
7.6　小结　　113
第8章　利用Deeplearning4j进行图像识别　　114
8.1　图像识别简介　　114
8.2　图像分类　　120
8.2.1　Deeplearning4j　　120
8.2.2　MNIST数据集　　121
8.2.3　加载数据　　121
8.2.4　创建模型　　122
8.3　小结　　128
第9章　利用手机传感器进行行为识别　　129
9.1　行为识别简介　　129
9.1.1　手机传感器　　130
9.1.2　行为识别流水线　　131
9.1.3　计划　　132
9.2　从手机收集数据　　133
9.2.1　安装Android Studio　　133
9.2.2　加载数据采集器　　133
9.2.3　收集训练数据　　136
9.3　创建分类器　　138
9.3.1　减少假性转换　　140
9.3.2　将分类器嵌入移动应用　　142
9.4　小结　　143
第10章　利用Mallet进行文本挖掘――主题模型与垃圾邮件检测　　144
10.1　文本挖掘简介　　144
10.1.1　主题模型　　145
10.1.2　文本分类　　145
10.2　安装Mallet　　146
10.3　使用文本数据　　147
10.3.1　导入数据　　149
10.3.2　对文本数据做预处理　　150
10.4　为BBC新闻做主题模型　　152
10.4.1　BBC数据集　　152
10.4.2　建模　　153
10.4.3　评估模型　　155
10.4.4　重用模型　　156
10.5　垃圾邮件检测　　157
10.5.1　垃圾邮件数据集　　158
10.5.2　特征生成　　159
10.5.3　训练与测试模型　　160
10.6　小结　　161
第11章　机器学习进阶　　162
11.1　现实生活中的机器学习　　162
11.1.1　噪声数据　　162
11.1.2　类不平衡　　162
11.1.3　特征选择困难　　163
11.1.4　模型链　　163
11.1.5　评价的重要性　　163
11.1.6　从模型到产品　　164
11.1.7　模型维护　　164
11.2　标准与标记语言　　165
11.2.1　CRISP-DM　　165
11.2.2　SEMMA方法　　166
11.2.3　预测模型标记语言　　166
11.3　云端机器学习　　167
11.4　Web资源与比赛　　168
11.4.1　数据集　　168
11.4.2　在线课程　　169
11.4.3　比赛　　170
11.4.4　网站与博客　　170
11.4.5　场馆与会议　　171
11.5　小结　　171
・ ・ ・ ・ ・ ・ (收起)第1部分 背景知识
第1章 机器学习概述 3
1.1 背景 3
1.2 发展现状 6
1.2.1 数据现状 6
1.2.2 机器学习算法现状 8
1.3 机器学习基本概念 12
1.3.1 机器学习流程 12
1.3.2 数据源结构 14
1.3.3 算法分类 16
1.3.4 过拟合问题 18
1.3.5 结果评估 20
1.4 本章小结 22
第2部分 算法流程
第2章 场景解析 25
2.1 数据探查 25
2.2 场景抽象 27
2.3 算法选择 29
2.4 本章小结 31
第3章 数据预处理 32
3.1 采样 32
3.1.1 随机采样 32
3.1.2 系统采样 34
3.1.3 分层采样 35
3.2 归一化 36
3.3 去除噪声 39
3.4 数据过滤 42
3.5 本章小结 43
第4章 特征工程 44
4.1 特征抽象 44
4.2 特征重要性评估 49
4.3 特征衍生 53
4.4 特征降维 57
4.4.1 特征降维的基本概念 57
4.4.2 主成分分析 59
4.5 本章小结 62
第5章 机器学习算法――常规算法 63
5.1 分类算法 63
5.1.1 K近邻 63
5.1.2 朴素贝叶斯 68
5.1.3 逻辑回归 74
5.1.4 支持向量机 81
5.1.5 随机森林 87
5.2 聚类算法 94
5.2.1 K-means 97
5.2.2 DBSCAN 103
5.3 回归算法 109
5.4 文本分析算法 112
5.4.1 分词算法――Hmm 112
5.4.2 TF-IDF 118
5.4.3 LDA 122
5.5 推荐类算法 127
5.6 关系图算法 133
5.6.1 标签传播 134
5.6.2 Dijkstra最短路径 138
5.7 本章小结 145
第6章 机器学习算法――深度学习 146
6.1 深度学习概述 146
6.1.1 深度学习的发展 147
6.1.2 深度学习算法与传统
算法的比较 148
6.2 深度学习的常见结构 152
6.2.1 深度神经网络 152
6.2.2 卷积神经网络 153
6.2.3 循环神经网络 156
6.3 本章小结 157
第3部分 工具介绍
第7章 常见机器学习工具介绍 161
7.1 概述 161
7.2 单机版机器学习工具 163
7.2.1 SPSS 163
7.2.2 R语言 167
7.2.3 工具对比 172
7.3 开源分布式机器学习工具 172
7.3.1 Spark MLib 172
7.3.2 TensorFlow 179
7.4 企业级云机器学习工具 190
7.4.1 亚马逊AWS ML 191
7.4.2 阿里云机器学习PAI 196
7.5 本章小结 205
第4部分 实战应用
第8章 业务解决方案 209
8.1 心脏病预测 209
8.1.1 场景解析 209
8.1.2 实验搭建 211
8.1.3 小结 216
8.2 商品推荐系统 216
8.2.1 场景解析 217
8.2.2 实验搭建 218
8.2.3 小结 220
8.3 金融风控案例 220
8.3.1 场景解析 221
8.3.2 实验搭建 222
8.3.3 小结 225
8.4 新闻文本分析 225
8.4.1 场景解析 225
8.4.2 实验搭建 226
8.4.3 小结 230
8.5 农业贷款发放预测 230
8.5.1 场景解析 230
8.5.2 实验搭建 232
8.5.3 小结 236
8.6 雾霾天气成因分析 236
8.6.1 场景解析 237
8.6.2 实验搭建 238
8.6.3 小结 243
8.7 图片识别 243
8.7.1 场景解析 243
8.7.2 实验搭建 245
8.7.3 小结 253
8.8 本章小结 253
第5部分 知识图谱
第9章 知识图谱 257
9.1 未来数据采集 257
9.2 知识图谱的概述 259
9.3 知识图谱开源
工具 261
9.4 本章小结 264
参考文献 265
・ ・ ・ ・ ・ ・ (收起)第一部分 场景化机器学习
第1章 机器学习如何应用于业务　2
1.1 为什么我们的业务系统如此糟糕　3
1.2 为什么如今自动化很重要　5
1.2.1 什么是生产率　6
1.2.2 机器学习如何提高生产率　6
1.3 机器如何做出决策　7
1.3.1 人：是否基于规则　7
1.3.2 你能相信一个基于模式的答案吗　8
1.3.3 机器学习如何能提升你的业务系统　8
1.4 机器能帮Karen做决策吗　9
1.4.1 目标变量　10
1.4.2 特征　10
1.5 机器如何学习　10
1.6 在你的公司落实使用机器学习进行决策　13
1.7 工具　14
1.7.1 AWS和SageMaker是什么，它们如何帮助你　14
1.7.2 Jupyter笔记本是什么　15
1.8 配置SageMaker为解决第2~7章中的场景做准备　15
1.9 是时候行动了　16
1.10 小结　16
第二部分 公司机器学习的六个场景
第2章 你是否应该将采购订单发送给技术审批人　18
2.1 决策　18
2.2 数据　19
2.3 开始你的训练过程　20
2.4 运行Jupyter笔记本并进行预测　21
2.4.1 第一部分：加载并检查数据　24
2.4.2 第二部分：将数据转换为正确的格式　27
2.4.3 第三部分：创建训练集、验证集和测试集　30
2.4.4 第四部分：训练模型　32
2.4.5 第五部分：部署模型　33
2.4.6 第六部分：测试模型　34
2.5 删除端点并停止你的笔记本实例　35
2.5.1 删除端点　36
2.5.2 停止笔记本实例　37
2.6 小结　38
第3章 你是否应该致电客户以防客户流失　39
3.1 你在决策什么　40
3.2 处理流程　40
3.3 准备数据集　41
3.3.1 转换操作1：标准化数据　42
3.3.2 转换操作2：计算周与周之间的变化　43
3.4 XGBoost基础　43
3.4.1 XGBoost的工作原理　43
3.4.2 机器学习模型如何确定函数的AUC的好坏　45
3.5 准备构建模型　47
3.5.1 将数据集上传到S3　47
3.5.2 在SageMaker上设置笔记本　48
3.6 构建模型　49
3.6.1 第一部分：加载并检查数据　50
3.6.2 第二部分：将数据转换为正确的格式　52
3.6.3 第三部分：创建训练集、验证集和测试集　53
3.6.4 第四部分：训练模型　55
3.6.5 第五部分：部署模型　57
3.6.6 第六部分：测试模型　57
3.7 删除端点并停止笔记本实例　60
3.7.1 删除端点　60
3.7.2 停止笔记本实例　60
3.8 检查以确保端点已被删除　60
3.9 小结　61
第4章 你是否应该将事件上报给支持团队　62
4.1 你在决策什么　62
4.2 处理流程　63
4.3 准备数据集　63
4.4 NLP　65
4.4.1 生成词向量　65
4.4.2 决定每组包含多少单词　67
4.5 BlazingText及其工作原理　68
4.6 准备构建模型　69
4.6.1 将数据集上传到S3　69
4.6.2 在SageMaker上设置笔记本　70
4.7 构建模型　70
4.7.1 第一部分：加载并检查数据　71
4.7.2 第二部分：将数据转换为正确的格式　74
4.7.3 第三部分：创建训练集和验证集　76
4.7.4 第四部分：训练模型　77
4.7.5 第五部分：部署模型　79
4.7.6 第六部分：测试模型　79
4.8 删除端点并停止你的笔记本实例　80
4.8.1 删除端点　80
4.8.2 停止笔记本实例　80
4.9 检查以确保端点已被删除　81
4.10 小结　81
第5章 你是否应该质疑供应商发送给你的发票　82
5.1 你在决策什么　82
5.2 处理流程　84
5.3 准备数据集　85
5.4 什么是异常　86
5.5 监督机器学习与无监督机器学习　87
5.6 随机裁剪森林及其工作原理　88
5.6.1 样本1　88
5.6.2 样本2　90
5.7 准备构建模型　94
5.7.1 将数据集上传到S3　94
5.7.2 在SageMaker上设置笔记本　94
5.8 构建模型　95
5.8.1 第一部分：加载并检查数据　96
5.8.2 第二部分：将数据转换为正确的格式　99
5.8.3 第三部分：创建训练集和验证集　100
5.8.4 第四部分：训练模型　100
5.8.5 第五部分：部署模型　101
5.8.6 第六部分：测试模型　102
5.9 删除端点并停止笔记本实例　104
5.9.1 删除端点　104
5.9.2 停止笔记本实例　104
5.10 检查以确保端点已被删除　105
5.11 小结　105
第6章 预测你公司的每月能耗　106
6.1 你在决策什么　106
6.1.1 时间序列数据介绍　107
6.1.2 Kiara的时间序列数据：每日能耗　109
6.2 加载处理时间序列数据的Jupyter笔记本　109
6.3 准备数据集：绘制时间序列数据　111
6.3.1 通过循环展示数据列　113
6.3.2 创建多个图表　114
6.4 神经网络是什么　116
6.5 准备构建模型　116
6.5.1 将数据集上传到S3　117
6.5.2 在SageMaker上设置笔记本　117
6.6 构建模型　117
6.6.1 第一部分：加载并检查数据　118
6.6.2 第二部分：将数据转换为正确的格式　119
6.6.3 第三部分：创建训练集和测试集　122
6.6.4 第四部分：训练模型　125
6.6.5 第五部分：部署模型　128
6.6.6 第六部分：进行预测并绘制结果　128
6.7 删除端点并停止你的笔记本实例　132
6.7.1 删除端点　133
6.7.2 停止笔记本实例　133
6.8 检查以确保端点已被删除　133
6.9 小结　134
第7 章 优化你公司的每月能耗预测　135
7.1 DeepAR对周期性事件的处理能力　135
7.2 DeepAR的最大优势：整合相关的时间序列　137
7.3 整合额外的数据集到Kiara的能耗模型　137
7.4 准备构建模型　138
7.4.1 下载我们准备的笔记本　138
7.4.2 在SageMaker上设置文件夹　139
7.4.3 将笔记本上传到SageMaker　139
7.4.4 从S3存储桶下载数据集　139
7.4.5 在S3上创建文件夹以保存你的数据　139
7.4.6 将数据集上传到你的AWS存储桶　139
7.5 构建模型　140
7.5.1 第一部分：设置笔记本　140
7.5.2 第二部分：导入数据集　141
7.5.3 第三部分：将数据转换为正确的格式　143
7.5.4 第四部分：创建训练集和测试集　145
7.5.5 第五部分：配置模型并设置服务器以构建模型　147
7.5.6 第六部分：进行预测并绘制结果　151
7.6 删除端点并停止你的笔记本实例　154
7.6.1 删除端点　154
7.6.2 停止笔记本实例　154
7.7 检查以确保端点已被删除　154
7.8 小结　155
第三部分 将机器学习应用到生产环境中
第8章 通过Web提供预测服务　158
8.1 为什么通过Web提供决策和预测服务这么难　158
8.2 本章的步骤概述　159
8.3 SageMaker端点　159
8.4 设置SageMaker端点　160
8.4.1 上传笔记本　161
8.4.2 上传数据　163
8.4.3 运行笔记本并创建端点　165
8.5 设置无服务器API端点　166
8.5.1 在AWS账户上设置AWS证书　167
8.5.2 在本地计算机上设置AWS证书　168
8.5.3 配置证书　169
8.6 创建Web端点　170
8.6.1 安装Chalice　171
8.6.2 创建Hello World API　172
8.6.3 添加为SageMaker端点提供服务的代码　173
8.6.4 配置权限　175
8.6.5 更新requirements.txt文件　176
8.6.6 部署Chalice　176
8.7 提供决策服务　176
8.8 小结　177
第9章 案例研究　179
9.1 案例研究1：WorkPac　180
9.1.1 项目设计　181
9.1.2 第一阶段：准备并测试模型　181
9.1.3 第二阶段：实施POC　183
9.1.4 第三阶段：将流程嵌入公司的运营中　183
9.1.5 接下来的工作　183
9.1.6 吸取的教训　183
9.2 案例研究2：Faethm　184
9.2.1 AI核心　184
9.2.2 使用机器学习优化Faethm公司的流程　184
9.2.3 第一阶段：获取数据　185
9.2.4 第二阶段：识别特征　186
9.2.5 第三阶段：验证结果　186
9.2.6 第四阶段：应用到生产环境中　186
9.3 结论　187
9.3.1 观点1：建立信任　187
9.3.2 观点2：正确获取数据　187
9.3.3 观点3：设计操作模式以充分利用机器学习能力　187
9.3.4 观点4：在各个方面都使用了机器学习后，你的公司看起来怎么样　187
9.4 小结　188
附录A 注册AWS　189
附录B 设置并使用S3以存储文件　195
附录C 设置并使用AWS SageMaker来构建机器学习系统　204
附录D 停止全部服务　208
附录E 安装Python　211
・ ・ ・ ・ ・ ・ (收起)第 1章 MATLAB机器学习初体验 1
1.1　机器学习基础　1
1.2　机器学习算法的分类　4
1.2.1　监督学习　4
1.2.2　非监督学习　5
1.2.3　强化学习　5
1.3　选择正确的算法　6
1.4　构建机器学习模型的流程　7
1.5　MATLAB中的机器学习支持简介　8
1.5.1　操作系统、硬件平台要求　10
1.5.2　MATLAB安装要求　11
1.6　统计机器学习工具箱　11
1.6.1　数据类型　13
1.6.2　统计机器学习工具箱功能简介　13
1.7　神经网络工具箱　18
1.8　MATLAB中的统计学和线性代数　19
1.9　总结　21
第　2章 使用MATLAB导入数据和组织数据　22
2.1　熟悉MATLAB桌面　22
2.2　将数据导入MATLAB　27
2.2.1　导入向导　27
2.2.2　通过程序语句导入数据　29
2.3　从MATLAB导出数据　36
2.4　处理媒体文件　37
2.4.1　处理图像数据　37
2.4.2　音频的导入/导出　39
2.5　数据组织　39
2.5.1　元胞数组　40
2.5.2　结构体数组　42
2.5.3　table类型　44
2.5.4　分类数组　46
2.6　总结　47
第3章　从数据到知识挖掘　49
3.1　区分变量类别　50
3.1.1　定量变量　50
3.1.2　定性变量　50
3.2　数据准备　51
3.2.1　初步查看数据　51
3.2.2　找到缺失值　53
3.2.3　改变数据类型　54
3.2.4　替换缺失值　54
3.2.5　移除缺失值　55
3.2.6　为表格排序　56
3.2.7　找到数据中的异常值　56
3.2.8　将多个数据源合并成一个数据源　57
3.3　探索性统计指标―数值测量　59
3.3.1　位置测量　59
3.3.2　分散度的测量　61
3.3.3　分布形状的测量　64
3.4　探索性可视化　66
3.4.1　图形数据统计分析对话框　67
3.4.2　柱状图　70
3.4.3　箱形图　75
3.4.4　散点图　77
3.5　总结　78
第4章　找到变量之间的关系―回归方法　80
4.1　寻找线性关系　80
4.1.1　最小二乘回归　81
4.1.2　基本拟合接口　86
4.2　如何创建一个线性回归模型　88
4.2.1　通过稳健回归消除异常值的影响　93
4.2.2　多元线性回归　96
4.3　多项式回归　101
4.4　回归学习器App　103
4.5　总结　107
第5章　模式识别之分类算法　108
5.1　决策树分类　108
5.2　概率分类模型―朴素贝叶斯分类　115
5.2.1　概率论基础　116
5.2.2　使用朴素贝叶斯进行分类　119
5.2.3　MATLAB中的贝叶斯方法　120
5.3　判别分析分类　123
5.4　k邻近算法　128
5.5　MATLAB分类学习器App　132
5.6　总结　136
第6章　无监督学习　137
6.1　聚类分析简介　137
6.1.1　相似度与离散度指标　138
6.1.2　聚类方法类型简介　139
6.2　层次聚类算法　141
6.2.1　层次聚类中的相似度指标　141
6.2.2　定义层次聚类中的簇　143
6.2.3　如何理解层次聚类图　145
6.2.4　验证聚类结果　147
6.3　k均值聚类―基于均值聚类　148
6.3.1　k均值算法　148
6.3.2　函数kmeans()　149
6.3.3　silhouette图―可视化聚类结果　152
6.4　k中心点聚类―基于样本中心聚类　153
6.4.1　什么是中心点　154
6.4.2　函数kmedoids()　154
6.4.3　评估聚类结果　156
6.5　高斯混合模型聚类　156
6.5.1　高斯分布　156
6.5.2　MATLAB中的GMM支持　157
6.5.3　使用后验概率分布进行聚类　159
6.6　总结　160
第7章　人工神经网络――模拟人脑的思考方式　162
7.1　神经网络简介　162
7.2　神经网络基础构成　165
7.2.1　隐藏层数量　170
7.2.2　每层的节点数量　170
7.2.3　神经网络训练方法　170
7.3　神经网络工具箱　171
7.4　工具箱的用户界面　175
7.5　使用神经网络进行数据拟合　176
7.5.1　如何使用拟合App（nftool）　178
7.5.2　脚本分析　186
7.6　总结　188
第8章　降维――改进机器学习模型的性能　190
8.1　特征选择　190
8.1.1　分步回归　191
8.1.2　MATLAB中的分步回归　192
8.2　特征提取　199
8.3　总结　210
第9章　机器学习实战　211
9.1　用于预测混凝土质量的数据拟合　211
9.2　使用神经网络诊断甲状腺疾病　222
9.3　使用模糊聚类对学生进行分簇　226
9.4　总结　231
・ ・ ・ ・ ・ ・ (收起)译者序
前 言
致 谢
第1章 引言1
1.1 传统机器学习范式1
1.2 案例3
1.3 终身学习简史7
1.4 终身学习的定义9
1.5 知识类型和关键挑战14
1.6 评估方法和大数据的角色17
1.7 本书大纲18
第2章 相关学习范式20
2.1 迁移学习20
2.1.1 结构对应学习21
2.1.2 朴素贝叶斯迁移分类器22
2.1.3 迁移学习中的深度学习23
2.1.4 迁移学习与终身学习的区别24
2.2 多任务学习25
2.2.1 多任务学习中的任务相关性25
2.2.2 GO-MTL：使用潜在基础任务的多任务学习26
2.2.3 多任务学习中的深度学习28
2.2.4 多任务学习与终身学习的区别30
2.3 在线学习30
2.4 强化学习31
2.5 元学习32
2.6 小结34
第3章 终身监督学习35
3.1 定义和概述36
3.2 基于记忆的终身学习37
3.2.1 两个基于记忆的学习方法37
3.2.2 终身学习的新表达37
3.3 终身神经网络39
3.3.1 MTL网络39
3.3.2 终身EBNN40
3.4 ELLA：高效终身学习算法41
3.4.1 问题设定41
3.4.2 目标函数42
3.4.3 解决一个低效问题43
3.4.4 解决第二个低效问题45
3.4.5 主动的任务选择46
3.5 终身朴素贝叶斯分类47
3.5.1 朴素贝叶斯文本分类47
3.5.2 LSC的基本思想49
3.5.3 LSC技术50
3.5.4 讨论52
3.6 基于元学习的领域词嵌入52
3.7 小结和评估数据集54
第4章 持续学习与灾难性遗忘56
4.1 灾难性遗忘56
4.2 神经网络中的持续学习58
4.3 无遗忘学习61
4.4 渐进式神经网络62
4.5 弹性权重合并63
4.6 iCaRL：增量分类器与表示学习65
4.6.1 增量训练66
4.6.2 更新特征表示67
4.6.3 为新类构建范例集68
4.6.4 在iCaRL中完成分类68
4.7 专家网关69
4.7.1 自动编码网关69
4.7.2 测量训练的任务相关性70
4.7.3 为测试选择相关的专家71
4.7.4 基于编码器的终身学习71
4.8 生成式重放的持续学习72
4.8.1 生成式对抗网络72
4.8.2 生成式重放73
评估灾难性遗忘74
4.10 小结和评估数据集75
第5章 开放式学习79
5.1 问题定义和应用80
5.2 基于中心的相似空间学习81
5.2.1 逐步更新CBS学习模型82
5.2.2 测试CBS学习模型84
5.2.3 用于未知类检测的CBS学习84
5.3 DOC：深度开放式分类87
5.3.1 前馈层和一对其余层87
5.3.2 降低开放空间89
5.3.3 DOC用于图像分类90
5.3.4 发现未知类90
5.4 小结和评估数据集91
第6章 终身主题建模93
6.1 终身主题建模的主要思想93
6.2 LTM：终身主题模型97
6.2.1 LTM模型97
6.2.2 主题知识挖掘99
6.2.3 融合过去的知识100
6.2.4 Gibbs采样器的条件分布102
6.3 AMC：少量数据的终身主题模型102
6.3.1 AMC整体算法103
6.3.2 挖掘must-link知识104
6.3.3 挖掘cannot-link知识107
6.3.4 扩展的Pólya瓮模型108
6.3.5 Gibbs采样器的采样分布110
6.4 小结和评估数据集112
第7章 终身信息提取114
7.1 NELL：停止语言学习器114
7.1.1 NELL结构117
7.1.2 NELL中的提取器与学习118
7.1.3 NELL中的耦合约束120
7.2 终身评价目标提取121
7.2.1 基于的终身学习122
7.2.2 AER算法123
7.2.3 知识学习124
7.2.4 使用过去知识125
7.3 在工作中学习126
7.3.1 条件随机场127
7.3.2 一般依赖特征128
7.3.3 L-CRF算法130
7.4 Lifelong-RL：终身松弛标记法131
7.4.1 松弛标记法132
7.4.2 终身松弛标记法133
7.5 小结和评估数据集133
第8章 聊天机器人的持续知识学习135
8.1 LiLi：终身交互学习与推理136
8.2 LiLi的基本思想139
8.3 LiLi的组件141
8.4 运行示例142
8.5 小结和评估数据集142
第9章 终身强化学习144
9.1 基于多环境的终身强化学习146
9.2 层次贝叶斯终身强化学习147
9.2.1 动机147
9.2.2 层次贝叶斯方法148
9.2.3 MTRL算法149
9.2.4 更新层次模型参数150
9.2.5 对MDP进行采样151
9.3 PG-ELLA：终身策略梯度强化学习152
9.3.1 策略梯度强化学习152
9.3.2 策略梯度终身学习设置154
9.3.3 目标函数和优化154
9.3.4 终身学习的安全策略搜索156
9.3.5 跨领域终身强化学习156
9.4 小结和评估数据集157
第10章 结论及未来方向159
参考文献164
・ ・ ・ ・ ・ ・ (收起)第1章　Python机器学习实践入门 1
1.1　机器学习常用概念 1
1.2　数据的准备、处理和可视化
―NumPy、pandas和matplotlib教程 6
1.2.1　NumPy的用法 6
1.2.2　理解pandas模块 23
1.2.3　matplotlib教程 32
1.3　本书使用的科学计算库 35
1.4　机器学习的应用场景 36
1.5　小结 36
第2章　无监督机器学习 37
2.1　聚类算法 37
2.1.1　分布方法 38
2.1.2　质心点方法 40
2.1.3　密度方法 41
2.1.4　层次方法 44
2.2　降维 52
2.3　奇异值分解（SVD） 57
2.4　小结 58
第3章　有监督机器学习 59
3.1　模型错误评估 59
3.2　广义线性模型 60
3.2.1　广义线性模型的概率
解释 63
3.2.2　k近邻 63
3.3　朴素贝叶斯 64
3.3.1　多项式朴素贝叶斯 65
3.3.2　高斯朴素贝叶斯 66
3.4　决策树 67
3.5　支持向量机 70
3.6　有监督学习方法的对比 75
3.6.1　回归问题 75
3.6.2　分类问题 80
3.7　隐马尔可夫模型 84
3.8　小结 93
第4章　Web挖掘技术 94
4.1　Web结构挖掘 95
4.1.1　Web爬虫 95
4.1.2　索引器 95
4.1.3　排序―PageRank
算法 96
4.2　Web内容挖掘 97
句法解析 97
4.3　自然语言处理 98
4.4　信息的后处理 108
4.4.1　潜在狄利克雷分配 108
4.4.2　观点挖掘（情感
分析） 113
4.5　小结 117
第5章　推荐系统 118
5.1　效用矩阵 118
5.2　相似度度量方法 120
5.3　协同过滤方法 120
5.3.1　基于记忆的协同
过滤 121
5.3.2　基于模型的协同
过滤 126
5.4　CBF方法 130
5.4.1　商品特征平均得分
方法 131
5.4.2　正则化线性回归
方法 132
5.5　用关联规则学习，构建推荐
系统 133
5.6　对数似然比推荐方法 135
5.7　混合推荐系统 137
5.8　推荐系统评估 139
5.8.1　均方根误差（RMSE）
评估 140
5.8.2　分类效果的度量方法 143
5.9　小结 144
第6章　开始Django之旅 145
6.1　HTTP―GET和POST方法的
基础 145
6.1.1　Django的安装和
服务器的搭建 146
6.1.2　配置 147
6.2　编写应用―Django
最重要的功能 150
6.2.1　model 150
6.2.2　HTML网页背后的
URL和view 151
6.2.3　URL声明和view 154
6.3　管理后台 157
6.3.1　shell接口 158
6.3.2　命令 159
6.3.3　RESTful应用编程
接口（API） 160
6.4　小结 162
第7章　电影推荐系统Web应用 163
7.1　让应用跑起来 163
7.2　model 165
7.3　命令 166
7.4　实现用户的注册、登录和
登出功能 172
7.5　信息检索系统（电影查询） 175
7.6　打分系统 178
7.7　推荐系统 180
7.8　管理界面和API 182
7.9　小结 184
第8章　影评情感分析应用 185
8.1　影评情感分析应用用法
简介 185
8.2　搜索引擎的选取和应用的
代码 187
8.3　Scrapy的配置和情感分析
应用代码 189
8.3.1　Scrapy的设置 190
8.3.2　Scraper 190
8.3.3　Pipeline 193
8.3.4　爬虫 194
8.4　Django model 196
8.5　整合Django和Scrapy 197
8.5.1　命令（情感分析模型和
删除查询结果） 198
8.5.2　情感分析模型加载器 198
8.5.3　删除已执行过的查询 201
8.5.4　影评情感分析器―
Django view和HTML
代码 202
8.6　PageRank：Django view和
算法实现 206
8.7　管理后台和API 210
8.8　小结 212
・ ・ ・ ・ ・ ・ (收起)第1章　监督学习　　1
1.1　简介　　1
1.2　数据预处理技术　　2
1.2.1　准备工作　　2
1.2.2　详细步骤　　2
1.3　标记编码方法　　4
1.4　创建线性回归器　　6
1.4.1　准备工作　　6
1.4.2　详细步骤　　7
1.5　计算回归准确性　　9
1.5.1　准备工作　　9
1.5.2　详细步骤　　10
1.6　保存模型数据　　10
1.7　创建岭回归器　　11
1.7.1　准备工作　　11
1.7.2　详细步骤　　12
1.8　创建多项式回归器　　13
1.8.1　准备工作　　13
1.8.2　详细步骤　　14
1.9　估算房屋价格　　15
1.9.1　准备工作　　15
1.9.2　详细步骤　　16
1.10　计算特征的相对重要性　　17
1.11　评估共享单车的需求分布　　19
1.11.1　准备工作　　19
1.11.2　详细步骤　　19
1.11.3　更多内容　　21
第2章　创建分类器　　24
2.1　简介　　24
2.2　建立简单分类器　　25
2.2.1　详细步骤　　25
2.2.2　更多内容　　27
2.3　建立逻辑回归分类器　　27
2.4　建立朴素贝叶斯分类器　　31
2.5　将数据集分割成训练集和测试集　　32
2.6　用交叉验证检验模型准确性　　33
2.6.1　准备工作　　34
2.6.2　详细步骤　　34
2.7　混淆矩阵可视化　　35
2.8　提取性能报告　　37
2.9　根据汽车特征评估质量　　38
2.9.1　准备工作　　38
2.9.2　详细步骤　　38
2.10　生成验证曲线　　40
2.11　生成学习曲线　　43
2.12　估算收入阶层　　45
第3章　预测建模　　48
3.1　简介　　48
3.2　用SVM建立线性分类器　　49
3.2.1　准备工作　　49
3.2.2　详细步骤　　50
3.3　用SVM建立非线性分类器　　53
3.4　解决类型数量不平衡问题　　55
3.5　提取置信度　　58
3.6　寻找最优超参数　　60
3.7　建立事件预测器　　62
3.7.1　准备工作　　62
3.7.2　详细步骤　　62
3.8　估算交通流量　　64
3.8.1　准备工作　　64
3.8.2　详细步骤　　64
第4章　无监督学习――聚类　　67
4.1　简介　　67
4.2　用k-means算法聚类数据　　67
4.3　用矢量量化压缩图片　　70
4.4　建立均值漂移聚类模型　　74
4.5　用凝聚层次聚类进行数据分组　　76
4.6　评价聚类算法的聚类效果　　79
4.7　用DBSCAN算法自动估算集群数量　　82
4.8　探索股票数据的模式　　86
4.9　建立客户细分模型　　88
第5章　构建推荐引擎　　91
5.1　简介　　91
5.2　为数据处理构建函数组合　　92
5.3　构建机器学习流水线　　93
5.3.1　详细步骤　　93
5.3.2　工作原理　　95
5.4　寻找最近邻　　95
5.5　构建一个KNN分类器　　98
5.5.1　详细步骤　　98
5.5.2　工作原理　　102
5.6　构建一个KNN回归器　　102
5.6.1　详细步骤　　102
5.6.2　工作原理　　104
5.7　计算欧氏距离分数　　105
5.8　计算皮尔逊相关系数　　106
5.9　寻找数据集中的相似用户　　108
5.10　生成电影推荐　　109
第6章　分析文本数据　　112
6.1　简介　　112
6.2　用标记解析的方法预处理数据　　113
6.3　提取文本数据的词干　　114
6.3.1　详细步骤　　114
6.3.2　工作原理　　115
6.4　用词形还原的方法还原文本的基本形式　　116
6.5　用分块的方法划分文本　　117
6.6　创建词袋模型　　118
6.6.1　详细步骤　　118
6.6.2　工作原理　　120
6.7　创建文本分类器　　121
6.7.1　详细步骤　　121
6.7.2　工作原理　　123
6.8　识别性别　　124
6.9　分析句子的情感　　125
6.9.1　详细步骤　　126
6.9.2　工作原理　　128
6.10　用主题建模识别文本的模式　　128
6.10.1　详细步骤　　128
6.10.2　工作原理　　131
第7章　语音识别　　132
7.1　简介　　132
7.2　读取和绘制音频数据　　132
7.3　将音频信号转换为频域　　134
7.4　自定义参数生成音频信号　　136
7.5　合成音乐　　138
7.6　提取频域特征　　140
7.7　创建隐马尔科夫模型　　142
7.8　创建一个语音识别器　　143
第8章　解剖时间序列和时序数据　　147
8.1　简介　　147
8.2　将数据转换为时间序列格式　　148
8.3　切分时间序列数据　　150
8.4　操作时间序列数据　　152
8.5　从时间序列数据中提取统计数字　　154
8.6　针对序列数据创建隐马尔科夫模型　　157
8.6.1　准备工作　　158
8.6.2　详细步骤　　158
8.7　针对序列文本数据创建条件随机场　　161
8.7.1　准备工作　　161
8.7.2　详细步骤　　161
8.8　用隐马尔科夫模型分析股票市场数据　　164
第9章　图像内容分析　　166
9.1　简介　　166
9.2　用OpenCV-Pyhon操作图像　　167
9.3　检测边　　170
9.4　直方图均衡化　　174
9.5　检测棱角　　176
9.6　检测SIFT特征点　　178
9.7　创建Star特征检测器　　180
9.8　利用视觉码本和向量量化创建特征　　182
9.9　用极端随机森林训练图像分类器　　185
9.10　创建一个对象识别器　　187
第10章　人脸识别　　189
10.1　简介　　189
10.2　从网络摄像头采集和处理视频信息　　189
10.3　用Haar级联创建一个人脸识别器　　191
10.4　创建一个眼睛和鼻子检测器　　193
10.5　做主成分分析　　196
10.6　做核主成分分析　　197
10.7　做盲源分离　　201
10.8　用局部二值模式直方图创建一个人脸识别器　　205
第11章　深度神经网络　　210
11.1　简介　　210
11.2　创建一个感知器　　211
11.3　创建一个单层神经网络　　213
11.4　创建一个深度神经网络　　216
11.5　创建一个向量量化器　　219
11.6　为序列数据分析创建一个递归神经网络　　221
11.7　在光学字符识别数据库中将字符可视化　　225
11.8　用神经网络创建一个光学字符识别器　　226
第12章　可视化数据　　230
12.1　简介　　230
12.2　画3D散点图　　230
12.3　画气泡图　　232
12.4　画动态气泡图　　233
12.5　画饼图　　235
12.6　画日期格式的时间序列数据　　237
12.7　画直方图　　239
12.8　可视化热力图　　241
12.9　动态信号的可视化模拟　　242
・ ・ ・ ・ ・ ・ (收起)目录
第1章　机器学习基础　　1
1.1　机器学习概要　　2
什么是机器学习　　2
机器学习的种类　　3
机器学习的应用　　8
1.2　机器学习的步骤　　9
数据的重要性　　9
有监督学习（分类）的例子　　11
无监督学习（聚类）的例子　　16
可视化　　18
图形的种类和画法：使用Matplotlib显示图形的方法　　22
使用pandas理解和处理数据　　30
本章小结　　36
第2章　有监督学习　　37
2.1　算法1：线性回归　　38
概述　　38
算法说明　　39
详细说明　　41
2.2　算法2：正则化　　45
概述　　45
算法说明　　48
详细说明　　50
2.3　算法3：逻辑回归　　52
概述　　52
算法说明　　53
详细说明　　55
2.4　算法4：支持向量机　　58
概述　　58
算法说明　　59
详细说明　　60
2.5　算法5：支持向量机（核方法）　　63
概述　　63
算法说明　　64
详细说明　　65
2.6　算法6：朴素贝叶斯　　68
概述　　68
算法说明　　70
详细说明　　74
2.7　算法7：随机森林　　76
概述　　76
算法说明　　77
详细说明　　80
2.8　算法8：神经网络　　81
概述　　81
算法说明　　83
详细说明　　86
2.9　算法9：KNN　　88
概述　　88
算法说明　　89
详细说明　　90
第3章　无监督学习　　93
3.1　算法10：PCA　　94
概述　　94
算法说明　　95
详细说明　　98
3.2　算法11：LSA　　99
概述　　99
算法说明　　100
详细说明　　104
3.3　算法12：NMF　　105
概述　　105
算法说明　　106
详细说明　　108
3.4　算法13：LDA　　111
概述　　111
算法说明　　112
详细说明　　114
3.5　算法14：k-means算法　　117
概述　　117
算法说明　　117
详细说明　　119
3.6　算法15：混合高斯分布　　122
概述　　122
算法说明　　123
详细说明　　126
3.7　算法16：LLE　　127
概述　　127
算法说明　　128
详细说明　　131
3.8　算法17：t-SNE　　133
概述　　133
算法说明　　134
详细说明　　136
第4章　评估方法和各种数据的处理　　139
4.1　评估方法　　140
有监督学习的评估　　140
分类问题的评估方法　　140
回归问题的评估方法　　148
均方误差和决定系数指标的不同　　152
与其他算法进行比较　　152
超参数的设置　　154
模型的过拟合　　155
防止过拟合的方法　　155
将数据分为训练数据和验证数据　　156
交叉验证　　158
搜索超参数　　160
4.2　文本数据的转换处理　　163
基于单词出现次数的转换　　163
基于tf-idf的转换　　164
应用于机器学习模型　　165
4.3　图像数据的转换处理　　167
直接将像素信息作为数值使用　　167
将转换后的向量数据作为输入来应用机器学习模型　　168
第5章　环境搭建 171
5.1　Python 3的安装　　172
Windows　　172
macOS　　172
Linux　　173
使用Anaconda在Windows上安装　　174
5.2　虚拟环境　　175
通过官方安装程序安装Python的情况　　175
通过Anaconda安装Python的情况　　177
5.3　第三方包的安装　　178
什么是第三方包　　178
安装第三方包的方法　　178
参考文献　　180
・ ・ ・ ・ ・ ・ (收起)第1章　学习前的准备　　1
1.1　关于机器学习 2
1.1.1　学习机器学习的窍门 4
1.1.2　机器学习中问题的分类 5
1.1.3　本书的结构 6
1.2　安装Python 7
1.3　Jupyter Notebook 11
1.3.1　Jupyter Notebook的用法 11
1.3.2　输入Markdown格式文本 14
1.3.3　更改文件名 16
1.4　安装Keras和TensorFlow 17
第2章　Python基础知识　　19
2.1　四则运算 20
2.1.1　四则运算的用法 20
2.1.2　幂运算 20
2.2　变量 21
2.2.1　利用变量进行计算 21
2.2.2　变量的命名 21
2.3　类型 22
2.3.1　类型的种类 22
2.3.2　检查类型 22
2.3.3　字符串 23
2.4　print 语句 24
2.4.1　print语句的用法 24
2.4.2　同时显示数值和字符串的方法1 24
2.4.3　同时显示数值和字符串的方法2 25
2.5　list（数组变量） 26
2.5.1　list的用法 26
2.5.2　二维数组 27
2.5.3　创建连续的整数数组 28
2.6　tuple（数组） 29
2.6.1　tuple的用法 29
2.6.2　读取元素 29
2.6.3　长度为1的tuple 30
2.7　if 语句 31
2.7.1　if语句的用法 31
2.7.2　比较运算符 32
2.8　for 语句 33
2.8.1　for语句的用法 33
2.8.2　enumerate的用法 33
2.9　向量 34
2.9.1　NumPy的用法 34
2.9.2　定义向量 35
2.9.3　读取元素 36
2.9.4　替换元素 36
2.9.5　创建连续整数的向量 36
2.9.6　ndarray的注意事项 37
2.10　矩阵 38
2.10.1　定义矩阵 38
2.10.2　矩阵的大小 38
2.10.3　读取元素 39
2.10.4　替换元素 39
2.10.5　生成元素为0和1的ndarray 39
2.10.6　生成元素随机的矩阵 40
2.10.7　改变矩阵的大小 41
2.11　矩阵的四则运算 41
2.11.1　矩阵的四则运算 41
2.11.2　标量×矩阵 42
2.11.3　算术函数 42
2.11.4　计算矩阵乘积 43
2.12　切片 43
2.13　替换满足条件的数据 45
2.14　help 46
2.15　函数 47
2.15.1　函数的用法 47
2.15.2　参数与返回值 47
2.16　保存文件 49
2.16.1　保存一个ndarray类型变量 49
2.16.2　保存多个ndarray类型变量 49
第3章　数据可视化　　51
3.1　绘制二维图形 52
3.1.1　绘制随机图形 52
3.1.2　代码清单的格式 53
3.1.3　绘制三次函数f (x) = (x - 2) x (x + 2) 53
3.1.4　确定绘制范围 54
3.1.5　绘制图形 55
3.1.6　装饰图形 55
3.1.7　并列显示多张图形 58
3.2　绘制三维图形 59
3.2.1　包含两个变量的函数 59
3.2.2　用颜色表示数值：pcolor 60
3.2.3　绘制三维图形：surface 62
3.2.4　绘制等高线：contour 64
第4章　机器学习中的数学　　67
4.1　向量 68
4.1.1　什么是向量 68
4.1.2　用Python定义向量 69
4.1.3　列向量的表示方法 69
4.1.4　转置的表示方法 70
4.1.5　加法和减法 71
4.1.6　标量积 73
4.1.7　内积 74
4.1.8　向量的模 75
4.2　求和符号 76
4.2.1　带求和符号的数学式的变形 77
4.2.2　通过内积求和 79
4.3　累乘符号 79
4.4　导数 80
4.4.1　多项式的导数 80
4.4.2　带导数符号的数学式的变形 82
4.4.3　复合函数的导数 83
4.4.4　复合函数的导数：链式法则 84
4.5　偏导数 85
4.5.1　什么是偏导数 85
4.5.2　偏导数的图形 87
4.5.3　绘制梯度的图形 89
4.5.4　多变量的复合函数的偏导数 91
4.5.5　交换求和与求导的顺序 93
4.6　矩阵 95
4.6.1　什么是矩阵 95
4.6.2　矩阵的加法和减法 97
4.6.3　标量积 99
4.6.4　矩阵的乘积 100
4.6.5　单位矩阵 103
4.6.6　逆矩阵 105
4.6.7　转置 107
4.6.8　矩阵和联立方程式 109
4.6.9　矩阵和映射 111
4.7　指数函数和对数函数 113
4.7.1　指数 113
4.7.2　对数 115
4.7.3　指数函数的导数 118
4.7.4　对数函数的导数 120
4.7.5　Sigmoid函数 121
4.7.6　Softmax函数 123
4.7.7　Softmax函数和Sigmoid函数 127
4.7.8　高斯函数 128
4.7.9　二维高斯函数 129
第5章　有监督学习：回归　　135
5.1　一维输入的直线模型 136
5.1.1　直线模型 138
5.1.2　平方误差函数 139
5.1.3　求参数（梯度法） 142
5.1.4　直线模型参数的解析解 148
5.2　二维输入的平面模型 152
5.2.1　数据的表示方法 154
5.2.2　平面模型 155
5.2.3　平面模型参数的解析解 157
5.3　D维线性回归模型 159
5.3.1　D维线性回归模型 160
5.3.2　参数的解析解 160
5.3.3　扩展到不通过原点的平面 164
5.4　线性基底函数模型 165
5.5　过拟合问题 171
5.6　新模型的生成 181
5.7　模型的选择 185
5.8　小结 186
第6章　有监督学习：分类　　189
6.1　一维输入的二元分类 190
6.1.1　问题设置 190
6.1.2　使用概率表示类别分类 194
6.1.3　最大似然估计 196
6.1.4　逻辑回归模型 199
6.1.5　交叉熵误差 201
6.1.6　学习法则的推导 205
6.1.7　通过梯度法求解 209
6.2　二维输入的二元分类 210
6.2.1　问题设置 210
6.2.2　逻辑回归模型 214
6.3　二维输入的三元分类 219
6.3.1　三元分类逻辑回归模型 219
6.3.2　交叉熵误差 222
6.3.3　通过梯度法求解 223
第7章　神经网络与深度学习　　227
7.1　神经元模型 229
7.1.1　神经细胞 229
7.1.2　神经元模型 230
7.2　神经网络模型 234
7.2.1　二层前馈神经网络 234
7.2.2　二层前馈神经网络的实现 237
7.2.3　数值导数法 242
7.2.4　通过数值导数法应用梯度法 246
7.2.5　误差反向传播法 251
7.2.6　求.E / .vkj 252
7.2.7　求.E / .wji 256
7.2.8　误差反向传播法的实现 262
7.2.9　学习后的神经元的特性 268
7.3　使用Keras实现神经网络模型 270
7.3.1　二层前馈神经网络 271
7.3.2　Keras的使用流程 273
第8章　神经网络与深度学习的应用（手写数字识别）　　277
8.1　MINST数据集 278
8.2　二层前馈神经网络模型 279
8.3　ReLU激活函数 286
8.4　空间过滤器 291
8.5　卷积神经网络 295
8.6　池化 300
8.7　Dropout 301
8.8　融合了各种特性的MNIST识别网络模型 302
第9章　无监督学习　　307
9.1　二维输入数据 308
9.2　K-means算法 310
9.2.1　K-means算法的概要 310
9.2.2　步骤0：准备变量与初始化 311
9.2.3　步骤1：更新R 313
9.2.4　步骤2：更新μ 315
9.2.5　失真度量 318
9.3　混合高斯模型 320
9.3.1　基于概率的聚类 320
9.3.2　混合高斯模型 323
9.3.3　EM算法的概要 328
9.3.4　步骤0：准备变量与初始化 329
9.3.5　步骤1（步骤E）：更新γ 330
9.3.6　步骤2（步骤M）：更新π、μ和Σ 332
9.3.7　似然 336
第10章　本书小结　　339
后记　　349
・ ・ ・ ・ ・ ・ (收起)译者序
原书前言
致谢
第1章 文本机器学习导论1
1.1导论1
1.1.1本章内容组织结构2
1.2文本学习有何特别之处3
1.3文本分析模型4
1.3.1文本预处理和相似度计算4
1.3.2降维与矩阵分解6
1.3.3文本聚类6
1.3.4文本分类与回归建模8
1.3.5结合文本与异构数据的联合分析10
1.3.6信息检索与网页搜索11
1.3.7序列语言建模与嵌入11
1.3.8文本摘要11
1.3.9信息提取11
1.3.10意见挖掘与情感分析12
1.3.11文本分割与事件检测12
1.4本章小结12
1.5参考资料13
1.5.1软件资源13
1.6习题13
第2章 文本预处理与相似度计算15
2.1导论15
2.1.1本章内容组织结构16
2.2原始文本提取与词条化16
2.2.1文本提取中与网页相关的问题18
2.3从词条中提取词项19
2.3.1停用词移除19
2.3.2连字符19
2.3.3大小写转换20
2.3.4基于用法的合并20
2.3.5词干提取21
2.4向量空间表示与归一化21
2.5文本中的相似度计算23
2.5.1idf归一化和词干提取是否总是有用25
2.6本章小结26
2.7参考资料26
2.7.1软件资源26
2.8习题27
第3章 矩阵分解与主题建模28
3.1导论28
3.1.1本章内容组织结构30
3.1.2将二分解归一化为标准的三分解30
3.2奇异值分解（SVD)31
3.2.1SVD的例子33
3.2.2实现SVD的幂迭代法35
3.2.3SVD/LSA的应用35
3.2.4SVD/LSA的优缺点36
3.3非负矩阵分解36
3.3.1非负矩阵分解的可解释性38
3.3.2非负矩阵分解的例子39
3.3.3融入新文档40
3.3.4非负矩阵分解的优缺点41
3.4概率潜在语义分析（PLSA）41
3.4.1与非负矩阵分解的联系44
3.4.2与SVD的比较44
3.4.3PLSA的例子45
3.4.4PLSA的优缺点45
3.5隐含狄利克雷分布（LDA）概览46
3.5.1简化的LDA模型46
3.5.2平滑的LDA模型49
3.6非线性变换和特征工程50
3.6.1选择一个相似度函数52
3.6.2Nystrom估计58
3.6.3相似度矩阵的部分可用性60
3.7本章小结61
3.8参考资料62
3.8.1软件资源62
3.9习题63
第4章 文本聚类65
4.1导论65
4.1.1本章内容组织结构66
4.2特征选择与特征工程66
4.2.1特征选择67
4.2.2特征工程68
4.3主题建模和矩阵分解70
4.3.1混合隶属度模型与重叠簇70
4.3.2非重叠簇与双聚类：矩阵分解的角度70
4.4面向聚类的生成混合模型74
4.4.1伯努利模型75
4.4.2多项式模型76
4.4.3与混合隶属度主题模型的比较77
4.4.4与朴素贝叶斯分类模型的联系77
4.5k均值算法78
4.5.1收敛与初始化80
4.5.2计算复杂度80
4.5.3与概率模型的联系81
4.6层次聚类算法81
4.6.1高效实现与计算复杂度83
4.6.2与k均值的自然联姻84
4.7聚类集成85
4.7.1选择集成分量86
4.7.2混合来自不同分量的结果86
4.8将文本当作序列来进行聚类87
4.8.1面向聚类的核方法87
4.8.2数据相关的核方法：谱聚类90
4.9聚类到有监督学习的转换91
4.9.1实际问题92
4.10聚类评估93
4.10.1内部有效性度量的缺陷93
4.10.2外部有效性度量93
4.11本章小结97
4.12参考资料97
4.12.1软件资源98
4.13习题98
第5章 文本分类：基本模型100
5.1导论100
5.1.1标记的类型与回归建模101
5.1.2训练与测试102
5.1.3归纳、直推和演绎学习器102
5.1.4基本模型103
5.1.5分类器中与文本相关的挑战103
5.2特征选择与特征工程104
5.2.1基尼系数104
5.2.2条件熵105
5.2.3逐点互信息105
5.2.4紧密相关的度量方式106
5.2.5χ2-统计量106
5.2.6嵌入式特征选择模型108
5.2.7特征工程技巧108
5.3朴素贝叶斯模型109
5.3.1伯努利模型109
5.3.2多项式模型111
5.3.3实际观察113
5.3.4利用朴素贝叶斯对输出进行排序113
5.3.5朴素贝叶斯的例子113
5.3.6半监督朴素贝叶斯116
5.4最近邻分类器118
5.4.11-最近邻分类器的属性119
5.4.2Rocchio与最近质心分类121
5.4.3加权最近邻122
5.4.4自适应最近邻：一系列有效的方法124
5.5决策树与随机森林126
5.5.1构造决策树的基本步骤126
5.5.2分裂一个节点127
5.5.3多变量分裂128
5.5.4决策树在文本分类中的问题129
5.5.5随机森林129
5.5.6把随机森林看作自适应最近邻方法130
5.6基于规则的分类器131
5.6.1顺序覆盖算法131
5.6.2从决策树中生成规则133
5.6.3关联分类器134
5.6.4预测135
5.7本章小结135
5.8参考资料135
5.8.1软件资源137
5.9习题137
第6章 面向文本的线性分类与回归140
6.1导论140
6.1.1线性模型的几何解释141
6.1.2我们需要偏置变量吗142
6.1.3使用正则化的线性模型的一般定义143
6.1.4将二值预测推广到多类144
6.1.5面向文本的线性模型的特点145
6.2最小二乘回归与分类145
6.2.1使用L2正则化的最小二乘回归145
6.2.2LASSO:使用L1正则化的最小二乘回归148
6.2.3Fisher线性判别与最小二乘分类器150
6.3支持向量机(SVM)156
6.3.1正则优化解释156
6.3.2最大间隔解释157
6.3.3Pegasos：在原始空间中求解SVM 159
6.3.4对偶SVM优化形式160
6.3.5对偶SVM的学习算法162
6.3.6对偶SVM的自适应最近邻解释163
6.4对数几率回归165
6.4.1正则优化解释165
6.4.2对数几率回归的训练算法166
6.4.3对数几率回归的概率解释167
6.4.4多元对数几率回归与其他推广168
6.4.5关于对数几率回归性能的评述169
6.5线性模型的非线性推广170
6.5.1基于显式变换的核SVM171
6.5.2为什么传统的核函数能够提升线性可分性172
6.5.3不同核函数的优缺点174
6.5.4核技巧175
6.5.5核技巧的系统性应用176
6.6本章小结179
6.7参考资料180
6.7.1软件资源181
6.8习题181
第7章 分类器的性能与评估184
7.1导论184
7.1.1本章内容组织结构184
7.2偏置-方差权衡185
7.2.1一个形式化的观点186
7.2.2偏置和方差的迹象189
7.3偏置-方差权衡在性能方面可能的影响189
7.3.1训练数据规模的影响189
7.3.2数据维度的影响191
7.3.3文本中模型选择可能的影响191
7.4利用集成方法系统性地提升性能192
7.4.1bagging与子采样192
7.4.2boosting193
7.5分类器评估195
7.5.1分割为训练部分和测试部分196
7.5.2绝对准确率度量197
7.5.3面向分类和信息检索的排序度量199
7.6本章小结204
7.7参考资料205
7.7.1boosting与对数几率回归的联系205
7.7.2分类器评估205
7.7.3软件资源206
7.7.4用于评估的数据集206
7.8习题206
第8章 结合异构数据的联合文本挖掘208
8.1导论208
8.1.1本章内容组织结构210
8.2共享矩阵分解的技巧210
8.2.1分解图210
8.2.2应用：结合文本和网页链接进行共享分解211
8.2.3应用：结合文本与无向社交网络214
8.2.4应用：结合文本的图像迁移学习215
8.2.5应用：结合评分和文本的推荐系统217
8.2.6应用：跨语言文本挖掘218
8.3分解机219
8.4联合概率建模技术223
8.4.1面向聚类的联合概率模型223
8.4.2朴素贝叶斯分类器224
8.5到图挖掘技术的转换224
8.6本章小结226
8.7参考资料227
8.7.1软件资源227
8.8习题228
第9章 信息检索与搜索引擎229
9.1导论229
9.1.1本章内容组织结构230
9.2索引和查询处理230
9.2.1词典数据结构231
9.2.2倒排索引233
9.2.3线性时间的索引构建234
9.2.4查询处理236
9.2.5效率优化244
9.3信息检索模型的评分248
9.3.1基于tf-idf的向量空间模型248
9.3.2二值独立模型249
9.3.3使用词项频率的BM25模型251
9.3.4信息检索中的统计语言模型252
9.4网络爬虫与资源发现254
9.4.1一个基本的爬虫算法255
9.4.2带偏好的爬虫256
9.4.3多线程257
9.4.4避开蜘蛛陷阱258
9.4.5用于近似重复检测的Shingling方法258
9.5搜索引擎中的查询处理259
9.5.1分布式索引构建259
9.5.2动态索引更新260
9.5.3查询处理260
9.5.4信誉度的重要性261
9.6基于链接的排序算法262
9.6.1PageRank262
9.6.2HITS267
9.7本章小结269
9.8参考资料269
9.8.1软件资源270
9.9习题270
第10章 文本序列建模与深度学习272
10.1导论272
10.1.1本章内容组织结构274
10.2统计语言模型274
10.2.1skip-gram模型277
10.2.2与嵌入的关系278
10.3核方法279
10.4单词-上下文矩阵分解模型 280
10.4.1使用计数的矩阵分解280
10.4.2GloVe嵌入282
10.4.3PPMI矩阵分解283
10.4.4位移PPMI矩阵分解283
10.4.5融入句法和其他特征283
10.5单词距离的图形化表示284
10.6神经语言模型285
10.6.1神经网络简介285
10.6.2基于word2vec的神经嵌入295
10.6.3word2vec(SGNS)是对数几率矩阵分解302
10.6.4除了单词以外：基于doc2vec的段落嵌入304
10.7循环神经网络(RNN)305
10.7.1实际问题307
10.7.2RNN的语言建模示例308
10.7.3图像描述应用310
10.7.4序列到序列学习与机器翻译311
10.7.5句子级分类应用314
10.7.6使用语言特征的词条级分类315
10.7.7多层循环网络316
10.8本章小结319
10.9参考资料319
10.9.1软件资源320
10.10习题321
第11章 文本摘要323
11.1导论323
11.1.1提取式摘要与抽象式摘要324
11.1.2提取式摘要中的关键步骤324
11.1.3提取式摘要中的分割阶段324
11.1.4本章内容组织结构325
11.2提取式摘要的主题词方法325
11.2.1词项概率325
11.2.2归一化频率权重326
11.2.3主题签名327
11.2.4句子选择方法329
11.3提取式摘要的潜在方法329
11.3.1潜在语义分析330
11.3.2词汇链331
11.3.3基于图的方法332
11.3.4质心摘要333
11.4面向提取式摘要的机器学习334
11.4.1特征提取334
11.4.2使用哪种分类器335
11.5多文档摘要335
11.5.1基于质心的摘要335
11.5.2基于图的方法336
11.6抽象式摘要337
11.6.1句子压缩337
11.6.2信息融合338
11.6.3信息排列338
11.7本章小结338
11.8参考资料339
11.8.1软件资源339
11.9习题340
第12章 信息提取341
12.1导论341
12.1.1历史演变343
12.1.2自然语言处理的角色343
12.1.3本章内容组织结构345
12.2命名实体识别345
12.2.1基于规则的方法346
12.2.2转化为词条级分类任务349
12.2.3隐马尔可夫模型350
12.2.4最大熵马尔可夫模型354
12.2.5条件随机场355
12.3关系提取357
12.3.1转换为分类问题357
12.3.2利用显式的特征工程进行关系预测358
12.3.3利用隐式的特征工程进行关系预测：核方法361
12.4本章小结365
12.5参考资料365
12.5.1弱监督学习方法366
12.5.2无监督与开放式信息提取 366
12.5.3软件资源367
12.6习题367
第13章 意见挖掘与情感分析368
13.1导论368
13.1.1意见词典370
13.1.2把意见挖掘看作槽填充和信息提取任务371
13.1.3本章内容组织结构372
13.2文档级情感分析372
13.2.1面向分类的无监督方法374
13.3短语级与句子级情感分类375
13.3.1句子级与短语级分析的应用376
13.3.2主观性分类到最小割问题的归约376
13.3.3句子级与短语级极性分析中的上下文377
13.4把基于方面的意见挖掘看作信息提取任务377
13.4.1Hu和Liu的无监督方法378
13.4.2OPINE：一种无监督方法379
13.4.3把有监督意见提取看作词条级分类任务380
13.5虚假意见381
13.5.1面向虚假评论检测的有监督方法382
13.5.2面向虚假评论制造者检测的无监督方法384
13.6意见摘要384
13.6.1评分总结384
13.6.2情感总结385
13.6.3基于短语与句子的情感总结385
13.6.4提取式与抽象式总结385
13.7本章小结385
13.8参考资料385
13.8.1软件资源387
13.9习题387
第14章 文本分割与事件检测388
14.1导论388
14.1.1与话题检测和追踪的关系388
14.1.2本章内容组织结构389
14.2文本分割389
14.2.1TextTiling390
14.2.2C99方法390
14.2.3基于现成的分类器的有监督的分割392
14.2.4基于马尔可夫模型的有监督的分割393
14.3文本流挖掘395
14.3.1流式文本聚类395
14.3.2面向首次报道检测的应用 396
14.4事件检测397
14.4.1无监督的事件检测397
14.4.2把有监督的事件检测看作有监督的分割任务399
14.4.3把事件检测看作一个信息提取问题399
14.5本章小结402
14.6参考资料402
14.6.1软件资源402
14.7习题403
参考文献404
・ ・ ・ ・ ・ ・ (收起)译者序
前言
第1章探索数据分析1
1.1Scala入门2
1.2去除分类字段的重复值2
1.3数值字段概述4
1.4基本抽样、分层抽样和一致抽样5
1.5使用Scala和Spark的Note―book工作8
1.6相关性的基础12
1.7总结14
第2章数据管道和建模15
2.1影响图16
2.2序贯试验和风险处理17
2.3探索与利用问题21
2.4不知之不知23
2.5数据驱动系统的基本组件23
2.5.1数据收集24
2.5.2数据转换层25
2.5.3数据分析与机器学习26
2.5.4UI组件26
2.5.5动作引擎28
2.5.6关联引擎28
2.5.7监控28
2.6优化和交互28
2.7总结29
第3章使用Spark和MLlib30
3.1安装Spark31
3.2理解Spark的架构32
3.2.1任务调度32
3.2.2Spark的组件35
3.2.3MQTT、ZeroMQ、Flume和Kafka36
3.2.4HDFS、Cassandra、S3和Tachyon37
3.2.5Mesos、YARN和Standa―lone38
3.3应用38
3.3.1单词计数38
3.3.2基于流的单词计数41
3.3.3SparkSQL和数据框45
3.4机器学习库46
3.4.1SparkR47
3.4.2图算法：Graphx和Graph―Frames48
3.5Spark的性能调整48
3.6运行Hadoop的HDFS49
3.7总结54
第4章监督学习和无监督学习55
4.1记录和监督学习55
4.1.1Iirs数据集56
4.1.2类标签点57
4.1.3SVMWithSGD58
4.1.4logistic回归60
4.1.5决策树62
4.1.6bagging和boosting：集成学习方法66
4.2无监督学习66
4.3数据维度71
4.4总结73
第5章回归和分类74
5.1回归是什么74
5.2连续空间和度量75
5.3线性回归77
5.4logistic回归81
5.5正则化83
5.6多元回归84
5.7异方差84
5.8回归树85
5.9分类的度量87
5.10多分类问题87
5.11感知机87
5.12泛化误差和过拟合90
5.13总结90
第6章使用非结构化数据91
6.1嵌套数据92
6.2其他序列化格式100
6.3Hive和Impala102
6.4会话化104
6.5使用特质109
6.6使用模式匹配110
6.7非结构化数据的其他用途113
6.8概率结构113
6.9投影113
6.10总结113
第7章使用图算法115
7.1图简介115
7.2SBT116
7.3Scala的图项目119
7.3.1增加节点和边121
7.3.2图约束123
7.3.3JSON124
7.4GraphX126
7.4.1谁收到电子邮件130
7.4.2连通分量131
7.4.3三角形计数132
7.4.4强连通分量132
7.4.5PageRank133
7.4.6SVD++134
7.5总结138
第8章Scala与R和Python的集成139
8.1R的集成140
8.1.1R和SparkR的相关配置140
8.1.2数据框144
8.1.3线性模型150
8.1.4广义线性模型152
8.1.5在SparkR中读取JSON文件156
8.1.6在SparkR中写入Parquet文件157
8.1.7从R调用Scala158
8.2Python的集成161
8.2.1安装Python161
8.2.2PySpark162
8.2.3从Java／Scala调用Python163
8.3总结167
第9章Scala中的NLP169
9.1文本分析流程170
9.2Spark的MLlib库177
9.2.1TF―IDF177
9.2.2LDA178
9.3分词、标注和分块185
9.4POS标记186
9.5使用word2vec寻找词关系189
9.6总结192
第10章高级模型监控193
10.1系统监控194
10.2进程监控195
10.3模型监控201
10.3.1随时间变化的性能202
10.3.2模型停用标准202
10.3.3A／B测试202
10.4总结202
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 概 述
1.1 什么是机器学习――从一个小故事开始 / 002
1.2 机器学习的一些应用场景――蝙蝠公司的业务单元 / 003
1.3 机器学习应该如何入门――世上无难事 / 005
1.4 有监督学习与无监督学习 / 007
1.5 机器学习中的分类与回归 / 008
1.6 模型的泛化、过拟合与欠拟合 / 008
1.7 小结 / 009
第2章 基于Python语言的环境配置
2.1 Python的下载和安装 / 012
2.2 Jupyter Notebook的安装与使用方法 / 013
2.2.1 使用pip进行Jupyter Notebook的下载和安装 / 013
2.2.2 运行Jupyter Notebook / 014
2.2.3 Jupyter Notebook的使用方法 / 015
2.3 一些必需库的安装及功能简介 / 017
2.3.1 Numpy――基础科学计算库 / 017
2.3.2 Scipy――强大的科学计算工具集 / 018
2.3.3 pandas――数据分析的利器 / 019
2.3.4 matplotlib――画出优美的图形 / 020
深入浅出Python 机器学习
VIII
2.4 scikit-learn――非常流行的Python机器学习库 / 021
2.5 小结 / 022
第3章 K最近邻算法――近朱者赤，近墨者黑
3.1 K最近邻算法的原理 / 024
3.2 K最近邻算法的用法 / 025
3.2.1 K最近邻算法在分类任务中的应用 / 025
3.2.2 K最近邻算法处理多元分类任务 / 029
3.2.3 K最近邻算法用于回归分析 / 031
3.3 K最近邻算法项目实战――酒的分类 / 034
3.3.1 对数据集进行分析 / 034
3.3.2 生成训练数据集和测试数据集 / 036
3.3.3 使用K最近邻算法进行建模 / 038
3.3.4 使用模型对新样本的分类进行预测 / 039
3.4 小结 / 041
第4章 广义线性模型――“耿直”的算法模型
4.1 线性模型的基本概念 / 044
4.1.1 线性模型的一般公式 / 044
4.1.2 线性模型的图形表示 / 045
4.1.3 线性模型的特点 / 049
4.2 最基本的线性模型――线性回归 / 050
4.2.1 线性回归的基本原理 / 050
4.2.2 线性回归的性能表现 / 051
4.3 使用L2正则化的线性模型――岭回归 / 053
4.3.1 岭回归的原理 / 053
4.3.2 岭回归的参数调节 / 054
4.4 使用L1正则化的线性模型――套索回归 / 058
4.4.1 套索回归的原理 / 058
4.4.2 套索回归的参数调节 / 059
4.4.3 套索回归与岭回归的对比 / 060
目
录
IX
4.5 小结 / 062
第5章 朴素贝叶斯――打雷啦，收衣服啊
5.1 朴素贝叶斯基本概念 / 064
5.1.1 贝叶斯定理 / 064
5.1.2 朴素贝叶斯的简单应用 / 064
5.2 朴素贝叶斯算法的不同方法 / 068
5.2.1 贝努利朴素贝叶斯 / 068
5.2.2 高斯朴素贝叶斯 / 071
5.2.3 多项式朴素贝叶斯 / 072
5.3 朴素贝叶斯实战――判断肿瘤是良性还是恶性 / 075
5.3.1 对数据集进行分析 / 076
5.3.2 使用高斯朴素贝叶斯进行建模 / 077
5.3.3 高斯朴素贝叶斯的学习曲线 / 078
5.4 小结 / 080
第6章 决策树与随机森林――会玩读心术的算法
6.1 决策树 / 082
6.1.1 决策树基本原理 / 082
6.1.2 决策树的构建 / 082
6.1.3 决策树的优势和不足 / 088
6.2 随机森林 / 088
6.2.1 随机森林的基本概念 / 089
6.2.2 随机森林的构建 / 089
6.2.3 随机森林的优势和不足 / 092
6.3 随机森林实例――要不要和相亲对象进一步发展 / 093
6.3.1 数据集的准备 / 093
6.3.2 用get_dummies处理数据 / 094
6.3.3 用决策树建模并做出预测 / 096
6.4 小结 / 098
第7章 支持向量机SVM――专治线性不可分
7.1 支持向量机SVM基本概念 / 100
7.1.1 支持向量机SVM的原理 / 100
7.1.2 支持向量机SVM的核函数 / 102
7.2 SVM的核函数与参数选择 / 104
7.2.1 不同核函数的SVM对比 / 104
7.2.2 支持向量机的gamma参数调节 / 106
7.2.3 SVM算法的优势与不足 / 108
7.3 SVM实例――波士顿房价回归分析 / 108
7.3.1 初步了解数据集 / 109
7.3.2 使用SVR进行建模 / 110
7.4 小结 / 114
第8章 神经网络――曾入“冷宫”，如今得宠
8.1 神经网络的前世今生 / 116
8.1.1 神经网络的起源 / 116
8.1.2 第一个感知器学习法则 / 116
8.1.3 神经网络之父――杰弗瑞・欣顿 / 117
8.2 神经网络的原理及使用 / 118
8.2.1 神经网络的原理 / 118
8.2.2 神经网络中的非线性矫正 / 119
8.2.3 神经网络的参数设置 / 121
8.3 神经网络实例――手写识别 / 127
8.3.1 使用MNIST数据集 / 128
8.3.2 训练MLP神经网络 / 129
8.3.3 使用模型进行数字识别 / 130
8.4 小结 / 131
第9章 数据预处理、降维、特征提取及聚类――快
刀斩乱麻
9.1 数据预处理 / 134
9.1.1 使用StandardScaler进行数据预处理 / 134
9.1.2 使用MinMaxScaler进行数据预处理 / 135
9.1.3 使用RobustScaler进行数据预处理 / 136
9.1.4 使用Normalizer进行数据预处理 / 137
9.1.5 通过数据预处理提高模型准确率 / 138
9.2 数据降维 / 140
9.2.1 PCA主成分分析原理 / 140
9.2.2 对数据降维以便于进行可视化 / 142
9.2.3 原始特征与PCA主成分之间的关系 / 143
9.3 特征提取 / 144
9.3.1 PCA主成分分析法用于特征提取 / 145
9.3.2 非负矩阵分解用于特征提取 / 148
9.4 聚类算法 / 149
9.4.1 K均值聚类算法 / 150
9.4.2 凝聚聚类算法 / 153
9.4.3 DBSCAN算法 / 154
9.5 小结 / 157
第10章 数据表达与特征工程――锦上再添花
10.1 数据表达 / 160
10.1.1 使用哑变量转化类型特征 / 160
10.1.2 对数据进行装箱处理 / 162
10.2 数据“升维” / 166
10.2.1 向数据集添加交互式特征 / 166
10.2.2 向数据集添加多项式特征 / 170
10.3 自动特征选择 / 173
10.3.1 使用单一变量法进行特征选择 / 173
10.3.2 基于模型的特征选择 / 178
10.3.3 迭代式特征选择 / 180
10.4 小结 / 182
第11章 模型评估与优化――只有更好，没有最好
11.1 使用交叉验证进行模型评估 / 184
11.1.1 scikit-learn中的交叉验证法 / 184
11.1.2 随机拆分和“挨个儿试试” / 186
11.1.3 为什么要使用交叉验证法 / 188
11.2 使用网格搜索优化模型参数 / 188
11.2.1 简单网格搜索 / 189
11.2.2 与交叉验证结合的网格搜索 / 191
11.3 分类模型的可信度评估 / 193
11.3.1 分类模型中的预测准确率 / 194
11.3.2 分类模型中的决定系数 / 197
11.4 小结 / 198
第12章 建立算法的管道模型――团结就是力量
12.1 管道模型的概念及用法 / 202
12.1.1 管道模型的基本概念 / 202
12.1.2 使用管道模型进行网格搜索 / 206
12.2 使用管道模型对股票涨幅进行回归分析 / 209
12.2.1 数据集准备 / 209
12.2.2 建立包含预处理和MLP模型的管道模型 / 213
12.2.3 向管道模型添加特征选择步骤 / 214
12.3 使用管道模型进行模型选择和参数调优 / 216
12.3.1 使用管道模型进行模型选择 / 216
12.3.2 使用管道模型寻找更优参数 / 217
12.4 小结 / 220
第13章 文本数据处理――亲，见字如“数”
13.1 文本数据的特征提取、中文分词及词袋模型 / 222
13.1.1 使用CountVectorizer对文本进行特征提取 / 222
13.1.2 使用分词工具对中文文本进行分词 / 223
13.1.3 使用词袋模型将文本数据转为数组 / 224
13.2 对文本数据进一步进行优化处理 / 226
13.2.1 使用n-Gram改善词袋模型 / 226
13.2.2 使用tf-idf模型对文本数据进行处理 / 228
13.2.3 删除文本中的停用词 / 234
13.3 小结 / 236
第14章 从数据获取到话题提取――从“研究员”
到“段子手”
14.1 简单页面的爬取 / 238
14.1.1 准备Requests库和User Agent / 238
14.1.2 确定一个目标网站并分析其结构 / 240
14.1.3 进行爬取并保存为本地文件 / 241
14.2 稍微复杂一点的爬取 / 244
14.2.1 确定目标页面并进行分析 / 245
14.2.2 Python中的正则表达式 / 247
14.2.3 使用BeautifulSoup进行HTML解析 / 251
14.2.4 对目标页面进行爬取并保存到本地 / 256
14.3 对文本数据进行话题提取 / 258
14.3.1 寻找目标网站并分析结构 / 259
14.3.2 编写爬虫进行内容爬取 / 261
14.3.3 使用潜在狄利克雷分布进行话题提取 / 263
14.4 小结 / 265
第15章 人才需求现状与未来学习方向――你是不
是下一个“大牛”
15.1 人才需求现状 / 268
15.1.1 全球AI从业者达190万，人才需求3年翻8倍 / 268
15.1.2 AI人才需求集中于一线城市，七成从业者月薪过万 / 269
15.1.3 人才困境仍难缓解，政策支援亟不可待 / 269
15.2 未来学习方向 / 270
15.2.1 用于大数据分析的计算引擎 / 270
15.2.2 深度学习开源框架 / 271
15.2.3 使用概率模型进行推理 / 272
15.3 技能磨炼与实际应用 / 272
15.3.1 Kaggle算法大赛平台和OpenML平台 / 272
15.3.2 在工业级场景中的应用 / 273
15.3.3 对算法模型进行A/B测试 / 273
15.4 小结 / 274
参考文献 / 275
・ ・ ・ ・ ・ ・ (收起)对本书的赞誉
序一
序二
序三
前言
第1章　通向智能安全的旅程 1
1.1　人工智能、机器学习与深度学习 1
1.2　人工智能的发展 2
1.3　国内外网络安全形势 3
1.4　人工智能在安全领域的应用 5
1.5　算法和数据的辩证关系 9
1.6　本章小结 9
参考资源 10
第2章　打造机器学习工具箱 11
2.1　Python在机器学习领域的优势 11
2.1.1　NumPy 11
2.1.2　SciPy 15
2.1.3　NLTK 16
2.1.4　Scikit-Learn 17
2.2　TensorFlow简介与环境搭建 18
2.3　本章小结 19
参考资源 20
第3章　机器学习概述 21
3.1　机器学习基本概念 21
3.2　数据集 22
3.2.1　KDD 99数据 22
3.2.2　HTTP DATASET CSIC 2010 26
3.2.3　SEA数据集 26
3.2.4　ADFA-LD数据集 27
3.2.5　Alexa域名数据 29
3.2.6　Scikit-Learn数据集 29
3.2.7　MNIST数据集 30
3.2.8　Movie Review Data 31
3.2.9　SpamBase数据集 32
3.2.10　Enron数据集 33
3.3　特征提取 35
3.3.1　数字型特征提取 35
3.3.2　文本型特征提取 36
3.3.3　数据读取 37
3.4　效果验证 38
3.5　本章小结 40
参考资源 40
第4章　Web安全基础 41
4.1　XSS攻击概述 41
4.1.1　XSS的分类 43
4.1.2　XSS特殊攻击方式 48
4.1.3　XSS平台简介 50
4.1.4　近年典型XSS攻击事件分析 51
4.2　SQL注入概述 53
4.2.1　常见SQL注入攻击 54
4.2.2　常见SQL注入攻击载荷 55
4.2.3　SQL常见工具 56
4.2.4　近年典型SQL注入事件分析 60
4.3　WebShell概述 63
4.3.1　WebShell功能 64
4.3.2　常见WebShell 64
4.4　僵尸网络概述 67
4.4.1　僵尸网络的危害 68
4.4.2　近年典型僵尸网络攻击事件分析 69
4.5　本章小结 72
参考资源 72
第5章　K近邻算法 74
5.1　K近邻算法概述 74
5.2　示例：hello world！K近邻 75
5.3　示例：使用K近邻算法检测异常操作（一） 76
5.4　示例：使用K近邻算法检测异常操作（二） 80
5.5　示例：使用K近邻算法检测Rootkit 81
5.6　示例：使用K近邻算法检测WebShell 83
5.7　本章小结 85
参考资源 86
第6章　决策树与随机森林算法 87
6.1　决策树算法概述 87
6.2　示例：hello world！决策树 88
6.3　示例：使用决策树算法检测POP3暴力破解 89
6.4　示例：使用决策树算法检测FTP暴力破解 91
6.5　随机森林算法概述 93
6.6　示例：hello world！随机森林 93
6.7　示例：使用随机森林算法检测FTP暴力破解 95
6.8　本章小结 96
参考资源 96
第7章　朴素贝叶斯算法 97
7.1　朴素贝叶斯算法概述 97
7.2　示例：hello world！朴素贝叶斯 98
7.3　示例：检测异常操作 99
7.4　示例：检测WebShell（一） 100
7.5　示例：检测WebShell（二） 102
7.6　示例：检测DGA域名 103
7.7　示例：检测针对Apache的DDoS攻击 104
7.8　示例：识别验证码 107
7.9　本章小结 108
参考资源 108
第8章　逻辑回归算法 109
8.1　逻辑回归算法概述 109
8.2　示例：hello world！逻辑回归 110
8.3　示例：使用逻辑回归算法检测Java溢出攻击 111
8.4　示例：识别验证码 113
8.5　本章小结 114
参考资源 114
第9章　支持向量机算法 115
9.1　支持向量机算法概述 115
9.2　示例：hello world！支持向量机 118
9.3　示例：使用支持向量机算法识别XSS 120
9.4　示例：使用支持向量机算法区分僵尸网络DGA家族 124
9.4.1　数据搜集和数据清洗 124
9.4.2　特征化 125
9.4.3　模型验证 129
9.5　本章小结 130
参考资源 130
第10章　K-Means与DBSCAN算法 131
10.1　K-Means算法概述 131
10.2　示例：hello world！K-Means 132
10.3　示例：使用K-Means算法检测DGA域名 133
10.4　DBSCAN算法概述 135
10.5　示例：hello world！DBSCAN 135
10.6　本章小结 137
参考资源 137
第11章　Apriori与FP-growth算法 138
11.1　Apriori算法概述 138
11.2　示例：hello world！Apriori 140
11.3　示例：使用Apriori算法挖掘XSS相关参数 141
11.4　FP-growth算法概述 143
11.5　示例：hello world！FP-growth 144
11.6　示例：使用FP-growth算法挖掘疑似僵尸主机 145
11.7　本章小结 146
参考资源 146
第12章　隐式马尔可夫算法 147
12.1　隐式马尔可夫算法概述 147
12.2　hello world! 隐式马尔可夫 148
12.3　示例：使用隐式马尔可夫算法识别XSS攻击（一） 150
12.4　示例：使用隐式马尔可夫算法识别XSS攻击（二） 153
12.5　示例：使用隐式马尔可夫算法识别DGA域名 159
12.6　本章小结 162
参考资源 162
第13章　图算法与知识图谱 163
13.1　图算法概述 163
13.2　示例：hello world！有向图 164
13.3　示例：使用有向图识别WebShell 169
13.4　示例：使用有向图识别僵尸网络 173
13.5　知识图谱概述 176
13.6　示例：知识图谱在风控领域的应用 177
13.6.1　检测疑似账号被盗 178
13.6.2　检测疑似撞库攻击 179
13.6.3　检测疑似刷单 181
13.7　示例：知识图谱在威胁情报领域的应用 183
13.7.1　挖掘后门文件潜在联系 184
13.7.2　挖掘域名潜在联系 185
13.8　本章小结 187
参考资源 187
第14章　神经网络算法 188
14.1　神经网络算法概述 188
14.2　示例：hello world！神经网络 190
14.3　示例：使用神经网络算法识别验证码 190
14.4　示例：使用神经网络算法检测Java溢出攻击 191
14.5　本章小结 193
参考资源 194
第15章　多层感知机与DNN算法 195
15.1　神经网络与深度学习 195
15.2　TensorFlow编程模型 196
15.2.1　操作 197
15.2.2　张量 197
15.2.3　变量 198
15.2.4　会话 198
15.3　TensorFlow的运行模式 198
15.4　示例：在TensorFlow下识别验证码（一） 199
15.5　示例：在TensorFlow下识别验证码（二） 202
15.6　示例：在TensorFlow下识别验证码（三） 205
15.7　示例：在TensorFlow下识别垃圾邮件（一） 207
15.8　示例：在TensorFlow下识别垃圾邮件（二） 209
15.9　本章小结 210
参考资源 210
第16章　循环神经网络算法 212
16.1　循环神经网络算法概述 212
16.2　示例：识别验证码 213
16.3　示例：识别恶意评论 216
16.4　示例：生成城市名称 220
16.5　示例：识别WebShell 222
16.6　示例：生成常用密码 225
16.7　示例：识别异常操作 227
16.8　本章小结 230
参考资源 230
第17章　卷积神经网络算法 231
17.1　卷积神经网络算法概述 231
17.2　示例：hello world！卷积神经网络 234
17.3　示例：识别恶意评论 235
17.4　示例：识别垃圾邮件 237
17.5　本章小结 240
参考资源 242
・ ・ ・ ・ ・ ・ (收起)第一部分 初始
1 初识机器学习 2
1.1 学习机器学习的误区 2
1.2 什么是机器学习 3
1.3 Python 中的机器学习 3
1.4 学习机器学习的原则 5
1.5 学习机器学习的技巧 5
1.6 这本书不涵盖以下内容 6
1.7 代码说明 6
1.8 总结 6
2 Python 机器学习的生态圈 7
2.1 Python 7
2.2 SciPy 9
2.3 scikit-learn 9
2.4 环境安装 10
2.5 总结 12
3 第一个机器学习项目 13
3.1 机器学习中的 Hello World 项目 13
3.2 导入数据 14
3.3 概述数据 15
3.4 数据可视化 18
3.5 评估算法 20
3.6 实施预测 23
3.7 总结 24
4 Python 和 SciPy 速成 25
4.1 Python 速成 25
4.2 NumPy 速成 34
4.3 Matplotlib 速成 36
4.4 Pandas 速成 39
4.5 总结 41
第二部分 数据理解
5 数据导入 44
5.1 CSV 文件 44
5.2 Pima Indians 数据集 45
5.3 采用标准 Python 类库导入数据 46
5.4 采用 NumPy 导入数据 46
5.5 采用 Pandas 导入数据 47
5.6 总结 47
6 数据理解 48
6.1 简单地查看数据 48
6.2 数据的维度 49
6.3 数据属性和类型 50
6.4 描述性统计 50
6.5 数据分组分布（适用于分类算法） 51
6.6 数据属性的相关性 52
6.7 数据的分布分析 53
6.8 总结 54
7 数据可视化 55
7.1 单一图表 55
7.2 多重图表 58
7.3 总结 61
第三部分 数据准备
8 数据预处理 64
8.1 为什么需要数据预处理 64
8.2 格式化数据 65
8.3 调整数据尺度 65
8.4 正态化数据 67
8.5 标准化数据 68
8.6 二值数据 69
8.7 总结 70
9 数据特征选定 71
9.1 特征选定 72
9.2 单变量特征选定 72
9.3 递归特征消除 73
9.4 主要成分分析 75
9.5 特征重要性 76
9.6 总结 76
第四部分 选择模型
10 评估算法 78
10.1 评估算法的方法 78
10.2 分离训练数据集和评估数据集 79
10.3 K 折交叉验证分离 80
10.4 弃一交叉验证分离 81
10.5 重复随机分离评估数据集与训练数据集 82
10.6 总结 83
11 算法评估矩阵 85
11.1 算法评估矩阵 85
11.2 分类算法矩阵 86
11.3 回归算法矩阵 93
11.4 总结 96
12 审查分类算法 97
12.1 算法审查 97
12.2 算法概述 98
12.3 线性算法 98
12.4 非线性算法 101
12.5 总结 105
13 审查回归算法 106
13.1 算法概述 106
13.2 线性算法 107
13.3 非线性算法 111
13.4 总结 113
14 算法比较 115
14.1 选择最佳的机器学习算法 115
14.2 机器学习算法的比较 116
14.3 总结 118
15 自动流程 119
15.1 机器学习的自动流程 119
15.2 数据准备和生成模型的 Pipeline 120
15.3 特征选择和生成模型的 Pipeline 121
15.4 总结 122
第五部分 优化模型
16 集成算法 124
16.1 集成的方法 124
16.2 装袋算法 125
16.3 提升算法 129
16.4 投票算法 131
16.5 总结 132
17 算法调参 133
17.1 机器学习算法调参 133
17.2 网格搜索优化参数 134
17.3 随机搜索优化参数 135
17.4 总结 136
第六部分 结果部署
18 持久化加载模型 138
18.1 通过 pickle 序列化和反序列化机器学习的模型 138
18.2 通过 joblib 序列化和反序列化机器学习的模型 140
18.3 生成模型的技巧 141
18.4 总结 141
第七部分 项目实践
19 预测模型项目模板 144
19.1 在项目中实践机器学习 145
19.2 机器学习项目的 Python 模板 145
19.3 各步骤的详细说明 146
19.4 使用模板的小技巧 148
19.5 总结 149
20 回归项目实例 150
20.1 定义问题 150
20.2 导入数据 151
20.3 理解数据 152
20.4 数据可视化 155
20.5 分离评估数据集 159
20.6 评估算法 160
20.7 调参改善算法 164
20.8 集成算法 165
20.9 集成算法调参 167
20.10 确定最终模型 168
20.11 总结 169
21 二分类实例 170
21.1 问题定义 170
21.2 导入数据 171
21.3 分析数据 172
21.4 分离评估数据集 180
21.5 评估算法 180
21.6 算法调参 184
21.7 集成算法 187
21.8 确定最终模型 190
21.9 总结 190
22 文本分类实例 192
22.1 问题定义 192
22.2 导入数据 193
22.3 文本特征提取 195
22.4 评估算法 196
22.5 算法调参 198
22.6 集成算法 200
22.7 集成算法调参 201
22.8 确定最终模型 202
22.9 总结 203
・ ・ ・ ・ ・ ・ (收起)第 1章 走进机器学习1
1.1 机器学习概述1
1.2 机器学习过程2
第 2章 了解Python20
2.1 为什么选择Python20
2.2 下载和安装Python22
2.2.1 在Windows中安装Python22
2.2.2 Anaconda24
2.3 首个Python程序26
2.4 Python基础27
2.5 数据结构与循环36
第3章 特征工程42
3.1 什么是特征42
3.2 为什么执行特征工程43
3.3 特征提取43
3.4 特征选择43
3.5 特征工程方法――通用准则44
3.5.1 处理数值特征44
3.5.2 处理分类特征45
3.5.3 处理基于时间的特征47
3.5.4 处理文本特征47
3.5.5 缺失数据48
3.5.6 降维48
3.6 用Python进行特征工程49
3.6.1 Pandas基本操作49
3.6.2 常见任务57
第4章 数据可视化62
4.1 折线图63
4.2 条形图66
4.3 饼图67
4.4 直方图68
4.5 散点图69
4.6 箱线图70
4.7 采用面向对象的方式绘图71
4.8 Seaborn73
4.8.1 分布图74
4.8.2 双变量分布75
4.8.3 二元分布的核密度估计75
4.8.4 成对双变量分布76
4.8.5 分类散点图76
4.8.6 小提琴图77
4.8.7 点图78
第5章 回归79
5.1 简单回归80
5.2 多元回归92
5.3 模型评价94
5.3.1 训练误差95
5.3.2 泛化误差96
5.3.3 测试误差97
5.3.4 不可约误差98
5.3.5 偏差―方差权衡99
第6章 更多回归105
6.1 概述105
6.2 岭回归112
6.3 套索回归118
6.3.1 全子集算法118
6.3.2 用于特征选择的贪心算法119
6.3.3 特征选择的正则化119
6.4 非参数回归122
6.4.1 K-最近邻回归124
6.4.2 核回归127
第7章 分类128
7.1 线性分类器129
7.2 逻辑回归133
7.3 决策树147
7.3.1 关于树的术语148
7.3.2 决策树学习149
7.3.3 决策边界151
7.4 随机森林158
7.5 朴素贝叶斯164
第8章 无监督学习169
8.1 聚类170
8.2 K-均值聚类170
8.2.1 随机分配聚类质心的问题175
8.2.2 查找K的值175
8.3 分层聚类182
8.3.1 距离矩阵184
8.3.2 连接185
第9章 文本分析189
9.1 使用Python进行基本文本处理189
9.1.1 字符串比较191
9.1.2 字符串转换191
9.1.3 字符串操作192
9.2 正则表达式193
9.3 自然语言处理195
9.3.1 词干提取196
9.3.2 词形还原197
9.3.3 分词197
9.4 文本分类200
9.5 主题建模206
第 10章 神经网络与深度学习209
10.1 矢量化210
10.2 神经网络218
10.2.1 梯度下降220
10.2.2 激活函数221
10.2.3 参数初始化224
10.2.4 优化方法227
10.2.5 损失函数227
10.3 深度学习229
10.4 深度学习架构230
10.4.1 深度信念网络231
10.4.2 卷积神经网络231
10.4.3 循环神经网络231
10.4.4 长短期记忆网络231
10.4.5 深度堆栈网络232
10.5 深度学习框架232
第 11章 推荐系统237
11.1 基于流行度的推荐引擎237
11.2 基于内容的推荐引擎240
11.3 基于分类的推荐引擎243
11.4 协同过滤245
第 12章 时间序列分析249
12.1 处理日期和时间249
12.2 窗口函数254
12.3 相关性258
12.4 时间序列预测261
・ ・ ・ ・ ・ ・ (收起)前　言
第1部分　实时机器学习方法论
第1章　实时机器学习综述 2
1.1　什么是机器学习 2
1.2　机器学习发展的前世今生 3
1.2.1　历史上机器学习无法调和的难题 3
1.2.2　现代机器学习的新融合 4
1.3　机器学习领域分类 5
1.4　实时是个“万灵丹” 6
1.5　实时机器学习的分类 7
1.5.1　硬实时机器学习 7
1.5.2　软实时机器学习 7
1.5.3　批实时机器学习 8
1.6　实时应用对机器学习的要求 8
1.7　案例：Netflix在机器学习竞赛中学到的经验 9
1.7.1　Netflix 用户信息被逆向工程 9
1.7.2　Netflix 最终胜出者模型无法在生产环境中使用 9
1.8　实时机器学习模型的生存期 10
第2章　实时监督式机器学习 12
2.1　什么是监督式机器学习 12
2.1.1　“江湖门派”对预测模型的
不同看法 13
2.1.2　工业界的学术门派 14
2.1.3　实时机器学习实战的思路 15
2.2　怎样衡量监督式机器学习模型 16
2.2.1　统计量的优秀 16
2.2.2　应用业绩的优秀 20
2.3　实时线性分类器介绍 20
2.3.1　广义线性模型的定义 20
2.3.2　训练线性模型 21
2.3.3　冷启动问题 22
第3章　数据分析工具 Pandas 23
3.1　颠覆 R 的 Pandas 23
3.2　Pandas 的安装 24
3.3　利用 Pandas 分析实时股票报价数据 24
3.3.1　外部数据导入 25
3.3.2　数据分析基本操作 25
3.3.3　可视化操作 26
3.3.4　秒级收盘价变化率初探 28
3.4　数据分析的三个要点 30
3.4.1　不断验证假设 30
3.4.2　全面可视化，全面监控化 30
第4章　机器学习工具 Scikit-learn 31
4.1　如何站在风口上？向Scikit-learn 学习 31
4.1.1　传统的线下统计软件 R 31
4.1.2　底层软件黑盒子 Weka 32
4.1.3　跨界产品 Scikit-learn 33
4.1.4　Scikit-learn的优势 33
4.2　Scikit-learn 的安装 34
4.3　Scikit-learn 的主要模块 35
4.3.1　监督式、非监督式机器学习 35
4.3.2　建模函数fit和predict 36
4.3.3　数据预处理 38
4.3.4　自动化建模预测 Pipeline 39
4.4　利用 Scikit-learn 进行股票价格波动预测 40
4.4.1　数据导入和预处理 41
4.4.2　编写专有时间序列数据预处理模块 41
4.4.3　利用 Pipeline 进行建模 43
4.4.4　评价建模效果 43
4.4.5　引入成交量和高维交叉项进行建模 44
4.4.6　本书没有告诉你的 45
第2部分　实时机器学习架构
第5章　实时机器学习架构设计 48
5.1　设计实时机器学习架构的
四个要点 48
5.2　Lambda 架构和主要成员 49
5.2.1　实时响应层 49
5.2.2　快速处理层 50
5.2.3　批处理层 50
5.3　常用的实时机器学习架构 50
5.3.1　瀑布流架构 50
5.3.2　并行响应架构 51
5.3.3　实时更新模型混合架构 52
5.4　小结 53
第6章　集群部署工具 Docker 55
6.1　Docker 的前世今生 55
6.2　容器虚拟机的基本组成部分 56
6.3　Docker 引擎命令行工具 57
6.3.1　Docker 引擎的安装 57
6.3.2　Docker 引擎命令行的基本操作 58
6.4　通过 Dockerfile 配置容器虚拟机 61
6.4.1　利用 Dockerfile 配置基本容器虚拟机 62
6.4.2　利用 Dockerfile 进行虚拟机和宿主机之间的文件传输 62
6.5　服务器集群配置工具Docker Compose 64
6.5.1　Docker Compose 的安装 64
6.5.2　Docker Compose 的基本操作 64
6.5.3　利用 Docker Compose 创建网页计数器集群 65
6.6　远端服务器配置工具Docker Machine 68
6.6.1　Docker Machine 的安装 68
6.6.2　安装 Oracle VirtualBox 69
6.6.3　创建和管理 VirtualBox中的虚拟机 69
6.6.4　在 Docker Machine 和 VirtualBox的环境中运行集群 70
6.6.5　利用 Docker Machine 在 Digital Ocean 上配置运行集群 71
6.7　其他有潜力的 Docker 工具 73
第7章　实时消息队列和RabbitMQ 74
7.1　实时消息队列 74
7.2　AMQP 和 RabbitMQ 简介 76
7.3　RabbitMQ的主要构成部分 76
7.4　常用交换中心模式 78
7.4.1　直连结构 78
7.4.2　扇形结构 78
7.4.3　话题结构 79
7.4.4　报头结构 79
7.5　消息传导设计模式 79
7.5.1　任务队列 80
7.5.2　Pub/Sub 发布/监听 80
7.5.3　远程命令 81
7.6　利用 Docker 快速部署RabbitMQ 82
7.7　利用 RabbitMQ 开发队列服务 85
7.7.1　准备案例材料 86
7.7.2　实时报价存储服务 86
7.7.3　实时走势预测服务 89
7.7.4　整合运行实验 93
7.7.5　总结和改进 95
第8章　实战数据库综述 98
8.1　SQL 与 NoSQL，主流数据库分类 98
8.1.1　关系型数据库 99
8.1.2　非关系型数据库 NoSQL 99
8.2　数据库的性能 100
8.2.1　耐分割 100
8.2.2　 一致性 101
8.2.3　可用性 101
8.2.4　CAP 定理 101
8.3　SQL和NoSQL对比 102
8.3.1　数据存储、读取方式 102
8.3.2　数据库的扩展方式 103
8.3.3　性能比较 103
8.4　数据库的发展趋势 103
8.4.1　不同数据库之间自动化同步更为方便 103
8.4.2　云数据库的兴起 104
8.4.3　底层和应用层多层化 104
8.5　MySQL 简介 105
8.6　Cassandra简介 105
8.6.1　Cassandra交互方式简介 105
8.6.2　利用Docker安装Cassandra 106
8.6.3　使用Cassandra存储数据 106
第9章　实时数据监控 ELK 集群 107
9.1　Elasticsearch、LogStash和Kibana 的前世今生 107
9.1.1　Elasticsearch 的平凡起家 108
9.1.2　LogStash 卑微的起源 108
9.1.3　Kibana 惊艳登场 109
9.1.4　ELK 协同作战 109
9.2　Elasticsearch 基本架构 109
9.2.1　文档 110
9.2.2　索引和文档类型 111
9.2.3　分片和冗余 112
9.2.4　Elasticsearch 和数据库进行比较 113
9.3　Elasticsearch 快速入门 113
9.3.1　用 Docker 运行 Elasticsearch 容器虚拟机 113
9.3.2　创建存储文档、文档类型和索引 114
9.3.3　搜索文档 117
9.3.4　对偶搜索 120
9.4　Kibana 快速入门 124
9.4.1　利用 Docker 搭建ELK 集群 125
9.4.2　配置索引格式 127
9.4.3　交互式搜索 128
9.4.4　可视化操作 129
9.4.5　实时检测面板 132
第10章　机器学习系统设计模式 134
10.1　 设计模式的前世今生 134
10.1.1　单机设计模式逐渐式微 134
10.1.2　微服务取代设计模式的示例 135
10.1.3　微服务设计模式的兴起 137
10.2　读：高速键值模式 137
10.2.1　问题场景 137
10.2.2　解决方案 138
10.2.3　其他使用场景 139
10.3　读：缓存高速查询模式 139
10.3.1　问题场景 139
10.3.2　解决方案 139
10.3.3　适用场景 141
10.4　更新：异步数据库更新模式 141
10.4.1　问题场景 141
10.4.2　解决方案 141
10.4.3　使用场景案例 142
10.5　更新：请求重定向模式 144
10.5.1　问题场景 144
10.5.2　解决方案 144
10.5.3　更新流程 145
10.5.4　使用场景案例 146
10.6　处理：硬实时并行模式 146
10.6.1　问题场景 146
10.6.2　解决方案 147
10.6.3　使用场景案例 147
10.7　处理：分布式任务队列模式 148
10.7.1　问题场景 148
10.7.2　解决方案 149
10.7.3　Storm 作为分布式任务队列 150
10.7.4　适用场景 151
10.7.5　结构的演进 152
10.8　处理：批实时处理模式 152
10.8.1　问题场景 152
10.8.2　解决方案 152
10.8.3　适用场景 153
第3部分　未来展望
第11章　Serverless 架构 156
11.1　Serverless 架构的前世今生 156
11.2　Serverless 架构对实时
机器学习的影响 157
第12章　深度学习的风口 159
12.1　深度学习的前世今生 159
12.2　深度学习的难点 161
12.3　如何选择深度学习工具 161
12.3.1　与现有编程平台、技能整合的难易程度 162
12.3.2　此平台除做深度学习之外，还能做什么 163
12.3.3　深度学习平台的成熟程度 164
12.4　未来发展方向 165
・ ・ ・ ・ ・ ・ (收起)I IMAGE FORMATION 1
1 Geometric Camera Models 3
1.1 Image Formation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.1 Pinhole Perspective . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.2 Weak Perspective . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.1.3 Cameras with Lenses . . . . . . . . . . . . . . . . . . . . . . . 8
1.1.4 The Human Eye . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2 Intrinsic and Extrinsic Parameters . . . . . . . . . . . . . . . . . . . 14
1.2.1 Rigid Transformations and Homogeneous Coordinates . . . . 14
1.2.2 Intrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . . 16
1.2.3 Extrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . . 18
1.2.4 Perspective Projection Matrices . . . . . . . . . . . . . . . . . 19
1.2.5 Weak-Perspective Projection Matrices . . . . . . . . . . . . . 20
1.3 Geometric Camera Calibration . . . . . . . . . . . . . . . . . . . . . 22
1.3.1 ALinear Approach to Camera Calibration . . . . . . . . . . . 23
1.3.2 ANonlinear Approach to Camera Calibration . . . . . . . . . 27
1.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2 Light and Shading 32
2.1 Modelling Pixel Brightness . . . . . . . . . . . . . . . . . . . . . . . 32
2.1.1 Reflection at Surfaces . . . . . . . . . . . . . . . . . . . . . . 33
2.1.2 Sources and Their Effects . . . . . . . . . . . . . . . . . . . . 34
2.1.3 The Lambertian+Specular Model . . . . . . . . . . . . . . . . 36
2.1.4 Area Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.2 Inference from Shading . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.2.1 Radiometric Calibration and High Dynamic Range Images . . 38
2.2.2 The Shape of Specularities . . . . . . . . . . . . . . . . . . . 40
2.2.3 Inferring Lightness and Illumination . . . . . . . . . . . . . . 43
2.2.4 Photometric Stereo: Shape from Multiple Shaded Images . . 46
2.3 Modelling Interreflection . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.3.1 The Illumination at a Patch Due to an Area Source . . . . . 52
2.3.2 Radiosity and Exitance . . . . . . . . . . . . . . . . . . . . . 54
2.3.3 An Interreflection Model . . . . . . . . . . . . . . . . . . . . . 55
2.3.4 Qualitative Properties of Interreflections . . . . . . . . . . . . 56
2.4 Shape from One Shaded Image . . . . . . . . . . . . . . . . . . . . . 59
2.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3 Color 68
3.1 Human Color Perception . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.1.1 Color Matching . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.1.2 Color Receptors . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.2 The Physics of Color . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.2.1 The Color of Light Sources . . . . . . . . . . . . . . . . . . . 73
3.2.2 The Color of Surfaces . . . . . . . . . . . . . . . . . . . . . . 76
3.3 Representing Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.3.1 Linear Color Spaces . . . . . . . . . . . . . . . . . . . . . . . 77
3.3.2 Non-linear Color Spaces . . . . . . . . . . . . . . . . . . . . . 83
3.4 AModel of Image Color . . . . . . . . . . . . . . . . . . . . . . . . . 86
3.4.1 The Diffuse Term . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.4.2 The Specular Term . . . . . . . . . . . . . . . . . . . . . . . . 90
3.5 Inference from Color . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.5.1 Finding Specularities Using Color . . . . . . . . . . . . . . . 90
3.5.2 Shadow Removal Using Color . . . . . . . . . . . . . . . . . . 92
3.5.3 Color Constancy: Surface Color from Image Color . . . . . . 95
3.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
II EARLY VISION: JUST ONE IMAGE 105
4 Linear Filters 107
4.1 Linear Filters and Convolution . . . . . . . . . . . . . . . . . . . . . 107
4.1.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2 Shift Invariant Linear Systems . . . . . . . . . . . . . . . . . . . . . 112
4.2.1 Discrete Convolution . . . . . . . . . . . . . . . . . . . . . . . 113
4.2.2 Continuous Convolution . . . . . . . . . . . . . . . . . . . . . 115
4.2.3 Edge Effects in Discrete Convolutions . . . . . . . . . . . . . 118
4.3 Spatial Frequency and Fourier Transforms . . . . . . . . . . . . . . . 118
4.3.1 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . 119
4.4 Sampling and Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.4.1 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4.2 Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.4.3 Smoothing and Resampling . . . . . . . . . . . . . . . . . . . 126
4.5 Filters as Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.5.1 Convolution as a Dot Product . . . . . . . . . . . . . . . . . 131
4.5.2 Changing Basis . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.6 Technique: Normalized Correlation and Finding Patterns . . . . . . 132
4.6.1 Controlling the Television by Finding Hands by Normalized
Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.7 Technique: Scale and Image Pyramids . . . . . . . . . . . . . . . . . 134
4.7.1 The Gaussian Pyramid . . . . . . . . . . . . . . . . . . . . . 135
4.7.2 Applications of Scaled Representations . . . . . . . . . . . . . 136
4.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5 Local Image Features 141
5.1 Computing the Image Gradient . . . . . . . . . . . . . . . . . . . . . 141
5.1.1 Derivative of Gaussian Filters . . . . . . . . . . . . . . . . . . 142
5.2 Representing the Image Gradient . . . . . . . . . . . . . . . . . . . . 144
5.2.1 Gradient-Based Edge Detectors . . . . . . . . . . . . . . . . . 145
5.2.2 Orientations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.3 Finding Corners and Building Neighborhoods . . . . . . . . . . . . . 148
5.3.1 Finding Corners . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.3.2 Using Scale and Orientation to Build a Neighborhood . . . . 151
5.4 Describing Neighborhoods with SIFT and HOG Features . . . . . . 155
5.4.1 SIFT Features . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.4.2 HOG Features . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.5 Computing Local Features in Practice . . . . . . . . . . . . . . . . . 160
5.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6 Texture 164
6.1 Local Texture Representations Using Filters . . . . . . . . . . . . . . 166
6.1.1 Spots and Bars . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.1.2 From Filter Outputs to Texture Representation . . . . . . . . 168
6.1.3 Local Texture Representations in Practice . . . . . . . . . . . 170
6.2 Pooled Texture Representations by Discovering Textons . . . . . . . 171
6.2.1 Vector Quantization and Textons . . . . . . . . . . . . . . . . 172
6.2.2 K-means Clustering for Vector Quantization . . . . . . . . . . 172
6.3 Synthesizing Textures and Filling Holes in Images . . . . . . . . . . 176
6.3.1 Synthesis by Sampling Local Models . . . . . . . . . . . . . . 176
6.3.2 Filling in Holes in Images . . . . . . . . . . . . . . . . . . . . 179
6.4 Image Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.4.1 Non-local Means . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.4.2 Block Matching 3D (BM3D) . . . . . . . . . . . . . . . . . . 183
6.4.3 Learned Sparse Coding . . . . . . . . . . . . . . . . . . . . . 184
6.4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.5 Shape from Texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.5.1 Shape from Texture for Planes . . . . . . . . . . . . . . . . . 187
6.5.2 Shape from Texture for Curved Surfaces . . . . . . . . . . . . 190
6.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
III EARLY VISION: MULTIPLE IMAGES 195
7 Stereopsis 197
7.1 Binocular Camera Geometry and the Epipolar Constraint . . . . . . 198
7.1.1 Epipolar Geometry . . . . . . . . . . . . . . . . . . . . . . . . 198
7.1.2 The Essential Matrix . . . . . . . . . . . . . . . . . . . . . . . 200
7.1.3 The Fundamental Matrix . . . . . . . . . . . . . . . . . . . . 201
7.2 Binocular Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 201
7.2.1 Image Rectification . . . . . . . . . . . . . . . . . . . . . . . . 202
7.3 Human Stereopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.4 Local Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 205
7.4.1 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
7.4.2 Multi-Scale Edge Matching . . . . . . . . . . . . . . . . . . . 207
7.5 Global Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 210
7.5.1 Ordering Constraints and Dynamic Programming . . . . . . . 210
7.5.2 Smoothness and Graphs . . . . . . . . . . . . . . . . . . . . . 211
7.6 Using More Cameras . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
7.7 Application: Robot Navigation . . . . . . . . . . . . . . . . . . . . . 215
7.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
8 Structure from Motion 221
8.1 Internally Calibrated Perspective Cameras . . . . . . . . . . . . . . . 221
8.1.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 223
8.1.2 Euclidean Structure and Motion from Two Images . . . . . . 224
8.1.3 Euclidean Structure and Motion from Multiple Images . . . . 228
8.2 Uncalibrated Weak-Perspective Cameras . . . . . . . . . . . . . . . . 230
8.2.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 231
8.2.2 Affine Structure and Motion from Two Images . . . . . . . . 233
8.2.3 Affine Structure and Motion from Multiple Images . . . . . . 237
8.2.4 From Affine to Euclidean Shape . . . . . . . . . . . . . . . . 238
8.3 Uncalibrated Perspective Cameras . . . . . . . . . . . . . . . . . . . 240
8.3.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 241
8.3.2 Projective Structure and Motion from Two Images . . . . . . 242
8.3.3 Projective Structure and Motion from Multiple Images . . . . 244
8.3.4 From Projective to Euclidean Shape . . . . . . . . . . . . . . 246
8.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
IV MID-LEVEL VISION 253
9 Segmentation by Clustering 255
9.1 Human Vision: Grouping and Gestalt . . . . . . . . . . . . . . . . . 256
9.2 Important Applications . . . . . . . . . . . . . . . . . . . . . . . . . 261
9.2.1 Background Subtraction . . . . . . . . . . . . . . . . . . . . . 261
9.2.2 Shot Boundary Detection . . . . . . . . . . . . . . . . . . . . 264
9.2.3 Interactive Segmentation . . . . . . . . . . . . . . . . . . . . 265
9.2.4 Forming Image Regions . . . . . . . . . . . . . . . . . . . . . 266
9.3 Image Segmentation by Clustering Pixels . . . . . . . . . . . . . . . 268
9.3.1 Basic Clustering Methods . . . . . . . . . . . . . . . . . . . . 269
9.3.2 The Watershed Algorithm . . . . . . . . . . . . . . . . . . . . 271
9.3.3 Segmentation Using K-means . . . . . . . . . . . . . . . . . . 272
9.3.4 Mean Shift: Finding Local Modes in Data . . . . . . . . . . . 273
9.3.5 Clustering and Segmentation with Mean Shift . . . . . . . . . 275
9.4 Segmentation, Clustering, and Graphs . . . . . . . . . . . . . . . . . 277
9.4.1 Terminology and Facts for Graphs . . . . . . . . . . . . . . . 277
9.4.2 Agglomerative Clustering with a Graph . . . . . . . . . . . . 279
9.4.3 Divisive Clustering with a Graph . . . . . . . . . . . . . . . . 281
9.4.4 Normalized Cuts . . . . . . . . . . . . . . . . . . . . . . . . . 284
9.5 Image Segmentation in Practice . . . . . . . . . . . . . . . . . . . . . 285
9.5.1 Evaluating Segmenters . . . . . . . . . . . . . . . . . . . . . . 286
9.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
10 Grouping and Model Fitting 290
10.1 The Hough Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 290
10.1.1 Fitting Lines with the Hough Transform . . . . . . . . . . . . 290
10.1.2 Using the Hough Transform . . . . . . . . . . . . . . . . . . . 292
10.2 Fitting Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . 293
10.2.1 Fitting a Single Line . . . . . . . . . . . . . . . . . . . . . . . 294
10.2.2 Fitting Planes . . . . . . . . . . . . . . . . . . . . . . . . . . 295
10.2.3 Fitting Multiple Lines . . . . . . . . . . . . . . . . . . . . . . 296
10.3 Fitting Curved Structures . . . . . . . . . . . . . . . . . . . . . . . . 297
10.4 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
10.4.1 M-Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
10.4.2 RANSAC: Searching for Good Points . . . . . . . . . . . . . 302
10.5 Fitting Using Probabilistic Models . . . . . . . . . . . . . . . . . . . 306
10.5.1 Missing Data Problems . . . . . . . . . . . . . . . . . . . . . 307
10.5.2 Mixture Models and Hidden Variables . . . . . . . . . . . . . 309
10.5.3 The EM Algorithm for Mixture Models . . . . . . . . . . . . 310
10.5.4 Difficulties with the EM Algorithm . . . . . . . . . . . . . . . 312
10.6 Motion Segmentation by Parameter Estimation . . . . . . . . . . . . 313
10.6.1 Optical Flow and Motion . . . . . . . . . . . . . . . . . . . . 315
10.6.2 Flow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
10.6.3 Motion Segmentation with Layers . . . . . . . . . . . . . . . 317
10.7 Model Selection: Which Model Is the Best Fit? . . . . . . . . . . . . 319
10.7.1 Model Selection Using Cross-Validation . . . . . . . . . . . . 322
10.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
11 Tracking 326
11.1 Simple Tracking Strategies . . . . . . . . . . . . . . . . . . . . . . . . 327
11.1.1 Tracking by Detection . . . . . . . . . . . . . . . . . . . . . . 327
11.1.2 Tracking Translations by Matching . . . . . . . . . . . . . . . 330
11.1.3 Using Affine Transformations to Confirm a Match . . . . . . 332
11.2 Tracking Using Matching . . . . . . . . . . . . . . . . . . . . . . . . 334
11.2.1 Matching Summary Representations . . . . . . . . . . . . . . 335
11.2.2 Tracking Using Flow . . . . . . . . . . . . . . . . . . . . . . . 337
11.3 Tracking Linear Dynamical Models with Kalman Filters . . . . . . . 339
11.3.1 Linear Measurements and Linear Dynamics . . . . . . . . . . 340
11.3.2 The Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . 344
11.3.3 Forward-backward Smoothing . . . . . . . . . . . . . . . . . . 345
11.4 Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
11.4.1 Linking Kalman Filters with Detection Methods . . . . . . . 349
11.4.2 Key Methods of Data Association . . . . . . . . . . . . . . . 350
11.5 Particle Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
11.5.1 Sampled Representations of Probability Distributions . . . . 351
11.5.2 The Simplest Particle Filter . . . . . . . . . . . . . . . . . . . 355
11.5.3 The Tracking Algorithm . . . . . . . . . . . . . . . . . . . . . 356
11.5.4 A Workable Particle Filter . . . . . . . . . . . . . . . . . . . . 358
11.5.5 Practical Issues in Particle Filters . . . . . . . . . . . . . . . 360
11.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
V HIGH-LEVEL VISION 365
12 Registration 367
12.1 Registering Rigid Objects . . . . . . . . . . . . . . . . . . . . . . . . 368
12.1.1 Iterated Closest Points . . . . . . . . . . . . . . . . . . . . . . 368
12.1.2 Searching for Transformations via Correspondences . . . . . . 369
12.1.3 Application: Building Image Mosaics . . . . . . . . . . . . . . 370
12.2 Model-based Vision: Registering Rigid Objects with Projection . . . 375
12.2.1 Verification: Comparing Transformed and Rendered Source
to Target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
12.3 Registering Deformable Objects . . . . . . . . . . . . . . . . . . . . . 378
12.3.1 Deforming Texture with Active Appearance Models . . . . . 378
12.3.2 Active Appearance Models in Practice . . . . . . . . . . . . . 381
12.3.3 Application: Registration in Medical Imaging Systems . . . . 383
12.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
13 Smooth Surfaces and Their Outlines 391
13.1 Elements of Differential Geometry . . . . . . . . . . . . . . . . . . . 393
13.1.1 Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
13.1.2 Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
13.2 Contour Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
13.2.1 The Occluding Contour and the Image Contour . . . . . . . . 402
13.2.2 The Cusps and Inflections of the Image Contour . . . . . . . 403
13.2.3 Koenderink’s Theorem . . . . . . . . . . . . . . . . . . . . . . 404
13.3 Visual Events: More Differential Geometry . . . . . . . . . . . . . . 407
13.3.1 The Geometry of the Gauss Map . . . . . . . . . . . . . . . . 407
13.3.2 Asymptotic Curves . . . . . . . . . . . . . . . . . . . . . . . . 409
13.3.3 The Asymptotic Spherical Map . . . . . . . . . . . . . . . . . 410
13.3.4 Local Visual Events . . . . . . . . . . . . . . . . . . . . . . . 412
13.3.5 The Bitangent Ray Manifold . . . . . . . . . . . . . . . . . . 413
13.3.6 Multilocal Visual Events . . . . . . . . . . . . . . . . . . . . . 414
13.3.7 The Aspect Graph . . . . . . . . . . . . . . . . . . . . . . . . 416
13.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
14 Range Data 422
14.1 Active Range Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . 422
14.2 Range Data Segmentation . . . . . . . . . . . . . . . . . . . . . . . . 424
14.2.1 Elements of Analytical Differential Geometry . . . . . . . . . 424
14.2.2 Finding Step and Roof Edges in Range Images . . . . . . . . 426
14.2.3 Segmenting Range Images into Planar Regions . . . . . . . . 431
14.3 Range Image Registration and Model Acquisition . . . . . . . . . . . 432
14.3.1 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
14.3.2 Registering Range Images . . . . . . . . . . . . . . . . . . . . 434
14.3.3 Fusing Multiple Range Images . . . . . . . . . . . . . . . . . 436
14.4 Object Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
14.4.1 Matching Using Interpretation Trees . . . . . . . . . . . . . . 438
14.4.2 Matching Free-Form Surfaces Using Spin Images . . . . . . . 441
14.5 Kinect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
14.5.1 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
14.5.2 Technique: Decision Trees and Random Forests . . . . . . . . 448
14.5.3 Labeling Pixels . . . . . . . . . . . . . . . . . . . . . . . . . . 450
14.5.4 Computing Joint Positions . . . . . . . . . . . . . . . . . . . 453
14.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
15 Learning to Classify 457
15.1 Classification, Error, and Loss . . . . . . . . . . . . . . . . . . . . . . 457
15.1.1 Using Loss to Determine Decisions . . . . . . . . . . . . . . . 457
15.1.2 Training Error, Test Error, and Overfitting . . . . . . . . . . 459
15.1.3 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 460
15.1.4 Error Rate and Cross-Validation . . . . . . . . . . . . . . . . 463
15.1.5 Receiver Operating Curves . . . . . . . . . . . . . . . . . . . 465
15.2 Major Classification Strategies . . . . . . . . . . . . . . . . . . . . . 467
15.2.1 Example: Mahalanobis Distance . . . . . . . . . . . . . . . . 467
15.2.2 Example: Class-Conditional Histograms and Naive Bayes . . 468
15.2.3 Example: Classification Using Nearest Neighbors . . . . . . . 469
15.2.4 Example: The Linear Support Vector Machine . . . . . . . . 470
15.2.5 Example: Kernel Machines . . . . . . . . . . . . . . . . . . . 473
15.2.6 Example: Boosting and Adaboost . . . . . . . . . . . . . . . 475
15.3 Practical Methods for Building Classifiers . . . . . . . . . . . . . . . 475
15.3.1 Manipulating Training Data to Improve Performance . . . . . 477
15.3.2 Building Multi-Class Classifiers Out of Binary Classifiers . . 479
15.3.3 Solving for SVMS and Kernel Machines . . . . . . . . . . . . 480
15.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
16 Classifying Images 482
16.1 Building Good Image Features . . . . . . . . . . . . . . . . . . . . . 482
16.1.1 Example Applications . . . . . . . . . . . . . . . . . . . . . . 482
16.1.2 Encoding Layout with GIST Features . . . . . . . . . . . . . 485
16.1.3 Summarizing Images with Visual Words . . . . . . . . . . . . 487
16.1.4 The Spatial Pyramid Kernel . . . . . . . . . . . . . . . . . . . 489
16.1.5 Dimension Reduction with Principal Components . . . . . . . 493
16.1.6 Dimension Reduction with Canonical Variates . . . . . . . . 494
16.1.7 Example Application: Identifying Explicit Images . . . . . . 498
16.1.8 Example Application: Classifying Materials . . . . . . . . . . 502
16.1.9 Example Application: Classifying Scenes . . . . . . . . . . . . 502
16.2 Classifying Images of Single Objects . . . . . . . . . . . . . . . . . . 504
16.2.1 Image Classification Strategies . . . . . . . . . . . . . . . . . 505
16.2.2 Evaluating Image Classification Systems . . . . . . . . . . . . 505
16.2.3 Fixed Sets of Classes . . . . . . . . . . . . . . . . . . . . . . . 508
16.2.4 Large Numbers of Classes . . . . . . . . . . . . . . . . . . . . 509
16.2.5 Flowers, Leaves, and Birds: Some Specialized Problems . . . 511
16.3 Image Classification in Practice . . . . . . . . . . . . . . . . . . . . . 512
16.3.1 Codes for Image Features . . . . . . . . . . . . . . . . . . . . 513
16.3.2 Image Classification Datasets . . . . . . . . . . . . . . . . . . 513
16.3.3 Dataset Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
16.3.4 Crowdsourcing Dataset Collection . . . . . . . . . . . . . . . 515
16.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
17 Detecting Objects in Images 519
17.1 The Sliding Window Method . . . . . . . . . . . . . . . . . . . . . . 519
17.1.1 Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 520
17.1.2 Detecting Humans . . . . . . . . . . . . . . . . . . . . . . . . 525
17.1.3 Detecting Boundaries . . . . . . . . . . . . . . . . . . . . . . 527
17.2 Detecting Deformable Objects . . . . . . . . . . . . . . . . . . . . . . 530
17.3 The State of the Art of Object Detection . . . . . . . . . . . . . . . 535
17.3.1 Datasets and Resources . . . . . . . . . . . . . . . . . . . . . 538
17.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
18 Topics in Object Recognition 540
18.1 What Should Object Recognition Do? . . . . . . . . . . . . . . . . . 540
18.1.1 What Should an Object Recognition System Do? . . . . . . . 540
18.1.2 Current Strategies for Object Recognition . . . . . . . . . . . 542
18.1.3 What Is Categorization? . . . . . . . . . . . . . . . . . . . . . 542
18.1.4 Selection: What Should Be Described? . . . . . . . . . . . . . 544
18.2 Feature Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
18.2.1 Improving Current Image Features . . . . . . . . . . . . . . . 544
18.2.2 Other Kinds of Image Feature . . . . . . . . . . . . . . . . . . 546
18.3 Geometric Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
18.4 Semantic Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
18.4.1 Attributes and the Unfamiliar . . . . . . . . . . . . . . . . . . 550
18.4.2 Parts, Poselets and Consistency . . . . . . . . . . . . . . . . . 551
18.4.3 Chunks of Meaning . . . . . . . . . . . . . . . . . . . . . . . . 554
VI APPLICATIONS AND TOPICS 557
19 Image-Based Modeling and Rendering 559
19.1 Visual Hulls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
19.1.1 Main Elements of the Visual Hull Model . . . . . . . . . . . . 561
19.1.2 Tracing Intersection Curves . . . . . . . . . . . . . . . . . . . 563
19.1.3 Clipping Intersection Curves . . . . . . . . . . . . . . . . . . 566
19.1.4 Triangulating Cone Strips . . . . . . . . . . . . . . . . . . . . 567
19.1.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
19.1.6 Going Further: Carved Visual Hulls . . . . . . . . . . . . . . 572
19.2 Patch-Based Multi-View Stereopsis . . . . . . . . . . . . . . . . . . . 573
19.2.1 Main Elements of the PMVS Model . . . . . . . . . . . . . . 575
19.2.2 Initial Feature Matching . . . . . . . . . . . . . . . . . . . . . 578
19.2.3 Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
19.2.4 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
19.2.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
19.3 The Light Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
19.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
20 Looking at People 590
20.1 HMM’s, Dynamic Programming, and Tree-Structured Models . . . . 590
20.1.1 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . 590
20.1.2 Inference for an HMM . . . . . . . . . . . . . . . . . . . . . . 592
20.1.3 Fitting an HMM with EM . . . . . . . . . . . . . . . . . . . . 597
20.1.4 Tree-Structured Energy Models . . . . . . . . . . . . . . . . . 600
20.2 Parsing People in Images . . . . . . . . . . . . . . . . . . . . . . . . 602
20.2.1 Parsing with Pictorial Structure Models . . . . . . . . . . . . 602
20.2.2 Estimating the Appearance of Clothing . . . . . . . . . . . . 604
20.3 Tracking People . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
20.3.1 Why Human Tracking Is Hard . . . . . . . . . . . . . . . . . 606
20.3.2 Kinematic Tracking by Appearance . . . . . . . . . . . . . . . 608
20.3.3 Kinematic Human Tracking Using Templates . . . . . . . . . 609
20.4 3D from 2D: Lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
20.4.1 Reconstruction in an Orthographic View . . . . . . . . . . . . 611
20.4.2 Exploiting Appearance for Unambiguous Reconstructions . . 613
20.4.3 Exploiting Motion for Unambiguous Reconstructions . . . . . 615
20.5 Activity Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
20.5.1 Background: Human Motion Data . . . . . . . . . . . . . . . 617
20.5.2 Body Configuration and Activity Recognition . . . . . . . . . 621
20.5.3 Recognizing Human Activities with Appearance Features . . 622
20.5.4 Recognizing Human Activities with Compositional Models . . 624
20.6 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
20.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
21 Image Search and Retrieval 627
21.1 The Application Context . . . . . . . . . . . . . . . . . . . . . . . . . 627
21.1.1 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
21.1.2 User Needs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
21.1.3 Types of Image Query . . . . . . . . . . . . . . . . . . . . . . 630
21.1.4 What Users Do with Image Collections . . . . . . . . . . . . 631
21.2 Basic Technologies from Information Retrieval . . . . . . . . . . . . . 632
21.2.1 Word Counts . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
21.2.2 Smoothing Word Counts . . . . . . . . . . . . . . . . . . . . . 633
21.2.3 Approximate Nearest Neighbors and Hashing . . . . . . . . . 634
21.2.4 Ranking Documents . . . . . . . . . . . . . . . . . . . . . . . 638
21.3 Images as Documents . . . . . . . . . . . . . . . . . . . . . . . . . . 639
21.3.1 Matching Without Quantization . . . . . . . . . . . . . . . . 640
21.3.2 Ranking Image Search Results . . . . . . . . . . . . . . . . . 641
21.3.3 Browsing and Layout . . . . . . . . . . . . . . . . . . . . . . 643
21.3.4 Laying Out Images for Browsing . . . . . . . . . . . . . . . . 644
21.4 Predicting Annotations for Pictures . . . . . . . . . . . . . . . . . . 645
21.4.1 Annotations from Nearby Words . . . . . . . . . . . . . . . . 646
21.4.2 Annotations from the Whole Image . . . . . . . . . . . . . . 646
21.4.3 Predicting Correlated Words with Classifiers . . . . . . . . . 648
21.4.4 Names and Faces . . . . . . . . . . . . . . . . . . . . . . . . 649
21.4.5 Generating Tags with Segments . . . . . . . . . . . . . . . . . 651
21.5 The State of the Art of Word Prediction . . . . . . . . . . . . . . . . 654
21.5.1 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.2 Comparing Methods . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.3 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 656
21.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659
VII BACKGROUND MATERIAL 661
22 Optimization Techniques 663
22.1 Linear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . . . 663
22.1.1 Normal Equations and the Pseudoinverse . . . . . . . . . . . 664
22.1.2 Homogeneous Systems and Eigenvalue Problems . . . . . . . 665
22.1.3 Generalized Eigenvalues Problems . . . . . . . . . . . . . . . 666
22.1.4 An Example: Fitting a Line to Points in a Plane . . . . . . . 666
22.1.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . 667
22.2 Nonlinear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . 669
22.2.1 Newton’s Method: Square Systems of Nonlinear Equations. . 670
22.2.2 Newton’s Method for Overconstrained Systems . . . . . . . . 670
22.2.3 The Gauss―Newton and Levenberg―Marquardt Algorithms . 671
22.3 Sparse Coding and Dictionary Learning . . . . . . . . . . . . . . . . 672
22.3.1 Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . 672
22.3.2 Dictionary Learning . . . . . . . . . . . . . . . . . . . . . . . 673
22.3.3 Supervised Dictionary Learning . . . . . . . . . . . . . . . . . 675
22.4 Min-Cut/Max-Flow Problems and Combinatorial Optimization . . . 675
22.4.1 Min-Cut Problems . . . . . . . . . . . . . . . . . . . . . . . . 676
22.4.2 Quadratic Pseudo-Boolean Functions . . . . . . . . . . . . . . 677
22.4.3 Generalization to Integer Variables . . . . . . . . . . . . . . . 679
22.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
Bibliography 684
Index 737
List of Algorithms 760
・ ・ ・ ・ ・ ・ (收起)序言
前言
第1篇　基础知识
第1章　引言 2
1.1　人工智能的新焦点――深度学习 2
1.1.1　人工智能――神话传说到影视漫画 2
1.1.2　人工智能的诞生 3
1.1.3　神经科学的研究 4
1.1.4　人工神经网络的兴起 5
1.1.5　神经网络的第一次寒冬 6
1.1.6　神经网络的第一次复兴 8
1.1.7　神经网络的第二次寒冬 9
1.1.8　2006年――深度学习的起点 10
1.1.9　生活中的深度学习 11
1.1.10　常见深度学习框架简介 12
1.2　给计算机一双眼睛――计算机视觉 14
1.2.1　计算机视觉简史 14
1.2.2　2012年――计算机视觉的新起点 16
1.2.3　计算机视觉的应用 17
1.2.4　常见计算机视觉工具包 19
1.3　基于深度学习的计算机视觉 19
1.3.1　从ImageNet竞赛到AlphaGo战胜李世石――计算机视觉超越人类 19
1.3.2　GPU和并行技术――深度学习和计算视觉发展的加速器 21
1.3.3　基于卷积神经网络的计算机视觉应用 22
第2章　深度学习和计算机视觉中的基础数学知识 27
2.1　线性变换和非线性变换 27
2.1.1　线性变换的定义 27
2.1.2　高中教科书中的小例子 28
2.1.3　点积和投影 28
2.1.4　矩阵乘法的几何意义（1） 30
2.1.5　本征向量和本征值 34
2.1.6　矩阵乘法的几何意义（2） 37
2.1.7　奇异值分解 38
2.1.8　线性可分性和维度 39
2.1.9　非线性变换 42
2.2　概率论及相关基础知识 43
2.2.1　条件概率和独立 43
2.2.2　期望值、方差和协方差 44
2.2.3　熵 45
2.2.4　最大似然估计（Maximum Likelihood Estimation，MLE） 47
2.2.5　KL散度（KullbackCLeibler divergence） 49
2.2.6　KL散度和MLE的联系 49
2.3　维度的诅咒 50
2.3.1　采样和维度 50
2.3.2　高维空间中的体积 51
2.3.3　高维空间中的距离 53
2.3.4　中心极限定理和高维样本距离分布的近似 54
2.3.5　数据实际的维度 56
2.3.6　局部泛化 58
2.3.7　函数对实际维度的影响 59
2.3.8　PCA――什么是主成分 60
2.3.9　PCA――通过本征向量和本征值求主成分 60
2.3.10　PCA――通过主成分分析降维 61
2.3.11　PCA――归一化和相关性系数 63
2.3.12　PCA――什么样的数据适合PCA 64
2.3.13　其他降维手段 65
2.4　卷积 66
2.4.1　点积和卷积 66
2.4.2　一维卷积 67
2.4.3　卷积和互相关 68
2.4.4　二维卷积和图像响应 69
2.4.5　卷积的计算 70
2.5　数学优化基础 71
2.5.1　最小值和梯度下降 72
2.5.2　冲量（Momentum） 73
2.5.3　牛顿法 75
2.5.4　学习率和自适应步长 77
2.5.5　学习率衰减（Learning Rate Decay） 78
2.5.6　AdaGrad：每个变量有自己的节奏 78
2.5.7　AdaDelta的进一步改进 79
2.5.8　其他自适应算法 80
2.5.9　损失函数 81
2.5.10　分类问题和负对数似然 82
2.5.11　逻辑回归 83
2.5.12　Softmax：将输出转换为概率 84
2.5.13　链式求导法则 84
第3章　神经网络和机器学习基础 87
3.1　感知机 87
3.1.1　基本概念 87
3.1.2　感知机和线性二分类 87
3.1.3　激活函数 88
3.2　神经网络基础 89
3.2.1　从感知机到神经网络 89
3.2.2　最简单的神经网络二分类例子 90
3.2.3　隐层神经元数量的作用 93
3.2.4　更加复杂的样本和更复杂的神经网络 94
3.3　后向传播算法 95
3.3.1　求神经网络参数的梯度 95
3.3.2　计算图（Computational Graph） 95
3.3.3　利用后向传播算法计算一个神经网络参数的梯度 97
3.3.4　梯度消失 99
3.3.5　修正线性单元（ReLU） 100
3.3.6　梯度爆炸 101
3.3.7　梯度检查（gradient check） 102
3.3.8　从信息传播的角度看后向传播算法 103
3.4　随机梯度下降和批量梯度下降 104
3.4.1　全量数据（full-batch）梯度下降 104
3.4.2　随机梯度下降（SGD）和小批量数据（mini-batch） 104
3.4.3　数据均衡和数据增加（data augmentation） 106
3.5　数据、训练策略和规范化 108
3.5.1　欠拟合和过拟合 108
3.5.2　训练误差和测试误差 109
3.5.3　奥卡姆剃刀没有免费午餐 111
3.5.4　数据集划分和提前停止 112
3.5.5　病态问题和约束 113
3.5.6　L2规范化（L2 Regularization） 113
3.5.7　L1规范化（L1 Regularization） 114
3.5.8　集成（Ensemble）和随机失活（Dropout） 115
3.6　监督学习、非监督学习、半监督学习和强化学习 117
3.6.1　监督学习、非监督学习和半监督学习 117
3.6.2　强化学习（reinforcement learning） 118
第4章　深度卷积神经网络 120
4.1　卷积神经网络 120
4.1.1　基本概念 120
4.1.2　卷积层和特征响应图 121
4.1.3　参数共享 123
4.1.4　稀疏连接 124
4.1.5　多通道卷积 125
4.1.6　激活函数 125
4.1.7　池化、不变性和感受野 126
4.1.8　分布式表征（Distributed Representation） 128
4.1.9　分布式表征和局部泛化 130
4.1.10　分层表达 131
4.1.11　卷积神经网络结构 131
4.2　LeNet――第一个卷积神经网络 132
4.3　新起点――AlexNet 133
4.3.1　网络结构 133
4.3.2　局部响应归一化（Local Response Normalization，LRN） 136
4.4　更深的网络――GoogLeNet 136
4.4.1　1×1卷积和Network In Network 136
4.4.2　Inception结构 138
4.4.3　网络结构 138
4.4.4　批规一化（Batch Normalization，BN） 140
4.5　更深的网络――ResNet 142
4.5.1　困难的深层网络训练：退化问题 142
4.5.2　残差单元 142
4.5.3　深度残差网络 144
4.5.4　从集成的角度看待ResNet 144
4.5.5　结构更复杂的网络 146
第2篇　实例精讲
第5章　Python基础 148
5.1　Python简介 148
5.1.1　Python简史 148
5.1.2　安装和使用Python 149
5.2　Python基本语法 150
5.2.1　基本数据类型和运算 150
5.2.2　容器 153
5.2.3　分支和循环 156
5.2.4　函数、生成器和类 159
5.2.5　map、reduce和filter 162
5.2.6　列表生成（list comprehension） 163
5.2.7　字符串 163
5.2.8　文件操作和pickle 164
5.2.9　异常 165
5.2.10　多进程（multiprocessing） 165
5.2.11　os模块 166
5.3　Python的科学计算包――NumPy 167
5.3.1　基本类型（array） 167
5.3.2　线性代数模块（linalg） 172
5.3.3　随机模块（random） 173
5.4　Python的可视化包――matplotlib 175
5.4.1　2D图表 175
5.4.2　3D图表 178
5.4.3　图像显示 180
第6章　OpenCV基础 182
6.1　OpenCV简介 182
6.1.1　OpenCV的结构 182
6.1.2　安装和使用OpenCV 183
6.2　Python-OpenCV基础 184
6.2.1　图像的表示 184
6.2.2　基本图像处理 185
6.2.3　图像的仿射变换 188
6.2.4　基本绘图 190
6.2.5　视频功能 192
6.3　用OpenCV实现数据增加小工具 193
6.3.1　随机裁剪 194
6.3.2　随机旋转 194
6.3.3　随机颜色和明暗 196
6.3.4　多进程调用加速处理 196
6.3.5　代码：图片数据增加小工具 196
6.4　用OpenCV实现物体标注小工具 203
6.4.1　窗口循环 203
6.4.2　鼠标和键盘事件 205
6.4.3　代码：物体检测标注的小工具 206
第7章　Hello World! 212
7.1　用MXNet实现一个神经网络 212
7.1.1　基础工具、NVIDIA驱动和CUDA安装 212
7.1.2　安装MXNet 213
7.1.3　MXNet基本使用 214
7.1.4　用MXNet实现一个两层神经网络 215
7.2　用Caffe实现一个神经网络 219
7.2.1　安装Caffe 219
7.2.2　Caffe的基本概念 220
7.2.3　用Caffe实现一个两层神经网络 221
第8章　最简单的图片分类――手写数字识别 227
8.1　准备数据――MNIST 227
8.1.1　下载MNIST 227
8.1.2　生成MNIST的图片 227
8.2　基于Caffe的实现 228
8.2.1　制作LMDB数据 229
8.2.2　训练LeNet-5 230
8.2.3　测试和评估 235
8.2.4　识别手写数字 239
8.2.5　增加平移和旋转扰动 240
8.3　基于MXNet的实现 242
8.3.1　制作Image Recordio数据 242
8.3.2　用Module模块训练LeNet-5 243
8.3.3　测试和评估 245
8.3.4　识别手写数字 247
第9章　利用Caffe做回归 249
9.1　回归的原理 249
9.1.1　预测值和标签值的欧式距离 249
9.1.2　EuclideanLoss层 250
9.2　预测随机噪声的频率 250
9.2.1　生成样本：随机噪声 250
9.2.2　制作多标签HDF5数据 252
9.2.3　网络结构和Solver定义 253
9.2.4　训练网络 259
9.2.5　批量装载图片并利用GPU预测 260
9.2.6　卷积核可视化 262
第10章　迁移学习和模型微调 264
10.1　吃货必备――通过Python采集美食图片 264
10.1.1　通过关键词和图片搜索引擎下载图片 264
10.1.2　数据预处理――去除无效和不相关图片 267
10.1.3　数据预处理――去除重复图片 267
10.1.4　生成训练数据 269
10.2　美食分类模型 271
10.2.1　迁移学习 271
10.2.2　模型微调法（Finetune） 272
10.2.3　混淆矩阵（Confusion Matrix） 276
10.2.4　P-R曲线和ROC曲线 278
10.2.5　全局平均池化和激活响应图 284
第11章　目标检测 288
11.1　目标检测算法简介 288
11.1.1　滑窗法 288
11.1.2　PASCAL VOC、mAP和IOU简介 289
11.1.3　Selective Search和R-CNN简介 290
11.1.4　SPP、ROI Pooling和Fast R-CNN简介 291
11.1.5　RPN和Faster R-CNN简介 293
11.1.6　YOLO和SSD简介 294
11.2　基于PASCAL VOC数据集训练SSD模型 296
11.2.1　MXNet的SSD实现 296
11.2.2　下载PASCAL VOC数据集 297
11.2.3　训练SSD模型 298
11.2.4　测试和评估模型效果 299
11.2.5　物体检测结果可视化 299
11.2.6　制作自己的标注数据 302
第12章　度量学习 304
12.1　距离和度量学习 304
12.1.1　欧氏距离和马氏距离 304
12.1.2　欧式距离和余弦距离 305
12.1.3　非线性度量学习和Siamese网络 306
12.1.4　Contrastive Loss：对比损失函数 307
12.2　用MNIST训练Siamese网络 307
12.2.1　数据准备 307
12.2.2　参数共享训练 309
12.2.3　结果和可视化 314
12.2.4　用τ-SNE可视化高维特征 316
第13章　图像风格迁移 317
13.1　风格迁移算法简介 317
13.1.1　通过梯度下降法进行图像重建 317
13.1.2　图像风格重建和Gram矩阵 318
13.1.3　图像风格迁移 320
13.2　MXNet中的图像风格迁移例子 320
13.2.1　MXNet的风格迁移实现 321
13.2.2　对图片进行风格迁移 326
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 概述 1
1.1 什么是计算机视觉？ 2
1.2 简史 8
1.3 本书概述 16
1.4 课程大纲样例 21
1.5 标记法说明 22
1.6 扩展阅读 22
第2章 图像形成 25
2.1 几何基元和变换 26
2.1.1 几何基元 26
2.1.2 2D变换 29
2.1.3 3D变换 32
2.1.4 3D旋转 33
2.1.5 3D到2D投影 37
2.1.6 镜头畸变 46
2.2 光度测定学的图像形成 47
2.2.1 照明 48
2.2.2 反射和阴影 49
2.2.3 光学 54
2.3 数字摄像机 57
2.3.1 采样与混叠 60
2.3.2 色彩 63
2.3.3 压缩 71
2.4 补充阅读 72
2.5 习题 73
第3章 图像处理 77
3.1 点算子 78
3.1.1 像素变换 79
3.1.2 彩色变换 81
3.1.3 合成与抠图 81
3.1.4 直方图均衡化 83
3.1.5 应用：色调调整 86
3.2 线性滤波 86
3.2.1 可分离的滤波 89
3.2.2 线性滤波示例 90
3.2.3 带通和导向滤波器 91
3.3 更多的邻域算子 95
3.3.1 非线性滤波 95
3.3.2 形态学 99
3.3.3 距离变换 100
3.3.4 连通量 101
3.4 傅里叶变换 102
3.4.1 傅里叶变换对 105
3.4.2 二维傅里叶变换 107
3.4.3 维纳滤波 108
3.4.4 应用：锐化，模糊
和去噪 111
3.5 金字塔与小波 111
3.5.1 插值 112
3.5.2 降采样 114
3.5.3 多分辨率表达 116
3.5.4 小波 119
3.5.5 应用：图像融合 123
3.6 几何变换 125
3.6.1 参数化变换 125
3.6.2 基于网格的卷绕 131
3.6.3 应用：基于特征的变形 133
3.7 全局优化 133
3.7.1 正则化 134
3.7.2 马尔科夫随机场 138
3.7.3 应用：图像的恢复 147
3.8 补充阅读 147
3.9 习题 149
第4章 特征检测与匹配 157
4.1 点和块 159
4.1.1 特征检测器 160
4.1.2 特征描述子 169
4.1.3 特征匹配 172
4.1.4 特征跟踪 179
4.1.5 应用：表演驱动的动画 181
4.2 边缘 182
4.2.1 边缘检测 182
4.2.2 边缘连接 187
4.2.3 应用：边缘编辑和增强 189
4.3 线条 190
4.3.1 逐次近似 191
4.3.2 Hough变换 191
4.3.3 消失点 194
4.3.4 应用：矩形检测 196
4.4 扩展阅读 197
4.5 习题 198
第5章 分割 205
5.1 活动轮廓 206
5.1.1 蛇行 207
5.1.2 动态蛇行和
CONDENSATION 211
5.1.3 剪刀 214
5.1.4 水平集 215
5.1.5 应用：轮廓跟踪和
转描机 217
5.2 分裂与归并 218
5.2.1 分水岭 218
5.2.2 区域分裂(区分式聚类) 219
5.2.3 区域归并(凝聚式聚类) 219
5.2.4 基于图的分割 219
5.2.5 概率聚集 220
5.3 均值移位和模态发现 221
5.3.1 k-均值和高斯混合 222
5.3.2 均值移位 224
5.4 规范图割 227
5.5 图割和基于能量的方法 230
5.6 补充阅读 234
5.7 习题 235
第6章 基于特征的配准 237
6.1 基于2D和3D特征的配准 238
6.1.1 使用最小二乘的
2D配准 238
6.1.2 应用：全景图 240
6.1.3 迭代算法 241
6.1.4 鲁棒最小二乘
和RANSAC 243
6.1.5 3D配准 245
6.2 姿态估计 246
6.2.1 线性算法 246
6.2.2 迭代算法 248
6.2.3 应用：增强现实 249
6.3 几何内参数标定 250
6.3.1 标定模式 250
6.3.2 消失点 252
6.3.3 应用：单视图测量学 253
6.3.4 旋转运动 254
6.3.5 径向畸变 256
6.4 补充阅读 257
6.5 习题 258
第7章 由运动到结构 263
7.1 三角测量 264
7.2 二视图由运动到结构 266
7.2.1 投影(未标定的)重建 270
7.2.2 自标定 271
7.2.3 应用：视图变形 273
7.3 因子分解 274
7.3.1 透视与投影因子分解 276
7.3.2 应用：稀疏3D模型
提取 277
7.4 光束平差法 278
7.4.1 挖掘稀疏性 280
7.4.2 应用：匹配运动和增强
现实 282
7.4.3 不确定性和二义性 283
7.4.4 应用：由因特网照片
重建 284
7.5 限定结构和运动 287
7.5.1 基于线条的方法 287
7.5.2 基于平面的方法 288
7.6 补充阅读 289
7.7 习题 290
第8章 稠密运动估计 293
8.1 平移配准 294
8.1.1 分层运动估计 297
8.1.2 基于傅里叶的配准 298
8.1.3 逐次求精 300
8.2 参数化运动 305
8.2.1 应用：视频稳定化 308
8.2.2 学到的运动模型 308
8.3 基于样条的运动 309
8.4 光流 312
8.4.1 多帧运动估计 315
8.4.2 应用：视频去噪 316
8.4.3 应用：去隔行扫描 316
8.5 层次运动 317
8.5.1 应用：帧插值 319
8.5.2 透明层和反射 320
8.6 补充阅读 321
8.7 习题 322
第9章 图像拼接 327
9.1 运动模型 329
9.1.1 平面透视运动 329
9.1.2 应用：白板和文档扫描 330
9.1.3 旋转全景图 331
9.1.4 缝隙消除 333
9.1.5 应用：视频摘要和压缩 334
9.1.6 圆柱面和球面坐标 335
9.2 全局配准 338
9.2.1 光束平差法 338
9.2.2 视差消除 341
9.2.3 认出全景图 343
9.2.4 直接配准和基于特征的
?配准 345
9.3 合成 346
9.3.1 合成表面的选择 346
9.3.2 像素选择和加权
(去虚影) 348
9.3.3 应用：照片蒙太奇 352
9.3.4 融合 353
9.4 补充阅读 355
9.5 习题 356
第10章 计算摄影学 359
10.1 光度学标定 361
10.1.1 辐射度响应函数 362
10.1.2 噪声水平估计 363
10.1.3 虚影 364
10.1.4 光学模糊(空间响应)
估计 365
10.2 高动态范围成像 368
10.2.1 色调映射 374
10.2.2 应用：闪影术 380
10.3 超分辨率和模糊去除 381
10.3.1 彩色图像去马赛克 385
10.3.2 应用：彩色化 387
10.4 图像抠图和合成 388
10.4.1 蓝屏抠图 389
10.4.2 自然图像抠图 391
10.4.3 基于优化的抠图 394
10.4.4 烟、阴影和闪抠图 396
10.4.5 视频抠图 397
10.5 纹理分析与合成 398
10.5.1 应用：空洞填充
与修图 400
10.5.2 应用：非真实感绘制 401
10.6 补充阅读 403
10.7 习题 404
第11章 立体视觉对应 409
11.1 极线几何学 412
11.1.1 矫正 412
11.1.2 平面扫描 414
11.2 稀疏对应 416
11.3 稠密对应 418
11.4 局部方法 420
11.4.1 亚像素估计
与不确定性 422
11.4.2 应用：基于立体视觉的
头部跟踪 423
11.5 全局优化 424
11.5.1 动态规划 425
11.5.2 基于分割的方法 427
11.5.3 应用：z-键控与背景
替换 428
11.6 多视图立体视觉 429
11.6.1 体积与3D表面重建 432
11.6.2 由轮廓到形状 436
11.7 补充阅读 438
11.8 习题 439
第12章 3D重建 443
12.1 由X到形状 444
12.1.1 由阴影到形状与光度
测量立体视觉 445
12.1.2 由纹理到形状 447
12.1.3 由聚焦到形状 448
12.2 主动距离获取 449
12.2.1 距离数据归并 451
12.2.2 应用：数字遗产 453
12.3 表面表达 454
12.3.1 表面插值 454
12.3.2 表面简化 455
12.3.3 几何图像 456
12.4 基于点的表达 456
12.5 体积表达 457
12.6 基于模型的重建 459
12.6.1 建筑结构 459
12.6.2 头部和人脸 461
12.6.3 应用：脸部动画 463
12.6.4 完整人体建模与跟踪 465
12.7 恢复纹理映射与反照率 469
12.7.1 估计BRDF 470
12.7.2 应用：3D摄影学 471
12.8 补充阅读 472
12.9 习题 473
第13章 基于图像的绘制 477
13.1 视图插值 478
13.1.1 视图相关的纹理映射 480
13.1.2 应用：照片游览 481
13.2 层次深度图像 482
13.3 光场与发光图 484
13.3.1 非结构化发光图 487
13.3.2 表面光场 488
13.3.3 应用：同心拼图 489
13.4 环境影像形板 490
13.4.1 更高维光场 491
13.4.2 从建模到绘制 492
13.5 基于视频的绘制 493
13.5.1 基于视频的动画 493
13.5.2 视频纹理 494
13.5.3 应用：图片动画 497
13.5.4 3D视频 497
13.5.5 应用：基于视频的
游览 499
13.6 补充阅读 501
13.7 习题 503
第14章 识别 507
14.1 物体检测 509
14.1.1 人脸检测 509
14.1.2 行人检测 515
14.2 人脸识别 518
14.2.1 特征脸 518
14.2.2 活动表观与3D形状
模型 525
14.2.3 应用：个人照片收藏 528
14.3 实例识别 529
14.3.1 几何配准 530
14.3.2 大型数据库 531
14.3.3 应用：位置识别 535
14.4 类别识别 537
14.4.1 词袋 539
14.4.2 基于部件的模型 542
14.4.3 基于分割的识别 545
14.4.4 应用：智能照片编辑 548
14.5 上下文与场景理解 550
14.5.1 学习与大型图像收集 552
14.5.2 应用：图像搜索 554
14.6 识别数据库和测试集 555
14.7 补充阅读 559
14.8 习题 562
第15章 结语 567
附录A 线性代数与数值方法 569
A.1 矩阵分解 570
A.1.1 奇异值分解 570
A.1.2 特征值分解 571
A.1.3 QR因子分解 573
A.1.4 乔里斯基分解 574
A.2 线性最小二乘 575
A.3 非线性最小二乘 578
A.4 直接稀疏矩阵方法 579
A.5 迭代方法 580
A.5.1 共轭梯度 581
A.5.2 预处理 582
A.5.3 多重网格 583
附录B 贝叶斯建模与推断 585
B.1 估计理论 586
B.2 最大似然估计与最小二乘 589
B.3 鲁棒统计学 590
B.4 先验模型与贝叶斯推断 591
B.5 马尔科夫随机场 592
B.5.1 梯度下降与模拟退火 594
B.5.2 动态规划 595
B.5.3 置信传播 596
B.5.4 图割 598
B.5.5 线性规划 601
B.6 不确定性估计(误差分析) 602
附录C 补充材料 604
C.1 数据集 605
C.2 软件 607
C.3 幻灯片与讲座 615
C.4 参考文献 615
词汇表 617
・ ・ ・ ・ ・ ・ (收起)《python计算机视觉编程》
推荐序 xi
前言 xiii
第1章　基本的图像操作和处理 1
1.1　pil：python图像处理类库 1
1.1.1　转换图像格式 2
1.1.2　创建缩略图 3
1.1.3　复制和粘贴图像区域 3
1.1.4　调整尺寸和旋转 3
1.2　matplotlib 4
1.2.1　绘制图像、点和线 4
1.2.2　图像轮廓和直方图 6
1.2.3　交互式标注 7
1.3　numpy 8
1.3.1　图像数组表示 8
1.3.2　灰度变换 9
1.3.3　图像缩放 11
1.3.4　直方图均衡化 11
1.3.5　图像平均 13
1.3.6　图像的主成分分析（pca） 14
1.3.7　使用pickle模块 16
1.4　scipy 17
1.4.1　图像模糊 18
1.4.2　图像导数 19
1.4.3　形态学：对象计数 22
1.4.4　一些有用的scipy模块 23
1.5　高级示例：图像去噪 24
练习 28
代码示例约定 29
第2章　局部图像描述子 31
2.1　harris角点检测器 31
2.2　sift（尺度不变特征变换） 39
2.2.1　兴趣点 39
2.2.2　描述子 39
2.2.3　检测兴趣点 40
2.2.4　匹配描述子 43
2.3　匹配地理标记图像 47
2.3.1　从panoramio下载地理标记图像 47
2.3.2　使用局部描述子匹配 50
2.3.3　可视化连接的图像 52
练习 54
第3章　图像到图像的映射 57
3.1　单应性变换 57
3.1.1　直接线性变换算法 59
3.1.2　仿射变换 60
3.2　图像扭曲 61
3.2.1　图像中的图像 63
3.2.2　分段仿射扭曲 67
3.2.3　图像配准 70
3.3　创建全景图 76
3.3.1　ransac 77
3.3.2　稳健的单应性矩阵估计 78
3.3.3　拼接图像 81
练习 84
第4章　照相机模型与增强现实 85
4.1　针孔照相机模型 85
4.1.1　照相机矩阵 86
4.1.2　三维点的投影 87
4.1.3　照相机矩阵的分解 89
4.1.4　计算照相机中心 90
4.2　照相机标定 91
4.3　以平面和标记物进行姿态估计 93
4.4　增强现实 97
4.4.1　pygame和pyopengl 97
4.4.2　从照相机矩阵到opengl格式 98
4.4.3　在图像中放置虚拟物体 100
4.4.4　综合集成 102
4.4.5　载入模型 104
练习 106
第5章　多视图几何 107
5.1　外极几何 107
5.1.1　一个简单的数据集 109
5.1.2　用matplotlib绘制三维数据 111
5.1.3　计算f：八点法 112
5.1.4　外极点和外极线 113
5.2　照相机和三维结构的计算 116
5.2.1　三角剖分 116
5.2.2　由三维点计算照相机矩阵 118
5.2.3　由基础矩阵计算照相机矩阵 120
5.3　多视图重建 122
5.3.1　稳健估计基础矩阵 123
5.3.2　三维重建示例 125
5.3.3　多视图的扩展示例 129
5.4　立体图像 130
练习 135
第6章　图像聚类 137
6.1　k-means聚类 137
6.1.1　scipy聚类包 138
6.1.2　图像聚类 139
6.1.3　在主成分上可视化图像 140
6.1.4　像素聚类 142
6.2　层次聚类 144
6.3　谱聚类 152
练习 157
第7章　图像搜索 159
7.1　基于内容的图像检索 159
7.2　视觉单词 160
7.3　图像索引 164
7.3.1　建立数据库 164
7.3.2　添加图像 165
7.4　在数据库中搜索图像 167
7.4.1　利用索引获取候选图像 168
7.4.2　用一幅图像进行查询 169
7.4.3　确定对比基准并绘制结果 171
7.5　使用几何特性对结果排序 172
7.6　建立演示程序及web应用 176
7.6.1　用cherrypy创建web应用 176
7.6.2　图像搜索演示程序 176
练习 179
第8章　图像内容分类 181
8.1　k邻近分类法（knn） 181
8.1.1　一个简单的二维示例 182
8.1.2　用稠密sift作为图像特征 185
8.1.3　图像分类：手势识别 187
8.2　贝叶斯分类器 190
8.3　支持向量机 195
8.3.1　使用libsvm 196
8.3.2　再论手势识别 198
8.4　光学字符识别 199
8.4.1　训练分类器 200
8.4.2　选取特征 200
8.4.3　多类支持向量机 201
8.4.4　提取单元格并识别字符 202
8.4.5　图像校正 205
练习 206
第9章　图像分割 209
9.1　图割（graph cut） 209
9.1.1　从图像创建图 211
9.1.2　用户交互式分割 216
9.2　利用聚类进行分割 218
9.3　变分法 224
练习 226
第10章　opencv 227
10.1　opencv的python接口 227
10.2　opencv基础知识 228
10.2.1　读取和写入图像 228
10.2.2　颜色空间 228
10.2.3　显示图像及结果 229
10.3　处理视频 232
10.3.1　视频输入 232
10.3.2　将视频读取到numpy数组中 234
10.4　跟踪 234
10.4.1　光流 235
10.4.2　lucas-kanade算法 237
10.5　更多示例 243
10.5.1　图像修复 243
10.5.2　利用分水岭变换进行分割 244
10.5.3　利用霍夫变换检测直线 245
练习 246
附录a　安装软件包 247
a.1　numpy和scipy 247
a.1.1　windows 247
a.1.2　mac os x 247
a.1.3　linux 248
a.2　matplotlib 248
a.3　pil 248
a.4　libsvm 249
a.5　opencv 249
a.5.1　windows 和 unix 249
a.5.2　mac os x 249
a.5.3　linux 250
a.6　vlfeat 250
a.7　pygame 250
a.8　pyopengl 250
a.9　pydot 251
a.10　python-graph 251
a.11　simplejson 252
a.12　pysqlite 252
a.13　cherrypy 252
附录b　图像集 253
b.1　flickr 253
b.2　panoramio 254
b.3　牛津大学视觉几何组 255
b.4　肯塔基大学识别基准图像 255
b.5　其他 256
b.5.1　prague texture segmentation datagenerator与基准 256
b.5.2　微软研究院grab cut数据集 256
b.5.3　caltech 101 256
b.5.4　静态手势数据库 256
b.5.5　middlebury stereo数据集 256
附录c　图片来源 257
c.1　来自flickr的图像 257
c.2　其他图像 258
c.3　插图 258
参考文献 259
索引 263
・ ・ ・ ・ ・ ・ (收起)第1章　图像编程入门　　1
1.1　简介　　1
1.2　安装OpenCV库　　1
1.2.1　准备工作　　1
1.2.2　如何实现　　2
1.2.3　实现原理　　4
1.2.4　扩展阅读　　5
1.2.5　参阅　　6
1.3　装载、显示和存储图像　　6
1.3.1　准备工作　　6
1.3.2　如何实现　　6
1.3.3　实现原理　　8
1.3.4　扩展阅读　　9
1.3.5　参阅　　11
1.4　深入了解cv::Mat　　11
1.4.1　如何实现　　11
1.4.2　实现原理　　13
1.4.3　扩展阅读　　16
1.4.4　参阅　　17
1.5　定义感兴趣区域　　17
1.5.1　准备工作　　17
1.5.2　如何实现　　17
1.5.3　实现原理　　18
1.5.4　扩展阅读　　18
1.5.5　参阅　　19
第2章　操作像素　　20
2.1　简介　　20
2.2　访问像素值　　21
2.2.1　准备工作　　21
2.2.2　如何实现　　21
2.2.3　实现原理　　23
2.2.4　扩展阅读　　24
2.2.5　参阅　　24
2.3　用指针扫描图像　　24
2.3.1　准备工作　　25
2.3.2　如何实现　　25
2.3.3　实现原理　　26
2.3.4　扩展阅读　　27
2.3.5　参阅　　31
2.4　用迭代器扫描图像　　31
2.4.1　准备工作　　31
2.4.2　如何实现　　31
2.4.3　实现原理　　32
2.4.4　扩展阅读　　33
2.4.5　参阅　　33
2.5　编写高效的图像扫描循环　　33
2.5.1　如何实现　　34
2.5.2　实现原理　　34
2.5.3　扩展阅读　　36
2.5.4　参阅　　36
2.6　扫描图像并访问相邻像素　　36
2.6.1　准备工作　　36
2.6.2　如何实现　　36
2.6.3　实现原理　　38
2.6.4　扩展阅读　　38
2.6.5　参阅　　39
2.7　实现简单的图像运算　　39
2.7.1　准备工作　　39
2.7.2　如何实现　　40
2.7.3　实现原理　　40
2.7.4　扩展阅读　　41
2.8　图像重映射　　42
2.8.1　如何实现　　42
2.8.2　实现原理　　43
2.8.3　参阅　　44
第3章　处理图像的颜色　　45
3.1　简介　　45
3.2　用策略设计模式比较颜色　　45
3.2.1　如何实现　　46
3.2.2　实现原理　　47
3.2.3　扩展阅读　　50
3.2.4　参阅　　53
3.3　用GrabCut算法分割图像　　53
3.3.1　如何实现　　54
3.3.2　实现原理　　56
3.3.3　参阅　　56
3.4　转换颜色表示法　　56
3.4.1　如何实现　　57
3.4.2　实现原理　　58
3.4.3　参阅　　59
3.5　用色调、饱和度和亮度表示颜色　　59
3.5.1　如何实现　　59
3.5.2　实现原理　　61
3.5.3　拓展阅读　　64
3.5.4　参阅　　66
第4章　用直方图统计像素　　67
4.1　简介　　67
4.2　计算图像直方图　　67
4.2.1　准备工作　　68
4.2.2　如何实现　　68
4.2.3　实现原理　　72
4.2.4　扩展阅读　　72
4.2.5　参阅　　74
4.3　利用查找表修改图像外观　　74
4.3.1　如何实现　　74
4.3.2　实现原理　　75
4.3.3　扩展阅读　　76
4.3.4　参阅　　78
4.4　直方图均衡化　　78
4.4.1　如何实现　　78
4.4.2　实现原理　　79
4.5　反向投影直方图检测特定图像内容　　79
4.5.1　如何实现　　80
4.5.2　实现原理　　81
4.5.3　扩展阅读　　82
4.5.4　参阅　　84
4.6　用均值平移算法查找目标　　85
4.6.1　如何实现　　85
4.6.2　实现原理　　87
4.6.3　参阅　　88
4.7　比较直方图搜索相似图像　　88
4.7.1　如何实现　　88
4.7.2　实现原理　　90
4.7.3　参阅　　90
4.8　用积分图像统计像素　　91
4.8.1　如何实现　　91
4.8.2　实现原理　　92
4.8.3　扩展阅读　　93
4.8.4　参阅　　99
第5章　用形态学运算变换图像　　100
5.1　简介　　100
5.2　用形态学滤波器腐蚀和膨胀图像　　100
5.2.1　准备工作　　101
5.2.2　如何实现　　101
5.2.3　实现原理　　102
5.2.4　扩展阅读　　103
5.2.5　参阅　　104
5.3　用形态学滤波器开启和闭合图像　　104
5.3.1　如何实现　　104
5.3.2　实现原理　　105
5.3.3　参阅　　106
5.4　在灰度图像中应用形态学运算　　106
5.4.1　如何实现　　106
5.4.2　实现原理　　107
5.4.3　参阅　　108
5.5　用分水岭算法实现图像分割　　108
5.5.1　如何实现　　109
5.5.2　实现原理　　111
5.5.3　扩展阅读　　112
5.5.4　参阅　　114
5.6　用MSER算法提取特征区域　　114
5.6.1　如何实现　　114
5.6.2　实现原理　　116
5.6.3　参阅　　118
第6章　图像滤波　　119
6.1　简介　　119
6.2　低通滤波器　　120
6.2.1　如何实现　　120
6.2.2　实现原理　　121
6.2.3　参阅　　123
6.3　用滤波器进行缩减像素采样　　124
6.3.1　如何实现　　124
6.3.2　实现原理　　125
6.3.3　扩展阅读　　126
6.3.4　参阅　　127
6.4　中值滤波器　　128
6.4.1　如何实现　　128
6.4.2　实现原理　　129
6.5　用定向滤波器检测边缘　　129
6.5.1　如何实现　　130
6.5.2　实现原理　　132
6.5.3　扩展阅读　　135
6.5.4　参阅　　136
6.6　计算拉普拉斯算子　　136
6.6.1　如何实现　　137
6.6.2　实现原理　　138
6.6.3　扩展阅读　　141
6.6.4　参阅　　142
第7章　提取直线、轮廓和区域　　143
7.1　简介　　143
7.2　用Canny算子检测图像轮廓　　143
7.2.1　如何实现　　143
7.2.2　实现原理　　145
7.2.3　参阅　　146
7.3　用霍夫变换检测直线　　146
7.3.1　准备工作　　146
7.3.2　如何实现　　147
7.3.3　实现原理　　151
7.3.4　扩展阅读　　153
7.3.5　参阅　　155
7.4　点集的直线拟合　　155
7.4.1　如何实现　　155
7.4.2　实现原理　　157
7.4.3　扩展阅读　　158
7.5　提取连续区域　　158
7.5.1　如何实现　　159
7.5.2　实现原理　　160
7.5.3　扩展阅读　　161
7.6　计算区域的形状描述子　　161
7.6.1　如何实现　　162
7.6.2　实现原理　　163
7.6.3　扩展阅读　　164
第8章　检测兴趣点　　166
8.1　简介　　166
8.2　检测图像中的角点　　166
8.2.1　如何实现　　167
8.2.2　实现原理　　171
8.2.3　扩展阅读　　172
8.2.4　参阅　　174
8.3　快速检测特征　　174
8.3.1　如何实现　　174
8.3.2　实现原理　　175
8.3.3　扩展阅读　　176
8.3.4　参阅　　178
8.4　尺度不变特征的检测　　178
8.4.1　如何实现　　179
8.4.2　实现原理　　180
8.4.3　扩展阅读　　181
8.4.4　参阅　　183
8.5　多尺度FAST特征的检测　　183
8.5.1　如何实现　　183
8.5.2　实现原理　　184
8.5.3　扩展阅读　　185
8.5.4　参阅　　186
第9章　描述和匹配兴趣点　　187
9.1　简介　　187
9.2　局部模板匹配　　187
9.2.1　如何实现　　188
9.2.2　实现原理　　190
9.2.3　扩展阅读　　191
9.2.4　参阅　　192
9.3　描述并匹配局部强度值模式　　192
9.3.1　如何实现　　193
9.3.2　实现原理　　195
9.3.3　扩展阅读　　196
9.3.4　参阅　　199
9.4　用二值描述子匹配关键点　　199
9.4.1　如何实现　　199
9.4.2　实现原理　　200
9.4.3　扩展阅读　　201
9.4.4　参阅　　202
第10章　估算图像之间的投影关系　　203
10.1　简介　　203
10.2　计算图像对的基础矩阵　　205
10.2.1　准备工作　　205
10.2.2　如何实现　　206
10.2.3　实现原理　　208
10.2.4　参阅　　209
10.3　用RANSAC（随机抽样一致性）算法匹配图像　　209
10.3.1　如何实现　　209
10.3.2　实现原理　　212
10.3.3　扩展阅读　　213
10.4　计算两幅图像之间的单应矩阵　　214
10.4.1　准备工作　　214
10.4.2　如何实现　　215
10.4.3　实现原理　　217
10.4.4　扩展阅读　　218
10.4.5　参阅　　219
10.5　检测图像中的平面目标　　219
10.5.1　如何实现　　219
10.5.2　实现原理　　221
10.5.3　参阅　　224
第11章　三维重建　　225
11.1　简介　　225
11.2　相机标定　　226
11.2.1　如何实现　　227
11.2.2　实现原理　　230
11.2.3　扩展阅读　　232
11.2.4　参阅　　233
11.3　相机姿态还原　　233
11.3.1　如何实现　　233
11.3.2　实现原理　　235
11.3.3　扩展阅读　　236
11.3.4　参阅　　238
11.4　用标定相机实现三维重建　　238
11.4.1　如何实现　　238
11.4.2　实现原理　　241
11.4.3　扩展阅读　　243
11.4.4　参阅　　244
11.5　计算立体图像的深度　　244
11.5.1　准备工作　　244
11.5.2　如何实现　　245
11.5.3　实现原理　　247
11.5.4　参阅　　247
第12章　处理视频序列　　248
12.1　简介　　248
12.2　读取视频序列　　248
12.2.1　如何实现　　248
12.2.2　实现原理　　250
12.2.3　扩展阅读　　251
12.2.4　参阅　　251
12.3　处理视频帧　　251
12.3.1　如何实现　　251
12.3.2　实现原理　　252
12.3.3　扩展阅读　　256
12.3.4　参阅　　258
12.4　写入视频帧　　258
12.4.1　如何实现　　259
12.4.2　实现原理　　259
12.4.3　扩展阅读　　262
12.4.4　参阅　　263
12.5　提取视频中的前景物体　　263
12.5.1　如何实现　　264
12.5.2　实现原理　　266
12.5.3　扩展阅读　　266
12.5.4　参阅　　268
第13章　跟踪运动目标　　269
13.1　简介　　269
13.2　跟踪视频中的特征点　　269
13.2.1　如何实现　　269
13.2.2　实现原理　　274
13.2.3　参阅　　274
13.3　估算光流　　275
13.3.1　准备工作　　275
13.3.2　如何实现　　276
13.3.3　实现原理　　278
13.3.4　参阅　　279
13.4　跟踪视频中的物体　　279
13.4.1　如何实现　　279
13.4.2　实现原理　　282
13.4.3　参阅　　284
第14章　实用案例　　285
14.1　简介　　285
14.2　人脸识别　　286
14.2.1　如何实现　　286
14.2.2　实现原理　　288
14.2.3　参阅　　290
14.3　人脸定位　　291
14.3.1　准备工作　　291
14.3.2　如何实现　　292
14.3.3　实现原理　　295
14.3.4　扩展阅读　　297
14.3.5　参阅　　298
14.4　行人检测　　298
14.4.1　准备工作　　298
14.4.2　如何实现　　299
14.4.3　实现原理　　302
14.4.4　扩展阅读　　304
14.4.5　参阅　　308
・ ・ ・ ・ ・ ・ (收起)第一章 绪论
1・1 生物视觉通路简介
1・2 Marr的计算视觉理论框架
1・3 本书各章内容简介
1・4 计算机视觉的现状与阅读本书需注意的问题
思考题
参考文献
第二章 边缘检测
2・1 边缘检测与微分滤波器
2・2 边缘检测与正则化方法
2・3 多尺度滤波器与过零点定理
2・4 最优边缘检测滤波器
2・5 边缘检测快速算法
2・6 图像低层次处理的其他问题
思考题
参考文献
第三章 射影几何与几何元素表达
3・1 仿射变换与射影变换的几何表达
3・2 仿射坐标系与射影坐标系
3・3 仿射变换与射影变换的代数表达
3・4 不变量
3・5 由对应点求射影变换
3・6 点
3・7 指向和方向
3・8 平面直线及点线对偶关系
3・9 空间平面及点面对偶关系
3・10 空间直线
3・11 二次曲线与二次曲面
思考题
参考文献
第四章 摄像机定标
4・1 线性模型摄像机定标
4・2 非线性模型摄像机定标
4・3 立体视觉摄像机定标
4・4 机器人手眼定标
4・5 摄像机自定标技术
思考题
参考文献
第五章 立体视觉
5・1 立体视觉与三维重建
5・2 极线约束
5・3 对应基元匹配
5・4 射影几何意义下的三维重建
思考题
参考文献
第六章 运动与不确定性表达
6・1 欧氏平面上的刚体运动
6・2 欧氏空间中的刚体运动
6・3 不确定性的描述
6・4 不确定性的运算
6・5 不确定性运算的几个例子
6・6 三维直线段的不确定性
6・7 不确定性的显示
思考题
参考文献
第七章 基于光流场的运动分析
7・1 光流场和运动场
7・2 光流的约束方程
7・3 微分技术
7・4 其他方法
7・5 基于光流场的定性运动解释
思考题
参考文献
第八章 长序列运动图像特征跟踪
8・1 引论
8・2 参数估计理论初步
8・3 特征运动模型
8・4 特征跟踪的阐述
8・5 匹配
8・6 实际应用中需要考虑的问题
思考题
参考文献
第九章 基于二维特征对应的运动分析
9・1 极线方程和本质矩阵
9・2 基于点匹配的运动计算
9・3 图像是一个空间平面的投影时的运动计算
9・4 基于直线匹配的运动计算
9・5 基本矩阵的估计
思考题
参考文献
第十章 基于三维特征对应的运动分析
10・1 由三维点匹配估计运动
10・2 不需显式匹配的方法
10・3 从三维直线匹配估计运动
10・4 从平面匹配估计运动
10・5 二维-三维的物体定位
思考题
参考文献
第十一章 由图像灰度恢复三维物体形状
11・1 辐射度学与光度学
11・2 光照模型
11・3 由多幅图像恢复三维物体形状
11・4 由单幅图像恢复三维物体形状
思考题
参考文献
第十二章 建模与识别
12・1 CAD系统中的三维模型表达
12・2 曲线与曲面的表达
12・3 三维世界的多层次模型
12・4 由二维图像建模
12・5 识别的一般原则――问题与策略
12・6 特征关系图匹配
12・7 “假设检验”识别方法
思考题
参考文献
第十三章 距离图像获取与处理
13・1 距离传感器
13・2 数据预处理
13・3 深度图分割
思考题
参考文献
第十四章 计算机视觉系统体系结构讨论与展望
14・1 计算机视觉系统的基本体系结构
14・2 视觉系统体系结构讨论
14・3 主动视觉
14・4 计算机视觉的应用展望
参考文献
附录A 实验数据及参考结构
A・1 图像的格式
A・2 摄像机定标
A・3 立体视觉
A・4 基于光流场的运动分析
A・5 长序列运动图像特征跟踪
A・6 基于二维特征对应的运动分析
A・7 基于三维特征对应的运动分析
・ ・ ・ ・ ・ ・ (收起)译者序
前言
致老师
第一部分　导论
第1章　计算机视觉的定义及其历史2
1.1　简介2
1.2　定义2
1.3　局部全局问题3
1.4　生物视觉4
1.4.1　生物动因4
1.4.2　视觉感知6
参考文献7
第2章　编写图像处理程序8
2.1　简介8
2.2　图像处理的基本程序结构8
2.3　良好的编程风格9
2.4　计算机视觉的重点9
2.5　图像分析软件工具包10
2.6　makefile10
2.7　作业11
参考文献11
第3章　数学原理回顾12
3.1　简介12
3.2　线性代数简要回顾12
3.2.1　向量12
3.2.2　向量空间14
3.2.3　零空间15
3.2.4　函数空间16
3.2.5　线性变换17
3.2.6　导数和导数算子19
3.2.7　特征值和特征向量20
3.2.8　特征分解21
3.2.9　奇异值分解21
3.3　函数最小化简要回顾23
3.3.1　梯度下降23
3.3.2　局部最小值和全局最小值26
3.3.3　模拟退火27
3.4　概率论简要回顾28
3.5　作业30
参考文献31
第4章　图像：表示和创建32
4.1　简介32
4.2　图像表示32
4.2.1　标志性表示（图像）32
4.2.2　函数表示(方程)34
4.2.3　线性表示(向量)34
4.2.4　概率表示（随机场）35
4.2.5　图形表示（图）35
4.2.6　邻接悖论和六边形像素36
4.3　作为曲面的图像38
4.3.1　梯度38
4.3.2　等值线38
4.3.3　脊39
4.4　作业39
参考文献40
第二部分　预处理
第5章　卷积核算子42
5.1　简介42
5.2　线性算子42
5.3　图像的向量表示44
5.4　导数估计45
5.4.1　使用核估计导数46
5.4.2　通过函数拟合来估计导数46
5.4.3　图像基向量49
5.4.4　核作为采样可微分函数50
5.4.5　其他高阶导数53
5.4.6　尺度简介54
5.5　边缘检测55
5.6　尺度空间58
5.6.1　金字塔58
5.6.2　没有重采样的尺度空间59
5.7　示例61
5.8　数字梯度检测器的性能63
5.8.1　方向导数63
5.8.2　方向估计67
5.8.3　讨论70
5.9　总结71
5.10　作业71
参考文献76
第6章　去噪78
6.1　简介78
6.2　图像平滑78
6.2.1　一维情况79
6.2.2　二维情况79
6.3　使用双边滤波器实现保边平滑82
6.4　使用扩散方程实现保边平滑84
6.4.1　一维空间的扩散方程84
6.4.2　PDE模拟85
6.4.3　二维空间的扩散方程85
6.4.4　可变电导扩散86
6.5　使用优化实现保边平滑87
6.5.1　噪声消除的目标函数87
6.5.2　寻找一个先验项90
6.5.3　MAP算法实现和均场退火92
6.5.4　病态问题和正则化94
6.6　等效算法95
6.7　总结97
6.8　作业97
参考文献99
第7章　数学形态学101
7.1　简介101
7.2　二值形态学101
7.2.1　膨胀101
7.2.2　腐蚀106
7.2.3　膨胀和腐蚀的性质107
7.2.4　开运算和闭运算108
7.2.5　开运算和闭运算的性质109
7.3　灰度形态学109
7.3.1　使用平面结构元素的灰度图像110
7.3.2　使用灰度结构元素的灰度图像113
7.3.3　使用集合运算的灰度形态学114
7.4　距离变换114
7.4.1　使用迭代最近邻计算DT115
7.4.2　使用二值形态运算计算DT115
7.4.3　使用掩码计算DT115
7.4.4　使用维诺图计算DT117
7.5　边缘链接的应用117
7.6　总结120
7.7　作业121
参考文献122
第三部分　图像理解
第8章　分割124
8.1　简介124
8.2　阈值：仅基于亮度的分割125
8.2.1　阈值的局部性质125
8.2.2　通过直方图分析选择阈值126
8.2.3　用高斯和拟合直方图129
8.2.4　高斯混合模型与期望最大化130
8.3　聚类：基于颜色相似度的分割132
8.3.1　k-均值聚类133
8.3.2　均值移位聚类135
8.4　连接组件：使用区域增长的空间分割136
8.4.1　递归方法136
8.4.2　迭代方法138
8.4.3　示例应用139
8.5　使用主动轮廓进行分割140
8.5.1　snake：离散和连续140
8.5.2　水平集：包含边或者不包含边144
8.6　分水岭：基于亮度曲面的分割151
8.7　图割：基于图论的分割156
8.7.1　目标函数157
8.7.2　求解目标函数158
8.8　使用MFA进行分割159
8.9　评估分割的质量160
8.10　总结161
8.11　作业162
参考文献163
第9章　参数变换167
9.1　简介167
9.2　霍夫变换168
9.2.1　垂线问题169
9.2.2　如何找到交点――累加器数组169
9.2.3　使用梯度降低计算复杂度170
9.3　寻找圆171
9.3.1　由任意三个非共线像素表示的圆的位置推导171
9.3.2　当原点未知但半径已知时找圆172
9.3.3　利用梯度信息减少找圆的计算172
9.4　寻找椭圆172
9.5　广义霍夫变换174
9.6　寻找峰值175
9.7　寻找三维形状――高斯图176
9.8　寻找对应体――立体视觉中的参数一致性177
9.9　总结179
9.10　作业179
参考文
・ ・ ・ ・ ・ ・ (收起)第1章　图像编程入门　　1
1.1　简介　　1
1.2　安装OpenCV库　　1
1.2.1　准备工作　　1
1.2.2　安装　　2
1.2.3　实现原理　　3
1.2.4　扩展阅读　　4
1.2.5　参阅　　6
1.3　装载、显示和存储图像　　6
1.3.1　准备工作　　6
1.3.2　如何实现　　6
1.3.3　实现原理　　8
1.3.4　扩展阅读　　9
1.3.5　参阅　　12
1.4　深入了解cv::Mat　　12
1.4.1　如何实现　　12
1.4.2　实现原理　　14
1.4.3　扩展阅读　　16
1.4.4　参阅　　17
1.5　定义兴趣区域　　18
1.5.1　准备工作　　18
1.5.2　如何实现　　18
1.5.3　实现原理　　19
1.5.4　扩展阅读　　19
1.5.5　参阅　　20
第2章　操作像素　　21
2.1　简介　　21
2.2　访问像素值　　22
2.2.1　准备工作　　22
2.2.2　如何实现　　22
2.2.3　实现原理　　24
2.2.4　扩展阅读　　24
2.2.5　参阅　　25
2.3　用指针扫描图像　　25
2.3.1　准备工作　　25
2.3.2　如何实现　　26
2.3.3　实现原理　　27
2.3.4　扩展阅读　　28
2.3.5　参阅　　31
2.4　用迭代器扫描图像　　31
2.4.1　准备工作　　32
2.4.2　如何实现　　32
2.4.3　实现原理　　32
2.4.4　扩展阅读　　33
2.4.5　参阅　　34
2.5　编写高效的图像扫描循环　　34
2.5.1　如何实现　　34
2.5.2　实现原理　　34
2.5.3　扩展阅读　　36
2.5.4　参阅　　36
2.6　扫描图像并访问相邻像素　　36
2.6.1　准备工作　　36
2.6.2　如何实现　　37
2.6.3　实现原理　　38
2.6.4　扩展阅读　　39
2.6.5　参阅　　39
2.7　实现简单的图像运算　　40
2.7.1　准备工作　　40
2.7.2　如何实现　　40
2.7.3　实现原理　　41
2.7.4　扩展阅读　　41
2.8　图像重映射　　42
2.8.1　如何实现　　43
2.8.2　实现原理　　43
2.8.3　参阅　　44
第3章　用类处理彩色图像　　45
3.1　简介　　45
3.2　在算法设计中使用策略模式　　45
3.2.1　准备工作　　46
3.2.2　如何实现　　46
3.2.3　实现原理　　47
3.2.4　扩展阅读　　50
3.2.5　参阅　　52
3.3　用控制器设计模式实现功能模块间通信　　52
3.3.1　准备工作　　53
3.3.2　如何实现　　53
3.3.3　实现原理　　55
3.3.4　扩展阅读　　56
3.4　转换颜色表示法　　57
3.4.1　准备工作　　57
3.4.2　如何实现　　57
3.4.3　实现原理　　58
3.4.4　参阅　　59
3.5　用色调、饱和度、亮度表示颜色.59
3.5.1　如何实现　　60
3.5.2　实现原理　　61
3.5.3　扩展阅读　　63
第4章　用直方图统计像素　　66
4.1　简介　　66
4.2　计算图像直方图　　66
4.2.1　准备工作　　67
4.2.2　如何实现　　67
4.2.3　实现原理　　71
4.2.4　扩展阅读　　71
4.2.5　参阅　　73
4.3　利用查找表修改图像外观　　73
4.3.1　如何实现　　74
4.3.2　实现原理　　74
4.3.3　扩展阅读　　75
4.3.4　参阅　　77
4.4　直方图均衡化　　78
4.4.1　如何实现　　78
4.4.2　实现原理　　79
4.5　反向投影直方图检测特定图像内容　　 79
4.5.1　如何实现　　79
4.5.2　实现原理　　81
4.5.3　扩展阅读　　81
4.5.4　参阅　　84
4.6　均值平移算法查找目标　　84
4.6.1　如何实现　　85
4.6.2　实现原理　　87
4.6.3　参阅　　88
4.7　比较直方图搜索相似图像　　88
4.7.1　如何实现　　88
4.7.2　实现原理　　90
4.7.3　参阅　　90
4.8　用积分图像统计像素　　91
4.8.1　如何实现　　91
4.8.2　实现原理　　92
4.8.3　扩展阅读　　93
4.8.4　参阅　　99
第5章　用形态学运算变换图像　　100
5.1　简介　　100
5.2　形态学滤波器腐蚀和膨胀图像　　100
5.2.1　准备工作　　101
5.2.2　如何实现　　101
5.2.3　实现原理　　102
5.2.4　扩展阅读　　104
5.2.5　参阅　　104
5.3　用形态学滤波器开启和闭合图像　　 104
5.3.1　如何实现　　104
5.3.2　实现原理　　105
5.3.3　参阅　　106
5.4　用形态学滤波器检测边缘和角点　　 106
5.4.1　准备工作　　106
5.4.2　如何实现　　107
5.4.3　实现原理　　109
5.4.4　参阅　　110
5.5　用分水岭算法实现图像分割　　110
5.5.1　如何实现　　111
5.5.2　实现原理　　114
5.5.3　扩展阅读　　115
5.5.4　参阅　　116
5.6　用MSER算法提取特征区域　　116
5.6.1　如何实现　　117
5.6.2　实现原理　　118
5.6.3　参阅　　121
5.7　用GrabCut算法提取前景物体　　121
5.7.1　如何实现　　121
5.7.2　实现原理　　123
5.7.3　参阅　　124
第6章　图像滤波　　125
6.1　简介　　125
6.2　低通滤波器　　126
6.2.1　如何实现　　126
6.2.2　实现原理　　127
6.2.3　扩展阅读　　129
6.2.4　参阅　　132
6.3　中值滤波器　　133
6.3.1　如何实现　　133
6.3.2　实现原理　　134
6.4　用定向滤波器检测边缘　　134
6.4.1　如何实现　　135
6.4.2　实现原理　　137
6.4.3　扩展阅读　　139
6.4.4　参阅　　141
6.5　计算拉普拉斯算子　　141
6.5.1　如何实现　　141
6.5.2　实现原理　　143
6.5.3　扩展阅读　　145
6.5.4　参阅　　146
第7章　提取直线、轮廓和区域　　147
7.1　简介　　147
7.2　用Canny算子检测图像轮廓　　147
7.2.1　如何实现　　147
7.2.2　实现原理　　148
7.2.3　参阅　　150
7.3　用霍夫变换检测直线　　150
7.3.1　准备工作　　150
7.3.2　如何实现　　150
7.3.3　实现原理　　154
7.3.4　扩展阅读　　157
7.3.5　参阅　　158
7.4　点集的直线拟合　　158
7.4.1　如何实现　　159
7.4.2　实现原理　　161
7.4.3　扩展阅读　　161
7.5　提取区域的轮廓　　161
7.5.1　如何实现　　162
7.5.2　实现原理　　163
7.5.3　扩展阅读　　164
7.6　计算区域的形状描述子　　164
7.6.1　如何实现　　165
7.6.2　实现原理　　166
7.6.3　扩展阅读　　167
第8章　检测兴趣点　　169
8.1　简介　　169
8.2　检测图像中的角点　　169
8.2.1　如何实现　　170
8.2.2　实现原理　　174
8.2.3　扩展阅读　　176
8.2.4　参阅　　177
8.3　快速检测特征　　178
8.3.1　如何实现　　178
8.3.2　实现原理　　179
8.3.3　扩展阅读　　180
8.3.4　参阅　　182
8.4　尺度不变特征的检测　　182
8.4.1　如何实现　　183
8.4.2　实现原理　　184
8.4.3　扩展阅读　　185
8.4.4　参阅　　186
8.5　多尺度FAST 特征的检测　　187
8.5.1　如何实现　　187
8.5.2　实现原理　　188
8.5.3　扩展阅读　　188
8.5.4　参阅　　190
第9章　描述和匹配兴趣点　　191
9.1　简介　　191
9.2　局部模板匹配　　191
9.2.1　如何实现　　192
9.2.2　实现原理　　194
9.2.3　扩展阅读　　195
9.2.4　参阅　　196
9.3　描述局部强度值模式　　196
9.3.1　如何实现　　197
9.3.2　实现原理　　198
9.3.3　扩展阅读　　200
9.3.4　参阅　　203
9.4　用二值特征描述关键点　　203
9.4.1　如何实现　　203
9.4.2　实现原理　　204
9.4.3　扩展阅读　　205
9.4.4　参阅　　206
第10章　估算图像之间的投影关系　　207
10.1　简介　　207
10.2　相机校准　　209
10.2.1　如何实现　　209
10.2.2　实现原理　　213
10.2.3　扩展阅读　　216
10.2.4　参阅　　216
10.3　计算图像对的基础矩阵　　216
10.3.1　准备工作　　217
10.3.2　如何实现　　218
10.3.3　实现原理　　219
10.3.4　参阅　　220
10.4　用RANSAC（随机抽样一致性）算法匹配图像　　220
10.4.1　如何实现　　221
10.4.2　实现原理　　224
10.4.3　扩展阅读　　225
10.5　计算两幅图像之间的单应矩阵.226
10.5.1　准备工作　　226
10.5.2　如何实现　　227
10.5.3　实现原理　　229
10.5.4　扩展阅读　　230
10.5.5　参阅　　232
第11章　处理视频序列　　233
11.1　简介　　233
11.2　读取视频序列　　233
11.2.1　如何实现　　233
11.2.2　实现原理　　235
11.2.3　扩展阅读　　236
11.2.4　参阅　　236
11.3　处理视频帧　　236
11.3.1　如何实现　　236
11.3.2　实现原理　　237
11.3.3　扩展阅读　　241
11.3.4　参阅　　244
11.4　写入视频帧　　244
11.4.1　如何实现　　244
11.4.2　实现原理　　245
11.4.3　扩展阅读　　247
11.4.4　参阅　　249
11.5　跟踪视频中的特征点　　249
11.5.1　如何实现　　249
11.5.2　实现原理　　253
11.5.3　参阅　　254
11.6　提取视频中的前景物体　　254
11.6.1　如何实现　　255
11.6.2　实现原理　　257
11.6.3　扩展阅读　　257
11.6.4　参阅　　259
・ ・ ・ ・ ・ ・ (收起)第1章 浅谈人工智能、神经网络和计算机视觉 1
1.1 人工还是智能 1
1.2 人工智能的三起两落 2
1.2.1 两起两落 2
1.2.2 卷土重来 3
1.3 神经网络简史 5
1.3.1 生物神经网络和人工神经网络 5
1.3.2 M-P模型 6
1.3.3 感知机的诞生 9
1.3.4 你好，深度学习 10
1.4 计算机视觉 11
1.5 深度学习+ 12
1.5.1 图片分类 12
1.5.2 图像的目标识别和语义分割 13
1.5.3 自动驾驶 13
1.5.4 图像风格迁移 14
第2章 相关的数学知识 15
2.1 矩阵运算入门 15
2.1.1 标量、向量、矩阵和张量 15
2.1.2 矩阵的转置 17
2.1.3 矩阵的基本运算 18
2.2 导数求解 22
2.2.1 一阶导数的几何意义 23
2.2.2 初等函数的求导公式 24
2.2.3 初等函数的和、差、积、商求导 26
2.2.4 复合函数的链式法则 27
第3章 深度神经网络基础 29
3.1 监督学习和无监督学习 29
3.1.1 监督学习 30
3.1.2 无监督学习 32
3.1.3 小结 33
3.2 欠拟合和过拟合 34
3.2.1 欠拟合 34
3.2.2 过拟合 35
3.3 后向传播 36
3.4 损失和优化 38
3.4.1 损失函数 38
3.4.2 优化函数 39
3.5 激活函数 42
3.5.1 Sigmoid 44
3.5.2 tanh 45
3.5.3 ReLU 46
3.6 本地深度学习工作站 47
3.6.1 GPU和CPU 47
3.6.2 配置建议 49
第4章 卷积神经网络 51
4.1 卷积神经网络基础 51
4.1.1 卷积层 51
4.1.2 池化层 54
4.1.3 全连接层 56
4.2 LeNet模型 57
4.3 AlexNet模型 59
4.4 VGGNet模型 61
4.5 GoogleNet 65
4.6 ResNet 69
第5章 Python基础 72
5.1 Python简介 72
5.2 Jupyter Notebook 73
5.2.1 Anaconda的安装与使用 73
5.2.2 环境管理 76
5.2.3 环境包管理 77
5.2.4 Jupyter Notebook的安装 79
5.2.5 Jupyter Notebook的使用 80
5.2.6 Jupyter Notebook常用的快捷键 86
5.3 Python入门 88
5.3.1 Python的基本语法 88
5.3.2 Python变量 92
5.3.3 常用的数据类型 94
5.3.4 Python运算 99
5.3.5 Python条件判断语句 107
5.3.6 Python循环语句 109
5.3.7 Python中的函数 113
5.3.8 Python中的类 116
5.4 Python中的NumPy 119
5.4.1 NumPy的安装 119
5.4.2 多维数组 119
5.4.3 多维数组的基本操作 125
5.5 Python中的Matplotlib 133
5.5.1 Matplotlib的安装 133
5.5.2 创建图 133
第6章 PyTorch基础 142
6.1 PyTorch中的Tensor 142
6.1.1 Tensor的数据类型 143
6.1.2 Tensor的运算 146
6.1.3 搭建一个简易神经网络 153
6.2 自动梯度 156
6.2.1 torch.autograd和Variable 156
6.2.2 自定义传播函数 159
6.3 模型搭建和参数优化 162
6.3.1 PyTorch之torch.nn 162
6.3.2 PyTorch之torch.optim 167
6.4 实战手写数字识别 169
6.4.1 torch和torchvision 170
6.4.2 PyTorch之torch.transforms 171
6.4.3 数据预览和数据装载 173
6.4.4 模型搭建和参数优化 174
第7章 迁移学习 180
7.1 迁移学习入门 180
7.2 数据集处理 181
7.2.1 验证数据集和测试数据集 182
7.2.2 数据预览 182
7.3 模型搭建和参数优化 185
7.3.1 自定义VGGNet 185
7.3.2 迁移VGG16 196
7.3.3 迁移ResNet50 203
7.4 小结 219
第8章 图像风格迁移实战 220
8.1 风格迁移入门 220
8.2 PyTorch图像风格迁移实战 222
8.2.1 图像的内容损失 222
8.2.2 图像的风格损失 223
8.2.3 模型搭建和参数优化 224
8.2.4 训练新定义的卷积神经网络 226
8.3 小结 232
第9章 多模型融合 233
9.1 多模型融合入门 233
9.1.1 结果多数表决 234
9.1.2 结果直接平均 236
9.1.3 结果加权平均 237
9.2 PyTorch之多模型融合实战 239
9.3 小结 246
第10章 循环神经网络 247
10.1 循环神经网络入门 247
10.2 PyTorch之循环神经网络实战 249
10.3 小结 257
第11章 自动编码器 258
11.1 自动编码器入门 258
11.2 PyTorch之自动编码实战 259
11.2.1 通过线性变换实现自动编码器模型 260
11.2.2 通过卷积变换实现自动编码器模型 267
11.3 小结 273
・ ・ ・ ・ ・ ・ (收起)第1章 视觉系统实践――图像显示、输入/输出和库函数调用 1
1.1 OpenCV 1
1.2 基本的OpenCV代码 2
1.2.1 IplImage数据结构 3
1.2.2 读写图像 5
1.2.3 图像显示 6
1.2.4 示例 6
1.3 图像捕捉 9
1.4 和AIPCV库的接口 11
1.5 网站文件 15
1.6 参考文献 15
第2章 边缘检测技术 17
2.1 边缘检测的目的 17
2.2 传统的方法和理论 19
2.2.1 边缘的模型 20
2.2.2 噪声 21
2.2.3 导数算子 24
2.2.4 基于模板的边缘检测 29
2.3 边缘模型：Marr-Hildreth边缘检测器 31
2.4 Canny Edge边缘检测器 34
2.5 Shen-Castan(ISEF)边缘检测器 39
2.6 两种最优边缘检测器的比较 41
2.7 彩色边缘 44
2.8 Marr-Hildreth边缘检测器的源代码 46
2.9 Canny边缘检测器的源代码 50
2.10 Shen-Castan边缘检测器的源代码 58
2.11 网站文件 67
2.12 参考文献 69
第3章 数码形态学 73
3.1 形态学定义 73
3.2 连通性 73
3.3 数码形态学的基本元素――二值操作 75
3.3.1 二值膨胀 75
3.3.2 实现二值膨胀 79
3.3.3 二值腐蚀 82
3.3.4 二值腐蚀的实现 86
3.3.5 开启和闭合 88
3.3.6 MAX――用于形态学的高级程序设计语言 93
3.3.7 “命中/不命中”变换 97
3.3.8 识别区域边缘 99
3.3.9 条件膨胀 100
3.3.10 区域计数 102
3.4 灰阶形态学 103
3.4.1 开启操作和闭合操作 105
3.4.2 平滑操作 108
3.4.3 梯度 109
3.4.4 纹理的分割 110
3.4.5 对象的大小分布 111
3.5 彩色形态学 112
3.6 网站文件 113
3.7 参考文献 115
第4章 灰阶分割 117
4.1 灰阶分割的基础 117
4.1.1 使用边缘像素 119
4.1.2 迭代选择法 119
4.1.3 灰阶直方图法 120
4.1.4 使用熵 121
4.1.5 模糊集合 124
4.1.6 最小误差阈值法 126
4.1.7 单阈值选择的示例结果 127
4.2 使用区域阈值 129
4.2.1 Chow-Kaneko算法 130
4.2.2 通过边缘对光照进行
建模 133
4.2.3 实现和结果 135
4.2.4 对比 136
4.3 松弛法 137
4.4 移动平均法 142
4.5 基于聚类的阈值 145
4.6 多重阈值 146
4.7 网站文件 147
4.8 参考文献 148
第5章 纹理和色彩 151
5.1 纹理和分割 151
5.2 灰阶图像中纹理的简单分析 152
5.3 灰阶共生矩阵 155
5.3.1 最大概率 157
5.3.2 矩 157
5.3.3 对比度 157
5.3.4 同质性 157
5.3.5 熵 158
5.3.6 GLCM描述符的测试结果 158
5.3.7 纹理操作符的加速 159
5.4 边缘和纹理 161
5.5 能量和纹理 162
5.6 表面和纹理 164
5.6.1 向量散射算法 164
5.6.2 表面曲度算法 166
5.7 分形维度 168
5.8 彩色分割 171
5.9 彩色纹理 174
5.10 网站文件 174
5.11 参考文献 175
第6章 图像细化 179
6.1 骨架概述 179
6.2 中轴变换 180
6.3 迭代式形态学方法 181
6.4 等高线的使用 188
6.5 把对象看做多边形 192
6.6 基于力的图像细化 194
6.6.1 定义 195
6.6.2 力场的使用 195
6.6.3 子像素骨架 198
6.7 Zhang-Suen/Stentiford/Holt组合算法的源代码 200
6.8 网站文件 210
6.9 参考文献 211
第7章 图像还原 215
7.1 图像降质――真实世界 215
7.2 频域 217
7.2.1 傅里叶变换 217
7.2.2 快速傅里叶变换 219
7.2.3 逆傅里叶变换 222
7.2.4 二维傅里叶变换 223
7.2.5 OpenCV中的傅里叶变换 224
7.2.6 创建人工模糊 226
7.3 逆滤波器 231
7.4 Wiener滤波器 232
7.5 结构化噪声 233
7.6 运动模糊――一种特殊情况 236
7.7 同态滤波器――过滤照度 237
7.7.1 通用频率过滤器 238
7.7.2 分离光照产生的效果 240
7.8 网站文件 241
7.9 参考文献 242
第8章 分类 245
8.1 对象、模式和统计数据 245
8.1.1 特征和区域 247
8.1.2 训练和测试 251
8.1.3 类别内和类别外的差异 253
8.2 最小距离分类器 256
8.2.1 距离度量 257
8.2.2 特征之间的距离 259
8.3 交叉验证 260
8.4 支持向量机 262
8.5 多重分类器――整合分类器 264
8.5.1 合并多种方法 264
8.5.2 整合类型1的响应 265
8.5.3 评估 266
8.5.4 响应类型之间的转换 267
8.5.5 整合类型2的响应 267
8.5.6 整合类型3的响应 269
8.6 bagging和boosting 269
8.6.1 bagging 269
8.6.2 boosting 269
8.7 网站文件 271
8.8 参考文献 271
第9章 符号识别 273
9.1 问题描述 273
9.2 对简单的完美图像进行
OCR 274
9.3 在扫描的图像上进行OCR――图像分割 277
9.3.1 噪声 277
9.3.2 分离独立的字形 279
9.3.3 匹配模板 282
9.3.4 统计识别 284
9.4 传真图像的OCR――针对印刷字符 287
9.4.1 朝向――倾斜检测 287
9.4.2 使用边缘 291
9.5 手写字符 294
9.5.1 字符轮廓的属性 295
9.5.2 凸缺 297
9.5.3 向量模板 301
9.5.4 神经网络 305
9.6 使用多重分类器 312
9.6.1 合并多种方法 312
9.6.2 多重分类器的结果 314
9.7 印刷乐谱识别――案例研究 315
9.7.1 五线谱线 315
9.7.2 分割 317
9.7.3 音乐符号识别 319
9.8 神经网络识别系统的源代码 320
9.9 网站文件 327
9.10 参考文献 328
第10章 基于内容的搜索――通过示例搜索图像 333
10.1 搜索图像 333
10.2 维护图像集合 334
10.3 通过示例搜索的特征 336
10.3.1 彩色图像的特征 336
10.3.2 灰阶图像特征 343
10.4 考虑空间因素 345
10.4.1 整体区域 346
10.4.2 矩形区域 346
10.4.3 角度区域 346
10.4.4 环状区域 347
10.4.5 混合区域 348
10.4.6 空间采样的测试 348
10.5 其他要考虑的因素 350
10.5.1 纹理 351
10.5.2 对象、等高线和边缘 351
10.5.3 数据集 351
10.6 网站文件 352
10.7 参考文献 353
第11章 将高性能计算用于视觉处理和图像处理 357
11.1 多处理器计算的范式 358
11.1.1 共享内存 358
11.1.2 消息传递 359
11.2 执行时间 359
11.2.1 使用clock()函数 359
11.2.2 使用QueryPerformance-Counter函数 361
11.3 消息传递接口系统 363
11.3.1 安装MPI 363
11.3.2 使用MPI 364
11.3.3 进程间通信 364
11.3.4 运行MPI程序 366
11.3.5 真实的图像计算 367
11.3.6 使用计算机网络――集群计算 370
11.4 共享内存系统――使用PC的图形处理器 372
11.4.1 GLSL 373
11.4.2 OpenGL基础 373
11.4.3 OpenGL中的纹理实践 375
11.4.4 着色器编程基础 378
11.4.5 读入并转换图像 381
11.4.6 向着色程序传递参数 382
11.4.7 整合以上内容 384
11.4.8 通过GPU加速 385
11.4.9 开发和测试着色器代码 385
11.5 寻找所需的软件 386
11.6 网站文件 387
11.7 参考文献 387
・ ・ ・ ・ ・ ・ (收起)第1章 深度学习基础 1
1.1 神经网络 1
1.1.1 感知机 1
1.1.2 神经网络原理 2
1.2 卷积神经网络 3
1.2.1 CNN基本操作 3
1.2.2 CNN原理 6
1.3 循环神经网络 7
1.3.1 RNN 7
1.3.2 LSTM与GRU 8
1.4 经典网络 9
1.4.1 AlexNet 9
1.4.2 VGG 10
1.4.3 GoogLeNet 11
1.4.4 ResNet 12
1.4.5 MobileNet 13
1.5 进阶必备：如何学习深度学习并“落地”求职 16
1.5.1 深度学习如何快速入门 16
1.5.2 深度学习行业求职技巧 17
第2章 计算机视觉基础 18
2.1 目标检测Two-stage算法 18
2.1.1 R-CNN算法 18
2.1.2 Fast R-CNN算法 20
2.1.3 Faster R-CNN算法 21
2.2 目标检测One-stage算法 23
2.2.1 YOLO系列算法 23
2.2.2 SSD算法 29
2.3 图像分割算法 31
2.3.1 FCN算法 31
2.3.2 U-Net算法 33
2.3.3 DeepLab系列算法 34
2.3.4 Mask R-CNN算法 37
2.4 进阶必备：计算机视觉方向知多少 38
第3章 基础图像处理 40
3.1 线性滤波 40
3.1.1 案例1：使用方框滤波 41
3.1.2 案例2：使用均值滤波 46
3.1.3 案例3：使用高斯滤波 48
3.2 非线性滤波 50
3.2.1 案例4：使用中值滤波例 50
3.2.2 案例5：使用双边滤波 52
3.3　OpenCV形态学运算 54
3.3.1 案例6：进行膨胀操作 55
3.3.2 案例7：进行腐蚀操作 57
3.3.3 案例8：使用形态学运算 58
3.4　案例9：使用漫水填充 63
3.5　图像金字塔 67
3.5.1 案例10：使用高斯金字塔 67
3.5.2 案例11：使用拉普拉斯金字塔 70
3.6　阈值化 73
3.6.1 案例12：使用基本阈值 74
3.6.2 案例13：使用自适应阈值 78
3.7　进阶必备：选择一款合适的图像处理工具 80
3.7.1 OpenCV 80
3.7.2 Matlab 81
第4章 图像变换 83
4.1　边缘检测 83
4.1.1 案例14：Sobel算法 83
4.1.2 案例15：Scharr算法 87
4.1.3 案例16：Laplacian算法 90
4.1.4 案例17：Canny算法 91
4.2　案例18：绘制轮廓 94
4.3　霍夫变换 97
4.3.1 案例19：霍夫线变换 97
4.3.2 案例20：霍夫圆变换 101
4.4　案例21：重映射 103
4.5　案例22：仿射变换 106
4.6　案例23：透视变换 109
4.7　直方图 111
4.7.1 案例24：直方图的计算与绘制 111
4.7.2 案例25：直方图均衡化 113
4.8　进阶必备：图像变换应用之文本图像矫正 114
4.8.1 图像变换知识总结 114
4.8.2 案例26：文本图像矫正 115
第5章 角点检测 117
5.1 案例27：Harris角点检测 117
5.2　案例28：Shi-Tomasi角点检测 119
5.3　案例29：亚像素级角点检测 122
5.4　进阶必备：角点检测知识总结 125
第6章 特征点检测与匹配 127
6.1　特征点检测 127
6.1.1 opencv-contrib环境安装 127
6.1.2 案例30：SIFT特征点检测 130
6.1.3 案例31：SURF特征点检测 137
6.2　特征匹配 139
6.2.1 案例32：BruteForce匹配 139
6.2.2 案例33：FLANN匹配 146
6.3　案例34：ORB特征提取 148
6.4　进阶必备：利用特征点拼接图像 151
6.4.1 特征点检测算法汇总 151
6.4.2 案例35：基于特征点检测与匹配的图像拼接 151
第7章 手写数字识别 155
7.1　Keras的应用 155
7.1.1 Keras模型 155
7.1.2 Keras层 156
7.1.3 模型编译 157
7.1.4 模型训练 158
7.2　LeNet算法 159
7.3　案例36：使用Keras实现手写数字识别 160
7.3.1 模型训练 160
7.3.2 手写数字识别模型推理 164
7.4　进阶必备：算法模型开发流程 167
7.4.1 数据准备 167
7.4.2 网络搭建 169
7.4.3 模型训练 170
第8章 CIFAR-10图像分类 171
8.1　图像分类数据集 171
8.1.1 CIFAR-10数据集和CIFAR-100数据集 171
8.1.2 ImageNet数据集 172
8.1.3 PASCAL VOC数据集 173
8.2　案例37：CIFAR-10图像分类 173
8.2.1 模型训练过程 174
8.2.2 模型推理 179
8.3　进阶必备：COCO数据集与使用HOGTSVM方法实现图像分类 180
8.3.1 COCO数据集 180
8.3.2 案例38：使用HOG+SVM方法实现图像分类 180
第9章 验证码识别 184
9.1　TensorFlow应用 184
9.1.1 案例39：TensorFlow的基本使用 184
9.1.2 TensorFlow的常用模块 186
9.2　案例40：验证码识别 188
9.2.1 生成验证码图片 188
9.2.2 基于TensorFlow的验证码识别 189
9.3　进阶必备：算法模型开发技巧 194
9.3.1 数据预处理技巧 194
9.3.2 网络搭建技巧 195
9.3.3 模型训练技巧 196
第10章 文本检测实战 197
10.1　文本检测算法 197
10.1.1 CTPN算法 198
10.1.2 EAST算法 200
10.2　案例41：基于EAST算法的文本检测 202
10.2.1 数据预处理 202
10.2.2 网络搭建 205
10.2.3 模型训练 212
10.2.4 文本检测验证 217
10.3　进阶必备：在不同场景下文本检测的应对方式 218
10.3.1 复杂场景文本检测 219
10.3.2 案例42：使用形态学运算实现简单场景文本检测 220
10.3.3 案例43：使用MSER+NMS实现简单场景文本检测 223
第11章 文本识别实战 226
11.1　文本识别算法 226
11.1.1 CRNN算法 226
11.1.2 Attention OCR算法 229
11.2　案例44：基于C-RNN算法的文本识别 231
11.2.1 数据预处理 231
11.2.2 网络搭建 232
11.2.3 模型训练 236
11.2.4 文本识别验证 237
11.3　进阶必备：单字OCR 238
11.3.1 OCR探究 238
11.3.2 案例45：文本图片字符切割 238
第12章 TensorFlow Lite 244
12.1　TensorFlow Lite介绍 244
12.1.1 TensorFlow Lite基础 245
12.1.2 TensorFlow Lite源码分析 246
12.2　模型转换 248
12.2.1 FlatBuffers文件格式 248
12.2.2 案例46：其他格式转换为.tflite模型 250
12.3　模型量化 252
12.3.1 案例47：量化感知训练 252
12.3.2 案例48：训练后量化 255
12.4　进阶必备：模型转换与模型部署优化答疑 257
12.4.1 模型转换问题 257
12.4.2 模型部署优化 258
第13章 基于TensorFlow Lite的AI功能部署实战 260
13.1　部署流程 260
13.2　案例49：移动端部署 261
13.2.1 搭建开发环境 262
13.2.2 编译运行项目 262
13.2.3 调用过程解析 264
13.3　PC端部署 266
13.3.1 案例50：Windows端部署 266
13.3.2 案例51：Linux端部署 278
13.3.3 案例52：ARM平台部署 282
13.3.4 案例53：MIPS平台部署 285
13.4　进阶必备：推理框架拓展与OpenCV编译部署 286
13.4.1 其他深度学习推理框架 286
13.4.2 OpenCV编译 286
・ ・ ・ ・ ・ ・ (收起)第 1章 图像的获取和表示 1
1.1 图像传感器技术 1
1.1.1 传感器材料 2
1.1.2 传感器光电二极管元件 3
1.1.3 传感器配置：马赛克、Faveon和BSI 3
1.1.4 动态范围和噪声 5
1.1.5 传感器处理 5
1.1.6 去马赛克 6
1.1.7 坏像素的校正 6
1.1.8 颜色和照明校正 6
1.1.9 几何校正 7
1.2 摄像机和计算成像 7
1.2.1 计算成像概述 7
1.2.2 单像素的摄像头计算 8
1.2.3 二维可计算摄像机 9
1.2.4 三维深度的摄像机系统 10
1.3 三维深度处理 21
1.3.1 方法概述 21
1.3.2 深度感知和处理中存在的问题 22
1.3.3 单目深度处理 27
1.4 三维表示：体元、深度图、网格和点云 31
1.5 总结 32
第 2章 图像预处理 33
2.1 图像处理概述 33
2.2 图像预处理要解决的问题 34
2.2.1 计算机视觉的流程和图像预处理 34
2.2.2 图像校正 36
2.2.3 图像增强 36
2.2.4 为特征提取准备图像 37
2.3 图像处理方法分类 41
2.3.1 点运算 42
2.3.2 直线运算 42
2.3.3 区域运算 42
2.3.4 算法 42
2.3.5 数据转换 43
2.4 色度学 43
2.4.1 色彩管理系统概述 44
2.4.2 光源、白点、黑点和中性轴 44
2.4.3 设备色彩模型 45
2.4.4 颜色空间与色彩感知 45
2.4.5 色域映射与渲染目的 46
2.4.6 色彩增强的实际考虑 47
2.4.7 色彩的准确度与精度 48
2.5 空间滤波 48
2.5.1 卷积滤波与检测 48
2.5.2 核滤波与形状选择 50
2.5.3 点滤波 51
2.5.4 噪声与伪像滤波 52
2.5.5 积分图与盒式滤波器 53
2.6 边缘检测器 54
2.6.1 核集合: Sobel, Scharr, Prewitt, Roberts, Kirsch, Robinson和Frei-Chen 54
2.6.2 Canny检测器 55
2.7 变换滤波、Fourier变换及其他 56
2.7.1 Fourier变换 56
2.7.2 其他变换 58
2.8 形态学与分割 59
2.8.1 二值形态学 59
2.8.2 灰度和彩色形态学 61
2.8.3 形态学优化和改进 61
2.8.4 欧氏距离映射 61
2.8.5 超像素分割 62
2.8.6 深度图分割 63
2.8.7 色彩分割 64
2.9 阈值化 64
2.9.1 全局阈值化 65
2.9.2 局部阈值化 68
2.10 总结 69
第3章 全局特征和区域特征 70
3.1 视觉特征的历史概述 70
3.1.1 核心思想：全局、区域和局部 71
3.1.2 纹理分析 73
3.1.3 统计方法 76
3.2 纹理区域度量 77
3.2.1 边缘度量 77
3.2.2 互相关和自相关 79
3.2.3 Fourier频谱、小波和基签名 79
3.2.4 共生矩阵和Haralick特征 80
3.2.5 Laws纹理度量 89
3.2.6 LBP局部二值模式 90
3.2.7 动态纹理 91
3.3 统计区域度量 91
3.3.1 图像矩特征 92
3.3.2 点度量特征 92
3.3.3 全局直方图 94
3.3.4 局部区域直方图 94
3.3.5 散点图和3D直方图 95
3.3.6 多分辨率和多尺度直方图 97
3.3.7 径向直方图 98
3.3.8 轮廓或边缘直方图 99
3.4 基空间度量 99
3.4.1 Fourier描述 101
3.4.2 Walsh-Hadamard变换 102
3.4.3 HAAR变换 103
3.4.4 斜变换 103
3.4.5 Zernike多项式 103
3.4.6 导向滤波器 104
3.4.7 Karhunen-Loeve变换与Hotelling变换 104
3.4.8 小波变换和Gabor滤波器 105
3.4.9 Hough变换与Radon变换 106
3.5 总结 108
第4章 局部特征设计、分类和学习 109
4.1 局部特征 109
4.1.1 检测器、兴趣点、关键点、锚点、标注 110
4.1.2 描述子、特征描述、特征提取 110
4.1.3 稀疏局部模式方法 111
4.2 局部特征属性 111
4.2.1 选择特征描述子和兴趣点 111
4.2.2 特征描述子和特征匹配 112
4.2.3 好特征的标准 112
4.2.4 可重复性，相对于困难的查找算容易 113
4.2.5 判别性与非判别性 114
4.2.6 相对和绝 对位置 114
4.2.7 匹配代价和一致性 114
4.3 距离函数 115
4.3.1 关于距离函数的早期研究成果 115
4.3.2 欧氏或笛卡儿距离度量 116
4.3.3 网格距离度量 118
4.3.4 基于统计学的差异性度量 119
4.3.5 二值或布尔距离度量 120
4.4 描述子的表示 121
4.4.1 坐标空间和复数空间 121
4.4.2 笛卡儿坐标 121
4.4.3 极坐标和对数极坐标 121
4.4.4 径向坐标 122
4.4.5 球面坐标 122
4.4.6 Gauge坐标 122
4.4.7 多元空间和多模数据 122
4.4.8 特征金字塔 123
4.5 描述子的密度 123
4.5.1 丢弃兴趣点和描述子 124
4.5.2 稠密与稀疏特征描述 124
4.6 描述子形状拓扑 125
4.6.1 关联性模板 125
4.6.2 块和形状 125
4.6.3 对象多边形 127
4.7 局部二值描述与点对模式 128
4.7.1 FREAK视网膜模式 129
4.7.2 Brisk 模式 130
4.7.3 ORB和BRIEF模式 131
4.8 描述子判别性 131
4.8.1 谱的判别性 132
4.8.2 区域、形状和模式的判别性 133
4.8.3 几何判别因素 133
4.8.4 通过特征可视化来评价判别性 134
4.8.5 精度与可跟踪 136
4.8.6 精度优化、子区域重叠、Gaussian权重和池化 138
4.8.7 亚像素精度 138
4.9 搜索策略与优化 139
4.9.1 密集搜索 139
4.9.2 网格搜索 139
4.9.3 多尺度金字塔搜索 140
4.9.4 尺度空间和图像金字塔 140
4.9.5 特征金字塔 142
4.9.6 稀疏预测搜索与跟踪 142
4.9.7 跟踪区域限制搜寻 143
4.9.8 分割限制搜索 143
4.9.9 深度或Z限制搜索 143
4.10 计算机视觉、模型和结构 144
4.10.1 特征空间 144
4.10.2 对象模型 145
4.10.3 约束 146
4.10.4 选择检测器和特征 146
4.10.5 训练概述 147
4.10.6 特征和对象的分类 148
4.10.7 特征学习、稀疏编码和卷积网络 154
4.11 总结 158
第5章 特征描述属性的分类学 159
5.1 特征描述子系列 160
5.2 计算机视觉分类学方面的早期研究成果 161
5.3 鲁棒性和精度 161
5.4 通用的鲁棒性分类学 162
5.4.1 光照 163
5.4.2 颜色准则 163
5.4.3 不完全性 164
5.4.4 分辨率和精度 164
5.4.5 几何失真 165
5.4.6 效率变量、费用和效益 165
5.4.7 判别性和唯 一性 165
5.5 通用的视觉度量分类学 166
5.5.1 特征描述子族 168
5.5.2 频谱维度 168
5.5.3 频谱类型 168
5.5.4 兴趣点 171
5.5.5 存储格式 171
5.5.6 数据类型 172
5.5.7 描述子内存 172
5.5.8 特征形状 173
5.5.9 特征模式 173
5.5.10 特征密度 174
5.5.11 特征搜索方法 174
5.5.12 模式对采样 175
5.5.13 模式区域大小 176
5.5.14 距离函数 176
5.6 特征度量评估 177
5.6.1 效率变量、成本和效益 177
5.6.2 图像重建的效率度量 178
5.6.3 特征度量评估举例 178
5.7 总结 180
第6章 兴趣点检测与特征描述子研究 181
6.1 兴趣点调整 181
6.2 兴趣点概念 182
6.3 兴趣点方法概述 184
6.3.1 Laplacian 和Gaussian -Laplacian 185
6.3.2 Moravac角点检测器 185
6.3.3 Harris方法、Harris-Stephens、Shi-Tomasi以及Hessian类型的检测器 186
6.3.4 Hessian矩阵检测器和Hessian-Laplace 186
6.3.5 Gaussian差 187
6.3.6 显著性区域 187
6.3.7 SUSAN、Trajkovic 以及 Hedly 187
6.3.8 Fast、Faster以及 AGHAST 188
6.3.9 局部曲率方法 189
6.3.10 形态兴趣区域 189
6.4 特征描述子介绍 190
6.4.1 局部二值描述子 190
6.4.2 Census 197
6.4.3 BRIEF 198
6.4.4 ORB 199
6.4.5 BRISK 200
6.4.6 FREAK 201
6.5 谱描述子 202
6.5.1 SIFT 202
6.5.2 SIFT-PCA 206
6.5.3 SIFT-GLOH 207
6.5.4 改进的SIF-SIFER 207
6.5.5 SIFT CS-LBP改造 208
6.5.6 RootSIFT改造 208
6.5.7 CenSurE和STAR 209
6.5.8 相关模板 210
6.5.9 HAAR特征 212
6.5.10 使用类HAAR特征的Viola Jones算法 213
6.5.11 SURF 214
6.5.12 其他SURF算法 215
6.5.13 梯度直方图及变种 216
6.5.14 PHOG和相关方法 217
6.5.15 Daisy和O-Daisy 218
6.5.16 CARD 219
6.5.17 具有鲁棒性的快速特征匹配 221
6.5.18 RIFF和CHOG 222
6.5.19 链码直方图 223
6.5.20 D-NETS 224
6.5.21 局部梯度模式 225
6.5.22 局部相位量化 225
6.6 基空间描述子 226
6.6.1 傅里叶描述子 227
6.6.2 用其他基函数来构建描述子 228
6.6.3 稀疏编码方法 228
6.7 多边形形状描述 229
6.7.1 MSER方法 229
6.7.2 针对斑点和多边形的物体形状度量 230
6.7.3 形状上下文 233
6.8 3D、4D、体积以及多模态描述子 234
6.8.1 3D HOG 235
6.8.2 HON 4D 235
6.8.3 3D SIFT 236
6.9 总结 237
第7章 基准数据、内容、度量和分析 238
7.1 什么是基准数据？ 238
7.2 先前关于标注数据方面的研究：艺术与科学 240
7.2.1 质量性能的一般度量 240
7.2.2 算法性能的衡量 241
7.2.3 Rosin关于角点方面的研究工作 242
7.3 构造基准数据的关键问题 243
7.3.1 内容：采用、修改或创建 243
7.3.2 可用的基准数据介绍 243
7.3.3 使用数据拟合算法 244
7.3.4 场景构成和标记 245
7.4 定义目标和预期 247
7.4.1 Mikolajczyk和Schmid的方法学 247
7.4.2 开放式评价系统 248
7.4.3 极 端情况和限制 248
7.4.4 兴趣点和特征 248
7.5 基准数据的鲁棒性准则 249
7.5.1 举例说明鲁棒性准则 249
7.5.2 将鲁棒性准则用于实际应用 250
7.6 度量与基准数据的配对 252
7.6.1 兴趣点、特征和基准数据的配对和优化 252
7.6.2 一般的视觉分类学的例子 253
7.7 合成的特征字母表 254
7.7.1 合成数据集的目标 254
7.7.2 合成兴趣点字母表 256
7.7.3 将合成字母表叠加到真实图像上 258
7.8 总结 260
第8章 可视流程及优化 261
8.1 阶段、操作和资源 261
8.2 计算资源预算 263
8.2.1 计算单元、ALU和加速器 265
8.2.2 能耗的使用 266
8.2.3 内存的利用 266
8.2.4 I/O性能 269
8.3 计算机视觉流程的实例 270
8.3.1 汽车识别 270
8.3.2 人脸检测、情感识别以及年龄识别 277
8.3.3 图像分类 285
8.3.4 增强现实 289
8.4 可选的加速方案 294
8.4.1 内存优化 294
8.4.2 粗粒度并行 296
8.4.3 细粒度数据并行 297
8.4.4 高 级指令集和加速器 300
8.5 计算机视觉算法的优化与调整 301
8.5.1 编译器优化与手工优化 301
8.5.2 特征描述子改造、检测器和距离函数 302
8.5.3 Boxlets与卷积加速 303
8.5.4 数据类型优化，整型与浮点型 303
8.6 优化资源 304
8.7 总结 304
附录A 合成特征分析 306
A.1 目标的背景与期望 307
A.2 测试方法和结果 309
A.3 合成字母基准图像概述 311
A.4 测试1：合成兴趣点字母检测 313
A.5 测试2：合成角点字母检测 323
A.6 测试3：叠加到真实图像上的合成字母检测 333
A.7 测试4：字母的旋转不变性 333
A.8 结果分析和不可重复性异常 336
附录B 基准数据集概述 339
附录C 成像和计算机视觉资源 347
C.1 商业产品 347
C.2 开放源码 348
C.3 组织、机构和标准 350
C.4 在线资源 351
附录D 扩展SDM准则 353
译后记 370
参考文献 372
・ ・ ・ ・ ・ ・ (收起)第 1章 机器学习与sklearn 1
1．1 sklearn 环境配置 2
1．1．1 环境要求 2
1．1．2 安装方法 2
1．1．3 修改pip 源 3
1．1．4 安装Jupyter Notebook 4
1．2 数据集 5
1．2．1 自带的小型数据集 6
1．2．2 在线下载的数据集 8
1．2．3 计算机生成的数据集 8
1．3 分类 9
1．3．1 加载数据与模型 10
1．3．2 建立分类模型 11
1．3．3 模型的训练及预测 12
1．3．4 模型评价 12
1．4 回归 14
1．4．1 线性回归 15
1．4．2 回归模型评价 16
1．5 聚类 17
1．5．1 K-means 17
1．5．2 DBSCAN 17
1．5．3 聚类实例 18
1．6 降维 19
1．6．1 PCA 降维 19
1．6．2 LDA 降维 22
1．7 模型验证 23
1．8 模型持久化 27
1．8．1 joblib 27
1．8．2 pickle 28
1．9 小结 28
第 2章 传统图像处理方法 29
2．1 图像分类 29
2．1．1 HOG 的原理 29
2．1．2 工具介绍 30
2．1．3 CIFAR-10 分类 31
2．1．4 手写字符分类 33
2．2 目标检测 36
2．3 图像分割 40
2．4 图像搜索 41
2．5 小结 43
第3章 深度学习与PyTorch 44
3．1 框架介绍 44
3．2 环境配置 46
3．3 运算基本单元 48
3．3．1 Tensor 数据类型 48
3．3．2 Tensor 与ndarray 49
3．3．3 CPU 与GPU 运算 49
3．3．4 PyTorch 实现K-means 51
3．4 自动求导 55
3．5 数据加载 57
3．5．1 Dataset 58
3．5．2 DataLoader 59
3．6 神经网络工具包 60
3．6．1 Module 模块 61
3．6．2 线性层 62
3．6．3 卷积层 62
3．6．4 池化层 64
3．6．5 BatchNorm 层 65
3．6．6 激活层 65
3．6．7 神经网络各层输出的可视化 72
3．6．8 循环神经网络 76
3．6．9 Sequential 和ModuleList 78
3．6．10 损失函数 79
3．7 模型优化器optim 82
3．7．1 optim 用法 82
3．7．2 优化器的选择 82
3．7．3 学习率的选择 86
3．8 参数初始化init 94
3．9 模型持久化 96
3．10 JIT 编译器 98
3．11 模型迁移ONNX 99
3．12 数据可视化TensorBoard 101
3．13 机器视觉工具包torchvision 103
3．13．1 数据 103
3．13．2 模型 104
3．13．3 图像处理 106
3．14 小结 110
第4章 卷积神经网络中的分类与回归 111
4．1 卷积神经网络中的分类问题 112
4．1．1 CIFAR-10 图像分类 112
4．1．2 卷积神经网络的发展 117
4．1．3 分类网络的实现 121
4．1．4 模型训练 127
4．1．5 模型展示 132
4．1．6 多标签分类 134
4．2 卷积神经网络中的回归问题 142
4．2．1 生成数据集 142
4．2．2 模型训练 145
4．2．3 模型展示 146
4．3 小结 148
第5章 目标检测 149
5．1 深度学习物体检测算法 149
5．1．1 两段式检测 150
5．1．2 一段式检测 153
5．2 数据集构建 155
5．2．1 选择目标物体图片 155
5．2．2 背景图片下载 156
5．2．3 图片合成 156
5．3 数据加载 162
5．4 数据标记与损失函数构建 166
5．4．1 数据标记 167
5．4．2 损失函数 167
5．5 模型搭建与训练 172
5．6 模型预测 175
5．7 小结 180
第6章 图像分割 181
6．1 数据加载 184
6．2 模型搭建 189
6．3 模型训练 191
6．4 模型展示 194
6．5 智能弹幕 195
6．6 像素级回归问题：超分辨率重建 196
6．6．1 超分辨率重建算法的发展 197
6．6．2 数据加载 198
6．6．3 模型搭建与训练 202
6．6．4 模型展示 205
6．7 小结 206
第7章 图像搜索 207
7．1 分类网络的特征 208
7．2 深度学习人脸识别技术 208
7．2．1 FaceNet 209
7．2．2 CosFace 和ArcFace 209
7．3 数据处理 210
7．3．1 数据下载 210
7．3．2 数据检查 212
7．3．3 数据提取 213
7．4 模型训练 214
7．4．1 普通分类模型 214
7．4．2 CosFace 218
7．5 图像搜索 219
7．5．1 图像比对 219
7．5．2 KD-Tree 搜索 221
7．6 小结 224
第8章 图像压缩 225
8．1 AutoEncoder 226
8．1．1 AutoEncoder 的原理 226
8．1．2 AutoEncoder 模型搭建 226
8．1．3 数据加载 229
8．1．4 模型训练 230
8．1．5 结果展示 232
8．2 GAN 234
8．2．1 GAN 原理 234
8．2．2 GAN 训练流程 235
8．2．3 GAN 随机生成人脸图片 235
8．2．4 GAN 与AutoEncoder 的结合 242
8．2．5 图像修复 247
8．3 小结 250
第9章 不定长文本识别 251
9．1 循环神经网络概述 251
9．2 时间序列预测 252
9．2．1 创建模型 253
9．2．2 生成数据 253
9．2．3 模型训练 255
9．2．4 模型预测 256
9．3 CRNN 模型 257
9．3．1 CRNN 算法简介 257
9．3．2 CTCLoss 函数 258
9．3．3 模型结构 259
9．3．4 数据预处理 261
9．3．5 模型训练 264
9．3．6 模型预测 266
9．4 小结 267
第 10章 神经网络压缩与部署 268
10．1 剪枝 268
10．1．1 模型设计 269
10．1．2 训练基础模型 271
10．1．3 模型稀疏化 273
10．1．4 压缩模型通道 276
10．2 量化 283
10．3 混合精度训练 287
10．4 深度学习模型的服务端部署 289
10．4．1 创建接口 289
10．4．2 访问接口 291
10．5 小结 292
・ ・ ・ ・ ・ ・ (收起)前言
作者简介
审校者简介
第1章　OpenCV入门1
1.1　了解人类视觉系统1
1.2　人类如何理解图像内容3
1.3　你能用OpenCV做什么4
1.3.1　内置数据结构和输入/输出4
1.3.2　图像处理操作5
1.3.3　GUI5
1.3.4　视频分析6
1.3.5　3D重建6
1.3.6　特征提取7
1.3.7　对象检测7
1.3.8　机器学习8
1.3.9　计算摄影8
1.3.10　形状分析9
1.3.11　光流算法9
1.3.12　人脸和对象识别9
1.3.13　表面匹配10
1.3.14　文本检测和识别10
1.3.15　深度学习10
1.4　安装OpenCV10
1.4.1　Windows11
1.4.2　Mac OS X11
1.4.3　Linux13
1.5　总结14
第2章　OpenCV基础知识导论15
2.1　技术要求15
2.2　基本CMake配置文件16
2.3　创建一个库16
2.4　管理依赖项17
2.5　让脚本更复杂18
2.6　图像和矩阵20
2.7　读/写图像22
2.8　读取视频和摄像头25
2.9　其他基本对象类型27
2.9.1　Vec对象类型27
2.9.2　Scalar对象类型28
2.9.3　Point对象类型28
2.9.4　Size对象类型29
2.9.5　Rect对象类型29
2.9.6　RotatedRect对象类型29
2.10　基本矩阵运算30
2.11　基本数据存储32
2.12　总结34
第3章　学习图形用户界面35
3.1　技术要求35
3.2　OpenCV用户界面介绍36
3.3　OpenCV的基本图形用户界面36
3.4　Qt图形用户界面44
3.5　OpenGL支持50
3.6　总结54
第4章　深入研究直方图和滤波器55
4.1　技术要求56
4.2　生成CMake脚本文件56
4.3　创建图形用户界面57
4.4　绘制直方图59
4.5　图像颜色均衡62
4.6　Lomography效果64
4.7　卡通效果68
4.8　总结72
第5章　自动光学检查、对象分割和检测73
5.1　技术要求73
5.2　隔离场景中的对象74
5.3　为AOI创建应用程序76
5.4　预处理输入图像78
5.4.1　噪声消除78
5.4.2　用光模式移除背景进行分割79
5.4.3　阈值84
5.5　分割输入图像85
5.5.1　连通组件算法85
5.5.2　findContours算法90
5.6　总结92
第6章　学习对象分类94
6.1　技术要求94
6.2　机器学习概念介绍95
6.3　计算机视觉和机器学习工作流程98
6.4　自动对象检查分类示例100
6.4.1　特征提取102
6.4.2　训练SVM模型105
6.4.3　输入图像预测109
6.5　总结111
第7章　检测面部部位与覆盖面具112
7.1　技术要求112
7.2　了解Haar级联112
7.3　什么是积分图像114
7.4　在实时视频中覆盖面具115
7.5　戴上太阳镜118
7.6　跟踪鼻子、嘴巴和耳朵121
7.7　总结122
第8章　视频监控、背景建模和形态学操作123
8.1　技术要求123
8.2　理解背景减除124
8.3　直接的背景减除124
8.4　帧差分128
8.5　高斯混合方法131
8.6　形态学图像处理133
8.7　使形状变细134
8.8　使形状变粗135
8.9　其他形态运算符136
8.9.1　形态开口136
8.9.2　形态闭合137
8.9.3　绘制边界138
8.9.4　礼帽变换139
8.9.5　黑帽变换140
8.10　总结140
第9章　学习对象跟踪141
9.1　技术要求141
9.2　跟踪特定颜色的对象141
9.3　构建交互式对象跟踪器143
9.4　用Harris角点检测器检测点148
9.5　用于跟踪的好特征151
9.6　基于特征的跟踪153
9.6.1　Lucas-Kanade方法153
9.6.2　Farneback算法157
9.7　总结161
第10章　开发用于文本识别的分割算法162
10.1　技术要求162
10.2　光学字符识别介绍162
10.3　预处理阶段164
10.3.1　对图像进行阈值处理164
10.3.2　文本分割165
10.4　在你的操作系统上安装Tesseract OCR172
10.4.1　在Windows上安装Tesseract172
10.4.2　在Mac上安装Tesseract173
10.5　使用Tesseract OCR库173
10.6　总结177
第11章　用Tesseract进行文本识别178
11.1　技术要求178
11.2　文本API的工作原理179
11.2.1　场景检测问题179
11.2.2　极值区域180
11.2.3　极值区域过滤181
11.3　使用文本API182
11.3.1　文本检测182
11.3.2　文本提取187
11.3.3　文本识别189
11.4　总结193
第12章　使用OpenCV进行深度学习194
12.1　技术要求194
12.2　深度学习简介195
12.2.1　什么是神经网络，我们如何从数据中学习195
12.2.2　卷积神经网络197
12.3　OpenCV中的深度学习198
12.4　YOLO用于实时对象检测199
12.4.1　YOLO v3深度学习模型架构200
12.4.2　YOLO数据集、词汇表和模型200
12.4.3　将YOLO导入OpenCV201
12.5　用SSD进行人脸检测204
12.5.1　SSD模型架构204
12.5.2　将SSD人脸检测导入OpenCV204
12.6　总结208
・ ・ ・ ・ ・ ・ (收起)译者序
前言
作者简介
审校者简介
译者简介
第1章　安装OpenCV 1
1.1　选择和使用合适的安装工具 2
1.1.1　在Windows上安装 2
1.1.2　在OS X系统中安装 6
1.1.3　在Ubuntu及其衍生版本中安装 11
1.1.4　在其他类Unix系统中安装 12
1.2　安装Contrib模块 13
1.3　运行示例 13
1.4　查找文档、帮助及更新 14
1.5　总结 15
第2章　处理文件、摄像头和图形用户界面 16
2.1　基本I/O脚本 16
2.1.1　读/写图像文件 16
2.1.2　图像与原始字节之间的转换 19
2.1.3　使用numpy.array访问图像数据 20
2.1.4　视频文件的读/写 22
2.1.5　捕获摄像头的帧 23
2.1.6　在窗口显示图像 24
2.1.7　在窗口显示摄像头帧 25
2.2　Cameo项目（人脸跟踪和图像处理） 26
2.3　Cameo―面向对象的设计 27
2.3.1　使用managers. CaptureManager提取视频流 27
2.3.2　使用managers.WindowManager抽象窗口和键盘 32
2.3.3　cameo.Cameo的强大实现 33
2.4　总结 34
第3章　使用OpenCV 3处理图像 36
3.1　不同色彩空间的转换 36
3.2　傅里叶变换 37
3.2.1　高通滤波器 37
3.2.2　低通滤波器 39
3.3　创建模块 39
3.4　边缘检测 40
3.5　用定制内核做卷积 41
3.6　修改应用 43
3.7　Canny边缘检测 44
3.8　轮廓检测 45
3.9　边界框、最小矩形区域和最小闭圆的轮廓 46
3.10　凸轮廓与Douglas-Peucker算法 48
3.11　直线和圆检测 50
3.11.1　直线检测 50
3.11.2　圆检测 51
3.12　检测其他形状 52
3.13　总结 52
第4章　深度估计与分割 53
4.1　创建模块 53
4.2　捕获深度摄像头的帧 54
4.3　从视差图得到掩模 56
4.4　对复制操作执行掩模 57
4.5　使用普通摄像头进行深度估计 59
4.6　使用分水岭和GrabCut算法进行物体分割 63
4.6.1　用GrabCut进行前景检测的例子 64
4.6.2　使用分水岭算法进行图像分割 66
4.7　总结 69
第5章　人脸检测和识别 70
5.1　Haar级联的概念 70
5.2　获取Haar级联数据 71
5.3　使用OpenCV进行人脸检测 72
5.3.1　静态图像中的人脸检测 72
5.3.2　视频中的人脸检测 74
5.3.3　人脸识别 76
5.4　总结 82
第6章　图像检索以及基于图像描述符的搜索 83
6.1　特征检测算法 83
6.1.1　特征定义 84
6.1.2　使用DoG和SIFT进行特征提取与描述 86
6.1.3　使用快速Hessian算法和SURF来提取和检测特征 89
6.1.4　基于ORB的特征检测和特征匹配 91
6.1.5　ORB特征匹配 93
6.1.6　K-最近邻匹配 95
6.1.7　FLANN匹配 96
6.1.8　FLANN的单应性匹配 99
6.1.9　基于文身取证的应用程序示例 102
6.2　总结 105
第7章　目标检测与识别 106
7.1　目标检测与识别技术 106
7.1.1　HOG描述符 107
7.1.2　检测人 112
7.1.3　创建和训练目标检测器 113
7.2　汽车检测 116
7.2.1　代码的功能 118
7.2.2　SVM和滑动窗口 122
7.3　总结 134
第8章　目标跟踪 135
8.1　检测移动的目标 135
8.2　背景分割器：KNN、MOG2和GMG 138
8.2.1　均值漂移和CAMShift 142
8.2.2　彩色直方图 144
8.2.3　返回代码 146
8.3　CAMShift 147
8.4　卡尔曼滤波器 149
8.4.1　预测和更新 149
8.4.2　范例 150
8.4.3　一个基于行人跟踪的例子 153
8.4.4　Pedestrian类 154
8.4.5　主程序 157
8.5　总结 159
第9章　基于OpenCV的神经网络简介 160
9.1　人工神经网络 160
9.2　人工神经网络的结构 161
9.2.1　网络层级示例 162
9.2.2　学习算法 163
9.3　OpenCV中的ANN 164
9.3.1　基于ANN的动物分类 166
9.3.2　训练周期 169
9.4　用人工神经网络进行手写数字识别 170
9.4.1　MNIST―手写数字数据库 170
9.4.2　定制训练数据 170
9.4.3　初始参数 171
9.4.4　迭代次数 171
9.4.5　其他参数 171
9.4.6　迷你库 172
9.4.7　主文件 175
9.5　可能的改进和潜在的应用 180
9.5.1　改进 180
9.5.2　应用 181
9.6　总结 181
・ ・ ・ ・ ・ ・ (收起)第 1 部分 基于 OpenCV 的传统视觉应用
第 1 章 图像生成 /2
1.1 图像显示 /3
1.1.1 使用 OpenCV 显示图像 /3
1.1.2 使用 Matplotlib 显示图像 /3
1.1.3 案例实现――使用OpenCV 显示图像 /3
1.1.4 案例实现――使用Matplotlib 显示图像 /5
1.2 图像读取 /6
1.2.1 使用 OpenCV 读取图像 /6
1.2.2 使用 Matplotlib 读取图像 /7
1.2.3 案例实现――使用OpenCV 读取图像 /7
1.2.4 案例实现――使用Matplotlib 读取图像 /9
1.3 图像保存 /10
1.3.1 使用 OpenCV 保存图像 /10
1.3.2 使用 Matplotlib 保存图像/11
1.3.3 案例实现――使用OpenCV 保存图像 /11
1.3.4 案例实现――使用Matplotlib 保存图像 /14
本章总结 /16
作业与练习 /16
第 2 章 OpenCV 图像处理（1） /17
2.1 图像模糊 /17
2.1.1 均值滤波 /17
2.1.2 中值滤波 /18
2.1.3 高斯滤波 /18
2.1.4 案例实现 /18
2.2 图像锐化 /21
2.2.1 图像锐化简介 /21
2.2.2 案例实现 /21
本章总结 /24
作业与练习 /24
第 3 章 OpenCV 图像处理（2） /26
3.1 OpenCV 绘图 /26
3.1.1 使用 OpenCV 绘制各种图形 /26
3.1.2 案例实现 /27
3.2 图像的几何变换 /31
3.2.1 几何变换操作 /31
3.2.2 案例实现 /32
本章总结 /38
作业与练习 /38
第 4 章 图像特征检测 /40
4.1 边缘编辑和增强 /41
4.1.1 Canny 边缘检测简介 /41
4.1.2 案例实现 /42
4.2 图像轮廓检测 /44
4.2.1 轮廓查找步骤 /45
4.2.2 查找轮廓函数 /45
4.2.3 绘制轮廓函数 /45
4.2.4 案例实现 /46
4.3 图像角点和线条检测 /48
4.3.1 角点的定义 /48
4.3.2 Harris 角点简介 /48
4.3.3 Harris 角点检测函数 /49
4.3.4 案例实现 /49
本章总结 /51
作业与练习 /52
第 5 章 图像特征匹配 /53
5.1 ORB 关键点检测与匹配 /53
5.1.1 FAST 算法 /54
5.1.2 BRIEF 算法 /55
5.1.3 特征匹配 /56
5.1.4 代码流程 /56
5.2 案例实现 /57
本章总结 /59
作业与练习 /59
第 6 章 图像对齐与拼接 /60
6.1 全景图像拼接 /60
6.1.1 全景图像的拼接原理 /61
6.1.2 算法步骤 /61
6.1.3 Ransac 算法介绍 /62
6.1.4 全景图像剪裁 /63
6.2 案例实现 /64
本章总结 /67
作业与练习 /67
第 7 章 相机运动估计 /68
7.1 双目相机运动估计 /68
7.1.1 相机测距流程 /68
7.1.2 双目相机成像模型 /69
7.1.3 极限约束 /70
7.1.4 双目测距的优势 /70
7.1.5 双目测距的难点 /70
7.2 案例实现 /72
本章总结 /82
作业与练习 /83
第 2 部分 基于机器学习和深度学习的视觉应用
第 8 章 基于 SVM 模型的手写数字识别/85
8.1 手写数字识别 /85
8.1.1 手写数字图像 /85
8.1.2 图像处理 /86
8.2 案例实现 /87
本章总结 /95
作业与练习 /95
第 9 章 基于 HOG+SVM 的行人检测 /96
9.1 行人检测 /96
9.1.1 HOG+SVM /96
9.1.2 检测流程 /97
9.1.3 滑动窗口 /98
9.1.4 非极大值抑制 /100
9.2 案例实现 /101
本章总结 /109
作业与练习 /109
第 10 章 数据标注 /110
10.1 目标检测数据标注 /110
10.1.1 数据收集与数据标注 /111
10.1.2 数据标注的通用规则 /112
10.1.3 案例实现 /113
10.2 视频目标跟踪数据标注 /118
10.2.1 视频与图像数据标注的差异 /118
10.2.2 案例实现 /119
本章总结 /127
作业与练习 /127
第 11 章 水果识别 /128
11.1 LeNet-5 模型的训练与评估 /128
11.1.1 卷积层 /129
11.1.2 池化层 /130
11.1.3 ReLU 层 /131
11.1.4 LeNet-5 模型 /131
11.1.5 Keras /132
11.1.6 案例实现 /133
11.2 LeNet-5 模型的应用 /139
11.2.1 使用 OpenCV 操作摄像头 /139
11.2.2 OpenCV 的绘图功能 /140
11.2.3 OpenCV 绘图函数的常见参数 /140
11.2.4 Keras 模型的保存和加载 /140
11.2.5 案例实现 /142
本章总结 /145
作业与练习 /145
第 12 章 病虫害识别 /147
12.1 植物叶子病虫害识别 /147
12.1.1 PlantVillage 数据集 /148
12.1.2 性能评估 /148
12.1.3 感受野 /149
12.2 案例实现 /149
本章总结 /161
作业与练习 /162
第 13 章 相似图像搜索 /163
13.1 以图搜图 /163
13.1.1 VGG 模型 /164
13.1.2 H5 模型文件 /165
13.1.3 案例实现 /166
13.2 人脸识别 /173
13.2.1 人脸检测 /173
13.2.2 分析面部特征 /174
13.2.3 人脸识别特征提取 /175
13.2.4 人脸相似性比较 /176
13.2.5 案例实现 /176
本章总结 /184
作业与练习 /185
第 14 章 多目标检测 /186
14.1 人脸口罩佩戴检测 /186
14.1.1 目标检测 /187
14.1.2 YOLO 模型 /188
14.1.3 YOLOv3 模型 /190
14.1.4 YOLOv3-Tiny 模型 /191
14.2 案例实现 /192
本章总结 /198
作业与练习 /198
第 15 章 可采摘作物检测 /199
15.1 番茄成熟度检测 /199
15.1.1 数据集 /200
15.1.2 RCNN 模型 /201
15.1.3 SPP-Net 模型 /202
15.1.4 Fast-RCNN 模型 /202
15.1.5 Faster-RCNN 模型 /202
15.1.6 Mask-RCNN 模型 /203
15.2 案例实现 /204
本章总结 /213
作业与练习 /213
第 16 章 智能照片编辑 /214
16.1 图像自动着色 /214
16.1.1 GAN 模型的基本结构与原理 /215
16.1.2 构建 GAN 模型 /216
16.2 案例实现 /218
本章总结 /225
作业与练习 /225
第 17 章 超分辨率 /227
17.1 图像超分辨率 /227
17.1.1 SRGAN 模型的结构 /228
17.1.2 SRGAN 模型的损失函数 /229
17.1.3 SRGAN 模型的评价指标 /230
17.2 案例实现 /230
本章总结 /237
作业与练习 /238
第 18 章 医学图像分割 /239
18.1 眼底血管图像分割 /239
18.1.1 图像分割 /240
18.1.2 语义分割 /241
18.1.3 全卷积神经网络 /243
18.1.4 反卷积 /244
18.1.5 U-Net 模型 /244
18.2 案例实现 /245
本章总结 /253
作业与练习 /253
第 19 章 医学图像配准 /255
19.1 头颈部 CT 图像配准 /255
19.1.1 图像配准方法 /256
19.1.2 VoxelMorph 配准框架 /257
19.1.3 TensorFlow-pix2pix /259
19.2 案例实现 /260
本章总结 /265
作业与练习 /265
第 20 章 视频内容分析 /267
20.1 人体动作识别 /267
20.1.1 视频动作识别模型 /268
20.1.2 UCF-101 数据集 /269
20.2 案例实现 /270
本章总结 /275
作业与练习 /275
第 21 章 图像语义理解 /277
21.1 视觉问答 /277
21.1.1 编码器-解码器模型 /279
21.1.2 光束搜索 /281
21.2 案例实现 /282
本章总结 /288
作业与练习 /288
第 3 部分 基于深度学习的新兴视觉应用
第 22 章 三维空间重建 /291
22.1 3D-R2N2 算法 /291
22.1.1 算法简介 /292
22.1.2 算法的优势 /292
22.1.3 算法的结构 /292
22.2 案例实现 /294
本章总结 /298
作业与练习 /298
第 23 章 视频稳定 /300
23.1 人脸视频稳定 /300
23.1.1 MobileNet 模型 /301
23.1.2 SSD 模型 /302
23.1.3 MobileNet-SSD 模型 /303
23.1.4 模型评估 /303
23.1.5 实时影响 /303
23.2 案例实现 /304
本章总结 /317
作业与练习 /317
第 24 章 目标检测与跟踪 /319
24.1 车辆检测与跟踪 /319
24.1.1 UA-DETRAC 数据集 /320
24.1.2 目标跟踪 /322
24.1.3 DeepSORT 目标跟踪 /323
24.2 案例实现 /324
本章总结 /337
作业与练习 /337
第 25 章 风格迁移 /339
25.1 图像与视频风格迁移 /339
25.1.1 理解图像内容和图像风格 /340
25.1.2 图像重建 /341
25.1.3 风格重建 /342
25.2 案例实现 /343
本章总结 /354
作业与练习 /355
附录 A 企业级综合教学项目介绍 /356
1.1 智慧停车场管理系统 /356
1.1.1 项目概述 /356
1.1.2 技能目标 /358
1.2 智慧景区管理系统 /358
1.2.1 项目概述 /358
1.2.2 技能目标 /359
1.3 智能考勤打卡系统 /360
1.3.1 项目概述 /360
1.3.2 技能目标 /361
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
作者简介
第一部分预备知识
第1章数据
1.1可视化
1.2离散化
1.2.1采样
1.2.2量化
1.3表示
1.4噪声
1.5本章小结
参考文献
习题
第2章技术
2.1插值
2.1.1线性插值
2.1.2双线性插值
2.2几何相交
2.3本章小结
参考文献
习题
第二部分基于图像的视觉计算
第3章卷积
3.1线性系统
3.1.1线性系统的响应
3.1.2卷积的性质
3.2线性滤波器
3.2.1全通、低通、带通和高通滤波器
3.2.2设计新滤波器
3.2.3二维滤波器的可分性
3.2.4相关和模式匹配
3.3实现细节
3.4本章小结
参考文献
习题
第4章谱分析
4.1离散傅里叶变换
4.2极坐标
4.2.1性质
4.2.2信号分析示例
4.3频域的周期性
4.4混叠
4.5推广到二维插值
4.5.1周期性的影响
4.5.2陷波器
4.5.3混叠效应示例
4.6对偶性
4.7本章小结
参考文献
习题
第5章特征检测
5.1边缘检测
5.1.1边缘子检测器
5.1.2多分辨率边缘检测
5.1.3边缘子聚合
5.2特征检测
5.3其他非线性滤波器
5.4本章小结
参考文献
习题
第三部分基于几何的视觉计算
第6章几何变换
6.1齐次坐标
6.2线性变换
6.3欧氏和仿射变换
6.3.1平移
6.3.2旋转
6.3.3缩放
6.3.4剪切
6.3.5一些现象
6.4变换的串联
6.4.1相对于中心点的缩放
6.4.2相对于任意轴的旋转
6.5坐标系
6.6串联的性质
6.7透视变换
6.8自由度
6.9非线性变换
6.10本章小结
参考文献
习题
第7章针孔相机
7.1针孔相机模型
7.1.1相机标定
7.1.2三维深度估计
7.1.3单应性
7.2实际相机的一些考虑
7.3本章小结
参考文献
习题
第8章对极几何
8.1背景
8.2多视几何中的匹配
8.3基础矩阵
8.3.1性质
8.3.2基础矩阵的估计
8.3.3仿前置双眼的相机设置
8.4本质矩阵
8.5整流
8.6应用对极几何
8.6.1根据视差恢复深度
8.6.2根据光流恢复深度
8.7本章小结
参考文献
习题
第四部分基于辐射度的视觉计算
第9章光照
9.1辐射度学
9.1.1双向反射分布函数
9.1.2光传播方程
9.2光度学与色彩
9.2.1CIE XYZ色彩空间
9.2.2CIE XYZ空间的认知结构
9.2.3认知一致色彩空间
9.3本章小结
参考文献
习题
第10章色彩还原
10.1加性色彩混合的建模
10.1.1设备的色域
10.1.2色调映射算子
10.1.3强度分辨率
10.1.4显示器示例
10.2色彩管理
10.2.1色域变换
10.2.2色域匹配
10.3减性色彩混合的建模
10.4局限性
10.4.1高动态范围成像
10.4.2多光谱成像
10.5本章小结
参考文献
习题
第11章光度处理
11.1直方图处理
11.2图像融合
11.2.1图像混合
11.2.2图像割
11.3光度立体视觉
11.3.1阴影处理
11.3.2光照方向计算
11.3.3色彩处理
11.4本章小结
参考文献
习题
第五部分视觉内容合成
第12章多样化域
12.1建模
12.2处理
12.3渲染
12.4应用
12.5本章小结
参考文献
第13章交互式图形流程
13.1顶点的几何变换
13.1.1模型变换
13.1.2视图变换
13.1.3透视投影变换
13.1.4遮挡处理
13.1.5窗口坐标变换
13.1.6最终变换
13.2裁剪和属性的顶点插值
13.3光栅化和属性的像素插值
13.4本章小结
参考文献
习题
第14章真实感与性能
14.1光照
14.2着色
14.3阴影
14.4纹理贴图
14.4.1纹理至对象空间映射
14.4.2对象至屏幕空间映射
14.4.3分级细化贴图
14.5凹凸贴图
14.6环境贴图
14.7透明度
14.8累积缓存
14.9背面剔除
14.10可见性剔除
14.10.1包围体
14.10.2空间细分
14.10.3其他用途
14.11本章小结
参考文献
习题
第15章图形编程
15.1图形处理单元的发展
15.2图形API和程序库的发展
15.3现代GPU和CUDA
15.3.1GPU架构
15.3.2CUDA编程模型
15.3.3CUDA存储模型
15.4本章小结
参考文献
・ ・ ・ ・ ・ ・ (收起)第 1章 图像的获取和表示1
1．1 图像传感器技术 1
1．1．1 传感器材料 2
1．1．2 传感器光电二极管元件 3
1．1．3 传感器配置：马赛克、Foveon和BSI 3
1．1．4 动态范围、噪声和超分辨率 4
1．1．5 传感器处理 5
1．1．6 去马赛克 5
1．1．7 坏像素校正 5
1．1．8 色彩和光照校正 6
1．1．9 几何校正 6
1．2 照相机和计算成像 6
1．2．1 计算成像概述 7
1．2．2 单像素可计算相机 7
1．2．3 二维可计算照相机 8
1．2．4 三维深度的照相机系统 9
1．3 三维深度处理 18
1．3．1 方法概述 18
1．3．2 深度感知和处理中存在的问题 18
1．3．3 单目深度处理 23
1．4 三维表示：体元、深度图、网格和点云 26
1．5 总结 27
1．6 习题 27
第 2章 图像预处理 29
2．1 图像处理概述 29
2．2 图像预处理要解决的问题 29
2．2．1 计算机视觉的流程和图像预处理 30
2．2．2 图像校正 31
2．2．3 图像增强 31
2．2．4 为特征提取准备图像 32
2．3 图像处理方法分类 36
2．3．1 点运算 36
2．3．2 直线运算 36
2．3．3 区域运算 37
2．3．4 算法 37
2．3．5 数据转换 37
2．4 色彩学 37
2．4．1 色彩管理系统概述 38
2．4．2 光源、白点、黑点和中性轴 38
2．4．3 设备颜色模型 39
2．4．4 色彩空间与色彩感知 39
2．4．5 色域映射与渲染的目标 40
2．4．6 色彩增强的实际考虑 41
2．4．7 色彩的准确度与精度 41
2．5 空间滤波 41
2．5．1 卷积滤波与检测 41
2．5．2 核滤波与形状选择 43
2．5．3 点滤波 44
2．5．4 噪声与伪像滤波 45
2．5．5 积分图与方框滤波器 46
2．6 边缘检测器 46
2．6．1 核集合 47
2．6．2 Canny检测器 48
2．7 变换滤波、Fourier变换及其他 48
2．7．1 Fourier变换 48
2．7．2 其他变换 50
2．8 形态学与分割 51
2．8．1 二值形态学 51
2．8．2 灰度和彩色形态学 52
2．8．3 形态学优化和改进 53
2．8．4 欧氏距离映射 53
2．8．5 超像素分割 53
2．8．6 深度图分割 54
2．8．7 色彩分割 55
2．9 阈值化 55
2．9．1 全局阈值化 56
2．9．2 局部阈值化 59
2．10 总结 60
2．11 习题 60
第3章 全局特征和区域特征 63
3．1 视觉特征的历史概述 63
3．1．1 全局度量、区域度量和局部度量的核心思想 64
3．1．2 纹理分析 65
3．1．3 统计方法 68
3．2 纹理区域度量 68
3．2．1 边缘度量 69
3．2．2 互相关性和自相关性 70
3．2．3 Fourier谱、小波和基签名 71
3．2．4 共生矩阵、Haralick特征 71
3．2．5 Laws纹理度量 78
3．2．6 LBP局部二值模式 79
3．2．7 动态纹理 80
3．3 统计区域度量 81
3．3．1 图像矩特征 81
3．3．2 点度量特征 81
3．3．3 全局直方图 83
3．3．4 局部区域直方图 83
3．3．5 散点图、3D直方图 84
3．3．6 多分辨率、多尺度直方图 85
3．3．7 径向直方图 87
3．3．8 轮廓或边缘直方图 87
3．4 基空间度量 88
3．4．1 Fourier描述 90
3．4．2 Walsh-Hadamard变换 90
3．4．3 HAAR变换 91
3．4．4 斜变换 91
3．4．5 Zernike多项式 91
3．4．6 导向滤波器 92
3．4．7 Karhunen-Loeve变换与Hotelling变换 93
3．4．8 小波变换和Gabor滤波器 93
3．4．9 Hough变换与Radon变换 95
3．5 总结 96
3．6 习题 96
第4章 局部特征设计 97
4．1 局部特征 97
4．1．1 检测器、兴趣点、关键点、锚点和特征点 98
4．1．2 描述子、特征描述和特征提取 98
4．1．3 稀疏局部模式方法 98
4．2 局部特征属性 99
4．2．1 选择特征描述子和兴趣点 99
4．2．2 特征描述子和特征匹配 99
4．2．3 好特征的标准 99
4．2．4 可重复性，困难和容易的查找 101
4．2．5 判别性与非判别性 101
4．2．6 相对位置和绝对位置 101
4．2．7 匹配代价和一致性 101
4．3 距离函数 102
4．3．1 距离函数的早期工作 102
4．3．2 欧氏或笛卡儿距离度量 103
4．3．3 网格距离度量 104
4．3．4 基于统计学的差异性度量 105
4．3．5 二值或布尔距离度量 106
4．4 描述子的表示 107
4．4．1 坐标空间和复合空间 107
4．4．2 笛卡儿坐标 107
4．4．3 极坐标和对数极坐标 107
4．4．4 径向坐标 107
4．4．5 球面坐标 108
4．4．6 Gauge坐标 108
4．4．7 多元空间和多模数据 108
4．4．8 特征金字塔 109
4．5 描述子的密度 109
4．5．1 丢弃兴趣点和描述子 109
4．5．2 稠密与稀疏特征描述 110
4．6 描述子形状 110
4．6．1 关联性模板 111
4．6．2 块和形状 111
4．6．3 对象多边形 113
4．7 局部二值描述子与点对模式 113
4．7．1 FREAK视网膜模式 114
4．7．2 BRISK模式 115
4．7．3 ORB和BRIEF模式 116
4．8 描述子的判别性 116
4．8．1 谱的判别性 117
4．8．2 区域、形状和模式的判别性 118
4．8．3 几何判别因素 118
4．8．4 通过特征可视化来评价判别性 119
4．8．5 精度与可跟踪性 121
4．8．6 精度优化、子区域重叠、Gaussian加权和池化 122
4．8．7 亚像素精度 123
4．9 搜索策略与优化 123
4．9．1 密集搜索 124
4．9．2 网格搜索 124
4．9．3 多尺度金字塔搜索 124
4．9．4 尺度空间和图像金字塔 125
4．9．5 特征金字塔 126
4．9．6 稀疏预测搜索与跟踪 127
4．9．7 跟踪区域限制搜寻 127
4．9．8 分割限制搜索 127
4．9．9 深度或Z限制搜索 127
4．10 计算机视觉、模型和结构 128
4．10．1 特征空间 128
4．10．2 对象模型 129
4．10．3 约束 130
4．10．4 选择检测器和特征 131
4．10．5 训练概述 131
4．10．6 特征和对象的分类 132
4．10．7 特征学习、稀疏编码和卷积网络 136
4．11 总结 139
4．12 习题 139
第5章 特征描述属性的分类 141
5．1 一般的鲁棒性分类 143
5．2 一般的视觉度量分类 146
5．3 特征度量评估 155
5．3．1 SIFT的示例 156
5．3．2 LBP的示例 156
5．3．3 形状因子的示例 157
5．4 总结 158
5．5 习题 158
第6章 兴趣点检测与特征描述子 159
6．1 兴趣点调整 159
6．2 兴趣点的概念 160
6．3 兴趣点方法概述 162
6．3．1 Laplacian和LoG 163
6．3．2 Moravac角点检测器 163
6．3．3 Harris方法、Harris-Stephens、Shi-Tomasi和Hessian类型的检测器 163
6．3．4 Hessian矩阵检测器和Hessian-Laplace 164
6．3．5 Gaussian差 164
6．3．6 显著性区域 164
6．3．7 SUSAN、Trajkovic-Hedly 165
6．3．8 FAST 165
6．3．9 局部曲率方法 166
6．3．10 形态兴趣区域 167
6．4 特征描述简介 167
6．4．1 局部二值描述子 168
6．4．2 Census 173
6．4．3 改进的Census变换 174
6．4．4 BRIEF 174
6．4．5 ORB 175
6．4．6 BRISK 176
6．4．7 FREAK 176
6．5 谱描述子 177
6．5．1 SIFT 177
6．5．2 SIFT-PCA 181
6．5．3 SIFT-GLOH 181
6．5．4 SIFT-SIFER 182
6．5．5 SIFT CS-LBP 182
6．5．6 ROOTSIFT 183
6．5．7 CenSurE和STAR 183
6．5．8 相关模板 185
6．5．9 HAAR特征 186
6．5．10 使用类HAAR特征的Viola和Jones算法 187
6．5．11 SURF 187
6．5．12 改进的SURF算法 189
6．5．13 梯度直方图（HOG）及改进方法 189
6．5．14 PHOG和相关方法 190
6．5．15 Daisy和O-Daisy 191
6．5．16 CARD 193
6．5．17 具有鲁棒性的快速特征匹配 194
6．5．18 RIFF和CHOG 195
6．5．19 链码直方图 196
6．5．20 D-NETS 196
6．5．21 局部梯度模式 197
6．5．22 局部相位量化 198
6．6 基空间描述子 198
6．6．1 Fourier描述子 199
6．6．2 用其他基函数来构建描述子 200
6．6．3 稀疏编码方法 200
6．7 多边形形状描述 200
6．7．1 MSER方法 201
6．7．2 针对斑点和多边形的目标形状度量 202
6．7．3 形状上下文 204
6．8 3D和4D描述子 205
6．8．1 3D HOG 206
6．8．2 HON 4D 206
6．8．3 3D SIFT 207
6．9 总结 208
6．10 习题 208
第7章 基准数据、内容、度量和分析 210
7．1 基准数据 210
7．2 先前关于基准数据方面的工作：艺术与科学 212
7．2．1 质量的一般度量 212
7．2．2 算法性能的度量 212
7．2．3 Rosin关于角点方面的工作 213
7．3 构造基准数据的关键问题 214
7．3．1 内容：采用、修改或创建 214
7．3．2 可用的基准数据集 215
7．3．3 拟合基准数据的算法 215
7．3．4 场景构成和标注 216
7．4 定义目标和预期 218
7．4．1 Mikolajczyk和Schmid的方法 218
7．4．2 开放式评价系统 219
7．4．3 极端情况和限制 219
7．4．4 兴趣点和特征 219
7．5 基准数据的鲁棒性准则 220
7．5．1 举例说明鲁棒性标准 220
7．5．2 将鲁棒性标准用于实际应用 221
7．6 度量与基准数据配对 222
7．6．1 兴趣点、特征和基准数据的配对和优化 222
7．6．2 一般的视觉分类例子 223
7．7 合成的特征字母表 224
7．7．1 合成数据集的目标 224
7．7．2 合成兴趣点字母表 226
7．7．3 将合成字母表叠加到真实图像上 228
7．8 总结 229
7．9 习题 230
第8章 可视流程及优化 231
8．1 阶段、操作和资源 231
8．2 计算资源预算 233
8．2．1 计算单元、ALU和加速器 234
8．2．2 能耗的使用 235
8．2．3 内存的利用 235
8．2．4 I O性能 238
8．3 计算机视觉流程的实例 238
8．3．1 汽车识别 239
8．3．2 人脸检测、情感识别和年龄识别 244
8．3．3 图像分类 250
8．3．4 增强现实 254
8．4 可选的加速方案 258
8．4．1 内存优化 258
8．4．2 粗粒度并行 260
8．4．3 细粒度数据并行 261
8．4．4 高级指令集和加速器 263
8．5 视觉算法的优化与调整 263
8．5．1 编译器优化与手工优化 264
8．5．2 特征描述子改进、检测器和距离函数 265
8．5．3 Boxlets与卷积加速 265
8．5．4 数据类型优化（整数与浮点） 265
8．6 优化资源 266
8．7 总结 266
第9章 特征学习的架构分类和神经科学背景 267
9．1 计算机视觉中的神经科学思想 268
9．2 特征生成与特征学习 269
9．3 计算机视觉中所使用的神经科学术语 269
9．4 特征学习的分类 274
9．4．1 卷积特征权重学习 275
9．4．2 局部特征描述子学习 275
9．4．3 基本特征的组合和字典学习 275
9．4．4 特征学习方法总结 276
9．5 计算机视觉中的机器学习模型 276
9．5．1 专家系统 277
9．5．2 统计和数学分析方法 278
9．5．3 受神经科学启发的方法 278
9．5．4 深度学习 278
9．6 机器学习和特征学习的历史 280
9．6．1 历史回顾：20世纪40年代至21世纪初 280
9．6．2 人工神经网络（ANN）分类 284
9．7 特征学习概述 285
9．7．1 通过学习得到的各类描述子 285
9．7．2 层次特征学习 285
9．7．3 要学习多少特征 286
9．7．4 深度神经网络的优势 286
9．7．5 特征编码的有效性 286
9．7．6 手工设计的特征与深度学习 287
9．7．7 特征学习的不变性和鲁棒性 288
9．7．8 最好的特征和学习架构 288
9．7．9 大数据、分析和计算机视觉的统一 289
9．7．10 关键技术的推动因素 291
9．8 神经科学的概念 292
9．8．1 生物学及其整体结构 293
9．8．2 难以找到统一的学习理论 294
9．8．3 人类视觉系统的架构 295
9．9 特征学习的结构分类 299
9．9．1 架构拓扑 301
9．9．2 架构组件和层 302
9．10 总结 313
9．11 习题 313
第 10章 特征学习和深度学习架构概述 315
10．1 架构概述 315
10．1．1 FNN架构简介 316
10．1．2 RNN的结构简介 372
10．1．3 BFN的结构简介 395
10．2 集成方法 427
10．3 深度神经网络的未来 429
10．3．1 增加最大深度―深度残差学习 429
10．3．2 使用更简单的MLP来近似复杂模型（模型压缩） 430
10．3．3 分类器的分解与重组 431
10．4 总结 432
10．5 习题 432
附录A 合成特征分析 435
附录B 基准数据集概述 464
附录C 成像和计算机视觉资源 470
附录D 扩展SDM准则 474
附录E 视觉基因组模型（VGM） 487
参考文献 508
译后记 541
・ ・ ・ ・ ・ ・ (收起)目　录
第1章　人工智能的发展概况　1
1．1　人工智能的诞生与初兴　2
1．2　人工智能的复兴与计算机视觉的初露端倪　4
1．3　数据被重视，人工智能崛起　4
第2　章 数据标注行业的国内现状与未来展望　7
2．1　国内数据标注行业的现状　8
2．2　数据标注工程师简介　9
2．3　数据标注行业的发展前景　12
第3　章 人工智能治理　15
3．1　人工智能的可持续发展　16
3．2　数据是AI治理的第一道防火墙　17
3．3　数据服务产业是AI治理落地的试验田　17
3．3．1　数据来源的合法合规问题　18
3．3．2　技术的安全性　19
3．3．3　问责机制　20
3．4　旷视，AI发展与治理双轮驱动　20
第4章　数据标注服务产品及旷视Data++数据标注平台　23
4．1　数据标注服务产品　24
4．2　数据服务标注平台流程　26
4．2．1　创建项目　26
4．2．2　数据上传　27
4．2．3　项目发布　27
4．2．4　项目交付　28
4．3　旷视Data++数据标注平台　28
4．3．1　用户注册　29
4．3．2　标注操作流程　31
第5章　通用标注工具　35
5．1　行人属性筛选　36
5．1．1　行人属性筛选定义　36
5．1．2　行人属性筛选工具介绍　37
5．1．3　行人属性筛选分类　39
5．1．4　标注注意事项　43
5．1．5　标注难点　46
5．1．6　实际中的应用　46
5．1．7　思考与讨论　48
5．1．8　行人属性筛选工具现状及展望　48
5．1．9　小结　50
5．2　属性标注　50
5．2．1　属性标注工具介绍　50
5．2．2　标注内容　52
5．2．3　标注方法　53
5．2．4　标注难点　54
5．2．5　生活中的应用　56
5．2．6　属性标注在Objects365中的应用　57
5．2．7　小结　59
5．3　框+属性　59
5．3．1　“框+属性”工具介绍　60
5．3．2　标注方法　61
5．3．3　标注难点　64
5．3．4　生活中的应用　66
5．3．5　小实验　67
5．3．6　小结　67
5．4　多边形+属性　68
5．4．1　多边形+属性工具介绍　68
5．4．2　标注标准　70
5．4．3　标注难点　74
5．4．4　“多边形+属性”工具在生活中的应用　75
5．4．5　小结　76
第6章　检测标注工具　79
6．1　人脸8点　80
6．1．1　人脸关键点检测定义　80
6．1．2　人脸8点工具介绍　81
6．1．3　标注方法　83
6．1．4　标注难点　84
6．1．5　生活中的应用　90
6．1．6　小实验　92
6．1．7　人脸8点工具现状及展望　92
6．1．8　小结　93
6．2　人体骨骼点　94
6．2．1　人体骨骼点14点定义　94
6．2．2　人体骨骼点工具介绍　95
6．2．3　标注方法　97
6．2．4　标注难点　99
6．2．5　生活中的应用　101
6．2．6　人体骨骼点工具现状及未来展望　104
6．2．7　小结　104
6．3　手部关键点　105
6．3．1　手部关键点21点定义　105
6．3．2　手部关键点标注工具介绍　107
6．3．3　标注方法　108
6．3．4　标注难点　110
6．3．5　手部关键点标注提升方法　114
6．3．6　生活中的应用　115
6．3．7　手部关键点工具现状及展望　117
6．3．8　小结　118
第7章　识别标注工具　121
7．1　一人所属照片清洗　122
7．1．1　一人所属照片清洗工具介绍　122
7．1．2　标注方法　124
7．1．3　标注难点　129
7．1．4　一人所属照片清洗工具在生活中的应用　131
7．1．5　照片清洗工具现状　133
7．1．6　小实验　134
7．1．7　小结　134
7．2　行人重识别　135
7．2．1　行人重识别合并标注工具介绍　135
7．2．2　标注方法　142
7．2．3　标注难点　147
7．2．4　生活中的应用　151
7．2．5　行人重识别技术现状与发展　152
7．2．6　小结　152
第8章　其他标注工具　155
8．1　视频人脸8点　156
8．1．1　视频人脸8点工具介绍　157
8．1．2　标注方法　159
8．1．3　生活中的应用　161
8．1．4　视频人脸8点工具的现状与发展　164
8．1．5　小结　165
8．2　人脸3D朝向　165
8．2．1　人脸3D朝向工具　165
8．2．2　人脸3D朝向工具介绍　166
8．2．3　标注方法　167
8．2．4　标注难点　170
8．2．5　生活中的应用　170
8．2．6　人脸3D朝向工具现状与展望　171
8．2．7　小结　172
8．3　精细分割　173
8．3．1　人像抠图工具介绍　173
8．3．2　标注方法　176
8．3．3　标注难点　185
8．3．4　生活中的应用　186
8．3．5　精细分割标注工具的现状与发展　187
8．3．6　小结　188
声明　189
・ ・ ・ ・ ・ ・ (收起)第1部分 基础知识导读篇
第1章 数字图像基础 2
1.1 图像表示基础 2
1.1.1 艺术与生活 2
1.1.2 数字图像 3
1.1.3 二值图像的处理 5
1.1.4 像素值的范围 5
1.1.5 图像索引 7
1.2 彩色图像的表示 8
1.3 应用基础 9
1.3.1 量化 10
1.3.2 特征 10
1.3.3 距离 11
1.3.4 图像识别 13
1.3.5 信息隐藏 15
1.4 智能图像处理基础 16
1.5 抽象 18
第2章 Python基础 21
2.1 如何开始 21
2.2 基础语法 22
2.2.1 变量的概念 22
2.2.2 变量的使用 22
2.3 数据类型 24
2.3.1 基础类型 25
2.3.2 列表 25
2.3.3 元组 28
2.3.4 字典 29
2.4 选择结构 31
2.5 循环结构 35
2.6 函数 39
2.6.1 什么是函数 39
2.6.2 内置函数 41
2.6.3 自定义函数 42
2.7 模块 44
2.7.1 标准模块 44
2.7.2 第三方模块 45
2.7.3 自定义模块 46
第3章 OpenCV基础 47
3.1 基础 47
3.1.1 安装OpenCV 47
3.1.2 读取图像 49
3.1.3 显示图像 50
3.1.4 保存图像 51
3.2 图像处理 52
3.2.1 像素处理 52
3.2.2 通道处理 57
3.2.3 调整图像大小 60
3.3 感兴趣区域 62
3.4 掩模 63
3.4.1 掩模基础及构造 64
3.4.2 乘法运算 65
3.4.3 逻辑运算 66
3.4.4 掩模作为函数参数 68
3.5 色彩处理 69
3.5.1 色彩空间基础 69
3.5.2 色彩空间转换 71
3.5.3 获取皮肤范围 72
3.6 滤波处理 73
3.6.1 均值滤波 75
3.6.2 高斯滤波 78
3.6.3 中值滤波 82
3.7 形态学 84
3.7.1 腐蚀 85
3.7.2 膨胀 88
3.7.3 通用形态学函数 91
第2部分 基础案例篇
第4章 图像加密与解密 94
4.1 加密与解密原理 94
4.2 图像整体加密与解密 96
4.3 脸部打码及解码 98
4.3.1 掩模方式实现 98
4.3.2 ROI方式实现 101
第5章 数字水印 105
5.1 位平面 106
5.2 数字水印原理 114
5.3 实现方法 115
5.4 具体实现 119
5.5 可视化水印 121
5.5.1 ROI 121
5.5.2 加法运算 123
5.6 扩展学习 125
5.6.1 算术运算实现数字水印 125
5.6.2 艺术字 128
第6章 物体计数 131
6.1 理论基础 131
6.1.1 如何计算图像的中心点 131
6.1.2 获取图像的中心点 133
6.1.3 按照面积筛选前景对象 135
6.2 核心程序 138
6.2.1 核函数 138
6.2.2 zip函数 140
6.2.3 阈值处理函数threshold 140
6.3 程序设计 141
6.4 实现程序 142
第7章 缺陷检测 144
7.1 理论基础 144
7.1.1 开运算 144
7.1.2 距离变换函数distanceTransform 146
7.1.3 最小包围圆形 148
7.1.4 筛选标准 149
7.2 程序设计 150
7.3 实现程序 151
第8章 手势识别 153
8.1 理论基础 154
8.1.1 获取凸包 154
8.1.2 凸缺陷 156
8.1.3 凸缺陷占凸包面积比 159
8.2 识别过程 161
8.2.1 识别流程 162
8.2.2 实现程序 165
8.3 扩展学习：石头、剪刀、布的识别 167
8.3.1 形状匹配 167
8.3.2 实现程序 170
第9章 答题卡识别 173
9.1 单道题目的识别 173
9.1.1 基本流程及原理 173
9.1.2 实现程序 178
9.2 整张答题卡识别原理 180
9.2.1 图像预处理 180
9.2.2 答题卡处理 181
9.2.3 筛选出所有选项 189
9.2.4 将选项按照题目分组 190
9.2.5 处理每一道题目的选项 195
9.2.6 显示结果 195
9.3 整张答题卡识别程序 195
第10章 隐身术 201
10.1 图像的隐身术 201
10.1.1 基本原理与实现 201
10.1.2 实现程序 213
10.1.3 问题及优化方向 214
10.2 视频隐身术 215
第11章 以图搜图 217
11.1 原理与实现 218
11.1.1 算法原理 218
11.1.2 感知哈希值计算方法 220
11.1.3 感知哈希值计算函数 224
11.1.4 计算距离 224
11.1.5 计算图像库内所有图像的哈希值 225
11.1.6 结果显示 226
11.2 实现程序 228
11.3 扩展学习 230
第12章 手写数字识别 231
12.1 基本原理 232
12.2 实现细节 233
12.3 实现程序 235
12.4 扩展阅读 236
第13章 车牌识别 238
13.1 基本原理 238
13.1.1 提取车牌 238
13.1.2 分割车牌 240
13.1.3 识别车牌 242
13.2 实现程序 246
13.3 下一步学习 249
第14章 指纹识别 250
14.1 指纹识别基本原理 251
14.2 指纹识别算法概述 251
14.2.1 描述关键点特征 251
14.2.2 特征提取 252
14.2.3 MCC匹配方法 254
14.2.4 参考资料 258
14.3 尺度不变特征变换 258
14.3.1 尺度空间变换 260
14.3.2 关键点定位 266
14.3.3 通过方向描述关键点 267
14.3.4 显示关键点 271
14.4 基于SIFT的指纹识别 273
14.4.1 距离计算 273
14.4.2 特征匹配 274
14.4.3 算法及实现程序 277
第3部分 机器学习篇
第15章 机器学习导读 282
15.1 机器学习是什么 283
15.2 机器学习基础概念 284
15.2.1 机器学习的类型 284
15.2.2 泛化能力 289
15.2.3 数据集的划分 290
15.2.4 模型的拟合 291
15.2.5 性能度量 292
15.2.6 偏差与方差 293
15.3 OpenCV中的机器学习模块 294
15.3.1 人工神经网络 295
15.3.2 决策树 296
15.3.3 EM模块 300
15.3.4 K近邻模块 300
15.3.5 logistic回归 303
15.3.6 贝叶斯分类器 305
15.3.7 支持向量机 308
15.3.8 随机梯度下降 SVM 分类器 310
15.4 OpenCV机器学习模块的使用 312
15.4.1 使用KNN模块分类 312
15.4.2 使用SVM模块分类 314
第16章 KNN实现字符识别 317
16.1 手写数字识别 317
16.2 英文字母识别 319
第17章 求解数独图像 322
17.1 基本过程 322
17.2 定位数独图像内的单元格 323
17.3 构造KNN模型 327
17.4 识别数独图像内的数字 330
17.5 求解数独 332
17.6 绘制数独求解结果 334
17.7 实现程序 335
17.8 扩展学习 338
第18章 SVM数字识别 339
18.1 基本流程 339
18.2 倾斜校正 340
18.3 HOG特征提取 343
18.4 数据处理 348
18.5 构造及使用SVM分类器 351
18.6 实现程序 352
18.7 参考学习 354
第19章 行人检测 355
19.1 方向梯度直方图特征 355
19.2 基础实现 358
19.2.1 基本流程 359
19.2.2 实现程序 359
19.3 函数detectMultiScale参数及优化 360
19.3.1 参数winStride 360
19.3.2 参数padding 362
19.3.3 参数scale 364
19.3.4 参数useMeanshiftGrouping 366
19.4 完整程序 369
19.5 参考学习 370
第20章 K均值聚类实现艺术画 371
20.1 理论基础 371
20.1.1 案例 371
20.1.2 K均值聚类的基本步骤 373
20.2 K均值聚类模块 374
20.3 艺术画 377
第4部分 深度学习篇
第21章 深度学习导读 384
21.1 从感知机到人工神经网络 384
21.1.1 感知机 384
21.1.2 激活函数 385
21.1.3 人工神经网络 387
21.1.4 完成分类 388
21.2 人工神经网络如何学习 389
21.3 深度学习是什么 390
21.3.1 深度的含义 390
21.3.2 表示学习 391
21.3.3 端到端 392
21.3.4 深度学习可视化 393
21.4 激活函数的分类 394
21.4.1 sigmoid函数 394
21.4.2 tanh函数 395
21.4.3 ReLU函数 395
21.4.4 Leaky ReLU函数 396
21.4.5 ELU函数 396
21.5 损失函数 397
21.5.1 为什么要用损失值 397
21.5.2 损失值如何起作用 398
21.5.3 均方误差 399
21.5.4 交叉熵误差 400
21.6 学习的技能与方法 401
21.6.1 全连接 401
21.6.2 随机失活 402
21.6.3 One-hot编码 403
21.6.4 学习率 403
21.6.5 正则化 404
21.6.6 mini-batch方法 405
21.6.7 超参数 406
21.7 深度学习游乐场 406
第22章 卷积神经网络基础 407
22.1 卷积基础 407
22.2 卷积原理 409
22.2.1 数值卷积 409
22.2.2 图像卷积 410
22.2.3 如何获取卷积核 411
22.3 填充和步长 412
22.4 池化操作 413
22.5 感受野 414
22.6 预处理与初始化 416
22.6.1 扩充数据集 416
22.6.2 标准化与归一化 417
22.6.3 网络参数初始化 418
22.7 CNN 418
22.7.1 LeNet 418
22.7.2 AlexNet 419
22.7.3 VGG网络 420
22.7.4 NiN 420
22.7.5 GooLeNet 421
22.7.6 残差网络 423
第23章 DNN模块 426
23.1 工作流程 427
23.2 模型导入 428
23.3 图像预处理 429
23.4 推理相关函数 438
第24章 深度学习应用实践 440
24.1 图像分类 441
24.1.1 图像分类模型 441
24.1.2 实现程序 442
24.2 目标检测 443
24.2.1 YOLO 444
24.2.2 SSD 447
24.3 图像分割 450
24.3.1 语义分割 450
24.3.2 实例分割 453
24.4 风格迁移 458
24.5 姿势识别 460
24.6 说明 463
第5部分 人脸识别篇
第25章 人脸检测 466
25.1 基本原理 466
25.2 级联分类器的使用 469
25.3 函数介绍 470
25.4 人脸检测实现 471
25.5 表情检测 473
第26章 人脸识别 475
26.1 人脸识别基础 475
26.1.1 人脸识别基本流程 475
26.1.2 OpenCV人脸识别基础 476
26.2 LBPH人脸识别 478
26.2.1 基本原理 478
26.2.2 函数介绍 482
26.2.3 案例介绍 482
26.3 EigenFaces人脸识别 484
26.3.1 基本原理 484
26.3.2 函数介绍 485
26.3.3 案例介绍 485
26.4 FisherFaces人脸识别 487
26.4.1 基本原理 487
26.4.2 函数介绍 489
26.4.3 案例介绍 489
26.5 人脸数据库 491
第27章 dlib库 493
27.1 定位人脸 493
27.2 绘制关键点 494
27.3 勾勒五官轮廓 497
27.4 人脸对齐 500
27.5 调用CNN实现人脸检测 502
第28章 人脸识别应用案例 504
28.1 表情识别 504
28.2 驾驶员疲劳检测 507
28.3 易容术 511
28.3.1 仿射 511
28.3.2 算法流程 512
28.3.3 实现程序 514
28.4 年龄和性别识别 517
・ ・ ・ ・ ・ ・ (收起)第1章 基于直方图优化的图像去雾技术 1
1.1 案例背景 1
1.2 理论基础 1
1.2.1 空域图像增强 1
1.2.2 直方图均衡化 2
1.3 程序实现 3
1.3.1 设计GUI界面 4
1.3.2 全局直方图处理 4
1.3.3 局部直方图处理 6
1.3.4 Retinex增强处理 8
1.4 延伸阅读 12
第2章 基于形态学的权重自适应图像去噪 13
2.1 案例背景 13
2.2 理论基础 14
2.2.1 图像去噪的方法 14
2.2.2 数学形态学的原理 15
2.2.3 权重自适应的多结构形态学去噪 15
2.3 程序实现 16
2.4 延伸阅读 22
第3章 基于多尺度形态学提取眼前节组织 24
3.1 案例背景 24
3.2 理论基础 25
3.3 程序实现 28
3.3.1 多尺度结构设计 28
3.3.2 多尺度边缘提取 29
3.3.3 多尺度边缘融合 31
3.4 延伸阅读 33
第4章 基于Hough变化的答题卡识别 34
4.1 案例背景 34
4.2 理论基础 34
4.2.1 图像二值化 35
4.2.2 倾斜校正 35
4.2.3 图像分割 38
4.3 程序实现 40
4.3.1 图像灰度化 40
4.3.2 灰度图像二值化 41
4.3.3 图像平滑滤波 41
4.3.4 图像矫正 41
4.3.5 完整性核查 42
4.4 延伸阅读 51
第5章 基于阈值分割的车牌定位识别 53
5.1 案例背景 53
5.2 理论基础 53
5.2.1 车牌图像处理 54
5.2.2 车牌定位原理 58
5.2.3 车牌字符处理 58
5.2.4 车牌字符识别 60
5.3 程序实现 62
5.4 延伸阅读 69
第6章 基于分水岭分割进行肺癌诊断 71
6.1 案例背景 71
6.2 理论基础 71
6.2.1 模拟浸水的过程 72
6.2.2 模拟降水的过程 72
6.2.3 过度分割问题 72
6.2.4 标记分水岭分割算法 72
6.3 程序实现 73
6.4 延伸阅读 77
第7章 基于主成分分析的人脸二维码识别 79
7.1 案例背景 79
7.2 理论基础 79
7.2.1 QR二维码简介 80
7.2.2 QR二维码的编码和译码流程 82
7.2.3 主成分分析方法 84
7.3 程序实现 85
7.3.1 人脸建库 85
7.3.2 人脸识别 87
7.3.3 人脸二维码 87
7.4 延伸阅读 92
第8章 基于知识库的手写体数字识别 94
8.1 案例背景 94
8.2 理论基础 94
8.2.1 算法流程 94
8.2.2 特征提取 95
8.2.3 模式识别 96
8.3 程序实现 97
8.3.1 图像处理 97
8.3.2 特征提取 98
8.3.3 模式识别 101
8.4 延伸阅读 102
8.4.1 识别器选择 102
8.4.2 特征库改善 102
第9章 基于特征匹配的英文印刷字符识别 103
9.1 案例背景 103
9.2 理论基础 104
9.2.1 图像预处理 104
9.2.2 图像识别技术 105
9.3 程序实现 106
9.3.1 界面设计 106
9.3.2 回调识别 111
9.4 延伸阅读 112
第10章 基于不变矩的数字验证码识别 113
10.1 案例背景 113
10.2 理论基础 114
10.3 程序实现 114
10.3.1 设计GUI界面 114
10.3.2 载入验证码图像 115
10.3.3 验证码图像去噪 116
10.3.4 验证码数字定位 118
10.3.5 验证码归一化 120
10.3.6 验证码数字识别 121
10.3.7 手动确认并入库 124
10.3.8 重新生成模板库 125
10.4 延伸阅读 128
第11章 基于小波技术进行图像融合 129
11.1 案例背景 129
11.2 理论基础 130
11.3 程序实现 132
11.3.1 设计GUI界面 132
11.3.2 图像载入 133
11.3.3 小波融合 135
11.4 延伸阅读 137
第12章 基于块匹配的全景图像拼接 138
12.1 案例背景 138
12.2 理论基础 138
12.2.1 图像匹配 139
12.2.2 图像融合 141
12.3 程序实现 142
12.3.1 设计GUI界面 142
12.3.2 载入图片 143
12.3.3 图像匹配 144
12.3.4 图像拼接 148
12.4 延伸阅读 153
第13章 基于霍夫曼图像编码的图像压缩和重建 155
13.1 案例背景 155
13.2 理论基础 155
13.2.1 霍夫曼编码的步骤 156
13.2.2 霍夫曼编码的特点 157
13.3 程序实现 158
13.3.1 设计GUI界面 158
13.3.2 压缩和重建 159
13.3.3 效果对比 164
13.4 延伸阅读 167
第14章 基于主成分分析的图像压缩和重建 168
14.1 案例背景 168
14.2 理论基础 168
14.2.1 主成分降维分析原理 168
14.2.2 由得分矩阵重建样本 169
14.2.3 主成分分析数据压缩比 170
14.2.4 基于主成分分析的图像压缩 170
14.3 程序实现 171
14.3.1 主成分分析的源代码 171
14.3.2 图像数组和样本矩阵之间的转换 172
14.3.3 基于主成分分析的图像压缩 173
14.4 延伸阅读 176
第15章 基于小波的图像压缩技术 177
15.1 案例背景 177
15.2 理论基础 178
15.3 程序实现 180
15.4 延伸阅读 188
第16章 基于融合特征的以图搜图技术 189
16.1 案例背景 189
16.2 理论基础 189
16.3 程序实现 191
16.3.1 图像预处理 191
16.3.2 计算特征 191
16.3.3 图像检索 194
16.3.4 结果分析 194
16.4 延伸阅读 196
第17章 基于Harris的角点特征检测 198
17.1 案例背景 198
17.2 理论基础 199
17.2.1 Harris的基本原理 199
17.2.2 Harris算法的流程 201
17.2.3 Harris角点的性质 201
17.3 程序实现 202
17.3.1 Harris算法的代码 202
17.3.2 角点检测实例 204
17.4 延伸阅读 205
第18章 基于GUI搭建通用视频处理工具 206
18.1 案例背景 206
18.2 理论基础 206
18.3 程序实现 208
18.3.1 设计GUI界面 208
18.3.2 实现GUI界面 209
18.4 延伸阅读 220
第19章 基于语音识别的信号灯图像
模拟控制技术 221
19.1 案例背景 221
19.2 理论基础 221
19.3 程序实现 223
19.4 延伸阅读 232
第20章 基于帧间差法进行视频目标检测 234
20.1 案例背景 234
20.2 理论基础 234
20.2.1 帧间差分法 235
20.2.2 背景差分法 236
20.2.3 光流法 236
20.3 程序实现 237
20.4 延伸阅读 24
第21章 路面裂缝检测系统设计 247
21.1 案例背景 247
21.2 理论基础 247
21.2.1 图像灰度化 248
21.2.2 图像滤波 250
21.2.3 图像增强 252
21.2.4 图像二值化 253
21.3 程序实现 255
21.4 延伸阅读 267
第22章 基于K-means聚类算法的图像分割 268
22.1 案例背景 268
22.2 理论基础 268
22.2.1 K-means聚类算法的原理 268
22.2.2 K-means聚类算法的要点 269
22.2.3 K-means聚类算法的缺点 270
22.2.4 基于K-means聚类算法进行图像分割 270
22.3 程序实现 271
22.3.1 样本间的距离 271
22.3.2 提取特征向量 272
22.3.3 图像聚类分割 273
22.4 延伸阅读 275
第23章 基于光流场的车流量计数应用 276
23.1 案例背景 276
23.2 理论基础 276
23.2.1 基于光流法检测运动的原理 276
23.2.2 光流场的主要计算方法 277
23.2.3 梯度光流场约束方程 278
23.2.4 Horn-Schunck光流算法 280
23.3 程序实现 281
23.3.1 计算视觉系统工具箱简介 281
23.3.2 基于光流法检测汽车运动 282
23.4 延伸阅读 287
第24章 基于Simulink进行图像和视频处理 289
24.1 案例背景 289
24.2 模块介绍 289
24.2.1 分析和增强模块库（Analysis和Enhancement） 290
24.2.2 转化模块库（Conversions） 291
24.2.3 滤波模块库（Filtering） 292
24.2.4 几何变换模块库（Geometric Transformations） 292
24.2.5 形态学操作模块库（Morphological Operations） 292
24.2.6 输入模块库（Sources） 293
24.2.7 输出模块库（Sinks） 293
24.2.8 统计模块库（Statistics） 294
24.2.9 文本和图形模块库（Text 和 Graphic） 295
24.2.10 变换模块库（Transforms） 295
24.2.11 其他工具模块库（Utilities） 295
24.3 仿真案例 296
24.3.1 搭建组织模型 296
24.3.2 仿真执行模型 298
24.3.3 自动生成报告 299
24.4 延伸阅读 302
第25章 基于小波变换的数字水印技术 304
25.1 案例背景 304
25.2 理论基础 304
25.2.1 数字水印技术的原理 305
25.2.2 典型的数字水印算法 307
25.2.3 数字水印攻击和评价 309
25.2.4 基于小波的水印技术 310
25.3 程序实现 312
25.3.1 准备载体和水印图像 312
25.3.2 小波数字水印的嵌入 313
25.3.3 小波数字水印的提取 317
25.3.4 小波水印的攻击试验 319
25.4 延伸阅读 323
第26章 基于最小误差法的胸片分割技术 325
26.1 案例背景 325
26.2 理论基础 325
26.2.1 图像增强 326
26.2.2 区域选择 326
26.2.3 形态学滤波 327
26.2.4 基于最小误差法进行胸片分割 328
26.3 程序实现 329
26.3.1 设计GUI界面 329
26.3.2 图像预处理 330
26.3.3 基于最小误差法进行图像分割 333
26.3.4 形态学后处理 335
26.4 延伸阅读 338
第27章 基于区域生长的肝脏影像分割系统 339
27.1 案例背景 339
27.2 理论基础 340
27.2.1 阈值分割 340
27.2.2 区域生长 340
27.2.3 基于阈值预分割的区域生长 341
27.3 程序实现 342
27.4 延伸阅读 346
第28章 基于计算机视觉的自动驾驶应用 347
28.1 案例背景 347
28.2 理论基础 348
28.2.1 环境感知 348
28.2.2 行为决策 348
28.2.3 路径规划 349
28.2.4 运动控制 349
28.3 程序实现 349
28.3.1 传感器数据载入 349
28.3.2 追踪器创建 351
28.3.3 碰撞预警 353
28.4 延伸阅读 358
第29章 基于深度学习的汽车目标检测 359
29.1 案例背景 359
29.2 理论基础 360
29.2.1 基本架构 360
29.2.2 卷积层 360
29.2.3 池化层 362
29.3 程序实现 362
29.3.1 加载数据 362
29.3.2 构建CNN 364
29.3.3 训练CNN 365
29.3.4 评估训练效果 367
29.4 延伸阅读 368
第30章 基于深度学习的视觉场景
识别 370
30.1 案例背景 370
30.2 理论基础 371
30.3 程序实现 371
30.3.1 环境配置 372
30.3.2 数据集制作 373
30.3.3 网络训练 375
30.3.4 网络测试 381
30.4 延伸阅读 383
第31章 深度学习综合应用 385
31.1 应用背景 385
31.2 理论基础 387
31.2.1 分类识别 387
31.2.2 目标检测 391
31.3 案例实现1：基于CNN的数字识别 395
31.3.1 自定义CNN 397
31.3.2 AlexNet 399
31.3.3 基于MATLAB进行实验设计 405
31.3.4 基于TensorFlow进行实验设计 413
31.3.5 实验小结 418
31.4 案例实现2：基于CNN的物体识别 418
31.4.1 CIFAR-10数据集 418
31.4.2 VggNet 421
31.4.3 ResNet 422
31.4.4 实验设计 424
31.4.5 实验小结 432
31.5 案例实现3：基于CNN的图像矫正 432
31.5.1 倾斜数据集 432
31.5.2 自定义CNN回归网络 434
31.5.3 AlexNet回归网络 436
31.5.4 实验设计 437
31.5.5 实验小结 445
31.6 案例实现4：基于LSTM的时间序列分析 445
31.6.1 厄尔尼诺南方涛动指数数据 446
31.6.2 样条拟合分析 446
31.6.3 基于MATLAB进行LSTM分析 448
31.6.4 基于Keras进行LSTM分析 451
31.6.5 实验小结 455
31.7 案例实现5：基于深度学习的以图搜图技术 455
31.7.1 人脸的深度特征 455
31.7.2 AlexNet的特征 460
31.7.3 GoogleNet的特征 461
31.7.4 深度特征融合计算 462
31.7.5 实验设计 462
31.7.6 实验小结 46731.8 案例实现6：基于YOLO的交通目标检测应用 467
31.8.1 车辆目标的YOLO检测 468
31.8.2 交通标志的YOLO检测 475
31.9 延伸阅读 481
・ ・ ・ ・ ・ ・ (收起)Summary of Contents
Foreword 23
Preface 25
About the Authors 31
1 Introduction 35
I Words
2 Regular Expressions and Automata 51
3 Words and Transducers 　　 79
4 N-Grams 117
5 Part-of-Speech Tagging 　　 157
6 Hidden Markov and Maximum Entropy Models 207
7 Phonetics 249
8 Speech Synthesis 283
9 Automatic Speech Recognition 319
10 Speech Recognition: Advanced Topics 369
11 Computational Phonology 　　 395
12 Formal Grammars of English 　 419
13 Syntactic Parsing 461
14 Statistical Parsing 493
15 Features and Uni?cation 　　 523
16 Language and Complexity 　　 563
IV Semantics and Pragmatics
17 The Representation ofMeaning　 579
18 Computational Semantics 　　 617
19 Lexical Semantics　 645
20 Computational Lexical Semantics　 671
21 Computational Discourse 　　 715
V Applications
22 Information Extraction 　　 759
23 Question Answering and Summarization 799
24 Dialogue and Conversational Agents 847
25 Machine Translation 　　 895
Bibliography 945
Author Index 995
Subject Index 1007
Contents
Foreword 23
Preface 25
About the Authors 31
1 Introduction 35
1.1 Knowledge in Speech and Language Processing 　 36
1.2 Ambiguity 38
1.3 Models andAlgorithms 39
1.4 Language, Thought, and Understanding 　　 40
1.5 TheState of theArt 42
1.6 SomeBriefHistory 43
1.6.1 Foundational Insights: 1940s and 1950s 　 43
1.6.2 The Two Camps: 1957C1970 　　 44
1.6.3 Four Paradigms: 1970C1983 　　 45
1.6.4 Empiricism and Finite-State Models Redux: 1983C1993 　 46
1.6.5 The Field Comes Together: 1994C1999　 46
1.6.6 The Rise of Machine Learning: 2000C2008 　 46
1.6.7 On Multiple Discoveries 　 47
1.6.8 A Final Brief Note on Psychology 　　 48
1.7 Summary 　 48
Bibliographical and Historical Notes 　 49
I Words
2 Regular Expressions and Automata　 51
2.1 RegularExpressions 　 51
2.1.1 Basic Regular Expression Patterns 　　 52
2.1.2 Disjunction, Grouping, and Precedence　 55
2.1.3 ASimpleExample　 56
2.1.4 A More Complex Example　 57
2.1.5 AdvancedOperators 　 58
2.1.6 Regular Expression Substitution, Memory, and ELIZA 　 59
2.2 Finite-StateAutomata 　 60
2.2.1 Use of an FSA to Recognize Sheeptalk 　 61
2.2.2 Formal Languages　 64
2.2.3 Another Example 　 65
2.2.4 Non-Deterministic FSAs . 66
2.2.5 Use of an NFSA to Accept Strings 　 67
2.2.6 Recognition as Search 69
2.2.7 Relation of Deterministic and Non-Deterministic Automata 　 72
Foreword 　 23
Preface 　 25
About the Authors　 31
1 Introduction 　 35
1.1 Knowledge in Speech and Language Processing　 36
1.2 Ambiguity 　 38
1.3 Models andAlgorithms 　 39
1.4 Language, Thought, and Understanding 　　　40
1.5 TheState of theArt . 42
1.6 SomeBriefHistory . 43
1.6.1 Foundational Insights: 1940s and 1950s 43
1.6.2 The Two Camps: 1957C1970 　　 44
1.6.3 Four Paradigms: 1970C1983 　　 45
1.6.4 Empiricism and Finite-State Models Redux: 1983C1993 46
1.6.5 The Field Comes Together: 1994C1999 46
1.6.6 The Rise of Machine Learning: 2000C2008 46
1.6.7 On Multiple Discoveries 47
1.6.8 A Final Brief Note on Psychology 　　 48
1.7 Summary 　 48
Bibliographical and Historical Notes 49
I Words
2 Regular Expressions and Automata 51
2.1 RegularExpressions 51
2.1.1 Basic Regular Expression Patterns 　　 52
2.1.2 Disjunction, Grouping, and Precedence　 55
2.1.3 ASimpleExample　 56
2.1.4 A More Complex Example 　 57
2.1.5 AdvancedOperators 　 58
2.1.6 Regular Expression Substitution, Memory, and ELIZA　 59
2.2 Finite-StateAutomata　 60
2.2.1 Use of an FSA to Recognize Sheeptalk　 61
2.2.2 Formal Languages　 64
2.2.3 Another Example 　 65
2.2.4 Non-Deterministic FSAs 　 66
2.2.5 Use of an NFSA to Accept Strings 　　 67
2.2.6 Recognition as Search　 69
2.2.7 Relation of Deterministic and Non-Deterministic Automata　 72
2.3 Regular Languages and FSAs　 72
2.4 Summary 　 75
Bibliographical and Historical Notes 76
Exercises 76
3 Words and Transducers 79
3.1 Survey of (Mostly) English Morphology 　 81
3.1.1 In?ectional Morphology 　 82
3.1.2 Derivational Morphology　 84
3.1.3 Cliticization 　 85
3.1.4 Non-Concatenative Morphology 　　 85
3.1.5 Agreement 　 86
3.2 Finite-State Morphological Parsing　 86
3.3 Construction of a Finite-State Lexicon 　　 88
3.4 Finite-StateTransducers 　 91
3.4.1 Sequential Transducers and Determinism 　 93
3.5 FSTs for Morphological Parsing 　 94
3.6 Transducers and Orthographic Rules 　　 96
3.7 The Combination of an FST Lexicon and Rules 　 99
3.8 Lexicon-Free FSTs: The Porter Stemmer 　　 102
3.9 Word and Sentence Tokenization　 102
3.9.1 Segmentation in Chinese　 104
3.10 Detection and Correction of Spelling Errors 　 106
3.11 MinimumEditDistance 　 107
3.12 Human Morphological Processing 　 111
3.13 Summary 　 113
Bibliographical and Historical Notes 　 114
Exercises 115
4 N-Grams 　 117
4.1 WordCounting inCorpora　 119
4.2 Simple (Unsmoothed) N-Grams　 120
4.3 Training andTestSets 　 125
4.3.1 N-Gram Sensitivity to the Training Corpus　 126
4.3.2 Unknown Words: Open Versus Closed Vocabulary Tasks 　 129
4.4 Evaluating N-Grams: Perplexity 　 129
4.5 Smoothing 　 131
4.5.1 LaplaceSmoothing 　 132
4.5.2 Good-Turing Discounting　 135
4.5.3 Some Advanced Issues in Good-Turing Estimation 　 136
4.6 Interpolation 　 138
4.7 Backoff 　 139
4.7.1 Advanced: Details of Computing Katz Backoff α and P 141
4.8 Practical Issues: Toolkits and Data Formats 　　 142
4.9 Advanced Issues in Language Modeling 　　 143
4.9.1 Advanced Smoothing Methods: Kneser-Ney Smoothing 　 143
4.9.2 Class-Based N-Grams　 145
4.9.3 Language Model Adaptation and Web Use　 146
4.9.4 Using Longer-Distance Information: A Brief Summary 　 146
4.10 Advanced: Information Theory Background 　　148
4.10.1 Cross-Entropy for Comparing Models 　　 150
4.11 Advanced: The Entropy of English and Entropy Rate Constancy 152
4.12 Summary 　 153
Bibliographical and Historical Notes 154
Exercises 155
5 Part-of-Speech Tagging 　 157
5.1 (Mostly) English Word Classes　 158
5.2 Tagsets forEnglish 　 164
5.3 Part-of-Speech Tagging 　 167
5.4 Rule-Based Part-of-Speech Tagging 　169
5.5 HMM Part-of-Speech Tagging　 173
5.5.1 Computing the Most Likely Tag Sequence: An Example　 176
5.5.2 Formalizing Hidden Markov Model Taggers　 178
5.5.3 Using the Viterbi Algorithm for HMM Tagging 　 179
5.5.4 Extending the HMM Algorithm to Trigrams 　 183
5.6 Transformation-Based Tagging 　 185
5.6.1 How TBL Rules Are Applied 　　 186
5.6.2 How TBL Rules Are Learned 　　 186
5.7 Evaluation and Error Analysis 　 187
5.7.1 ErrorAnalysis　 190
5.8 Advanced Issues in Part-of-Speech Tagging 　　 191
5.8.1 Practical Issues: Tag Indeterminacy and Tokenization 　 191
5.8.2 Unknown Words . 192
5.8.3 Part-of-Speech Tagging for Other Languages　 194
5.8.4 Tagger Combination 197
5.9 Advanced: The Noisy Channel Model for Spelling 　 197
5.9.1 Contextual Spelling Error Correction 　　 201
5.10 Summary 　 202
Bibliographical and Historical Notes 203
Exercises 205
6 Hidden Markov and Maximum Entropy Models 207
6.1 MarkovChains 　 208
6.2 TheHiddenMarkovModel 　 210
6.3 Likelihood Computation: The Forward Algorithm 　 213
6.4 Decoding: The Viterbi Algorithm　 218
6.5 HMM Training: The Forward-Backward Algorithm 　 220
6.6 Maximum Entropy Models: Background 　 227
6.6.1 LinearRegression 　 228
6.6.2 Logistic Regression 231
6.6.3 Logistic Regression: Classi?cation 　　233
6.6.4 Advanced: Learning in Logistic Regression 　 234
6.7 Maximum Entropy Modeling 　 235
6.7.1 Why We Call It Maximum Entropy 　　 239
6.8 Maximum Entropy Markov Models 241
6.8.1 Decoding and Learning in MEMMs 　　 244
6.9 Summary 　 245
Bibliographical and Historical Notes 246
Exercises 247
II Speech
7 Phonetics 　 249
7.1 Speech Sounds and Phonetic Transcription　 250
7.2 Articulatory Phonetics 　 251
7.2.1 TheVocalOrgans 　 252
7.2.2 Consonants: Place of Articulation 　　254
7.2.3 Consonants: Manner of Articulation 　　 255
7.2.4 Vowels 256
7.2.5 Syllables 257
7.3 Phonological Categories and Pronunciation Variation 259
7.3.1 Phonetic Features . 261
7.3.2 Predicting Phonetic Variation 　　 . 262
7.3.3 Factors In?uencing Phonetic Variation 　　 263
7.4 Acoustic Phonetics and Signals 264
7.4.1 Waves 　 264
7.4.2 Speech Sound Waves 　 265
7.4.3 Frequency and Amplitude; Pitch and Loudness 　 267
7.4.4 Interpretation of Phones from a Waveform　 270
7.4.5 Spectra and the Frequency Domain 　 270
7.4.6 The Source-Filter Model 　　274
7.5 Phonetic Resources 　 275
7.6 Advanced: Articulatory and Gestural Phonology 　 278
7.7 Summary 　 279
Bibliographical and Historical Notes　 280
Exercises 　 281
8 Speech Synthesis 　283
8.1 TextNormalization 　 285
8.1.1 Sentence Tokenization 　 285
8.1.2 Non-Standard Words 　 286
8.1.3 Homograph Disambiguation 　　290
8.2 Phonetic Analysis 　 291
8.2.1 Dictionary Lookup 　 291
8.2.2 Names 　 292
8.2.3 Grapheme-to-Phoneme Conversion 　　 293
8.3 ProsodicAnalysis 　 296
8.3.1 ProsodicStructure　 296
8.3.2 Prosodic Prominence 　 297
8.3.3 Tune 　 299
8.3.4 More Sophisticated Models: ToBI 　 300
8.3.5 Computing Duration from Prosodic Labels 　302
8.3.6 Computing F0 from Prosodic Labels 　　303
8.3.7 Final Result of Text Analysis: Internal Representation　 305
8.4 Diphone Waveform Synthesis 　 306
8.4.1 Steps for Building a Diphone Database 306
8.4.2 Diphone Concatenation and TD-PSOLA for Prosody 　308
8.5 Unit Selection (Waveform) Synthesis　 310
8.6 Evaluation 　 314
Bibliographical and Historical Notes 　 315
Exercises 　 318
9 Automatic Speech Recognition 　　319
9.1 Speech Recognition Architecture 　 321
9.2 The Hidden Markov Model Applied to Speech 　 325
9.3 Feature Extraction: MFCC Vectors　 329
9.3.1 Preemphasis　 330
9.3.2 Windowing 　 330
9.3.3 Discrete Fourier Transform 　 332
9.3.4 Mel Filter Bank and Log 　 333
9.3.5 The Cepstrum: Inverse Discrete Fourier Transform　 334
9.3.6 Deltas andEnergy　 336
9.3.7 Summary:MFCC 　 336
9.4 Acoustic Likelihood Computation　 337
9.4.1 Vector Quantization 　 337
9.4.2 GaussianPDFs 　 340
9.4.3 Probabilities, Log-Probabilities, and Distance Functions　 347
9.5 The Lexicon and Language Model 　 348
9.6 Search andDecoding 　 348
9.7 EmbeddedTraining 　 358
9.8 Evaluation: Word Error Rate 362
9.9 Summary 　 364
Bibliographical and Historical Notes 　 365
Exercises 　 367
10 Speech Recognition: Advanced Topics　 369
10.1 Multipass Decoding: N-Best Lists and Lattices 　　 369
10.2 A? (“Stack”)Decoding　 375
10.3 Context-Dependent Acoustic Models: Triphones 　 379
10.4 DiscriminativeTraining　 383
10.4.1 Maximum Mutual Information Estimation　 384
10.4.2 Acoustic Models Based on Posterior Classi?ers 385
10.5 ModelingVariation 　 386
10.5.1 Environmental Variation and Noise 　　386
10.5.2 Speaker Variation and Speaker Adaptation 　 387
10.5.3 Pronunciation Modeling: Variation Due to Genre 388
10.6 Metadata: Boundaries, Punctuation, and Dis?uencies 　 390
10.7 Speech Recognition by Humans　 392
10.8 Summary 　 393
Bibliographical and Historical Notes 　 393
Exercises 　 394
11 Computational Phonology 　 395
11.1 Finite-State Phonology 　 395
11.2 Advanced Finite-State Phonology 　 399
11.2.1 Harmony 　 399
11.2.2 Templatic Morphology　 400
11.3 Computational Optimality Theory 　 401
11.3.1 Finite-State Transducer Models of Optimality Theory 　 403
11.3.2 Stochastic Models of Optimality Theory　 404
11.4 Syllabi?cation 　 406
11.5 Learning Phonology and Morphology 　 409
11.5.1 Learning Phonological Rules 　 409
11.5.2 Learning Morphology 411
11.5.3 Learning in Optimality Theory 　　414
11.6 Summary 415
Bibliographical and Historical Notes 　 415
Exercises 417
III Syntax
12 Formal Grammars of English 419
12.1 Constituency 420
12.2 Context-FreeGrammars 421
12.2.1 Formal De?nition of Context-Free Grammar 425
12.3 Some Grammar Rules for English 　 426
12.3.1 Sentence-Level Constructions 　　426
12.3.2 Clauses and Sentences 　 428
12.3.3 The Noun Phrase　 428
12.3.4 Agreement 　 432
12.3.5 The Verb Phrase and Subcategorization　 434
12.3.6 Auxiliaries 　 436
12.3.7 Coordination　 437
12.4 Treebanks 438
12.4.1 Example: The Penn Treebank Project 　　 438
12.4.2 Treebanks as Grammars 　 440
12.4.3 Treebank Searching　 442
12.4.4 Heads and Head Finding 　443
12.5 Grammar Equivalence and Normal Form　 446
12.6 Finite-State and Context-Free Grammars 　 447
12.7 DependencyGrammars 448
12.7.1 The Relationship Between Dependencies and Heads 449
12.7.2 Categorial Grammar 451
12.8 Spoken Language Syntax 　 451
12.8.1 Dis?uencies andRepair 　 452
12.8.2 Treebanks for Spoken Language 　　453
12.9 Grammars and Human Processing 　 454
12.10 Summary 455
Bibliographical and Historical Notes 　456
Exercises 　 458
13 Syntactic Parsing 　 461
13.1 Parsing asSearch 　 462
13.1.1 Top-DownParsing 　 463
13.1.2 Bottom-UpParsing　 464
13.1.3 Comparing Top-Down and Bottom-Up Parsing 465
13.2 Ambiguity 466
13.3 Search in the Face of Ambiguity . 468
13.4 Dynamic Programming Parsing Methods 　　 469
13.4.1 CKYParsing 470
13.4.2 The Earley Algorithm 477
13.4.3 ChartParsing 482
13.5 PartialParsing . 484
13.5.1 Finite-State Rule-Based Chunking 　　 486
13.5.2 Machine Learning-Based Approaches to Chunking 486
13.5.3 Chunking-System Evaluations 　　 . 489
13.6 Summary 　490
Bibliographical and Historical Notes 　 491
Exercises 　 492
14 Statistical Parsing 　　493
14.1 Probabilistic Context-Free Grammars 　 494
14.1.1 PCFGs for Disambiguation 　 495
14.1.2 PCFGs for Language Modeling 　 497
14.2 Probabilistic CKY Parsing of PCFGs 　 498
14.3 Ways to Learn PCFG Rule Probabilities 　 501
14.4 ProblemswithPCFGs 　502
14.4.1 Independence Assumptions Miss Structural Dependencies BetweenRules　 502
14.4.2 Lack of Sensitivity to Lexical Dependencies　 503
14.5 Improving PCFGs by Splitting Non-Terminals 　 505
14.6 Probabilistic Lexicalized CFGs 　507
14.6.1 The Collins Parser 　509
14.6.2 Advanced: Further Details of the Collins Parser 　 511
14.7 EvaluatingParsers 　513
14.8 Advanced: Discriminative Reranking 　 515
14.9 Advanced: Parser-Based Language Modeling 　　 516
14.10 HumanParsing 　517
14.11 Summary 　519
Bibliographical and Historical Notes 　 520
Exercises 522
15 Features and Uni?cation　 523
15.1 FeatureStructures 　524
15.2 Uni?cation of Feature Structures 　 526
15.3 Feature Structures in the Grammar 　531
15.3.1 Agreement 　532
15.3.2 HeadFeatures 　534
15.3.3 Subcategorization 　535
15.3.4 Long-Distance Dependencies 　　 540
15.4 Implementation of Uni?cation　 541
15.4.1 Uni?cation Data Structures 　 541
15.4.2 The Uni?cationAlgorithm 　 543
15.5 Parsing with Uni?cation Constraints 　 547
15.5.1 Integration of Uni?cation into an Earley Parser 　548
15.5.2 Uni?cation-Based Parsing 　 553
15.6 Types and Inheritance 　 555
15.6.1 Advanced: Extensions to Typing 　 558
15.6.2 Other Extensions to Uni?cation 　 559
15.7 Summary 　 559
Bibliographical and Historical Notes　 560
Exercises 561
16 Language and Complexity 　 563
16.1 TheChomskyHierarchy 　 564
16.2 Ways to Tell if a Language Isn’t Regular 　　 566
16.2.1 The Pumping Lemma 567
16.2.2 Proofs that Various Natural Languages Are Not Regular　 569
16.3 Is Natural Language Context Free?　 571
16.4 Complexity and Human Processing 　 573
16.5 Summary 576
Bibliographical and Historical Notes 577
Exercises 578
17 The Representation of Meaning 579
17.1 Computational Desiderata for Representations 　 581
17.1.1 Veri?ability 581
17.1.2 Unambiguous Representations 　582
17.1.3 Canonical Form 　 583
17.1.4 Inference and Variables　 584
17.1.5 Expressiveness 　585
17.2 Model-Theoretic Semantics　 586
17.3 First-OrderLogic 　 589
17.3.1 Basic Elements of First-Order Logic 　　 589
17.3.2 Variables and Quanti?ers . 591
17.3.3 LambdaNotation . 593
17.3.4 The Semantics of First-Order Logic 　594
17.3.5 Inference 　 595
17.4 Event and State Representations　 597
17.4.1 RepresentingTime　 600
17.4.2 Aspect 　 603
17.5 DescriptionLogics 　 606
17.6 Embodied and Situated Approaches to Meaning 　 612
17.7 Summary 　 614
Bibliographical and Historical Notes 　 614
Exercises 616
18 Computational Semantics　 617
18.1 Syntax-Driven Semantic Analysis 　 617
18.2 Semantic Augmentations to Syntactic Rules 　 619
18.3 Quanti?er Scope Ambiguity and Underspeci?cation 　 626
18.3.1 Store and Retrieve Approaches 　　 626
18.3.2 Constraint-Based Approaches 　　 629
18.4 Uni?cation-Based Approaches to Semantic Analysis 　 632
18.5 Integration of Semantics into the Earley Parser 　 638
18.6 Idioms and Compositionality 　 639
18.7 Summary 　 641
Bibliographical and Historical Notes　 641
Exercises 　 643
19 Lexical Semantics 　645
19.1 WordSenses 　 646
19.2 Relations Between Senses 　 649
19.2.1 Synonymy and Antonymy 　 649
19.2.2 Hyponymy 　 650
19.2.3 SemanticFields 　 651
19.3 WordNet: A Database of Lexical Relations 　　 651
19.4 EventParticipants　 653
19.4.1 ThematicRoles 　 654
19.4.2 Diathesis Alternations 　656
19.4.3 Problems with Thematic Roles 　　 657
19.4.4 The Proposition Bank　 658
19.4.5 FrameNet 　 659
19.4.6 Selectional Restrictions 　 661
19.5 Primitive Decomposition 　 663
19.6 Advanced: Metaphor 665
19.7 Summary 　 666
Bibliographical and Historical Notes 　 667
Exercises 　 668
20 Computational Lexical Semantics 　 671
20.1 Word Sense Disambiguation: Overview 　　 672
20.2 Supervised Word Sense Disambiguation 　　 673
20.2.1 Feature Extraction for Supervised Learning　 674
20.2.2 Naive Bayes and Decision List Classi?ers 　 675
20.3 WSD Evaluation, Baselines, and Ceilings 　 678
20.4 WSD: Dictionary and Thesaurus Methods 　　680
20.4.1 The Lesk Algorithm 　 680
20.4.2 Selectional Restrictions and Selectional Preferences 　 682
20.5 Minimally Supervised WSD: Bootstrapping 　　 684
20.6 Word Similarity: Thesaurus Methods 　　 686
20.7 Word Similarity: Distributional Methods 　　 692
20.7.1 De?ning a Word’s Co-Occurrence Vectors 　 693
20.7.2 Measuring Association with Context 　　695
20.7.3 De?ning Similarity Between Two Vectors　 697
20.7.4 Evaluating Distributional Word Similarity 　 701
20.8 Hyponymy and Other Word Relations 　 701
20.9 SemanticRoleLabeling 　 704
20.10 Advanced: Unsupervised Sense Disambiguation　 708
20.11 Summary 709
Bibliographical and Historical Notes 710
Exercises 713
21 Computational Discourse　 715
21.1 DiscourseSegmentation　 718
21.1.1 Unsupervised Discourse Segmentation 　718
21.1.2 Supervised Discourse Segmentation 　　720
21.1.3 Discourse Segmentation Evaluation 　 722
21.2 TextCoherence　 723
21.2.1 Rhetorical Structure Theory 　 724
21.2.2 Automatic Coherence Assignment 　 726
21.3 ReferenceResolution 　 729
21.4 ReferencePhenomena 　 732
21.4.1 Five Types of Referring Expressions 　　 732
21.4.2 Information Status 　 734
21.5 Features for Pronominal Anaphora Resolution 　　 735
21.5.1 Features for Filtering Potential Referents　 735
21.5.2 Preferences in Pronoun Interpretation 　 736
21.6 Three Algorithms for Anaphora Resolution 　 738
21.6.1 Pronominal Anaphora Baseline: The Hobbs Algorithm 　 738
21.6.2 A Centering Algorithm for Anaphora Resolution 　 740
21.6.3 A Log-Linear Model for Pronominal Anaphora Resolution 　 742
21.6.4 Features for Pronominal Anaphora Resolution 　743
21.7 Coreference Resolution 　 744
21.8 Evaluation of Coreference Resolution 　 746
21.9 Advanced: Inference-Based Coherence Resolution 　 747
21.10 Psycholinguistic Studies of Reference 　 752
21.11 Summary 　753
Bibliographical and Historical Notes 　 754
Exercises　 756
V Applications
22 Information Extraction 　 759
22.1 Named Entity Recognition 　 761
22.1.1 Ambiguity in Named Entity Recognition 　 763
22.1.2 NER as Sequence Labeling 　 763
22.1.3 Evaluation of Named Entity Recognition　 766
22.1.4 Practical NER Architectures 　　 768
22.2 Relation Detection and Classi?cation 　　 768
22.2.1 Supervised Learning Approaches to Relation Analysis 769
22.2.2 Lightly Supervised Approaches to Relation Analysis . 772
22.2.3 Evaluation of Relation Analysis Systems . 776
22.3 Temporal and Event Processing 777
22.3.1 Temporal Expression Recognition 　　 777
22.3.2 Temporal Normalization 　 780
22.3.3 Event Detection and Analysis 　　 783
22.3.4 TimeBank 　784
22.4 Template Filling 　786
22.4.1 Statistical Approaches to Template-Filling 　 786
22.4.2 Finite-State Template-Filling Systems 　　 788
22.5 Advanced: Biomedical Information Extraction 　　 791
22.5.1 Biological Named Entity Recognition 　　 792
22.5.2 Gene Normalization　 793
22.5.3 Biological Roles and Relations 　 794
22.6 Summary 　 796
Bibliographical and Historical Notes　 796
Exercises 　 797
23 Question Answering and Summarization　 799
23.1 InformationRetrieval 　 801
23.1.1 The Vector Space Model 　 802
23.1.2 TermWeighting 　 804
23.1.3 Term Selection and Creation 　 806
23.1.4 Evaluation of Information-Retrieval Systems 806
23.1.5 Homonymy, Polysemy, and Synonymy 　 810
23.1.6 Ways to Improve User Queries 　　810
23.2 Factoid Question Answering　 812
23.2.1 Question Processing 　 813
23.2.2 PassageRetrieval　 815
23.2.3 AnswerProcessing　 817
23.2.4 Evaluation of Factoid Answers 　　 821
23.3 Summarization 　 821
23.4 Single-Document Summarization 　 824
23.4.1 Unsupervised Content Selection 　　 824
23.4.2 Unsupervised Summarization Based on Rhetorical Parsing 　 826
23.4.3 Supervised Content Selection 　　 828
23.4.4 Sentence Simpli?cation 　 829
23.5 Multi-Document Summarization　 830
23.5.1 Content Selection in Multi-Document Summarization　 831
23.5.2 Information Ordering in Multi-Document Summarization 　 832
23.6 Focused Summarization and Question Answering 　 835
23.7 Summarization Evaluation 　 839
23.8 Summary 　 841
Bibliographical and Historical Notes 　 842
Exercises 844
24 Dialogue and Conversational Agents 　847
24.1 Properties of Human Conversations 　849
24.1.1 Turns and Turn-Taking 　849
24.1.2 Language as Action: Speech Acts 　　 851
24.1.3 Language as Joint Action: Grounding 　 852
24.1.4 Conversational Structure 　 854
24.1.5 Conversational Implicature　 855
24.2 Basic Dialogue Systems 　 857
24.2.1 ASR Component　 857
24.2.2 NLU Component 　 858
24.2.3 Generation and TTS Components 　 861
24.2.4 Dialogue Manager 　 863
24.2.5 Dealing with Errors: Con?rmation and Rejection 867
24.3 VoiceXML 868
24.4 Dialogue System Design and Evaluation 　　 872
24.4.1 Designing Dialogue Systems 　　 872
24.4.2 Evaluating Dialogue Systems 　 872
24.5 Information-State and Dialogue Acts 　 874
24.5.1 Using Dialogue Acts 　 876
24.5.2 Interpreting Dialogue Acts　 877
24.5.3 Detecting Correction Acts　 880
24.5.4 Generating Dialogue Acts: Con?rmation and Rejection 　881
24.6 Markov Decision Process Architecture 　　 882
24.7 Advanced: Plan-Based Dialogue Agents 　　 886
24.7.1 Plan-Inferential Interpretation and Production　 887
24.7.2 The Intentional Structure of Dialogue 　 889
24.8 Summary 　891
Bibliographical and Historical Notes 　 892
Exercises 　 894
25 Machine Translation　 895
25.1 Why Machine Translation Is Hard 　 898
25.1.1 Typology 　 898
25.1.2 Other Structural Divergences 　　 900
25.1.3 LexicalDivergences 　 901
25.2 Classical MT and the Vauquois Triangle 903
25.2.1 Direct Translation 　 904
25.2.2 Transfer 　 906
25.2.3 Combined Direct and Transfer Approaches in Classic MT　 908
25.2.4 The Interlingua Idea: Using Meaning 　　 909
25.3 StatisticalMT 　 910
25.4 P(F|E): The Phrase-Based Translation Model　　 913
25.5 Alignment inMT 　 915
25.5.1 IBMModel 1 　 916
25.5.2 HMMAlignment 　　919
25.6 Training Alignment Models　 921
25.6.1 EM for Training Alignment Models 　　922
25.7 Symmetrizing Alignments for Phrase-Based MT　 924
25.8 Decoding for Phrase-Based Statistical MT 　　 926
25.9 MTEvaluation 　 930
25.9.1 Using Human Raters 　 930
25.9.2 Automatic Evaluation: BLEU 　　 931
25.10 Advanced: Syntactic Models for MT 　　 934
25.11 Advanced: IBM Model 3 and Fertility 　　935
25.11.1 Training forModel 3 　939
25.12 Advanced: Log-Linear Models for MT 　　 939
25.13 Summary 　940
Bibliographical and Historical Notes 　 941
Exercises 943
Bibliography 　 945
Author Index　 995
Subject Index 　 1007
・ ・ ・ ・ ・ ・ (收起)推荐序III
推荐语IV
前言V
数学符号IX
第1 章绪论1
1.1 自然语言处理的概念 2
1.2 自然语言处理的难点2
1.2.1 抽象性 2
1.2.2 组合性 2
1.2.3 歧义性 3
1.2.4 进化性3
1.2.5 非规范性3
1.2.6 主观性3
1.2.7 知识性3
1.2.8 难移植性4
1.3 自然语言处理任务体系.4
1.3.1 任务层级4
1.3.2 任务类别5
1.3.3 研究对象与层次6
1.4 自然语言处理技术发展历史7
第2 章自然语言处理基础11
2.1 文本的表示.12
2.1.1 词的独热表示13
2.1.2 词的分布式表示13
2.1.3 词嵌入表示19
2.1.4 文本的词袋表示19
2.2 自然语言处理任务20
2.2.1 语言模型20
2.2.2 自然语言处理基础任务23
2.2.3 自然语言处理应用任务31
2.3 基本问题35
2.3.1 文本分类问题35
2.3.2 结构预测问题36
2.3.3 序列到序列问题38
2.4 评价指标40
2.5 小结43
第3 章基础工具集与常用数据集45
3.1 NLTK 工具集46
3.1.1 常用语料库和词典资源46
3.1.2 常用自然语言处理工具集.49
3.2 LTP 工具集51
3.2.1 中文分词51
3.2.2 其他中文自然语言处理功能.52
3.3 PyTorch 基础52
3.3.1 张量的基本概念53
3.3.2 张量的基本运算54
3.3.3 自动微分57
3.3.4 调整张量形状58
3.3.5 广播机制59
3.3.6 索引与切片60
3.3.7 降维与升维60
3.4 大规模预训练数据61
3.4.1 维基百科数据62
3.4.2 原始数据的获取62
3.4.3 语料处理方法62
3.4.4 Common Crawl 数据66
3.5 更多数据集.66
3.6 小结68
第4 章自然语言处理中的神经网络基础69
4.1 多层感知器模型70
4.1.1 感知器70
4.1.2 线性回归71
4.1.3 Logistic 回归71
4.1.4 Softmax 回归72
4.1.5 多层感知器74
4.1.6 模型实现76
4.2 卷积神经网络78
4.2.1 模型结构78
4.2.2 模型实现80
4.3 循环神经网络83
4.3.1 模型结构83
4.3.2 长短时记忆网络85
4.3.3 模型实现87
4.3.4 基于循环神经网络的序列到序列模型88
4.4 注意力模型.89
4.4.1 注意力机制89
4.4.2 自注意力模型90
4.4.3 Transformer 91
4.4.4 基于Transformer 的序列到序列模型93
4.4.5 Transformer 模型的优缺点94
4.4.6 模型实现94
4.5 神经网络模型的训练96
4.5.1 损失函数96
4.5.2 梯度下降98
4.6 情感分类实战101
4.6.1 词表映射101
4.6.2 词向量层102
4.6.3 融入词向量层的多层感知器103
4.6.4 数据处理106
4.6.5 多层感知器模型的训练与测试108
4.6.6 基于卷积神经网络的情感分类109
4.6.7 基于循环神经网络的情感分类110
4.6.8 基于Transformer 的情感分类111
4.7 词性标注实战113
4.7.1 基于前馈神经网络的词性标注114
4.7.2 基于循环神经网络的词性标注114
4.7.3 基于Transformer 的词性标注116
4.8 小结116
第5 章静态词向量预训练模型119
5.1 神经网络语言模型120
5.1.1 预训练任务120
5.1.2 模型实现124
5.2 Word2vec 词向量130
5.2.1 概述130
5.2.2 负采样133
5.2.3 模型实现134
5.3 GloVe 词向量140
5.3.1 概述140
5.3.2 预训练任务140
5.3.3 参数估计140
5.3.4 模型实现141
5.4 评价与应用.143
5.4.1 词义相关性144
5.4.2 类比性146
5.4.3 应用147
5.5 小结148
第6 章动态词向量预训练模型151
6.1 词向量――从静态到动态152
6.2 基于语言模型的动态词向量预训练153
6.2.1 双向语言模型153
6.2.2 ELMo 词向量155
6.2.3 模型实现156
6.2.4 应用与评价169
6.3 小结171
第7 章预训练语言模型173
7.1 概述174
7.1.1 大数据174
7.1.2 大模型175
7.1.3 大算力175
7.2 GPT 177
7.2.1 无监督预训练178
7.2.2 有监督下游任务精调179
7.2.3 适配不同的下游任务180
7.3 BERT 182
7.3.1 整体结构182
7.3.2 输入表示183
7.3.3 基本预训练任务184
7.3.4 更多预训练任务190
7.3.5 模型对比194
7.4 预训练语言模型的应用194
7.4.1 概述194
7.4.2 单句文本分类195
7.4.3 句对文本分类198
7.4.4 阅读理解201
7.4.5 序列标注206
7.5 深入理解BERT .211
7.5.1 概述211
7.5.2 自注意力可视化分析212
7.5.3 探针实验213
7.6 小结.215
第8 章预训练语言模型进阶217
8.1 模型优化.218
8.1.1 XLNet 218
8.1.2 RoBERTa .223
8.1.3 ALBERT .227
8.1.4 ELECTRA 229
8.1.5 MacBERT 232
8.1.6 模型对比234
8.2 长文本处理.234
8.2.1 概述234
8.2.2 Transformer-XL 235
8.2.3 Reformer .238
8.2.4 Longformer 242
8.2.5 BigBird .243
8.2.6 模型对比244
8.3 模型蒸馏与压缩244
8.3.1 概述244
8.3.2 DistilBERT 246
8.3.3 TinyBERT 248
8.3.4 MobileBERT 250
8.3.5 TextBrewer 252
8.4 生成模型257
8.4.1 BART 257
8.4.2 UniLM 260
8.4.3 T5 .263
8.4.4 GPT-3 264
8.4.5 可控文本生成265
8.5 小结.267
第9 章多模态融合的预训练模型269
9.1 多语言融合.270
9.1.1 多语言BERT .270
9.1.2 跨语言预训练语言模型272
9.1.3 多语言预训练语言模型的应用273
9.2 多媒体融合.274
9.2.1 VideoBERT 274
9.2.2 VL-BERT 275
9.2.3 DALL・E 275
9.2.4 ALIGN 276
9.3 异构知识融合276
9.3.1 融入知识的预训练277
9.3.2 多任务学习282
9.4 更多模态的预训练模型285
9.5 小结.285
参考文献287
术语表297
・ ・ ・ ・ ・ ・ (收起)第1章 新手上路 1
1.1 自然语言与编程语言 2
1.1.1 词汇量 2
1.1.2 结构化 2
1.1.3 歧义性 3
1.1.4 容错性 3
1.1.5 易变性 4
1.1.6 简略性 4
1.2 自然语言处理的层次 4
1.2.1 语音、图像和文本 5
1.2.2 中文分词、词性标注和命名实体识别 5
1.2.3 信息抽取 6
1.2.4 文本分类与文本聚类 6
1.2.5 句法分析 6
1.2.6 语义分析与篇章分析 7
1.2.7 其他高级任务 7
1.3 自然语言处理的流派 8
1.3.1 基于规则的专家系统 8
1.3.2 基于统计的学习方法 9
1.3.3 历史 9
1.3.4 规则与统计 11
1.3.5 传统方法与深度学习 11
1.4 机器学习 12
1.4.1 什么是机器学习 13
1.4.2 模型 13
1.4.3 特征 13
1.4.4 数据集 15
1.4.5 监督学习 16
1.4.6 无监督学习 17
1.4.7 其他类型的机器学习算法 18
1.5 语料库 19
1.5.1 中文分词语料库 19
1.5.2 词性标注语料库 19
1.5.3 命名实体识别语料库 20
1.5.4 句法分析语料库 20
1.5.5 文本分类语料库 20
1.5.6 语料库建设 21
1.6 开源工具 21
1.6.1 主流NLP工具比较 21
1.6.2 Python接口 23
1.6.3 Java接口 28
1.7 总结 31
第2章 词典分词 32
2.1 什么是词 32
2.1.1 词的定义 32
2.1.2 词的性质--齐夫定律 33
2.2 词典 34
2.2.1 HanLP词典 34
2.2.2 词典的加载 34
2.3 切分算法 36
2.3.1 完全切分 36
2.3.2 正向最长匹配 37
2.3.3 逆向最长匹配 39
2.3.4 双向最长匹配 40
2.3.5 速度评测 43
2.4 字典树 46
2.4.1 什么是字典树 46
2.4.2 字典树的节点实现 47
2.4.3 字典树的增删改查实现 48
2.4.4 首字散列其余二分的字典树 50
2.4.5 前缀树的妙用 53
2.5 双数组字典树 55
2.5.1 双数组的定义 55
2.5.2 状态转移 56
2.5.3 查询 56
2.5.4 构造* 57
2.5.5 全切分与最长匹配 60
2.6 AC自动机 60
2.6.1 从字典树到AC自动机 61
2.6.2 goto表 61
2.6.3 output表 62
2.6.4 fail表 63
2.6.5 实现 65
2.7 基于双数组字典树的AC自动机 67
2.7.1 原理 67
2.7.2 实现 67
2.8 HanLP的词典分词实现 71
2.8.1 DoubleArrayTrieSegment 72
2.8.2 AhoCorasickDoubleArrayTrie-Segment 73
2.9 准确率评测 74
2.9.1 准确率 74
2.9.2 混淆矩阵与TP/FN/FP/TN 75
2.9.3 精确率 76
2.9.4 召回率 76
2.9.5 F1值 77
2.9.6 中文分词中的P、R、F1计算 77
2.9.7 实现 78
2.9.8 第二届国际中文分词评测 79
2.9.9 OOVRecallRate与IVRecallRate 81
2.10 字典树的其他应用 83
2.10.1 停用词过滤 83
2.10.2 简繁转换 87
2.10.3 拼音转换 90
2.11 总结 91
第3章 二元语法与中文分词 92
3.1 语言模型 92
3.1.1 什么是语言模型 92
3.1.2 马尔可夫链与二元语法 94
3.1.3 n元语法 95
3.1.4 数据稀疏与平滑策略 96
3.2 中文分词语料库 96
3.2.11998 年《人民日报》语料库PKU 97
3.2.2 微软亚洲研究院语料库MSR 98
3.2.3 繁体中文分词语料库 98
3.2.4 语料库统计 99
3.3 训练 100
3.3.1 加载语料库 101
3.3.2 统计一元语法 101
3.3.3 统计二元语法 103
3.4 预测 104
3.4.1 加载模型 104
3.4.2 构建词网 107
3.4.3 节点间的距离计算 111
3.4.4 词图上的维特比算法 112
3.4.5 与用户词典的集成 115
3.5 评测 118
3.5.1 标准化评测 118
3.5.2 误差分析 118
3.5.3 调整模型 119
3.6 日语分词 122
3.6.1 日语分词语料 122
3.6.2 训练日语分词器 123
3.7 总结 124
第4章 隐马尔可夫模型与序列标注 125
4.1 序列标注问题 125
4.1.1 序列标注与中文分词 126
4.1.2 序列标注与词性标注 127
4.1.3 序列标注与命名实体识别 128
4.2 隐马尔可夫模型 129
4.2.1 从马尔可夫假设到隐马尔可夫模型 129
4.2.2 初始状态概率向量 130
4.2.3 状态转移概率矩阵 131
4.2.4 发射概率矩阵 132
4.2.5 隐马尔可夫模型的三个基本用法 133
4.3 隐马尔可夫模型的样本生成 133
4.3.1 案例--医疗诊断 133
4.3.2 样本生成算法 136
4.4 隐马尔可夫模型的训练 138
4.4.1 转移概率矩阵的估计 138
4.4.2 初始状态概率向量的估计 139
4.4.3 发射概率矩阵的估计 140
4.4.4 验证样本生成与模型训练 141
4.5 隐马尔可夫模型的预测 142
4.5.1 概率计算的前向算法 142
4.5.2 搜索状态序列的维特比算法 143
4.6 隐马尔可夫模型应用于中文分词 147
4.6.1 标注集 148
4.6.2 字符映射 149
4.6.3 语料转换 150
4.6.4 训练 151
4.6.5 预测 152
4.6.6 评测 153
4.6.7 误差分析 154
4.7 二阶隐马尔可夫模型* 154
4.7.1 二阶转移概率张量的估计 155
4.7.2 二阶隐马尔可夫模型中的维特比算法 156
4.7.3 二阶隐马尔可夫模型应用于中文分词 158
4.8 总结 159
第5章 感知机分类与序列标注 160
5.1 分类问题 160
5.1.1 定义 160
5.1.2 应用 161
5.2 线性分类模型与感知机算法 161
5.2.1 特征向量与样本空间 162
5.2.2 决策边界与分离超平面 164
5.2.3 感知机算法 167
5.2.4 损失函数与随机梯度下降* 169
5.2.5 投票感知机和平均感知机 171
5.3 基于感知机的人名性别分类 174
5.3.1 人名性别语料库 174
5.3.2 特征提取 174
5.3.3 训练 175
5.3.4 预测 176
5.3.5 评测 177
5.3.6 模型调优 178
5.4 结构化预测问题 180
5.4.1 定义 180
5.4.2 结构化预测与学习的流程 180
5.5 线性模型的结构化感知机算法 180
5.5.1 结构化感知机算法 180
5.5.2 结构化感知机与序列标注 182
5.5.3 结构化感知机的维特比解码算法 183
5.6 基于结构化感知机的中文分词 186
5.6.1 特征提取 187
5.6.2 多线程训练 189
5.6.3 特征裁剪与模型压缩* 190
5.6.4 创建感知机分词器 192
5.6.5 准确率与性能 194
5.6.6 模型调整与在线学习* 195
5.6.7 中文分词特征工程* 197
5.7 总结 199
第6章 条件随机场与序列标注 200
6.1 机器学习的模型谱系 200
6.1.1 生成式模型与判别式模型 201
6.1.2 有向与无向概率图模型 202
6.2 条件随机场 205
6.2.1 线性链条件随机场 205
6.2.2 条件随机场的训练* 207
6.2.3 对比结构化感知机 210
6.3 条件随机场工具包 212
6.3.1 CRF++的安装 212
6.3.2 CRF++语料格式 213
6.3.3 CRF++特征模板 214
6.3.4 CRF++命令行训练 215
6.3.5 CRF++模型格式* 216
6.3.6 CRF++命令行预测 217
6.3.7 CRF++代码分析* 218
6.4 HanLP中的CRF++API 220
6.4.1 训练分词器 220
6.4.2 标准化评测 220
6.5 总结 221
第7章 词性标注 222
7.1 词性标注概述 222
7.1.1 什么是词性 222
7.1.2 词性的用处 223
7.1.3 词性标注 223
7.1.4 词性标注模型 223
7.2 词性标注语料库与标注集 224
7.2.1 《人民日报》语料库与PKU标注集 225
7.2.2 国家语委语料库与863标注集 231
7.2.3 《诛仙》语料库与CTB标注集 234
7.3 序列标注模型应用于词性标注 236
7.3.1 基于隐马尔可夫模型的词性标注 237
7.3.2 基于感知机的词性标注 238
7.3.3 基于条件随机场的词性标注 240
7.3.4 词性标注评测 241
7.4 自定义词性 242
7.4.1 朴素实现 242
7.4.2 标注语料 243
7.5 总结 244
第8章 命名实体识别 245
8.1 概述 245
8.2 基于规则的命名实体识别 246
8.3 命名实体识别语料库 250
8.4 基于层叠隐马尔可夫模型的角色标注框架 252
8.5 基于序列标注的命名实体识别 260
8.6 自定义领域命名实体识别 266
8.7 总结 268
第9章 信息抽取 270
9.1 新词提取 270
9.2 关键词提取 276
9.3 短语提取 283
9.4 关键句提取 284
9.5 总结 287
第10章 文本聚类 288
10.1 概述 288
10.2 文档的特征提取 291
10.3 k均值算法 293
10.4 重复二分聚类算法 300
10.5 标准化评测 303
10.6 总结 305
第11章 文本分类 306
11.1 文本分类的概念 306
11.2 文本分类语料库 307
11.3 文本分类的特征提取 308
11.4 朴素贝叶斯分类器 312
11.5 支持向量机分类器 317
11.6 标准化评测 320
11.7 情感分析 321
11.8 总结 323
第12章 依存句法分析 324
12.1 短语结构树 324
12.1.3 宾州树库和中文树库 326
12.2 依存句法树 327
12.3 依存句法分析 333
12.4 基于转移的依存句法分析 334
12.5 依存句法分析API 340
12.6 案例：基于依存句法树的意见抽取 342
12.7 总结 344
第13章 深度学习与自然语言处理 345
13.1 传统方法的局限 345
13.2 深度学习与优势 348
13.3 word2vec 353
13.4 基于神经网络的高性能依存句法分析器 360
13.5 自然语言处理进阶 363
自然语言处理学习资料推荐 365
・ ・ ・ ・ ・ ・ (收起)扉页
版权声明
内容提要
译者简介
译者序
序
前言
致谢
关于本书
关于作者
关于封面插画
资源与支持
目录
第一部分 处理文本的机器
第1章 NLP概述
1.1 自然语言与编程语言
1.2 神奇的魔法
1.2.1 会交谈的机器
1.2.2 NLP中的数学
1.3 实际应用
1.4 计算机“眼”中的语言
1.4.1 锁的语言（正则表达式）
1.4.2 正则表达式
1.4.3 一个简单的聊天机器人
1.4.4 另一种方法
1.5 超空间简述
1.6 词序和语法
1.7 聊天机器人的自然语言流水线
1.8 深度处理
1.9 自然语言智商
1.10 小结
第2章 构建自己的词汇表――分词
2.1 挑战（词干还原预览）
2.2 利用分词器构建词汇表
2.2.1 点积
2.2.2 度量词袋之间的重合度
2.2.3 标点符号的处理
2.2.4 将词汇表扩展到 n-gram
2.2.5 词汇表归一化
2.3 情感
2.3.1 VADER：一个基于规则的情感分析器
2.3.2 朴素贝叶斯
2.4 小结
第3章 词中的数学
3.1 词袋
3.2 向量化
向量空间
3.3 齐普夫定律
3.4 主题建模
3.4.1 回到齐普夫定律
3.4.2 相关度排序
3.4.3 工具
3.4.4 其他工具
3.4.5 Okapi BM25
3.4.6 未来展望
3.5 小结
第4章 词频背后的语义
4.1 从词频到主题得分
4.1.1 TF-IDF向量及词形归并
4.1.2 主题向量
4.1.3 思想实验
4.1.4 一个主题评分算法
4.1.5 一个 LDA分类器
4.2 潜在语义分析
思想实验的实际实现
4.3 奇异值分解
4.3.1 左奇异向量 U
4.3.2 奇异值向量 S
4.3.3 右奇异向量 V
4.3.4 SVD矩阵的方向
4.3.5 主题约简
4.4 主成分分析
4.4.1 三维向量上的 PCA
4.4.2 回归 NLP
4.4.3 基于 PCA的短消息语义分析
4.4.4 基于截断的 SVD的短消息语义分析
4.4.5 基于 LSA的垃圾短消息分类的效果
4.5 潜在狄利克雷分布（LDiA）
4.5.1 LDiA思想
4.5.2 基于 LDiA主题模型的短消息语义分析
4.5.3 LDiA+LDA=垃圾消息过滤器
4.5.4 更公平的对比：32 个 LDiA主题
4.6 距离和相似度
4.7 反馈及改进
线性判别分析
4.8 主题向量的威力
4.8.1 语义搜索
4.8.2 改进
4.9 小结
第二部分 深度学习（神经网络）
第5章 神经网络初步（感知机与反向传播）
5.1 神经网络的组成
5.2 小结
第6章 词向量推理（Word2vec）
6.1 语义查询与类比
类比问题
6.2 词向量
6.2.1 面向向量的推理
6.2.2 如何计算 Word2vec 表示
6.2.3 如何使用 gensim.word2vec 模块
6.2.4 生成定制化词向量表示
6.2.5 Word2vec 和 GloVe
6.2.6 fastText
6.2.7 Word2vec 和 LSA
6.2.8 词关系可视化
6.2.9 非自然词
6.2.10 利用 Doc2vec 计算文档相似度
6.3 小结
第7章 卷积神经网络（CNN）
7.1 语义理解
7.2 工具包
7.3 卷积神经网络
7.3.1 构建块
7.3.2 步长
7.3.3 卷积核的组成
7.3.4 填充
7.3.5 学习
7.4 狭窄的窗口
7.4.1 Keras 实现：准备数据
7.4.2 卷积神经网络架构
7.4.3 池化
7.4.4 dropout
7.4.5 输出层
7.4.6 开始学习（训练）
7.4.7 在流水线中使用模型
7.4.8 前景展望
7.5 小结
第8章 循环神经网络（RNN）
8.1 循环网络的记忆功能
8.2 整合各个部分
8.3 自我学习
8.4 超参数
8.5 预测
8.5.1 有状态性
8.5.2 双向 RNN
8.5.3 编码向量
8.6 小结
第9章 改进记忆力：长短期记忆网络（LSTM）
9.1 长短期记忆（LSTM）
9.1.1 随时间反向传播
9.1.2 模型的使用
9.1.3 脏数据
9.1.4 “未知”词条的处理
9.1.5 字符级建模
9.1.6 生成聊天文字
9.1.7 进一步生成文本
9.1.8 文本生成的问题：内容不受控
9.1.9 其他记忆机制
9.1.10 更深的网络
9.2 小结
第10章 序列到序列建模和注意力机制
10.1 编码-解码架构
10.1.1 解码思想
10.1.2 似曾相识？
10.1.3 序列到序列对话
10.1.4 回顾 LSTM
10.2 组装一个序列到序列的流水线
10.2.1 为序列到序列训练准备数据集
10.2.2 Keras 中的序列到序列模型
10.2.3 序列编码器
10.2.4 思想解码器
10.2.5 组装一个序列到序列网络
10.3 训练序列到序列网络
生成输出序列
10.4 使用序列到序列网络构建一个聊天机器人
10.4.1 为训练准备语料库
10.4.2 建立字符字典
10.4.3 生成独热编码训练集
10.4.4 训练序列到序列聊天机器人
10.4.5 组装序列生成模型
10.4.6 预测输出序列
10.4.7 生成回复
10.4.8 与聊天机器人交谈
10.5 增强
10.5.1 使用装桶法降低训练复杂度
10.5.2 注意力机制
10.6 实际应用
10.7 小结
第三部分 进入现实世界（现实中的 NLP 挑战）
第11章 信息提取（命名实体识别与问答系统）
11.1 命名实体与关系
11.1.1 知识库
11.1.2 信息提取
11.2 正则模式
11.2.1 正则表达式
11.2.2 把信息提取当作机器学习里的特征提取任务
11.3 值得提取的信息
11.3.1 提取 GPS位置
11.3.2 提取日期
11.4 提取人物关系（事物关系）
11.4.1 词性标注
11.4.2 实体名称标准化
11.4.3 实体关系标准化和提取
11.4.4 单词模式
11.4.5 文本分割
11.4.6 为什么 split('.!?')函数不管用
11.4.7 使用正则表达式进行断句
11.5 现实世界的信息提取
11.6 小结
第12章 开始聊天（对话引擎）
12.1 语言技能
12.1.1 现代方法
12.1.2 混合方法
12.2 模式匹配方法
12.2.1 基于 AIML 的模式匹配聊天机器人
12.2.2 模式匹配的网络视图
12.3 知识方法
12.4 检索（搜索）方法
12.4.1 上下文挑战
12.4.2 基于示例检索的聊天机器人
12.4.3 基于搜索的聊天机器人
12.5 生成式方法
12.5.1 聊聊 NLPIA
12.5.2 每种方法的利弊
12.6 四轮驱动
Will的成功
12.7 设计过程
12.8 技巧
12.8.1 用带有可预测答案的问题提问
12.8.2 要有趣
12.8.3 当其他所有方法都失败时，搜索
12.8.4 变得受欢迎
12.8.5 成为连接器
12.8.6 变得有情感
12.9 现实世界
12.10 小结
第13章 可扩展性（优化、并行化和批处理）
13.1 太多（数据）未必是好事
13.2 优化NLP算法
13.2.1 索引
13.2.2 高级索引
13.2.3 基于 Annoy 的高级索引
13.2.4 究竟为什么要使用近似索引
13.2.5 索引变通方法：离散化
13.3 常数级内存算法
13.3.1 gensim
13.3.2 图计算
13.4 并行化NLP计算
13.4.1 在 GPU上训练 NLP模型
13.4.2 租与买
13.4.3 GPU租赁选择
13.4.4 张量处理单元 TPU
13.5 减少模型训练期间的内存占用
13.6 使用TensorBoard 了解模型
如何可视化词嵌入
13.7 小结
附录A 本书配套的NLP工具
附录B 有趣的Python和正则表达式
附录C 向量和矩阵（线性代数基础）
附录D 机器学习常见工具与技术
附录E 设置亚马逊云服务（ AWS）上的GPU
附录F 局部敏感哈希
资源
词汇表
・ ・ ・ ・ ・ ・ (收起)第1章导论
1.1语音与语言处理中的知识
1.2歧义
1.3模型和算法
1.4语言、思维和理解
1.5学科现状与近期发展
1.6语音和语言处理简史
1.6.1基础研究：20世纪40年代和20世纪50年代
1.6.2两个阵营：1957年至1970年
1.6.3四个范型：1970年至1983年
1.6.4经验主义和有限状态模型的复苏：1983年至1993年
1.6.5不同领域的合流：1994年至1999年
1.6.6机器学习的兴起：2000年至2008年
1.6.7关于多重发现
1.6.8心理学的简要注记
1.7小结
1.8文献和历史说明
第一部分词汇的计算机处理
第2章正则表达式与自动机
2.1正则表达式
2.1.1基本正则表达式模式
2.1.2析取、组合与优先关系
2.1.3一个简单的例子
2.1.4一个比较复杂的例子
2.1.5高级算符
2.1.6正则表达式中的替换、存储器与ELIZA
2.2有限状态自动机
2.2.1用FSA来识别羊的语言
2.2.2形式语言
2.2.3其他例子
2.2.4非确定FSA
2.2.5使用NFSA接收符号串
2.2.6识别就是搜索
2.2.7确定自动机与非确定自动机的关系
2.3正则语言与FSA
2.4小结
2.5文献和历史说明
第3章词与转录机
3.1英语形态学概观
3.1.1屈折形态学
3.1.2派生形态学
3.1.3附着
3.1.4非毗连形态学
3.1.5一致关系
3.2有限状态形态剖析
3.3有限状态词表的建造
3.4有限状态转录机
3.4.1定序转录机和确定性
3.5用于形态剖析的FST
3.6转录机和正词法规则
3.7把FST词表与规则相结合
3.8与词表无关的FST：Porter词干处理器
3.9单词和句子的词例还原
3.9.1中文的自动切词
3.10拼写错误的检查与更正
3.11最小编辑距离
3.12人是怎样进行形态处理的
3.13小结
3.14文献和历史说明
第4章N元语法
4.1语料库中单词数目的计算
4.2简单的（非平滑的）N元语法
4.3训练集和测试集
4.3.1N元语法及其对训练语料库的敏感性
4.3.2未知词：开放词汇与封闭词汇
4.4N元语法的评测：困惑度
4.5平滑
4.5.1Laplace平滑
4.5.2GoodTuring打折法
4.5.3GoodTuring估计的一些高级专题
4.6插值法
4.7回退法
4.7.1高级专题：计算Katz回退的α和P*
4.8实际问题：工具包和数据格式
4.9语言模型建模中的高级专题
4.9.1高级的平滑方法：KneserNey平滑法
4.9.2基于类别的N元语法
4.9.3语言模型的自适应和网络（Web）应用
4.9.4长距离信息的使用：简要的综述
4.10信息论背景
4.10.1用于比较模型的交叉熵
4.11高级问题：英语的熵和熵率均衡性
4.12小结
4.13文献和历史说明
第5章词类标注
5.1（大多数）英语词的分类
5.2英语的标记集
5.3词类标注
5.4基于规则的词类标注
5.5基于隐马尔可夫模型的词类标注
5.5.1计算最可能的标记序列：一个实例
5.5.2隐马尔可夫标注算法的形式化
5.5.3使用Viterbi算法来进行HMM标注
5.5.4把HMM扩充到三元语法
5.6基于转换的标注
5.6.1怎样应用TBL规则
5.6.2怎样学习TBL规则
5.7评测和错误分析
5.7.1错误分析
5.8词类标注中的高级专题
5.8.1实际问题：标记的不确定性与词例还原
5.8.2未知词
5.8.3其他语言中的词类标注
5.8.4标注算法的结合
5.9高级专题：拼写中的噪声信道模型
5.9.1上下文错拼更正
5.10小结
5.11文献和历史说明
第6章隐马尔可夫模型与最大熵模型
6.1马尔可夫链
6.2隐马尔可夫模型
6.3似然度的计算：向前算法
6.4解码：Viterbi算法
6.5HMM的训练：向前向后算法
6.6最大熵模型：背景
6.6.1线性回归
6.6.2逻辑回归
6.6.3逻辑回归：分类
6.6.4高级专题：逻辑回归的训练
6.7最大熵模型
6.7.1为什么称为最大熵
6.8最大熵马尔可夫模型
6.8.1MEMM的解码和训练
6.9小结
6.10文献和历史说明
第二部分语音的计算机处理
第7章语音学
7.1言语语音与语音标音法
7.2发音语音学
7.2.1发音器官
7.2.2辅音：发音部位
7.2.3辅音：发音方法
7.2.4元音
7.2.5音节
7.3音位范畴与发音变异
7.3.1语音特征
7.3.2语音变异的预测
7.3.3影响语音变异的因素
7.4声学语音学和信号
7.4.1波
7.4.2语音的声波
7.4.3频率与振幅：音高和响度
7.4.4从波形来解释音子
7.4.5声谱和频域
7.4.6声源滤波器模型
7.5语音资源
7.6高级问题：发音音系学与姿态音系学
7.7小结
7.8文献和历史说明
第8章语音合成
8.1文本归一化
8.1.1句子的词例还原
8.1.2非标准词
8.1.3同形异义词的排歧
8.2语音分析
8.2.1查词典
8.2.2名称
8.2.3字位―音位转换
8.3韵律分析
8.3.1韵律的结构
8.3.2韵律的突显度
8.3.3音调
8.3.4更精巧的模型：ToBI
8.3.5从韵律标记计算音延
8.3.6从韵律标记计算F0
8.3.7文本分析的最后结果：内部表示
8.4双音子波形合成
8.4.1建立双音子数据库的步骤
8.4.2双音子毗连和用于韵律的TD―PSOLA
8.5单元选择（波形）合成
8.6评测
8.7文献和历史说明
第9章语音自动识别
9.1语音识别的总体结构
9.2隐马尔可夫模型应用于语音识别
9.3特征抽取：MFCC矢量
9.3.1预加重
9.3.2加窗
9.3.3离散傅里叶变换
9.3.4Mel滤波器组和对数
9.3.5倒谱：逆向傅里叶变换
9.3.6Delta特征与能量
9.3.7总结：MFCC
9.4声学似然度的计算
9.4.1矢量量化
9.4.2高斯概率密度函数
9.4.3概率、对数概率和距离函数
9.5词典和语言模型
9.6搜索与解码
9.7嵌入式训练
9.8评测：词错误率
9.9小结
9.10文献和历史说明
第10章语音识别：高级专题
10.1多遍解码：N最佳表和格
10.2A*解码算法（“栈”解码算法）
10.3依赖于上下文的声学模型：三音子
10.4分辨训练
10.4.1最大互信息估计
10.4.2基于后验分类器的声学模型
10.5语音变异的建模
10.5.1环境语音变异和噪声
10.5.2说话人变异和说话人适应
10.5.3发音建模：由于语类的差别而产生的变异
10.6元数据：边界、标点符号和不流利现象
10.7人的语音识别
10.8小结
10.9文献和历史说明
第11章计算音系学
11.1有限状态音系学
11.2高级有限状态音系学
11.2.1元音和谐
11.2.2模板式形态学
11.3计算优选理论
11.3.1优选理论中的有限状态转录机模型
11.3.2优选理论的随机模型
11.4音节切分
11.5音位规则和形态规则的机器学习
11.5.1音位规则的机器学习
11.5.2形态规则的机器学习
11.5.3优选理论中的机器学习
11.6小结
11.7文献和历史说明
第三部分句法的计算机处理
第12章英语的形式语法
12.1组成性
12.2上下文无关语法
12.2.1上下文无关语法的形式定义
12.3英语的一些语法规则
12.3.1句子一级的结构
12.3.2子句与句子
12.3.3名词短语
12.3.4一致关系
12.3.5动词短语和次范畴化
12.3.6助动词
12.3.7并列关系
12.4树库
12.4.1树库的例子：宾州树库课题
12.4.2作为语法的树库
12.4.3树库搜索
12.4.4中心词与中心词的发现
12.5语法等价与范式
12.6有限状态语法和上下文无关语法
12.7依存语法
12.7.1依存和中心词之间的关系
12.7.2范畴语法
12.8口语的句法
12.8.1不流畅现象与口语修正
12.8.2口语树库
12.9语法和人的语言处理
12.10小结
12.11文献和历史说明
第13章句法剖析
13.1剖析就是搜索
13.1.1自顶向下剖析
13.1.2自底向上剖析
13.1.3自顶向下剖析与自底向上剖析比较
13.2歧义
13.3面对歧义的搜索
13.4动态规划剖析方法
13.4.1CKY剖析
13.4.2Earley算法
13.4.3线图剖析
13.5局部剖析
13.5.1基于规则的有限状态组块分析
13.5.2基于机器学习的组块分析方法
13.5.3组块分析系统的评测
13.6小结
13.7文献和历史说明
第14章统计剖析
14.1概率上下文无关语法
14.1.1PCFG用于排歧
14.1.2PCFG用于语言建模
14.2PCFG的概率CKY剖析
14.3PCFG规则概率的学习途径
14.4PCFG的问题
14.4.1独立性假设忽略了规则之间的结构依存关系
14.4.2缺乏对词汇依存关系的敏感性
14.5使用分离非终极符号的办法来改进PCFG
14.6概率词汇化的CFG
14.6.1Collins剖析器
14.6.2高级问题：Collins剖析器更多的细节
14.7剖析器的评测
14.8高级问题：分辨再排序
14.9高级问题：基于剖析器的语言模型
14.10人的剖析
14.11小结
14.12文献和历史说明
第15章特征与合一
15.1特征结构
15.2特征结构的合一
15.3语法中的特征结构
15.3.1一致关系
15.3.2中心语特征
15.3.3次范畴化
15.3.4长距离依存关系
15.4合一的实现
15.4.1合一的数据结构
15.4.2合一算法
15.5带有合一约束的剖析
15.5.1把合一结合到Earley剖析器中
15.5.2基于合一的剖析
15.6类型与继承
15.6.1高级问题：类型的扩充
15.6.2合一的其他扩充
15.7小结
15.8文献和历史说明
第16章语言和复杂性
16.1Chomsky层级
16.2怎么判断一种语言不是正则的
16.2.1抽吸引理
16.2.2证明各种自然语言不是正则语言
16.3自然语言是上下文无关的吗
16.4计算复杂性和人的语言处理
16.5小结
16.6文献和历史说明
第四部分语义和语用的计算机处理
第17章意义的表示
17.1意义表示的计算要求
17.1.1可验证性
17.1.2无歧义性
17.1.3规范形式
17.1.4推理与变量
17.1.5表达能力
17.2模型论语义学
17.3一阶逻辑
17.3.1一阶逻辑基础
17.3.2变量和量词
17.3.3λ表示法
17.3.4一阶逻辑的语义
17.3.5推理
17.4事件与状态的表示
17.4.1时间表示
17.4.2体
17.5描述逻辑
17.6意义的具体化与情境表示方法
17.7小结
17.8文献和历史说明
第18章计算语义学
18.1句法驱动的语义分析
18.2句法规则的语义扩充
18.3量词辖域歧义及非确定性
18.3.1存储与检索方法
18.3.2基于约束的方法
18.4基于合一的语义分析方法
18.5语义与Earley分析器的集成
18.6成语和组成性
18.7小结
18.8文献和历史说明
第19章词汇语义学
19.1词义
19.2含义间的关系
19.2.1同义关系和反义关系
19.2.2上下位关系
19.2.3语义场
19.3WordNet：词汇关系信息库
19.4事件参与者
19.4.1题旨角色
19.4.2因素交替（DiathesisAlternations）
19.4.3题旨角色的问题
19.4.4命题库
19.4.5FrameNet
19.4.6选择限制
19.5基元分解
19.6高级问题：隐喻
19.7小结
19.8文献和历史说明
第20章计算词汇语义学
20.1词义排歧：综述
20.2有监督词义排歧
20.2.1监督学习的特征抽取
20.2.2朴素贝叶斯分类器和决策表分类器
20.3WSD评价方法、基准线和上限
20.4WSD：字典方法和同义词库方法
20.4.1Lesk算法
20.4.2选择限制和选择优先度
20.5最低限度的监督WSD：自举法
20.6词语相似度：语义字典方法
20.7词语相似度：分布方法
20.7.1定义词语的共现向量
20.7.2度量与上下文的联系
20.7.3定义两个向量之间的相似度
20.7.4评价分布式词语相似度
20.8下位关系和其他词语关系
20.9语义角色标注
20.10高级主题：无监督语义排歧
20.11小结
20.12文献和历史说明
第21章计算话语学
21.1话语分割
21.1.1无监督话语分割
21.1.2有监督话语分割
21.1.3话语分割的评价
21.2文本连贯性
21.2.1修辞结构理论
21.2.2自动连贯指派
21.3指代消解
21.4指代现象
21.4.1指示语的五种类型
21.4.2信息状态
21.5代词指代消解所使用的特征
21.5.1用来过滤潜在指代对象的特征
21.5.2代词解释中的优先关系
21.6指代消解的三种算法
21.6.1代词指代基准系统：Hobbs算法
21.6.2指代消解的中心算法
21.6.3代词指代消解的对数线性模型
21.6.4代词指代消解的特征
21.7共指消解
21.8共指消解的评价
21.9高级问题：基于推理的连贯判定
21.10所指的心理语言学研究
21.11小结
21.12文献和历史说明
第五部分应用
第22章信息抽取
22.1命名实体识别
22.1.1命名实体识别中的歧义
22.1.2基于序列标注的命名实体识别
22.1.3命名实体识别的评价
22.1.4实用NER架构
22.2关系识别和分类
22.2.1用于关系分析的有监督学习方法
22.2.2用于关系分析的弱监督学习方法
22.2.3关系分析系统的评价
22.3时间和事件处理
22.3.1时间表达式的识别
22.3.2时间的归一化
22.3.3事件检测和分析
22.3.4TimeBank
22.4模板填充
22.4.1模板填充的统计方法
22.4.2有限状态机模板填充系统
22.5高级话题：生物医学信息的抽取
22.5.1生物学命名实体识别
22.5.2基因归一化
22.5.3生物学角色和关系
22.6小结
22.7文献和历史说明
第23章问答和摘要
23.1信息检索
23.1.1向量空间模型
23.1.2词语权重计算
23.1.3词语选择和建立
23.1.4信息检索系统的评测
23.1.5同形关系、多义关系和同义关系
23.1.6改进用户查询的方法
23.2事实性问答
23.2.1问题处理
23.2.2段落检索
23.2.3答案处理
23.2.4事实性答案的评价
23.3摘要
23.4单文档摘要
23.4.1无监督的内容选择
23.4.2基于修辞分析的无监督摘要
23.4.3有监督的内容选择
23.4.4句子简化
23.5多文档摘要
23.5.1多文档摘要的内容选择
23.5.2多文档摘要的信息排序
23.6主题摘要和问答
23.7摘要的评价
23.8小结
23.9文献和历史说明
第24章对话与会话智能代理
24.1人类会话的属性
24.1.1话轮和话轮转换
24.1.2语言作为行动：言语行为
24.1.3语言作为共同行动：对话的共同基础
24.1.4会话结构
24.1.5会话隐含
24.2基本的对话系统
24.2.1ASR组件
24.2.2NLU组件
24.2.3生成和TTS组件
24.2.4对话管理器
24.2.5错误处理：确认和拒绝
24.3VoiceXML
24.4对话系统的设计和评价
24.4.1设计对话系统
24.4.2评价对话系统
24.5信息状态和对话行为
24.5.1使用对话行为
24.5.2解释对话行为
24.5.3检测纠正行为
24.5.4生成对话行为：确认和拒绝
24.6马尔可夫决策过程架构
24.7高级问题：基于规划的对话行为
24.7.1规划推理解释和生成
24.7.2对话的意图结构
24.8小结
24.9文献和历史说明
第25章机器翻译
25.1为什么机器翻译如此困难
25.1.1类型学
25.1.2其他的结构差异
25.1.3词汇的差异
25.2经典的机器翻译方法与Vauquois三角形
25.2.1直接翻译
25.2.2转换方法
25.2.3传统机器翻译系统中的直接和转换相融合的方法
25.2.4中间语言的思想：使用意义
25.3统计机器翻译
25.4P（F|E）：基于短语的翻译模型
25.5翻译中的对齐
25.5.1IBM模型1
25.5.2HMM对齐
25.6对齐模型的训练
25.6.1训练对齐模型的EM算法
25.7用于基于短语机器翻译的对称对齐
25.8基于短语统计机器翻译的解码
25.9机器翻译评价
25.9.1使用人工评价者
25.9.2自动评价：BLEU
25.10高级问题：机器翻译的句法模型
25.11高级问题：IBM模型3和繁衍度
25.11.1模型3的训练
25.12高级问题：机器翻译的对数线性模型
25.13小结
25.14文献和历史说明
参考文献
・ ・ ・ ・ ・ ・ (收起)Preface
1.Language Processing and Python
1.1 Computing with Language： Texts and Words
1.2 A Closer Look at Python： Texts as Lists of Words
1.3 Computing with Language： Simple Statistics
1.4 Back to Python： Making Decisions and Taking Control
1.5 Automatic Natural Language Understanding
1.6 Summary
1.7 Further Reading
1.8 Exercises
2.Accessing Text Corpora and Lexical Resources
2.1 Accessing Text Corpora
2.2 Conditional Frequency Distributions
2.3 More Python： Reusing Code
2.4 Lexical Resources
2.5 WordNet
2.6 Summary
2.7 Further Reading
2.8 Exercises
3.Processing Raw Text
3.1 Accessing Text from the Web and from Disk
3.2 Strings： Text Processing at the Lowest Level
3.3 Text Processing with Unicode
3.4 Regular Expressions for Detecting Word Patterns
3.5 Useful Applications of Regular Expressions
3.6 Normalizing Text
3.7 Regular Expressions for Tokenizing Text
3.8 Segmentation
3.9 Formatting： From Lists to Strings
3.10 Summary
3.11 Further Reading
3.12 Exercises
4.Writing Structured Programs
4.1 Back to the Basics
4.2 Sequences
4.3 Questions of Style
4.4 Functions： The Foundation of Structured Programming
4.5 Doing More with Functions
4.6 Program Development
4.7 Algorithm Design
4.8 A Sample of Python Libraries
4.9 Summary
4.10 Further Reading
4.11 Exercises
5.Categorizing andTagging Words
5.1 Using a Tagger
5.2 Tagged Corpora
5.3 Mapping Words to Properties Using Python Dictionaries
5.4 Automatic Tagging
5.5 N-Gram Tagging
5.6 Transformation-Based Tagging
5.7 How to Determine the Category of a Word
5.8 Summary
5.9 Further Reading
5.10 Exercises
6.Learning to Classify Text
6.1 Supervised Classification
6.2 Further Examples of Supervised Classification
6.3 Evaluation
6.4 Decision Trees
6.5 Naive Bayes Classifiers
6.6 Maximum Entropy Classifiers
6.7 Modeling Linguistic Patterns
6.8 Summary
6.9 Further Reading
6.10 Exercises
7.Extracting Information from Text
7.1 Information Extraction
7.2 Chunking
7.3 Developing and Evaluating Chunkers
7.4 Recursion in Linguistic Structure
7.5 Named Entity Recognition
7.6 Relation Extraction
7.7 Summary
7.8 Further Reading
7.9 Exercises
8.Analyzing Sentence Structure
8.1 Some Grammatical Dilemmas
8.2 Whats the Use of Syntax?
8.3 Context-Free Grammar
8.4 Parsing with Context-Free Grammar
8.5 Dependencies and Dependency Grammar
8.6 Grammar Development
8.7 Summary
8.8 Further Reading
8.9 Exercises
9.Building Feature-Based Grammars
9.1 Grammatical Features
9.2 Processing Feature Structures
9.3 Extending a Feature-Based Grammar
9.4 Summary
9.5 Further Reading
9.6 Exercises
10.Analyzing the Meaning of Sentences
10.1 Natural Language Understanding
10.2 Propositional Logic
10.3 First-Order Logic
10.4 The Semantics of English Sentences
10.5 Discourse Semantics
10.6 Summary
10.7 Further Reading
10.8 Exercises
11.Managing Linguistic Data
11.1 Corpus Structure： A Case Study
11.2 The Life Cycle of a Corpus
11.3 Acquiring Data
11.4 Working with XML
11.5 Working with Toolbox Data
11.6 Describing Language Resources Using OLAC Metadata
11.7 Summary
11.8 Further Reading
11.9 Exercises
Afterword： The Language Challenge
Bibliography
NLTK Index
General Index
・ ・ ・ ・ ・ ・ (收起)《python自然语言处理》
第1章 语言处理与python 1
1.1 语言计算：文本和词汇 1
1.2 近观python：将文本当做词链表 10
1.3 计算语言：简单的统计 17
1.4 回到python:决策与控制 24
1.5 自动理解自然语言 29
1.6 小结 35
1.7 深入阅读 36
1.8 练习 37
第2章 获得文本语料和词汇资源 41
2.1 获取文本语料库 41
2.2 条件频率分布 55
2.3 更多关于python：代码重用 60
2.4 词典资源 63
2.5 wordnet 72
2.6 小结 78
2.7 深入阅读 79
2.8 练习 80
第3章 处理原始文本 84
3.1 从网络和硬盘访问文本 84
3.2 字符串：最底层的文本处理 93
3.3 使用unicode进行文字处理 100
3.4 使用正则表达式检测词组搭配 105
3.5 正则表达式的有益应用 109
3.6 规范化文本 115
3.7 用正则表达式为文本分词 118
3.8 分割 121
3.9 格式化：从链表到字符串 126
3.10 小结 132
3.11 深入阅读 133
3.12 练习 134
第4章 编写结构化程序 142
4.1 回到基础 142
4.2 序列 147
4.3 风格的问题 152
4.4 函数：结构化编程的基础 156
4.5 更多关于函数 164
4.6 程序开发 169
4.7 算法设计 175
4.8 python库的样例 183
4.9 小结 188
4.10 深入阅读 189
4.11 练习 189
第5章 分类和标注词汇 195
5.1 使用词性标注器 195
5.2 标注语料库 197
5.3 使用python字典映射词及其属性 206
5.4 自动标注 216
5.5 n-gram标注 221
5.6 基于转换的标注 228
5.7 如何确定一个词的分类 230
5.8 小结 233
5.9 深入阅读 234
5.10 练习 235
第6章 学习分类文本 241
6.1 监督式分类 241
6.2 监督式分类的举例 254
6.3 评估 258
6.4 决策树 263
6.5 朴素贝叶斯分类器 266
6.6 最大熵分类器 271
6.7 为语言模式建模 275
6.8 小结 276
6.9 深入阅读 277
6.10 练习 278
第7章 从文本提取信息 281
7.1 信息提取 281
7.2 分块 284
7.3 开发和评估分块器 291
7.4 语言结构中的递归 299
7.5 命名实体识别 302
7.6 关系抽取 306
7.7 小结 307
7.8 深入阅读 308
7.9 练习 308
第8章 分析句子结构 312
8.1 一些语法困境 312
8.2 文法的用途 316
8.3 上下文无关文法 319
8.4 上下文无关文法分析 323
8.5 依存关系和依存文法 332
8.6 文法开发 336
8.7 小结 343
8.8 深入阅读 344
8.9 练习 344
第9章 建立基于特征的文法 349
9.1 文法特征 349
9.2 处理特征结构 359
9.3 扩展基于特征的文法 367
9.4 小结 379
9.5 深入阅读 380
9.6 练习 381
第10章 分析语句的含义 384
10.1 自然语言理解 384
10.2 命题逻辑 391
10.3 一阶逻辑 395
10.4 英语语句的语义 409
10.5 段落语义层 422
10.6 小结 428
10.7 深入阅读 429
10.8 练习 430
第11章 语言数据管理 434
11.1 语料库结构：案例研究 434
11.2 语料库生命周期 439
11.3 数据采集 443
11.4 使用xml 452
11.5 使用toolbox数据 459
11.6 使用olac元数据描述语言资源 463
11.7 小结 466
11.8 深入阅读 466
11.9 练习 467
后记 470
参考文献 476
・ ・ ・ ・ ・ ・ (收起)译者序
推荐序
作者介绍
关于审校人员
前言
第1章 引言 1
1.1 自然语言处理 1
1.2 基础应用 5
1.3 高级应用 6
1.4 NLP和Python相结合的优势 7
1.5 nltk环境搭建 7
1.6 读者提示 8
1.7 总结 9
第2章 实践理解语料库和数据集 10
2.1 语料库 10
2.2 语料库的作用 11
2.3 语料分析 13
2.4 数据属性的类型 16
2.4.1 分类或定性数据属性 16
2.4.2 数值或定量数据属性 17
2.5 不同文件格式的语料 18
2.6 免费语料库资源 19
2.7 为NLP应用准备数据集 20
2.7.1 挑选数据 20
2.7.2 预处理数据集 20
2.8 网页爬取 21
2.9 总结 23
第3章 理解句子的结构 24
3.1 理解NLP的组成 24
3.1.1 自然语言理解 24
3.1.2 自然语言生成 25
3.1.3 NLU和NLG的区别 25
3.1.4 NLP的分支 26
3.2 上下文无关文法 26
3.3 形态分析 28
3.3.1 形态学 28
3.3.2 词素 28
3.3.3 词干 28
3.3.4 形态分析 28
3.3.5 词 29
3.3.6 词素的分类 29
3.3.7 词干和词根的区别 32
3.4 词法分析 32
3.4.1 词条 33
3.4.2 词性标注 33
3.4.3 导出词条的过程 33
3.4.4 词干提取和词形还原的区别 34
3.4.5 应用 34
3.5 句法分析 34
3.6 语义分析 36
3.6.1 语义分析概念 36
3.6.2 词级别的语义 37
3.6.3 上下位关系和多义词 37
3.6.4 语义分析的应用 38
3.7 消歧 38
3.7.1 词法歧义 38
3.7.2 句法歧义 39
3.7.3 语义歧义 39
3.7.4 语用歧义 39
3.8 篇章整合 40
3.9 语用分析 40
3.10 总结 40
第4章 预处理 42
4.1 处理原始语料库文本 42
4.1.1 获取原始文本 42
4.1.2 小写化转换 44
4.1.3 分句 44
4.1.4 原始文本词干提取 46
4.1.5 原始文本词形还原 46
4.1.6 停用词去除 48
4.2 处理原始语料库句子 50
4.2.1 词条化 50
4.2.2 单词词形还原 51
4.3 基础预处理 52
4.4 实践和个性化预处理 57
4.4.1 由你自己决定 57
4.4.2 预处理流程 57
4.4.3 预处理的类型 57
4.4.4 理解预处理的案例 57
4.5 总结 62
第5章 特征工程和NLP算法 63
5.1 理解特征工程 64
5.1.1 特征工程的定义 64
5.1.2 特征工程的目的 64
5.1.3 一些挑战 65
5.2 NLP中的基础特征 65
5.2.1 句法分析和句法分析器 65
5.2.2 词性标注和词性标注器 81
5.2.3 命名实体识别 85
5.2.4 n元语法 88
5.2.5 词袋 89
5.2.6 语义工具及资源 91
5.3 NLP中的基础统计特征 91
5.3.1 数学基础 92
5.3.2 TF-IDF 96
5.3.3 向量化 99
5.3.4 规范化 100
5.3.5 概率模型 101
5.3.6 索引 103
5.3.7 排序 103
5.4 特征工程的优点 104
5.5 特征工程面临的挑战 104
5.6 总结 104
第6章 高级特征工程和NLP算法 106
6.1 词嵌入 106
6.2 word2vec基础 106
6.2.1 分布语义 107
6.2.2 定义word2vec 108
6.2.3 无监督分布语义模型中的必需品 108
6.3 word2vec模型从黑盒到白盒 109
6.4 基于表示的分布相似度 110
6.5 word2vec模型的组成部分 111
6.5.1 word2vec的输入 111
6.5.2 word2vec的输出 111
6.5.3 word2vec模型的构建模块 111
6.6 word2vec模型的逻辑 113
6.6.1 词汇表构建器 114
6.6.2 上下文环境构建器 114
6.6.3 两层的神经网络 116
6.6.4 算法的主要流程 119
6.7 word2vec模型背后的算法和数学理论 120
6.7.1 word2vec算法中的基本数学理论 120
6.7.2 词汇表构建阶段用到的技术 121
6.7.3 上下文环境构建过程中使用的技术 122
6.8 神经网络算法 123
6.8.1 基本神经元结构 123
6.8.2 训练一个简单的神经元 124
6.8.3 单个神经元的应用 126
6.8.4 多层神经网络 127
6.8.5 反向传播算法 127
6.8.6 word2vec背后的数学理论 128
6.9 生成最终词向量和概率预测结果的技术 130
6.10 word2vec相关的一些事情 131
6.11 word2vec的应用 131
6.11.1 实现一些简单例子 132
6.11.2 word2vec的优势 133
6.11.3 word2vec的挑战 133
6.11.4 在实际应用中使用word2vec 134
6.11.5 何时使用word2vec 135
6.11.6 开发一些有意思的东西 135
6.11.7 练习 138
6.12 word2vec概念的扩展 138
6.12.1 para2vec 139
6.12.2 doc2vec 139
6.12.3 doc2vec的应用 140
6.12.4 GloVe 140
6.12.5 练习 141
6.13 深度学习中向量化的重要性 141
6.14 总结 142
第7章 规则式自然语言处理系统 143
7.1 规则式系统 144
7.2 规则式系统的目的 146
7.2.1 为何需要规则式系统 146
7.2.2 使用规则式系统的应用 147
7.2.3 练习 147
7.2.4 开发规则式系统需要的资源 147
7.3 规则式系统的架构 148
7.3.1 从专家系统的角度来看规则式系统的通用架构 149
7.3.2 NLP应用中的规则式系统的实用架构 150
7.3.3 NLP应用中的规则式系统的定制架构 152
7.3.4 练习 155
7.3.5 Apache UIMA架构 155
7.4 规则式系统的开发周期 156
7.5 规则式系统的应用 156
7.5.1 使用规则式系统的NLP应用 156
7.5.2 使用规则式系统的通用AI应用 157
7.6 使用规则式系统来开发NLP应用 157
7.6.1 编写规则的思维过程 158
7.6.2 基于模板的聊天机器人应用 165
7.7 规则式系统与其他方法的对比 168
7.8 规则式系统的优点 169
7.9 规则式系统的缺点 169
7.10 规则式系统面临的挑战 170
7.11 词义消歧的基础 170
7.12 规则式系统近期发展的趋势 171
7.13 总结 171
第8章 自然语言处理中的机器学习方法 172
8.1 机器学习的基本概念 172
8.2 自然语言处理应用的开发步骤 176
8.2.1 第一次迭代时的开发步骤 177
8.2.2 从第二次到第N次迭代的开发步骤 177
8.3 机器学习算法和其他概念 179
8.3.1 有监督机器学习方法 179
8.3.2 无监督机器学习方法 206
8.3.3 半监督机器学习算法 210
8.3.4 一些重要概念 211
8.3.5 特征选择 215
8.3.6 维度约减 219
8.4 自然语言处理中的混合方法 221
8.5 总结 221
第9章 NLU和NLG问题中的深度学习 223
9.1 人工智能概览 223
9.1.1 人工智能的基础 223
9.1.2 人工智能的阶段 225
9.1.3 人工智能的种类 227
9.1.4 人工智能的目标和应用 227
9.2 NLU和NLG之间的区别 232
9.2.1 自然语言理解 232
9.2.2 自然语言生成 232
9.3 深度学习概览 233
9.4 神经网络基础 234
9.4.1 神经元的第一个计算模型 235
9.4.2 感知机 236
9.4.3 理解人工神经网络中的数学概念 236
9.5 实现神经网络 249
9.5.1 单层反向传播神经网络 249
9.5.2 练习 251
9.6 深度学习和深度神经网络 251
9.6.1 回顾深度学习 251
9.6.2 深度神经网络的基本架构 251
9.6.3 NLP中的深度学习 252
9.6.4 传统NLP和深度学习NLP技术的区别 253
9.7 深度学习技术和NLU 255
9.8 深度学习技术和NLG 262
9.8.1 练习 262
9.8.2 菜谱摘要和标题生成 262
9.9 基于梯度下降的优化 265
9.10 人工智能与人类智能 269
9.11 总结 269
第10章 高级工具 270
10.1 使用Apache Hadoop作为存储框架 270
10.2 使用Apache Spark作为数据处理框架 272
10.3 使用Apache Flink作为数据实时处理框架 274
10.4 Python中的可视化类库 274
10.5 总结 275
第11章 如何提高你的NLP技能 276
11.1 开始新的NLP职业生涯 276
11.2 备忘列表 277
11.3 确定你的领域 277
11.4 通过敏捷的工作来实现成功 278
11.5 NLP和数据科学方面一些有用的博客 278
11.6 使用公开的数据集 278
11.7 数据科学领域需要的数学知识 278
11.8 总结 279
第12章 安装指导 280
12.1 安装Python、pip和NLTK 280
12.2 安装PyCharm开发环境 280
12.3 安装依赖库 280
12.4 框架安装指导 281
12.5 解决你的疑问 281
12.6 总结 281
・ ・ ・ ・ ・ ・ (收起)译者序
作者简介
审校者简介
前言
第1章 NLP简介 1
1.1 什么是NLP 2
1.2 为何使用NLP 3
1.3 NLP的难点 4
1.4 NLP工具汇总 5
1.4.1 Apache OpenNLP 6
1.4.2 Stanford NLP 7
1.4.3 LingPipe 9
1.4.4 GATE 10
1.4.5 UIMA 10
1.5 文本处理概览 10
1.5.1 文本分词 11
1.5.2 文本断句 12
1.5.3 人物识别 14
1.5.4 词性判断 16
1.5.5 文本分类 17
1.5.6 关系提取 18
1.5.7 方法组合 20
1.6 理解NLP模型 20
1.6.1 明确目标 20
1.6.2 选择模型 21
1.6.3 构建、训练模型 21
1.6.4 验证模型 22
1.6.5 使用模型 22
1.7 准备数据 22
1.8 本章小结 24
第2章 文本分词 25
2.1 理解文本分词 25
2.2 什么是分词 26
2.3 一些简单的Java分词器 28
2.3.1 使用Scanner类 29
2.3.2 使用split方法 30
2.3.3 使用BreakIterator类 31
2.3.4 使用StreamTokenizer类 32
2.3.5 使用StringTokenizer类 34
2.3.6 使用Java核心分词法的性能考虑 34
2.4 NLP分词器的API 34
2.4.1 使用OpenNLPTokenizer类分词器 35
2.4.2 使用Stanford分词器 37
2.4.3 训练分词器进行文本分词 41
2.4.4 分词器的比较 44
2.5 理解标准化处理 45
2.5.1 转换为小写字母 45
2.5.2 去除停用词 46
2.5.3 词干化 49
2.5.4 词形还原 51
2.5.5 使用流水线进行标准化处理 54
2.6 本章小结 55
第3章 文本断句 56
3.1 SBD方法 56
3.2 SBD难在何处 57
3.3 理解LingPipe的HeuristicSen-tenceModel类的SBD规则 59
3.4 简单的Java SBD 60
3.4.1 使用正则表达式 60
3.4.2 使用BreakIterator类 62
3.5 使用NLP API 63
3.5.1 使用OpenNLP 64
3.5.2 使用Stanford API 66
3.5.3 使用LingPipe 74
3.6 训练文本断句模型 78
3.6.1 使用训练好的模型 80
3.6.2 使用SentenceDetector-Evaluator类评估模型 81
3.7 本章小结 82
第4章 人物识别 83
4.1 NER难在何处 84
4.2 NER的方法 84
4.2.1 列表和正则表达式 85
4.2.2 统计分类器 85
4.3 使用正则表达式进行NER 86
4.3.1 使用Java的正则表达式来寻找实体 86
4.3.2 使用LingPipe的RegEx-Chunker类 88
4.4 使用NLP API 89
4.4.1 使用OpenNLP进行NER 89
4.4.2 使用Stanford API进行NER 95
4.4.3 使用LingPipe进行NER 96
4.5 训练模型 100
4.6 本章小结 103
第5章 词性判断 104
5.1 词性标注 104
5.1.1 词性标注器的重要性 107
5.1.2 词性标注难在何处 107
5.2 使用NLP API 109
5.2.1 使用OpenNLP词性标注器 110
5.2.2 使用Stanford词性标注器 118
5.2.3 使用LingPipe词性标注器 125
5.2.4 训练OpenNLP词性标注模型 129
5.3 本章小结 131
第6章 文本分类 132
6.1 文本分类问题 132
6.2 情感分析介绍 134
6.3 文本分类技术 135
6.4 使用API进行文本分类 136
6.4.1 OpenNLP的使用 136
6.4.2 Stanford API的使用 140
6.4.3 使用LingPipe进行文本分类 145
6.5 本章小结 152
第7章 关系提取 153
7.1 关系类型 154
7.2 理解解析树 155
7.3 关系提取的应用 156
7.4 关系提取 159
7.5 使用NLP API 159
7.5.1 OpenNLP的使用 159
7.5.2 使用Stanford API 162
7.5.3 判断共指消解的实体 166
7.6 问答系统的关系提取 168
7.6.1 判断单词依赖关系 169
7.6.2 判断问题类型 170
7.6.3 搜索答案 171
7.7 本章小结 173
第8章 方法组合 174
8.1 准备数据 175
8.1.1 使用Boilerpipe从HTML中提取文本 175
8.1.2 使用POI从Word文档中提取文本 177
8.1.3 使用PDFBox从PDF文档中提取文本 181
8.2 流水线 182
8.2.1 使用Stanford流水线 182
8.2.2 在Standford流水线中使用多核处理器 187
8.3 创建一个文本搜索的流水线 188
8.4 本章小结 193
・ ・ ・ ・ ・ ・ (收起)第1 章基础入门 1
1.1 什么是自然语言处理 1
1.1.1 自然语言处理概述 1
1.1.2 自然语言处理的发展历史 3
1.1.3 自然语言处理的工作原理 6
1.1.4 自然语言处理的应用前景 7
1.2 开发工具与环境 7
1.2.1 Sublime Text 和Anaconda 介绍 7
1.2.2 开发环境的安装与配置 8
1.3 实战：第一个小程序的诞生 13
1.3.1 实例介绍 13
1.3.2 源码实现 13
第2 章快速上手Python 15
2.1 初识Python 编程语言 15
2.1.1 Python 概述 15
2.1.2 Python 能做什么 17
2.1.3 Python 的语法和特点 19
2.2 Python 进阶 24
2.2.1 Hello World 24
2.2.2 语句和控制流 24
2.2.3 函数 27
2.2.4 List 列表 29
2.2.5 元组 32
2.2.6 set 集合 33
2.2.7 字典 33
2.2.8 面向对象编程：类 34
2.2.9 标准库 36
2.3 Python 深入――第三方库 36
2.3.1 Web 框架 36
2.3.2 科学计算 37
2.3.3 GUI 37
2.3.4 其他库 37
第3 章线性代数 39
3.1 线性代数介绍 39
3.2 向量 40
3.2.1 向量定义 40
3.2.2 向量表示 42
3.2.3 向量定理 42
3.2.4 向量运算 43
3.3 矩阵 47
3.3.1 矩阵定义 47
3.3.2 矩阵表示 48
3.3.3 矩阵运算 48
3.3.4 线性方程组 51
3.3.5 行列式 51
3.3.6 特征值和特征向量 55
3.4 距离计算 56
3.4.1 余弦距离 56
3.4.2 欧氏距离 57
3.4.3 曼哈顿距离 58
3.4.4 明可夫斯基距离 59
3.4.5 切比雪夫距离 61
3.4.6 杰卡德距离 62
3.4.7 汉明距离 63
3.4.8 标准化欧式距离 64
3.4.9 皮尔逊相关系数 65
第4 章概率论 67
4.1 概率论介绍 67
4.2 事件 68
4.2.1 随机试验 68
4.2.2 随机事件和样本空间 69
4.2.3 事件的计算 70
4.3 概率 71
4.4 概率公理 73
4.5 条件概率和全概率 76
4.5.1 条件概率 76
4.5.2 全概率 77
4.6 贝叶斯定理 78
4.7 信息论 79
4.7.1 信息论的基本概念 79
4.7.2 信息度量 80
第5 章统计学 85
5.1 图形可视化 85
5.1.1 饼图 85
5.1.2 条形图 88
5.1.3 热力图 91
5.1.4 折线图 93
5.1.5 箱线图 96
5.1.6 散点图 99
5.1.7 雷达图 102
5.1.8 仪表盘 104
5.1.9 可视化图表用法 106
5.2 数据度量标准 108
5.2.1 平均值 108
5.2.2 中位数 108
5.2.3 众数 110
5.2.4 期望 111
5.2.5 方差 112
5.2.6 标准差 113
5.2.7 标准分 114
5.3 概率分布 115
5.3.1 几何分布 115
5.3.2 二项分布 116
5.3.3 正态分布 118
5.3.4 泊松分布 121
5.4 统计假设检验 123
5.5 相关和回归 125
5.5.1 相关 125
5.5.2 回归 127
5.5.3 相关和回归的联系 130
第6 章语言学 132
6.1 语音 132
6.1.1 什么是语音 132
6.1.2 语音的三大属性 133
6.1.3 语音单位 134
6.1.4 记音符号 135
6.1.5 共时语流音变 136
6.2 词汇 137
6.2.1 什么是词汇 137
6.2.2 词汇单位 137
6.2.3 词的构造 138
6.2.4 词义及其分类 140
6.2.5 义项与义素 141
6.2.6 语义场 142
6.2.7 词汇的构成 143
6.3 语法 143
6.3.1 什么是语法 143
6.3.2 词类 144
6.3.3 短语 148
6.3.4 单句 150
6.3.5 复句 152
第7 章自然语言处理 155
7.1 自然语言处理的任务和限制 155
7.2 自然语言处理的主要技术范畴 156
7.2.1 语音合成 156
7.2.2 语音识别 156
7.2.3 中文自动分词 157
7.2.4 词性标注 158
7.2.5 句法分析 158
7.2.6 文本分类 159
7.2.7 文本挖掘 160
7.2.8 信息抽取 161
7.2.9 问答系统 161
7.2.10 机器翻译 162
7.2.11 文本情感分析 163
7.2.12 自动摘要 164
7.2.13 文字蕴涵 165
7.3 自然语言处理的难点 165
7.3.1 语言环境复杂 165
7.3.2 文本结构形式多样 166
7.3.3 边界识别限制 166
7.3.4 词义消歧 167
7.3.5 指代消解 168
7.4 自然语言处理展望 169
第8 章语料库 173
8.1 语料库浅谈 173
8.2 语料库深入 174
8.3 自然语言处理工具包：NLTK 176
8.3.1 NLTK 简介 176
8.3.2 安装NLTK 177
8.3.3 使用NLTK 180
8.3.4 在Python NLTK 下使用Stanford NLP 186
8.4 获取语料库 194
8.4.1 国内外著名语料库 195
8.4.2 网络数据获取 197
8.4.3 NLTK 获取语料库 200
8.5 综合案例：走进大秦帝国 208
8.5.1 数据采集和预处理 208
8.5.2 构建本地语料库 208
8.5.3 大秦帝国语料操作 209
第9 章中文自动分词 216
9.1 中文分词简介 216
9.2 中文分词的特点和难点 218
9.3 常见中文分词方法 219
9.4 典型中文分词工具 220
9.4.1 HanLP 中文分词 220
9.4.2 其他中文分词工具 223
9.5 结巴中文分词 224
9.5.1 基于Python 的结巴中文分词 224
9.5.2 结巴分词工具详解 227
9.5.3 结巴分词核心内容 230
9.5.4 结巴分词基本用法 233
第10 章数据预处理 241
10.1 数据清洗 241
10.2 分词处理 242
10.3 特征构造 242
10.4 特征降维与选择 243
10.4.1 特征降维 243
10.4.2 特征选择 243
10.5 简单实例 244
10.6 本章小结 249
第11 章马尔可夫模型 250
11.1 马尔可夫链 250
11.1.1 马尔可夫简介 250
11.1.2 马尔可夫链的基本概念 251
11.2 隐马尔可夫模型 253
11.2.1 形式化描述 253
11.2.2 数学形式描述 255
11.3 向前算法解决HMM 似然度 256
11.3.1 向前算法定义 256
11.3.2 向前算法原理 256
11.3.3 现实应用：预测成都天气的冷热 258
11.4 文本序列标注案例：Viterbi 算法 259
第12 章条件随机场 263
12.1 条件随机场介绍 263
12.2 简单易懂的条件随机场 265
12.2.1 CRF 的形式化表示 265
12.2.2 CRF 的公式化表示 266
12.2.3 深度理解条件随机场 268
第13 章模型评估 269
13.1 从统计角度介绍模型概念 269
13.1.1 算法模型 269
13.1.2 模型评估和模型选择 270
13.1.3 过拟合与欠拟合的模型选择 272
13.2 模型评估与选择 275
13.2.1 模型评估的概念 275
13.2.2 模型评估的评测指标 275
13.2.3 以词性标注为例分析模型评估 276
13.2.4 模型评估的几种方法 278
13.3 ROC 曲线比较学习器模型 279
第14 章命名实体识别 281
14.1 命名实体识别概述 281
14.2 命名实体识别的特点与难点 284
14.3 命名实体识别方法 284
14.4 中文命名实体识别的核心技术 286
14.5 展望 295
第15 章自然语言处理实战 296
15.1 GitHub 数据提取与可视化分析 296
15.1.1 了解GitHub 的API 296
15.1.2 使用NetworkX 作图 299
15.1.3 使用NetworkX 构建兴趣图 301
15.1.4 NetWorkX 部分统计指标 304
15.1.5 构建GitHub 的兴趣图 305
15.1.6 可视化 318
15.2 微博话题爬取与存储分析 320
15.2.1 数据采集 320
15.2.2 数据提取 329
15.2.3 数据存储 332
15.2.4 项目运行与分析 333
附录A Python 与其他语言调用 337
附录B Git 项目上传简易教程 339
参考文献 341
・ ・ ・ ・ ・ ・ (收起)模块1　NLTK基础知识
第　1章 自然语言处理简介　3
1.1　为什么要学习NLP　4
1.2　从Python的基本知识开始　7
1.2.1　列表　7
1.2.2　自助　8
1.2.3　正则表达式　9
1.2.4　词典　11
1.2.5　编写函数　11
1.3　NLTK　13
1.4　试一试　18
1.5　本章小结　18
第　2章 文本的整理和清洗　19
2.1　文本整理　19
2.2　文本清洗　21
2.3　句子拆分器　22
2.4　标记解析　22
2.5　词干提取　24
2.6　词形还原　25
2.7　停用词删除　26
2.8　生僻字删除　27
2.9　拼写校正　27
2.10　试一试　28
2.11　本章小结　28
第3章　词性标注　30
3.1　什么是词性标注　30
3.1.1　斯坦福标注器　33
3.1.2　深入了解标注器　34
3.1.3　序列标注器　35
3.1.4　布里尔标注器　37
3.1.5　基于标注器的机器学习　37
3.2　命名实体识别　38
3.3　试一试　40
3.4　本章小结　41
第4章　对文本的结构进行语法分析　42
4.1　浅层语法分析与深层语法
分析　42
4.2　语法分析的两种方法　43
4.3　为什么需要语法分析　43
4.4　不同类型的语法分析器　45
4.4.1　递归下降的语法分析器　45
4.4.2　移位归约语法分析器　45
4.4.3　图表语法分析器　45
4.4.4　正则表达式语法
分析器　46
4.5　依存分析　47
4.6　组块化　49
4.7　信息抽取　51
4.7.1　命名实体识别　52
4.7.2　关系抽取　52
4.8　本章小结　53
第5章　NLP应用　54
5.1　构建第 一个NLP应用　54
5.2　其他的NLP应用　58
5.2.1　机器翻译　58
5.2.2　统计机器翻译　59
5.2.3　信息检索　59
5.2.4　语音识别　61
5.2.5　文本分类　62
5.2.6　信息提取　63
5.2.7　问答系统　64
5.2.8　对话系统　64
5.2.9　词义消歧　64
5.2.10　主题建模　64
5.2.11　语言检测　65
5.2.12　光学字符识别　65
5.3　本章小结　65
第6章　文本分类　66
6.1　机器学习　67
6.2　文本分类　68
6.3　采样　70
6.3.1　朴素贝叶斯　73
6.3.2　决策树　75
6.3.3　随机梯度下降　76
6.3.4　逻辑回归　77
6.3.5　支持向量机　78
6.4　随机森林算法　79
6.5　文本聚类　79
6.6　文本的主题建模　81
6.7　参考资料　83
6.8　本章小结　83
第7章　网络爬取　85
7.1　网络爬虫　85
7.2　编写第 一个爬虫程序　86
7.3　Scrapy中的数据流　89
7.3.1　Scrapy命令行界面　89
7.3.2　项　94
7.4　站点地图蜘蛛　96
7.5　项管道　97
7.6　外部参考　98
7.7　本章小结　99
第8章　与其他Python库一同
使用NLTK　100
8.1　NumPy　100
8.1.1　ndarray　101
8.1.2　基本操作　102
8.1.3　从数组中提取数据　103
8.1.4　复杂的矩阵运算　103
8.2　SciPy　107
8.2.1　线性代数　108
8.2.2　特征值和特征向量　108
8.2.3　稀疏矩阵　109
8.2.4　优化　110
8.3　Pandas　111
8.3.1　读取数据　112
8.3.2　时序数据　114
8.3.3　列转换　115
8.3.4　噪声数据　116
8.4　Matplotlib　117
8.4.1　subplot　118
8.4.2　添加轴　119
8.4.3　散点图　120
8.4.4　柱状图　120
8.4.5　3D图　121
8.5　外部参考　121
8.6　本章小结　121
第9章　使用Python进行社交媒体
挖掘　122
9.1　数据收集　122
9.2　数据提取　126
9.3　地理可视化　128
9.3.1　影响者检测　129
9.3.2　Facebook　130
9.3.3　影响者的朋友　134
9.4　本章小结　135
第　10章 大规模的文本挖掘　136
10.1　在Hadoop上使用Python的
不同方法　136
10.1.1　Python的流　137
10.1.2　Hive/Pig UDF　137
10.1.3　流包装器　137
10.2　在Hadoop上运行NLTK　138
10.2.1　UDF　138
10.2.2　Python流　140
10.3　在Hadoop上运行
Scikit-learn　141
10.4　PySpark　144
10.5　本章小结　146
模块2　使用Python 3的NLTK 3进行文本处理
第　1章 标记文本和WordNet的基础　149
1.1　引言　149
1.2　将文本标记成句子　150
1.2.1　准备工作　150
1.2.2　工作方式　151
1.2.3　工作原理　151
1.2.4　更多信息　151
1.2.5　请参阅　152
1.3　将句子标记成单词　152
1.3.1　工作方式　152
1.3.2　工作原理　153
1.3.3　更多信息　153
1.3.4　请参阅　154
1.4　使用正则表达式标记语句　154
1.4.1　准备工作　155
1.4.2　工作方式　155
1.4.3　工作原理　155
1.4.4　更多信息　155
1.4.5　请参阅　156
1.5　训练语句标记生成器　156
1.5.1　准备工作　156
1.5.2　工作方式　156
1.5.3　工作原理　157
1.5.4　更多信息　158
1.5.5　请参阅　158
1.6　在已标记的语句中过滤
停用词　158
1.6.1　准备工作　158
1.6.2　工作方式　159
1.6.3　工作原理　159
1.6.4　更多信息　159
1.6.5　请参阅　160
1.7　查找WordNet中单词的
Synset　160
1.7.1　准备工作　160
1.7.2　工作方式　160
1.7.3　工作原理　161
1.7.4　更多信息　161
1.7.5　请参阅　163
1.8　在WordNet中查找词元和
同义词　163
1.8.1　工作方式　163
1.8.2　工作原理　163
1.8.3　更多信息　163
1.8.4　请参阅　165
1.9　计算WordNet和Synset的
相似度　165
1.9.1　工作方式　165
1.9.2　工作原理　165
1.9.3　更多信息　166
1.9.4　请参阅　167
1.10　发现单词搭配　167
1.10.1　准备工作　167
1.10.2　工作方式　167
1.10.3　工作原理　168
1.10.4　更多信息　168
1.10.5　请参阅　169
第　2章 替换和校正单词　170
2.1　引言　170
2.2　词干提取　170
2.2.1　工作方式　171
2.2.2　工作原理　171
2.2.3　更多信息　171
2.2.4　请参阅　173
2.3　使用WordNet进行词形还原　173
2.3.1　准备工作　173
2.3.2　工作方式　173
2.3.3　工作原理　174
2.3.4　更多信息　174
2.3.5　请参阅　175
2.4　基于匹配的正则表达式替换
单词　175
2.4.1　准备工作　175
2.4.2　工作方式　175
2.4.3　工作原理　176
2.4.4　更多信息　177
2.4.5　请参阅　177
2.5　移除重复字符　177
2.5.1　准备工作　177
2.5.2　工作方式　178
2.5.3　工作原理　178
2.5.4　更多信息　179
2.5.5　请参阅　179
2.6　使用Enchant进行拼写校正　180
2.6.1　准备工作　180
2.6.2　工作方式　180
2.6.3　工作原理　181
2.6.4　更多信息　181
2.6.5　请参阅　183
2.7　替换同义词　183
2.7.1　准备工作　183
2.7.2　工作方式　183
2.7.3　工作原理　184
2.7.4　更多信息　184
2.7.5　请参阅　185
2.8　使用反义词替换否定形式　186
2.8.1　工作方式　186
2.8.2　工作原理　187
2.8.3　更多信息　187
2.8.4　请参阅　188
第3章　创建自定义语料库　189
3.1　引言　189
3.2　建立自定义语料库　190
3.2.1　准备工作　190
3.2.2　工作方式　190
3.2.3　工作原理　191
3.2.4　更多信息　192
3.2.5　请参阅　192
3.3　创建词汇表语料库　192
3.3.1　准备工作　192
3.3.2　工作方式　193
3.3.3　工作原理　193
3.3.4　更多信息　194
3.3.5　请参阅　194
3.4　创建已标记词性单词的
语料库　195
3.4.1　准备工作　195
3.4.2　工作方式　195
3.4.3　工作原理　196
3.4.4　更多信息　196
3.4.5　请参阅　199
3.5　创建已组块短语的语料库　199
3.5.1　准备工作　199
3.5.2　工作方式　199
3.5.3　工作原理　201
3.5.4　更多信息　201
3.5.5　请参阅　203
3.6　创建已分类文本的语料库　203
3.6.1　准备工作　204
3.6.2　工作方式　204
3.6.3　工作原理　204
3.6.4　更多信息　205
3.6.5　请参阅　206
3.7　创建已分类组块语料库
读取器　206
3.7.1　准备工作　206
3.7.2　工作方式　207
3.7.3　工作原理　208
3.7.4　更多信息　209
3.7.5　请参阅　213
3.8　懒惰语料库加载　213
3.8.1　工作方式　213
3.8.2　工作原理　214
3.8.3　更多信息　214
3.9　创建自定义语料库视图　215
3.9.1　工作方式　215
3.9.2　工作原理　216
3.9.3　更多信息　217
3.9.4　请参阅　218
3.10　创建基于MongoDB的
语料库读取器　218
3.10.1　准备工作　219
3.10.2　工作方式　219
3.10.3　工作原理　220
3.10.4　更多信息　221
3.10.5　请参阅　221
3.11　在加锁文件的情况下编辑
语料库　221
3.11.1　准备工作　221
3.11.2　工作方式　221
3.11.3　工作原理　222
第4章　词性标注　224
4.1　引言　224
4.2　默认标注　225
4.2.1　准备工作　225
4.2.2　工作方式　225
4.2.3　工作原理　226
4.2.4　更多信息　227
4.2.5　请参阅　228
4.3　训练一元组词性标注器　228
4.3.1　工作方式　228
4.3.2　工作原理　229
4.3.3　更多信息　230
4.3.4　请参阅　231
4.4　回退标注的组合标注器　231
4.4.1　工作方式　231
4.4.2　工作原理　232
4.4.3　更多信息　232
4.4.4　请参阅　233
4.5　训练和组合N元标注器　233
4.5.1　准备工作　233
4.5.2　工作方式　233
4.5.3　工作原理　234
4.5.4　更多信息　235
4.5.5　请参阅　236
4.6　创建似然单词标签的
模型　236
4.6.1　工作方式　236
4.6.2　工作原理　237
4.6.3　更多信息　237
4.6.4　请参阅　238
4.7　使用正则表达式标注　238
4.7.1　准备工作　238
4.7.2　工作方式　238
4.7.3　工作原理　239
4.7.4　更多信息　239
4.7.5　请参阅　239
4.8　词缀标签　239
4.8.1　工作方式　239
4.8.2　工作原理　240
4.8.3　更多信息　240
4.8.4　请参阅　241
4.9　训练布里尔标注器　241
4.9.1　工作方式　241
4.9.2　工作原理　242
4.9.3　更多信息　243
4.9.4　请参阅　244
4.10　训练TnT标注器　244
4.10.1　工作方式　244
4.10.2　工作原理　244
4.10.3　更多信息　245
4.10.4　请参阅　246
4.11　使用WordNet进行
标注　246
4.11.1　准备工作　246
4.11.2　工作方式　247
4.11.3　工作原理　248
4.11.4　请参阅　248
4.12　标注专有名词　248
4.12.1　工作方式　248
4.12.2　工作原理　249
4.12.3　请参阅　249
4.13　基于分类器的标注　249
4.13.1　工作方式　250
4.13.2　工作原理　250
4.13.3　更多信息　251
4.13.4　请参阅　252
4.14　使用NLTK训练器训练
标注器　253
4.14.1　工作方式　253
4.14.2　工作原理　254
4.14.3　更多信息　258
4.14.4　请参阅　260
第5章　提取组块　261
5.1　引言　261
5.2　使用正则表达式组块和
隔断　262
5.2.1　准备工作　262
5.2.2　工作方式　262
5.2.3　工作原理　263
5.2.4　更多信息　265
5.2.5　请参阅　267
5.3　使用正则表达式合并和拆分
组块　267
5.3.1　工作方式　267
5.3.2　工作原理　269
5.3.3　更多信息　270
5.3.4　请参阅　271
5.4　使用正则表达式扩展和删除
组块　271
5.4.1　工作方式　271
5.4.2　工作原理　272
5.4.3　更多信息　273
5.4.4　请参阅　273
5.5　使用正则表达式进行部分
解析　273
5.5.1　工作方式　273
5.5.2　工作原理　274
5.5.3　更多信息　275
5.5.4　请参阅　276
5.6　训练基于标注器的组块器　276
5.6.1　工作方式　276
5.6.2　工作原理　277
5.6.3　更多信息　278
5.6.4　请参阅　279
5.7　基于分类的分块　279
5.7.1　工作方式　279
5.7.2　工作原理　282
5.7.3　更多信息　282
5.7.4　请参阅　283
5.8　提取命名实体　283
5.8.1　工作方式　283
5.8.2　工作原理　284
5.8.3　更多信息　284
5.8.4　请参阅　285
5.9　提取专有名词组块　285
5.9.1　工作方式　286
5.9.2　工作原理　286
5.9.3　更多信息　286
5.10　提取部位组块　287
5.10.1　工作方式　288
5.10.2　工作原理　290
5.10.3　更多信息　290
5.10.4　请参阅　290
5.11　训练命名实体组块器　290
5.11.1　工作方式　290
5.11.2　工作原理　292
5.11.3　更多信息　292
5.11.4　请参阅　293
5.12　使用NLTK训练器训练
组块器　293
5.12.1　工作方式　293
5.12.2　工作原理　294
5.12.3　更多信息　295
5.12.4　请参阅　299
第6章　转换组块与树　300
6.1　引言　300
6.2　过滤句子中无意义的
单词　301
6.2.1　准备工作　301
6.2.2　工作方式　301
6.2.3　工作原理　302
6.2.4　更多信息　302
6.2.5　请参阅　303
6.3　纠正动词形式　303
6.3.1　准备工作　303
6.3.2　工作方式　303
6.3.3　工作原理　305
6.3.4　请参阅　306
6.4　交换动词短语　306
6.4.1　工作方式　306
6.4.2　工作原理　307
6.4.3　更多信息　307
6.4.4　请参阅　307
6.5　交换名词基数　308
6.5.1　工作方式　308
6.5.2　工作原理　309
6.5.3　请参阅　309
6.6　交换不定式短语　309
6.6.1　工作方式　309
6.6.2　工作原理　310
6.6.3　更多信息　310
6.6.4　请参阅　310
6.7　单数化复数名词　310
6.7.1　工作方式　310
6.7.2　工作原理　311
6.7.3　请参阅　311
6.8　链接组块变换　311
6.8.1　工作方式　311
6.8.2　工作原理　312
6.8.3　更多信息　312
6.8.4　请参阅　313
6.9　将组块树转换为文本　313
6.9.1　工作方式　313
6.9.2　工作原理　314
6.9.3　更多信息　314
6.9.4　请参阅　314
6.10　平展深度树　314
6.10.1　准备工作　315
6.10.2　工作方式　315
6.10.3　工作原理　316
6.10.4　更多信息　317
6.10.5　请参阅　318
6.11　创建浅树　318
6.11.1　工作方式　318
6.11.2　工作原理　320
6.11.3　请参阅　320
6.12　转换树标签　320
6.12.1　准备工作　320
6.12.2　工作方式　321
6.12.3　工作原理　322
6.12.4　请参阅　322
第7章　文本分类　323
7.1　引言　323
7.2　词袋特征提取　324
7.2.1　工作方式　324
7.2.2　工作原理　325
7.2.3　更多信息　325
7.2.4　请参阅　327
7.3　训练朴素贝叶斯
分类器　327
7.3.1　准备工作　327
7.3.2　工作方式　328
7.3.3　工作原理　329
7.3.4　更多信息　330
7.3.5　请参阅　333
7.4　训练决策树分类器　334
7.4.1　工作方式　334
7.4.2　工作原理　335
7.4.3　更多信息　335
7.4.4　请参阅　337
7.5　训练最大熵分类器　337
7.5.1　准备工作　337
7.5.2　工作方式　337
7.5.3　工作原理　338
7.5.4　更多信息　339
7.5.5　请参阅　340
7.6　训练scikit-learn
分类器　340
7.6.1　准备工作　341
7.6.2　工作方式　341
7.6.3　工作原理　342
7.6.4　更多信息　343
7.6.5　请参阅　345
7.7　衡量分类器的精准率和
召回率　346
7.7.1　工作方式　346
7.7.2　工作原理　347
7.7.3　更多信息　348
7.7.4　请参阅　349
7.8　计算高信息量单词　349
7.8.1　工作方式　350
7.8.2　工作原理　351
7.8.3　更多信息　352
7.8.4　请参阅　354
7.9　使用投票组合分类器　354
7.9.1　准备工作　355
7.9.2　工作方式　355
7.9.3　工作原理　356
7.9.4　请参阅　356
7.10　使用多个二元分类器
分类　357
7.10.1　准备工作　357
7.10.2　工作方式　357
7.10.3　工作原理　361
7.10.4　更多信息　362
7.10.5　请参阅　363
7.11　使用NLTK训练器训练
分类器　363
7.11.1　工作方式　363
7.11.2　工作原理　364
7.11.3　更多信息　365
7.11.4　请参阅　371
第8章　分布式进程和大型数据集的
处理　372
8.1　引言　372
8.2　使用execnet进行分布式
标注　372
8.2.1　准备工作　373
8.2.2　工作方式　373
8.2.3　工作原理　374
8.2.4　更多内容　375
8.2.5　请参阅　377
8.3　使用execnet进行分布式
组块　377
8.3.1　准备工作　377
8.3.2　工作方式　377
8.3.3　工作原理　378
8.3.4　更多内容　379
8.3.5　请参阅　379
8.4　使用execnet并行处理
列表　379
8.4.1　工作方式　379
8.4.2　工作原理　380
8.4.3　更多内容　381
8.4.4　请参阅　381
8.5　在Redis中存储频率分布　382
8.5.1　准备工作　382
8.5.2　工作方式　382
8.5.3　工作原理　384
8.5.4　更多内容　385
8.5.5　请参阅　386
8.6　在Redis中存储条件频率
分布　386
8.6.1　准备工作　386
8.6.2　工作方式　386
8.6.3　工作原理　387
8.6.4　更多内容　388
8.6.5　请参阅　388
8.7　在Redis中存储有序
字典　388
8.7.1　准备工作　388
8.7.2　工作方式　388
8.7.3　工作原理　390
8.7.4　更多内容　391
8.7.5　请参阅　392
8.8　使用Redis和execnet进行
分布式单词评分　392
8.8.1　准备工作　392
8.8.2　工作方式　392
8.8.3　工作原理　393
8.8.4　更多内容　396
8.8.5　请参阅　396
第9章　解析特定的数据类型　397
9.1　引言　397
9.2　使用dateutil解析日期和
时间　398
9.2.1　准备工作　398
9.2.2　工作方式　398
9.2.3　工作原理　399
9.2.4　更多信息　399
9.2.5　请参阅　399
9.3　时区的查找和转换　400
9.3.1　准备工作　400
9.3.2　工作方式　400
9.3.3　工作原理　402
9.3.4　更多信息　402
9.3.5　请参阅　403
9.4　使用lxml从HTML中提取
URL　403
9.4.1　准备工作　403
9.4.2　工作方式　403
9.4.3　工作原理　404
9.4.4　更多信息　404
9.4.5　请参阅　405
9.5　清理和剥离HTML　405
9.5.1　准备工作　405
9.5.2　工作方式　405
9.5.3　工作原理　405
9.5.4　更多信息　406
9.5.5　请参阅　406
9.6　使用BeautifulSoup转换
HTML实体　406
9.6.1　准备工作　406
9.6.2　工作方式　406
9.6.3　工作原理　407
9.6.4　更多信息　407
9.6.5　请参阅　407
9.7　检测和转换字符编码　407
9.7.1　准备工作　408
9.7.2　工作方式　408
9.7.3　工作原理　409
9.7.4　更多信息　409
9.7.5　请参阅　410
附录A　宾州treebank词性标签　411
模块3　使用Python掌握自然语言处理
第　1章 使用字符串　417
1.1　标记化　417
1.1.1　将文本标记为句子　418
1.1.2　其他语言文字的标记化　418
1.1.3　将句子标记为单词　419
1.1.4　使用TreebankWordTokenizer
进行标记化　420
1.1.5　使用正则表达式进行
标记化　421
1.2　规范化　424
1.2.1　消除标点符号　424
1.2.2　转化为小写和大写　425
1.2.3　处理停用词　425
1.2.4　计算英语中的停用词　426
1.3　替代和纠正标记　427
1.3.1　使用正则表达式替换
单词　427
1.3.2　使用一个文本替换另一个
文本的示例　428
1.3.3　在标记化之前进行
替代　428
1.3.4　处理重复的字符　428
1.3.5　删除重复字符的示例　429
1.3.6　使用单词的同义词替换
单词　430
1.4　在文本上应用齐夫定律　431
1.5　相似性量度　431
1.5.1　使用编辑距离算法应用
相似性量度　432
1.5.2　使用杰卡德系数应用
相似性量度　434
1.5.3　使用史密斯-沃特曼算法
应用相似性量度　434
1.5.4　其他字符串相似性指标　435
1.6　本章小结　436
第　2章 统计语言模型　437
2.1　单词频率　437
2.1.1　对给定文本进行最大
似然估计　441
2.1.2　隐马尔可夫模型估计　448
2.2　在MLE模型上应用平滑　450
2.2.1　加一平滑法　450
2.2.2　古德-图灵算法　451
2.2.3　聂氏估计　456
2.2.4　威滕 贝尔估计　457
2.3　为MLE指定回退机制　457
2.4　应用数据插值获得混合和
匹配　458
2.5　应用困惑度评估语言模型　458
2.6　在建模语言中应用
梅特罗波利斯-黑斯廷斯算法　459
2.7　在语言处理中应用
吉布斯采样　459
2.8　本章小结　461
第3章　词语形态学―试一试　462
3.1　词语形态学　462
3.2　词根还原器　463
3.3　词形还原　466
3.4　开发用于非英语语言的词根
还原器　467
3.5　词语形态分析器　469
3.6　词语形态生成器　471
3.7　搜索引擎　471
3.8　本章小结　475
第4章　词性标注―识别单词　476
4.1　词性标注　476
4.2　创建POS标注的语料库　482
4.3　选择某个机器学习算法　484
4.4　涉及n元组方法的统计建模　486
4.5　使用POS标注的语料库开发
组块器　491
4.6　本章小结　494
第5章　解析―分析训练数据　495
5.1　解析　495
5.2　构建树库　496
5.3　从树库中提取上下文无关文法的
规则　501
5.4　从CFG中创建概率上下文无关的
文法　507
5.5　CYK图解析算法　509
5.6　厄雷图解析算法　510
5.7　本章小结　516
第6章　语义分析―意义重大　517
6.1　语义分析　517
6.1.1　NER简介　521
6.1.2　使用隐马尔可夫模型的
NER系统　525
6.1.3　使用机器学习工具包训练
NER　530
6.1.4　使用POS标注的
NER　531
6.2　从Wordnet中生成同义词集
ID　534
6.3　使用Wordnet消除歧义　537
6.4　本章小结　541
第7章　情感分析―我很高兴　542
7.1　情感分析　542
7.2　使用机器学习的情感分析　548
7.3　本章小结　572
第8章　信息检索―访问信息　573
8.1　信息检索　573
8.1.1　停用词删除　574
8.1.2　利用向量空间模型进行
信息检索　576
8.2　向量空间评分以及与查询
操作器交互　583
8.3　利用隐含语义索引开发IR
系统　586
8.4　文本摘要　587
8.5　问答系统　588
8.6　本章小结　589
第9章　话语分析―知识就是信仰　590
9.1　话语分析　590
9.1.1　使用定中心理论进行
话语分析　595
9.1.2　回指解析　596
9.2　本章小结　601
第　10章 NLP系统的评估―
性能分析　602
10.1　对NLP系统进行评估的
需求　602
10.1.1　NLP工具（POS标注器、
词干还原器和形态分析器）
的评估　603
10.1.2　使用黄金数据评估
解析器　613
10.2　IR系统的评估　614
10.3　错误识别的指标　614
10.4　基于词汇匹配的指标　615
10.5　基于语法匹配的指标　619
10.6　使用浅层语义匹配的
指标　620
10.7　本章小结　621
参考书目　622
・ ・ ・ ・ ・ ・ (收起)第1章 中文语言的机器处理 1
1.1 历史回顾 2
1.1.1 从科幻到现实 2
1.1.2 早期的探索 3
1.1.3 规则派还是统计派 3
1.1.4 从机器学习到认知计算 5
1.2 现代自然语言系统简介 6
1.2.1 NLP流程与开源框架 6
1.2.2 哈工大NLP平台及其演示环境 9
1.2.3 Stanford NLP团队及其演示环境 11
1.2.4 NLTK开发环境 13
1.3 整合中文分词模块 16
1.3.1 安装Ltp Python组件 17
1.3.2 使用Ltp 3.3进行中文分词 18
1.3.3 使用结巴分词模块 20
1.4 整合词性标注模块 22
1.4.1 Ltp 3.3词性标注 23
1.4.2 安装StanfordNLP并编写Python接口类 24
1.4.3 执行Stanford词性标注 28
1.5 整合命名实体识别模块 29
1.5.1 Ltp 3.3命名实体识别 29
1.5.2 Stanford命名实体识别 30
1.6 整合句法解析模块 32
1.6.1 Ltp 3.3句法依存树 33
1.6.2 Stanford Parser类 35
1.6.3 Stanford短语结构树 36
1.6.4 Stanford依存句法树 37
1.7 整合语义角色标注模块 38
1.8 结语 40
第2章 汉语语言学研究回顾 42
2.1 文字符号的起源 42
2.1.1 从记事谈起 43
2.1.2 古文字的形成 47
2.2 六书及其他 48
2.2.1 象形 48
2.2.2 指事 50
2.2.3 会意 51
2.2.4 形声 53
2.2.5 转注 54
2.2.6 假借 55
2.3 字形的流变 56
2.3.1 笔与墨的形成与变革 56
2.3.2 隶变的方式 58
2.3.3 汉字的符号化与结构 61
2.4 汉语的发展 67
2.4.1 完整语义的基本形式DD句子 68
2.4.2 语言的初始形态与文言文 71
2.4.3 白话文与复音词 73
2.4.4 白话文与句法研究 78
2.5 三个平面中的语义研究 80
2.5.1 词汇与本体论 81
2.5.2 格语法及其框架 84
2.6 结语 86
第3章 词汇与分词技术 88
3.1 中文分词 89
3.1.1 什么是词与分词规范 90
3.1.2 两种分词标准 93
3.1.3 歧义、机械分词、语言模型 94
3.1.4 词汇的构成与未登录词 97
3.2 系统总体流程与词典结构 98
3.2.1 概述 98
3.2.2 中文分词流程 99
3.2.3 分词词典结构 103
3.2.4 命名实体的词典结构 105
3.2.5 词典的存储结构 108
3.3 算法部分源码解析 111
3.3.1 系统配置 112
3.3.2 Main方法与例句 113
3.3.3 句子切分 113
3.3.4 分词流程 117
3.3.5 一元词网 118
3.3.6 二元词图 125
3.3.7 NShort算法原理 130
3.3.8 后处理规则集 136
3.3.9 命名实体识别 137
3.3.10 细分阶段与最短路径 140
3.4 结语 142
第4章 NLP中的概率图模型 143
4.1 概率论回顾 143
4.1.1 多元概率论的几个基本概念 144
4.1.2 贝叶斯与朴素贝叶斯算法 146
4.1.3 文本分类 148
4.1.4 文本分类的实现 151
4.2 信息熵 154
4.2.1 信息量与信息熵 154
4.2.2 互信息、联合熵、条件熵 156
4.2.3 交叉熵和KL散度 158
4.2.4 信息熵的NLP的意义 159
4.3 NLP与概率图模型 160
4.3.1 概率图模型的几个基本问题 161
4.3.2 产生式模型和判别式模型 162
4.3.3 统计语言模型与NLP算法设计 164
4.3.4 极大似然估计 167
4.4 隐马尔科夫模型简介 169
4.4.1 马尔科夫链 169
4.4.2 隐马尔科夫模型 170
4.4.3 HMMs的一个实例 171
4.4.4 Viterbi算法的实现 176
4.5 最大熵模型 179
4.5.1 从词性标注谈起 179
4.5.2 特征和约束 181
4.5.3 最大熵原理 183
4.5.4 公式推导 185
4.5.5 对偶问题的极大似然估计 186
4.5.6 GIS实现 188
4.6 条件随机场模型 193
4.6.1 随机场 193
4.6.2 无向图的团（Clique）与因子分解 194
4.6.3 线性链条件随机场 195
4.6.4 CRF的概率计算 198
4.6.5 CRF的参数学习 199
4.6.6 CRF预测标签 200
4.7 结语 201
第5章 词性、语块与命名实体识别 202
5.1 汉语词性标注 203
5.1.1 汉语的词性 203
5.1.2 宾州树库的词性标注规范 205
5.1.3 stanfordNLP标注词性 210
5.1.4 训练模型文件 213
5.2 语义组块标注 219
5.2.1 语义组块的种类 220
5.2.2 细说NP 221
5.2.3 细说VP 223
5.2.4 其他语义块 227
5.2.5 语义块的抽取 229
5.2.6 CRF的使用 232
5.3 命名实体识别 240
5.3.1 命名实体 241
5.3.2 分词架构与专名词典 243
5.3.3 算法的策略DD词典与统计相结合 245
5.3.4 算法的策略DD层叠式架构 252
5.4 结语 259
第6章 句法理论与自动分析 260
6.1 转换生成语法 261
6.1.1 乔姆斯基的语言观 261
6.1.2 短语结构文法 263
6.1.3 汉语句类 269
6.1.4 谓词论元与空范畴 274
6.1.5 轻动词分析理论 279
6.1.6 NLTK操作句法树 280
6.2 依存句法理论 283
6.2.1 配价理论 283
6.2.2 配价词典 285
6.2.3 依存理论概述 287
6.2.4 Ltp依存分析介绍 290
6.2.5 Stanford依存转换、解析 293
6.3 PCFG短语结构句法分析 298
6.3.1 PCFG短语结构 298
6.3.2 内向算法和外向算法 301
6.3.3 Viterbi算法 303
6.3.4 参数估计 304
6.3.5 Stanford 的PCFG算法训练 305
6.4 结语 310
第7章 建设语言资源库 311
7.1 语料库概述 311
7.1.1 语料库的简史 312
7.1.2 语言资源库的分类 314
7.1.3 语料库的设计实例：国家语委语料库 315
7.1.4 语料库的层次加工 321
7.2 语法语料库 323
7.2.1 中文分词语料库 323
7.2.2 中文分词的测评 326
7.2.3 宾州大学CTB简介 327
7.3 语义知识库 333
7.3.1 知识库与HowNet简介 333
7.3.2 发掘义原 334
7.3.3 语义角色 336
7.3.4 分类原则与事件分类 344
7.3.5 实体分类 347
7.3.6 属性与分类 352
7.3.7 相似度计算与实例 353
7.4 语义网与百科知识库 360
7.4.1 语义网理论介绍 360
7.4.2 维基百科知识库 364
7.4.3 DBpedia抽取原理 365
7.5 结语 368
第8章 语义与认知 370
8.1 回顾现代语义学 371
8.1.1 语义三角论 371
8.1.2 语义场论 373
8.1.3 基于逻辑的语义学 376
8.2 认知语言学概述 377
8.2.1 象似性原理 379
8.2.2 顺序象似性 380
8.2.3 距离象似性 380
8.2.4 重叠象似性 381
8.3 意象图式的构成 383
8.3.1 主观性与焦点 383
8.3.2 范畴化：概念的认知 385
8.3.3 主体与背景 390
8.3.4 意象图式 392
8.3.5 社交中的图式 396
8.3.6 完形：压缩与省略 398
8.4 隐喻与转喻 401
8.4.1 隐喻的结构 402
8.4.2 隐喻的认知本质 403
8.4.3 隐喻计算的系统架构 405
8.4.4 隐喻计算的实现 408
8.5 构式语法 412
8.5.1 构式的概念 413
8.5.2 句法与构式 415
8.5.3 构式知识库 417
8.6 结语 420
第9章 NLP中的深度学习 422
9.1 神经网络回顾 422
9.1.1 神经网络框架 423
9.1.2 梯度下降法推导 425
9.1.3 梯度下降法的实现 427
9.1.4 BP神经网络介绍和推导 430
9.2 Word2Vec简介 433
9.2.1 词向量及其表达 434
9.2.2 Word2Vec的算法原理 436
9.2.3 训练词向量 439
9.2.4 大规模上下位关系的自动识别 443
9.3 NLP与RNN 448
9.3.1 Simple-RNN 449
9.3.2 LSTM原理 454
9.3.3 LSTM的Python实现 460
9.4 深度学习框架与应用 467
9.4.1 Keras框架介绍 467
9.4.2 Keras序列标注 471
9.4.3 依存句法的算法原理 478
9.4.4 Stanford依存解析的训练过程 483
9.5 结语 488
第10章 语义计算的架构 490
10.1 句子的语义和语法预处理 490
10.1.1 长句切分和融合 491
10.1.2 共指消解 496
10.2 语义角色 502
10.2.1 谓词论元与语义角色 502
10.2.2 PropBank简介 505
10.2.3 CPB中的特殊句式 506
10.2.4 名词性谓词的语义角色 509
10.2.5 PropBank展开 512
10.3 句子的语义解析 517
10.3.1 语义依存 517
10.3.2 完整架构 524
10.3.3 实体关系抽取 527
10.4 结语 531
・ ・ ・ ・ ・ ・ (收起)第1章 起步　　1
1.1 NLP中的基本概念和术语　　1
1.1.1 文本语料库　　1
1.1.2 段落　　2
1.1.3 句子　　2
1.1.4 短语和单词　　2
1.1.5 n元语法　　2
1.1.6 词袋　　2
1.2 NLP技术的应用　　3
1.2.1 情感分析　　3
1.2.2 命名实体识别　　4
1.2.3 实体链接　　5
1.2.4 文本翻译　　6
1.2.5 自然语言推理　　6
1.2.6 语义角色标记　　6
1.2.7 关系提取　　7
1.2.8 SQL查询生成或语义解析　　8
1.2.9 机器阅读理解　　8
1.2.10 文字蕴含　　10
1.2.11 指代消解　　10
1.2.12 搜索　　11
1.2.13 问答和聊天机器人　　11
1.2.14 文本转语音　　12
1.2.15 语音转文本　　13
1.2.16 说话人识别　　14
1.2.17 口语对话系统　　14
1.2.18 其他应用　　14
1.3 小结　　15
第2章 使用NLTK进行文本分类和词性标注　　16
2.1 安装NLTK 及其模块　　16
2.2 文本预处理及探索性分析　　18
2.2.1 分词　　18
2.2.2 词干提取　　19
2.2.3 去除停用词　　20
2.2.4 探索性分析　　20
2.3 词性标注　　24
2.3.1 词性标注定义　　24
2.3.2 词性标注的应用　　25
2.3.3 训练词性标注器　　25
2.4 训练影评情感分类器　　29
2.5 训练词袋分类器　　32
2.6 小结　　34
第3章 深度学习和TensorFlow　　35
3.1 深度学习　　35
3.1.1 感知器　　35
3.1.2 激活函数　　36
3.1.3 神经网络　　38
3.1.4 训练神经网络　　40
3.1.5 卷积神经网络　　43
3.1.6 递归神经网络　　44
3.2 TensorFlow　　45
3.2.1 通用图形处理单元　　45
3.2.2 安装　　46
3.2.3 Hello world !　　47
3.2.4 两数相加　　47
3.2.5 TensorBoard　　48
3.2.6 Keras库　　49
3.3 小结　　49
第4章 使用浅层模型进行语义嵌入　　50
4.1 词向量　　50
4.1.1 经典方法　　50
4.1.2 Word2vec　　51
4.1.3 连续词袋模型　　52
4.1.4 跳字模型　　53
4.2 从单词到文档嵌入　　59
4.3 Sentence2vec　　59
4.4 Doc2vec　　60
4.5 小结　　63
第5章 使用LSTM进行文本分类　　64
5.1 文本分类数据　　64
5.2 主题建模　　65
5.3 用于文本分类的深度学习元架构　　68
5.3.1 嵌入层　　68
5.3.2 深层表示　　68
5.3.3 全连接部分　　68
5.4 使用RNN识别YouTube视频垃圾评论　　69
5.5 使用CNN对新闻主题分类　　73
5.6 使用GloVe嵌入进行迁移学习　　76
5.7 多标签分类　　79
5.7.1 二元关联　　80
5.7.2 用于多标签分类的深度学习　　80
5.7.3 用于文档分类的attention网络　　81
5.8 小结　　83
第6章 使用CNN进行搜索和去重　　84
6.1 数据　　84
6.2 模型训练　　85
6.2.1 文本编码　　86
6.2.2 建立CNN模型　　87
6.2.3 训练　　89
6.2.4 推理　　91
6.3 小结　　92
第7章 使用字符级LSTM进行命名实体识别　　93
7.1 使用深度学习实现NER　　93
7.1.1 数据　　94
7.1.2 模型　　95
7.1.3 代码详解　　96
7.1.4 不同预训练词嵌入的影响　　98
7.1.5 改进空间　　105
7.2 小结　　105
第8章 使用GRU 进行文本生成和文本摘要　　106
8.1 使用RNN进行文本生成　　106
8.2 文本摘要　　112
8.2.1 提取式摘要　　112
8.2.2 抽象式摘要　　114
8.2.3 最新抽象式文本摘要　　123
8.3 小结　　125
第9章 使用记忆网络完成问答任务和编写聊天机器人　　127
9.1 QA任务　　127
9.2 用于QA任务的记忆网络　　128
9.2.1 记忆网络管道概述　　128
9.2.2 使用TensorFlow写一个记忆网络　　129
9.3 拓展记忆网络以进行对话建模　　134
9.3.1 对话数据集　　134
9.3.2 使用TensorFlow编写一个聊天机器人　　137
9.3.3 记忆网络相关文献　　146
9.4 小结　　146
第10章 使用基于attention的模型进行机器翻译　　147
10.1 机器翻译概述　　147
10.1.1 统计机器翻译　　147
10.1.2 神经机器翻译　　150
10.2 小结　　163
第11章 使用DeepSpeech进行语音识别　　164
11.1 语音识别概述　　164
11.2 建立用于语音识别的RNN模型　　165
11.2.1 语音信号表示　　165
11.2.2 用于语音数字识别的LSTM模型　　167
11.2.3 TensorBoard可视化　　168
11.2.4 使用DeepSpeech架构的语音转文本模型　　169
11.2.5 语音识别最新技术　　178
11.3 小结　　179
第12章 使用Tacotron进行文本转语音　　180
12.1 TTS领域概述　　181
12.1.1 自然性与可懂性　　181
12.1.2 TTS系统表现的评估方式　　181
12.1.3 传统技术――级联模型和参数模型　　182
12.1.4 关于频谱图和梅尔标度的一些提醒　　182
12.2 深度学习中的TTS　　185
12.2.1 WaveNet简介　　186
12.2.2 Tacotron　　186
12.3 利用Keras的Tacotron实现　　191
12.3.1 数据集　　192
12.3.2 数据准备　　192
12.3.3 架构实现　　196
12.3.4 训练与测试　　200
12.4 小结　　201
第13章 部署训练好的模型　　202
13.1 性能提升　　202
13.1.1 量化权重　　202
13.1.2 MobileNets　　203
13.2 TensorFlow Serving　　205
13.2.1 导出训练好的模型　　206
13.2.2 把导出模型投入服务　　207
13.3 在云上部署　　207
13.3.1 Amazon Web Services　　207
13.3.2 Google Cloud Platform　　210
13.4 在移动设备上部署　　213
13.4.1 iPhone　　213
13.4.2 Android　　213
13.5 小结　　213
・ ・ ・ ・ ・ ・ (收起)序一
序二
前言
第1章 NLP基础 1
1.1 什么是NLP 1
1.1.1 NLP的概念 1
1.1.2 NLP的研究任务 3
1.2 NLP的发展历程 5
1.3 NLP相关知识的构成 7
1.3.1 基本术语 7
1.3.2 知识结构 9
1.4 语料库 10
1.5 探讨NLP的几个层面 11
1.6 NLP与人工智能 13
1.7 本章小结 15
第2章 NLP前置技术解析 16
2.1 搭建Python开发环境 16
2.1.1 Python的科学计算发行版――Anaconda 17
2.1.2 Anaconda的下载与安装 19
2.2 正则表达式在NLP的基本应用 21
2.2.1 匹配字符串 22
2.2.2 使用转义符 26
2.2.3 抽取文本中的数字 26
2.3 Numpy使用详解 27
2.3.1 创建数组 28
2.3.2 获取Numpy中数组的维度 30
2.3.3 获取本地数据 31
2.3.4 正确读取数据 32
2.3.5 Numpy数组索引 32
2.3.6 切片 33
2.3.7 数组比较 33
2.3.8 替代值 34
2.3.9 数据类型转换 36
2.3.10 Numpy的统计计算方法 36
2.4 本章小结 37
第3章 中文分词技术 38
3.1 中文分词简介 38
3.2 规则分词 39
3.2.1 正向最大匹配法 39
3.2.2 逆向最大匹配法 40
3.2.3 双向最大匹配法 41
3.3 统计分词 42
3.3.1 语言模型 43
3.3.2 HMM模型 44
3.3.3 其他统计分词算法 52
3.4 混合分词 52
3.5 中文分词工具――Jieba 53
3.5.1 Jieba的三种分词模式 54
3.5.2 实战之高频词提取 55
3.6 本章小结 58
第4章 词性标注与命名实体识别 59
4.1 词性标注 59
4.1.1 词性标注简介 59
4.1.2 词性标注规范 60
4.1.3 Jieba分词中的词性标注 61
4.2 命名实体识别 63
4.2.1 命名实体识别简介 63
4.2.2 基于条件随机场的命名实体识别 65
4.2.3 实战一：日期识别 69
4.2.4 实战二：地名识别 75
4.3 总结 84
第5章 关键词提取算法 85
5.1 关键词提取技术概述 85
5.2 关键词提取算法TF/IDF算法 86
5.3 TextRank算法 88
5.4 LSA/LSI/LDA算法 91
5.4.1 LSA/LSI算法 93
5.4.2 LDA算法 94
5.5 实战提取文本关键词 95
5.6 本章小结 105
第6章 句法分析 106
6.1 句法分析概述 106
6.2 句法分析的数据集与评测方法 107
6.2.1 句法分析的数据集 108
6.2.2 句法分析的评测方法 109
6.3 句法分析的常用方法 109
6.3.1 基于PCFG的句法分析 110
6.3.2 基于最大间隔马尔可夫网络的句法分析 112
6.3.3 基于CRF的句法分析 113
6.3.4 基于移进C归约的句法分析模型 113
6.4 使用Stanford Parser的PCFG算法进行句法分析 115
6.4.1 Stanford Parser 115
6.4.2 基于PCFG的中文句法分析实战 116
6.5 本章小结 119
第7章 文本向量化 120
7.1 文本向量化概述 120
7.2 向量化算法word2vec 121
7.2.1 神经网络语言模型 122
7.2.2 C&W模型 124
7.2.3 CBOW模型和Skip-gram模型 125
7.3 向量化算法doc2vec/str2vec 127
7.4 案例：将网页文本向量化 129
7.4.1 词向量的训练 129
7.4.2 段落向量的训练 133
7.4.3 利用word2vec和doc2vec计算网页相似度 134
7.5 本章小结 139
第8章 情感分析技术 140
8.1 情感分析的应用 141
8.2 情感分析的基本方法 142
8.2.1 词法分析 143
8.2.2 机器学习方法 144
8.2.3 混合分析 144
8.3 实战电影评论情感分析 145
8.3.1 卷积神经网络 146
8.3.2 循环神经网络 147
8.3.3 长短时记忆网络 148
8.3.4 载入数据 150
8.3.5 辅助函数 154
8.3.6 模型设置 155
8.3.7 调参配置 158
8.3.8 训练过程 159
8.4 本章小结 159
第9章 NLP中用到的机器学习算法 160
9.1 简介 160
9.1.1 机器学习训练的要素 161
9.1.2 机器学习的组成部分 162
9.2 几种常用的机器学习方法 166
9.2.1 文本分类 166
9.2.2 特征提取 168
9.2.3 标注 169
9.2.4 搜索与排序 170
9.2.5 推荐系统 170
9.2.6 序列学习 172
9.3 分类器方法 173
9.3.1 朴素贝叶斯Naive Bayesian 173
9.3.2 逻辑回归 174
9.3.3 支持向量机 175
9.4 无监督学习的文本聚类 177
9.5 文本分类实战：中文垃圾邮件分类 180
9.5.1 实现代码 180
9.5.2 评价指标 187
9.6 文本聚类实战：用K-means对豆瓣读书数据聚类 190
9.7 本章小结 194
第10章 基于深度学习的NLP算法 195
10.1 深度学习概述 195
10.1.1 神经元模型 196
10.1.2 激活函数 197
10.1.3 感知机与多层网络 198
10.2 神经网络模型 201
10.3 多输出层模型 203
10.4 反向传播算法 204
10.5 最优化算法 208
10.5.1 梯度下降 208
10.5.2 随机梯度下降 209
10.5.3 批量梯度下降 210
10.6 丢弃法 211
10.7 激活函数 211
10.7.1 tanh函数 212
10.7.2 ReLU函数 212
10.8 实现BP算法 213
10.9 词嵌入算法 216
10.9.1 词向量 217
10.9.2 word2vec简介 217
10.9.3 词向量模型 220
10.9.4 CBOW和Skip-gram模型 222
10.10 训练词向量实践 224
10.11 朴素Vanilla-RNN 227
10.12 LSTM网络 230
10.12.1 LSTM基本结构 230
10.12.2 其他LSTM变种形式 234
10.13 Attention机制 236
10.13.1 文本翻译 237
10.13.2 图说模型 237
10.13.3 语音识别 239
10.13.4 文本摘要 239
10.14 Seq2Seq模型 240
10.15 图说模型 242
10.16 深度学习平台 244
10.16.1 Tensorflow 245
10.16.2 Mxnet 246
10.16.3 PyTorch 246
10.16.4 Caffe 247
10.16.5 Theano 247
10.17 实战Seq2Seq问答机器人 248
10.18 本章小结 254
第11章 Solr搜索引擎 256
11.1 全文检索的原理 257
11.2 Solr简介与部署 258
11.3 Solr后台管理描述 263
11.4 配置schema 267
11.5 Solr管理索引库 270
11.5.1 创建索引 270
11.5.2 查询索引 276
11.5.3 删除文档 279
11.6 本章小结 281
・ ・ ・ ・ ・ ・ (收起)第 1章　什么是文本分析 1
1.1　什么是文本分析　1
1.2　搜集数据　5
1.3　若输入错误数据，则输出亦为错误数据（garbage in，garbage out）　8
1.4　为什么你需要文本分析　9
1.5　总结　11
第　2章 Python文本分析技巧　12
2.1　为什么用Python来做文本分析　12
2.2　用Python进行文本操作　14
2.3　总结　18
第3章　spaCy语言模型　19
3.1　spaCy库　19
3.2　spaCy的安装步骤　21
3.3　故障排除　22
3.4　语言模型　22
3.5　安装语言模型　23
3.6　安装语言模型的方式及原因　25
3.7　语言模型的基本预处理操作　25
3.8　分词　26
3.9　词性标注　28
3.10　命名实体识别　29
3.11　规则匹配　30
3.12　预处理　31
3.13　总结　33
第4章　Gensim：文本向量化、向量变换和n-grams的工具　34
4.1　Gensim库介绍　34
4.2　向量以及为什么需要向量化　35
4.3　词袋（bag-of-words）　36
4.4　TF-IDF（词频-反向文档频率）　37
4.5　其他表示方式　38
4.6　Gensim中的向量变换　38
4.7　n-grams及其预处理技术　42
4.8　总结　44
第5章　词性标注及其应用　45
5.1　什么是词性标注　45
5.2　使用Python实现词性标注　49
5.3　使用spaCy进行词性标注　50
5.4　从头开始训练一个词性标注模型　53
5.5　词性标注的代码示例　57
5.6　总结　59
第6章　NER标注及其应用　60
6.1　什么是NER标注　60
6.2　用Python实现NER标注　64
6.3　使用spaCy实现NER标注　67
6.4　从头开始训练一个NER标注器　72
6.5　NER标注应用实例和可视化　77
6.6　总结　79
第7章　依存分析　80
7.1　依存分析　80
7.2　用Python实现依存分析　85
7.3　用spaCy实现依存分析　87
7.4　从头开始训练一个依存分析器　91
7.5　总结　98
第8章　主题模型　99
8.1　什么是主题模型　99
8.2　使用Gensim构建主题模型　101
8.3　隐狄利克雷分配（Latent Dirichlet Allocation）　102
8.4　潜在语义索引（Latent Semantic Indexing）　104
8.5　分层狄利特雷过程（Hierarchical Dirichlet Process）　105
8.6　动态主题模型　108
8.7　使用scikit-learn构建主题模型　109
8.8　总结　112
第9章　高级主题建模　113
9.1　高级训练技巧　113
9.2　探索文档　117
9.3　主题一致性和主题模型的评估　121
9.4　主题模型的可视化　123
9.5　总结　127
第　10章 文本聚类和文本分类　128
10.1　文本聚类　128
10.2　聚类前的准备工作　129
10.3　K-means　132
10.4　层次聚类　134
10.5　文本分类　136
10.6　总结　138
第　11章 查询词相似度计算和文本摘要　139
11.1　文本距离的度量　139
11.2　查询词相似度计算　145
11.3　文本摘要　147
11.4　总结　153
第　12章 Word2Vec、Doc2Vec和Gensim　154
12.1　Word2Vec　154
12.2　用Gensim实现Word2Vec　155
12.3　Doc2Vec　160
12.4　其他词嵌入技术　166
12.5　总结　172
第　13章 使用深度学习处理文本　173
13.1　深度学习　173
13.2　深度学习在文本上的应用　174
13.3　文本生成　177
13.4　总结　182
第　14章 使用Keras和spaCy进行深度学习　183
14.1　Keras和spaCy　183
14.2　使用Keras进行文本分类　185
14.3　使用spaCy进行文本分类　191
14.4　总结　201
第　15章 情感分析与聊天机器人　202
15.1　情感分析　202
15.2　基于Reddit的新闻数据挖掘　205
15.3　基于Twitter的微博数据挖掘　207
15.4　聊天机器人　209
15.5　总结　217
・ ・ ・ ・ ・ ・ (收起)译者序
前言
作者名单
第1章 延迟解释、浅层处理和构式：“尽可能解释”原则的基础 1
1.1 引言 1
1.2 延迟处理 2
1.3 工作记忆 5
1.4 如何识别语块：分词操作 7
1.5 延迟架构 10
1.5.1 分段和存储 11
1.5.2 内聚聚集 12
1.6 结论 15
1.7 参考文献 16
第2章 人类关联规范能否评估机器制造的关联列表 19
2.1 引言 19
2.2 人类语义关联 20
2.2.1 单词关联测试 20
2.2.2 作者的实验 21
2.2.3 人类关联拓扑 22
2.2.4 人类关联具有可比性 24
2.3 算法效率比较 26
2.3.1 语料库 26
2.3.2 LSA源关联列表 27
2.3.3 LDA源列表 28
2.3.4 基于关联比率的列表 28
2.3.5 列表比较 29
2.4 结论 33
2.5 参考文献 34
第3章 文本词如何在人类关联网络中选择相关词 37
3.1 引言 37
3.2 网络 40
3.3 基于文本的激励驱动的网络提取 42
3.3.1 子图提取算法 42
3.3.2 控制流程 43
3.3.3 最短路径提取 44
3.3.4 基于语料库的子图 46
3.4 网络提取流程的测试 46
3.4.1 进行测试的语料库 46
3.4.2 提取子图的评估 46
3.4.3 有向和无向子图提取：对比 48
3.4.4 每个激励产生的结果 49
3.5 对结果和相关工作的简要讨论 54
3.6 参考文献 57
第4章 反向关联任务 59
4.1 引言 59
4.2 计算前向关联 63
4.2.1 步骤 63
4.2.2 结果和评估 65
4.3 计算反向关联 67
4.3.1 问题 67
4.3.2 步骤 67
4.3.3 结果和评估 71
4.4 人类的表现 73
4.4.1 数据集 73
4.4.2 测试流程 75
4.4.3 评估 76
4.5 机器性能 77
4.6 讨论、结果和展望 78
4.6.1 人类的反向关联 78
4.6.2 机器的反向关联 80
4.7 致谢 82
4.8 参考文献 82
第5章 词汇的隐藏结构与功能 85
5.1 引言 86
5.2 方法 86
5.2.1 词典图 86
5.2.2 心理语言学变量 90
5.2.3 数据分析 91
5.3 内核、卫星、核心、MinSet以及词典余下部分的心理语言学属性 93
5.4 讨论 96
5.5 未来工作 99
5.6 参考文献 101
第6章 用于词义消歧的直推式学习博弈 103
6.1 引言 103
6.2 基于图的词义消歧 104
6.3 半监督学习方法 107
6.3.1 基于图的半监督学习 107
6.3.2 博弈论和博弈动态 108
6.4 词义消歧博弈 110
6.4.1 图构造 110
6.4.2 策略空间 111
6.4.3 收益矩阵 111
6.4.4 系统动力学 112
6.5 评估 113
6.5.1 实验设置 113
6.5.2 评估结果 114
6.5.3 对比先进水平算法 116
6.6 结论 117
6.7 参考文献 117
第7章 用心学写：生成连贯文本的问题 121
7.1 问题 121
7.2 次优文本及其相关原因 123
7.2.1 缺乏连贯性或凝聚力 124
7.2.2 错误引用 125
7.2.3 无动机的主题转移 126
7.3 如何解决任务的复杂性 127
7.4 相关研究 128
7.5 关于构建辅助写作过程的工具的假设 130
7.6 方法论 133
7.6.1 句法结构的识别 135
7.6.2 语义种子词的识别 135
7.6.3 单词对齐 137
7.6.4 确定对齐单词的相似性值 137
7.6.5 确定句子之间的相似性 141
7.6.6 基于句子相似性值的聚类 142
7.7 实验结果和评估 142
7.8 展望和总结 145
7.9 参考文献 146
第8章 面向著述属性的基于序贯规则挖掘的文体特征 149
8.1 引言和研究动机 149
8.2 著述属性过程 151
8.3 著述属性的文体特征 152
8.4 针对文体分析的时序数据挖掘 154
8.5 实验设置 155
8.5.1 数据集 156
8.5.2 分类方案 157
8.6 结果和讨论 158
8.7 结论 162
8.8 参考文献 162
第9章 一种并行的、面向认知的基频估计算法 165
9.1 引言 165
9.2 语音信号分割 167
9.2.1 语音和停顿段 168
9.2.2 浊音和清音区 169
9.2.3 稳定和不稳定区间 170
9.3 稳定区间的F0估计 171
9.4 F0传播 173
9.4.1 控制流 174
9.4.2 峰值传播 175
9.5 不稳定的浊音区域 178
9.6 并行化 178
9.7 实验和结果 179
9.8 结论 180
9.9 致谢 181
9.10 参考文献 182
第10章 基于完形填充、脑电图和眼球运动数据对n元语言模型、主题模型和循环神经网络的基准测试 185
10.1 引言 186
10.2 相关工作 187
10.3 方法 188
10.3.1 人类绩效评估 188
10.3.2 语言模型的三种风格 189
10.4 实验设置 192
10.5 结果 193
10.5.1 可预测性结果 193
10.5.2 N400振幅结果 196
10.5.3 单一注视时延结果 198
10.6 讨论和结论 200
10.7 致谢 202
10.8 参考文献 202
术语表 207
・ ・ ・ ・ ・ ・ (收起)第1章　应用自然语言处理技术 1
1.1　付出与回报 2
1.1.1　如何开始 2
1.1.2 招聘人员 2
1.1.3 学习 3
1.2 开发环境 3
1.3 技术基础 4
1.3.1 Java 4
1.3.2 规则方法 5
1.3.3 统计方法 5
1.3.4 计算框架 5
1.3.5 文本挖掘 7
1.3.6 语义库 7
1.4 本章小结 9
1.5 专业术语 9
第2章　中文分词原理与实现 11
2.1 接口 12
2.1.1 切分方案 13
2.1.2 词特征 13
2.2 查找词典算法 13
2.2.1 标准Trie树 14
2.2.2 三叉Trie树 18
2.2.3 词典格式 26
2.3 最长匹配中文分词 27
2.3.1 正向最大长度匹配法 28
2.3.2 逆向最大长度匹配法 33
2.3.3 处理未登录串 39
2.3.4 开发分词 43
2.4 概率语言模型的分词方法 45
2.4.1 一元模型 47
2.4.2 整合基于规则的方法 54
2.4.3 表示切分词图 55
2.4.4 形成切分词图 62
2.4.5 数据基础 64
2.4.6 改进一元模型 75
2.4.7 二元词典 79
2.4.8 完全二叉树组 85
2.4.9 三元词典 89
2.4.10 N元模型 90
2.4.11 N元分词 91
2.4.12 生成语言模型 99
2.4.13 评估语言模型 100
2.4.14 概率分词的流程与结构 101
2.4.15 可变长N元分词 102
2.4.16 条件随机场 103
2.5 新词发现 103
2.5.1 成词规则 109
2.6 词性标注 109
2.6.1 数据基础 114
2.6.2 隐马尔可夫模型 115
2.6.3 存储数据 124
2.6.4 统计数据 131
2.6.5 整合切分与词性标注 133
2.6.6 大词表 138
2.6.7 词性序列 138
2.6.8 基于转换的错误学习方法 138
2.6.9 条件随机场 141
2.7 词类模型 142
2.8 未登录词识别 144
2.8.1 未登录人名 144
2.8.2 提取候选人名 145
2.8.3 最长人名切分 153
2.8.4 一元概率人名切分 153
2.8.5 二元概率人名切分 156
2.8.6 未登录地名 159
2.8.7 未登录企业名 160
2.9 平滑算法 160
2.10 机器学习的方法 164
2.10.1 最大熵 165
2.10.2 条件随机场 170
2.11 有限状态机 171
2.12 地名切分 178
2.12.1 识别未登录地名 179
2.12.2 整体流程 185
2.13 企业名切分 187
2.13.1 识别未登录词 188
2.13.2 整体流程 190
2.14 结果评测 190
2.15 本章小结 191
2.16 专业术语 193
第3章　英文分析 194
3.1 分词 194
3.1.1 句子切分 194
3.1.2 识别未登录串 197
3.1.3 切分边界 198
3.2 词性标注 199
3.3 重点词汇 202
3.4 句子时态 203
3.5 本章小结 204
第4章　依存文法分析 205
4.1 句法分析树 205
4.2 依存文法 211
4.2.1 中文依存文法 211
4.2.2 英文依存文法 220
4.2.3 生成依存树 232
4.2.4 遍历 235
4.2.5 机器学习的方法 237
4.3 小结 237
4.4 专业术语 238
第5章　文档排重 239
5.1 相似度计算 239
5.1.1 夹角余弦 239
5.1.2 最长公共子串 242
5.1.3 同义词替换 246
5.1.4 地名相似度 248
5.1.5 企业名相似度 251
5.2 文档排重 251
5.2.1 关键词排重 251
5.2.2 SimHash 254
5.2.3 分布式文档排重 268
5.2.4 使用文本排重 269
5.3 在搜索引擎中使用文本排重 269
5.4 本章小结 270
5.5 专业术语 270
第6章　信息提取 271
6.1 指代消解 271
6.2 中文关键词提取 273
6.2.1 关键词提取的基本方法 273
6.2.2 HITS算法应用于关键词提取 275
6.2.3 从网页中提取关键词 277
6.3 信息提取 278
6.3.1 提取联系方式 280
6.3.2 从互联网提取信息 281
6.3.3 提取地名 282
6.4 拼写纠错 283
6.4.1 模糊匹配问题 285
6.4.2 正确词表 296
6.4.3 英文拼写检查 298
6.4.4 中文拼写检查 300
6.5 输入提示 302
6.6 本章小结 303
6.7 专业术语 303
第7章　自动摘要 304
7.1 自动摘要技术 305
7.1.1 英文文本摘要 307
7.1.2 中文文本摘要 309
7.1.3 基于篇章结构的自动摘要 314
7.1.4 句子压缩 314
7.2 指代消解 314
7.3 Lucene中的动态摘要 314
7.4 本章小结 317
7.5 专业术语 318
第8章　文本分类 319
8.1 地名分类 321
8.2 错误类型分类 321
8.3 特征提取 322
8.4 关键词加权法 326
8.5 朴素贝叶斯 330
8.6 贝叶斯文本分类 336
8.7 支持向量机 336
8.7.1 多级分类 345
8.7.2 规则方法 347
8.7.3 网页分类 350
8.8 最大熵 351
8.9 信息审查 352
8.10 文本聚类 353
8.10.1 K均值聚类方法 353
8.10.2 K均值实现 355
8.10.3 深入理解DBScan算法 359
8.10.4 使用DBScan算法聚类实例 361
8.11 本章小结 363
8.12 专业术语 363
第9章　文本倾向性分析 364
9.1 确定词语的褒贬倾向 367
9.2 实现情感识别 368
9.3 本章小结 372
9.4 专业术语 373
第10章　问答系统 374
10.1 问答系统的结构 375
10.1.1 提取问答对 376
10.1.2 等价问题 376
10.2 问句分析 377
10.2.1 问题类型 377
10.2.2 句型 381
10.2.3 业务类型 381
10.2.4 依存树 381
10.2.5 指代消解 383
10.2.6 二元关系 383
10.2.7 逻辑表示 386
10.2.8 问句模板 386
10.2.9 结构化问句模板 389
10.2.10 检索方式 390
10.2.11 问题重写 395
10.2.12 提取事实 395
10.2.13 验证答案 398
10.2.14 无答案的处理 398
10.3 知识库 398
10.4 聊天机器人 399
10.4.1 交互式问答 401
10.4.2 垂直领域问答系统 402
10.4.3 语料库 405
10.4.4 客户端 405
10.5 自然语言生成 405
10.6 依存句法 406
10.7 提取同义词 410
10.7.1 流程 410
10.8 本章小结 411
10.9 术语表 412
第11章　语音识别 413
11.1 总体结构 414
11.1.1 识别中文 416
11.1.2 自动问答 417
11.2 语音库 418
11.3 语音合成 419
11.3.1 归一化 420
11.4 语音 420
11.4.1 标注 424
11.4.2 相似度 424
11.5 Sphinx 424
11.5.1 中文训练集 426
11.6 Julius 429
11.7 本章小结 429
11.8 术语表 429
参考资源 430
后记 431
・ ・ ・ ・ ・ ・ (收起)出版者的话
译者序
前言
关于作者
第一部分 理论
第1章 找出词的结构
1.1 词及其部件
1.1.1 词元
1.1.2 词形
1.1.3 词素
1.1.4 类型学
1.2 问题和挑战
1.2.1 不规则性
1.2.2 歧义性
1.2.3 能产性
1.3 形态模型
1.3.1 查词典
1.3.2 有限状态形态
1.3.3 基于合一的形态
1.3.4 函数式形态
1.3.5 形态归纳
1.4 总结
第2章 找出文档的结构
2.1 概述
2.1.1 句子边界检测
2.1.2 主题边界检测
2.2 方法
2.2.1 生成序列分类方法
2.2.2 判别性局部分类方法
2.2.3 判别性序列分类方法
2.2.4 混合方法
2.2.5 句子分割的全局建模扩展
2.3 方法的复杂度
2.4 方法的性能
2.5 特征
2.5.1 同时用于文本与语音的特征
2.5.2 只用于文本的特征
2.5.3 语音特征
2.6 处理阶段
2.7 讨论
2.8 总结
第3章 句法
3.1 自然语言分析
3.2 树库：句法分析的数据驱动方法
3.3 句法结构的表示
3.3.1 使用依存图的句法分析
3.3.2 使用短语结构树的句法分析
3.4 分析算法
3.4.1 移进归约分析
3.4.2 超图和线图分析
3.4.3 最小生成树和依存分析
3.5 分析中的歧义消解模型
3.5.1 概率上下文无关文法
3.5.2 句法分析的生成模型
3.5.3 句法分析的判别模型
3.6 多语言问题：什么是词元
3.6.1 词元切分、实例和编码
3.6.2 分词
3.6.3 形态学
3.7 总结
第4章 语义分析
4.1 概述
4.2 语义解释
4.2.1 结构歧义
4.2.2 词义
4.2.3 实体与事件消解
4.2.4 谓词　论元结构
4.2.5 意义表示
4.3 系统范式
4.4 词义
4.4.1 资源
4.4.2 系统
4.4.3 软件
4.5 谓词　论元结构
4.5.1 资源
4.5.2 系统
4.5.3 软件
4.6 意义表示
4.6.1 资源
4.6.2 系统
4.6.3 软件
4.7 总结
4.7.1 词义消歧
4.7.2 谓词　论元结构
4.7.3 意义表示
第5章 语言模型
5.1 概述
5.2 n元模型
5.3 语言模型评价
5.4 参数估计
5.4.1 最大似然估计和平滑
5.4.2 贝叶斯参数估计
5.4.3 大规模语言模型
5.5 语言模型适应
5.6 语言模型的类型
5.6.1 基于类的语言模型
5.6.2 变长语言模型
5.6.3 判别式语言模型
5.6.4 基于句法的语言模型
5.6.5 最大熵语言模型
5.6.6 因子化语言模型
5.6.7 其他基于树的语言模型
5.6.8 基于主题的贝叶斯语言模型
5.6.9 神经网络语言模型
5.7 特定语言建模问题
5.7.1 形态丰富语言的建模
5.7.2 亚词单元的选择
5.7.3 形态类别建模
5.7.4 无分词语言
5.7.5 口语与书面语言
5.8 多语言和跨语言建模
5.8.1 多语言建模
5.8.2 跨语言建模
5.9 总结
第6章 文本蕴涵识别
6.1 概述
6.2 文本识别蕴涵任务
6.2.1 问题定义
6.2.2 RTE的挑战
6.2.3 评估文本蕴涵系统性能
6.2.4 文本蕴涵解决方案的应用
6.2.5 其他语言中的RTE研究
6.3 文本蕴涵识别的框架
6.3.1 要求
6.3.2 分析
6.3.3 有用的组件
6.3.4 通用模型
6.3.5 实现
6.3.6 对齐
6.3.7 推理
6.3.8 训练
6.4 案例分析
6.4.1 抽取语篇约束
6.4.2 基于编辑距离的RTE
6.4.3 基于转换的方法
6.4.4 逻辑表示及推理
6.4.5 独立于蕴涵学习对齐
6.4.6 在RTE中利用多对齐
6.4.7 自然逻辑
6.4.8 句法树核
6.4.9 使用有限依存上下文的全局相似度
6.4.10 RTE的潜在对齐推理
6.5 RTE的进一步研究
6.5.1 改进分析器
6.5.2 发明或解决新问题
6.5.3 开发知识库
6.5.4 更好的RTE评价
6.6 有用资源
6.6.1 文献
6.6.2 知识库
6.6.3 自然语言处理包
6.7 总结
第7章 多语情感与主观性分析
7.1 概述
7.2 定义
7.3 英语中的情感及主观性分析
7.3.1 词典
7.3.2 语料库
7.3.3 工具
7.4 词级和短语级标注
7.4.1 基于字典的方法
7.4.2 基于语料库的方法
7.5 句子级标注
7.5.1 基于字典
7.5.2 基于语料库
7.6 文档级标注
7.6.1 基于字典
7.6.2 基于语料库
7.7 什么有效，什么无效
7.7.1 最佳情况：已有人工标注的语料库
7.7.2 次优情形：基于语料库的跨语言映射
7.7.3 第三优情形：孳衍词典
7.7.4 第四优情形：翻译词典
7.7.5 各种可行方法的比较
7.8 总结
第二部分 实践
第8章 实体检测和追踪
8.1 概述
8.2 提及检测
8.2.1 数据驱动的分类
8.2.2 搜索提及
8.2.3 提及检测特征
8.2.4 提及检测实验
8.3 共指消解
8.3.1 Bell树的构建
8.3.2 共指模型：链接和引入模型
8.3.3 最大熵链接模型
8.3.4 共指消解实验
8.4 总结
第9章 关系和事件
9.1 概述
9.2 关系与事件
9.3 关系类别
9.4 将关系抽取视为分类
9.4.1 算法
9.4.2 特征
9.4.3 分类器
9.5 关系抽取的其他方法
9.5.1 无监督和半监督方法
9.5.2 核方法
9.5.3 实体和关系检测的联合方法
9.6 事件
9.7 事件抽取方法
9.8 超句
9.9 事件匹配
9.10 事件抽取的未来方向
9.11 总结
第10章 机器翻译
10.1 机器翻译现状
10.2 机器翻译评测
10.2.1 人工评测
10.2.2 自动评测
10.2.3 WER、BLEU、METEOR等
10.3 词对齐
10.3.1 共现
10.3.2 IBM模型1
10.3.3 期望最大化
10.3.4 对齐模型
10.3.5 对称化
10.3.6 作为机器学习问题的词对齐
10.4 基于短语的翻译模型
10.4.1 模型
10.4.2 训练
10.4.3 解码
10.4.4 立方剪枝
10.4.5 对数线性模型和参数调节
10.4.6 控制模型的大小
10.5 基于树的翻译模型
10.5.1 层次短语翻译模型
10.5.2 线图解码
10.5.3 基于句法的模型
10.6 语言学挑战
10.6.1 译词选择
10.6.2 形态学
10.6.3 词序
10.7 工具和数据资源
10.7.1 基本工具
10.7.2 机器翻译系统
10.7.3 平行语料
10.8 未来的方向
10.9 总结
第11章 跨语言信息检索
11.1 概述
11.2 文档预处理
11.2.1 文档句法和编码
11.2.2 词元化
11.2.3 规范化
11.2.4 预处理最佳实践
11.3 单语信息检索
11.3.1 文档表示
11.3.2 索引结构
11.3.3 检索模型
11.3.4 查询扩展
11.3.5 文档先验模型
11.3.6 模型选择的最佳实践
11.4 CLIR
11.4.1 基于翻译的方法
11.4.2 机器翻译
11.4.3 中间语言文档表示
11.4.4 最佳实践
11.5 多语言信息检索
11.5.1 语言识别
11.5.2 MLIR的索引建立
11.5.3 翻译查询串
11.5.4 聚合模型
11.5.5 最佳实践
11.6 信息检索的评价
11.6.1 建立实验环境
11.6.2 相关性评估
11.6.3 评价指标
11.6.4 已有数据集
11.6.5 最佳实践
11.7 工具、软件和资源
11.8 总结
第12章 多语自动文摘
12.1 概述
12.2 自动文摘方法
12.2.1 传统方法
12.2.2 基于图的方法
12.2.3 学习如何做摘要
12.2.4 多语自动摘要
12.3 评测
12.3.1 人工评价
12.3.2 自动评价
12.3.3 自动文摘评测系统的近期发展
12.3.4 多语自动文摘的自动评测方法
12.4 如何搭建自动文摘系统
12.4.1 材料
12.4.2 工具
12.4.3 说明
12.5 评测竞赛和数据集
12.5.1 评测竞赛
12.5.2 数据集
12.6 总结
第13章 问答系统
13.1 概述和历史
13.2 架构
13.3 源获取和预处理
13.4 问题分析
13.5 搜索及候选抽取
13.5.1 非结构化资源搜索
13.5.2 非结构化源文本的候选抽取
13.5.3 结构化源文本的候选抽取
13.6 回答评分
13.6.1 方法概述
13.6.2 证据结合
13.6.3 扩展到列表型问题
13.7 跨语言问答
13.8 案例研究
13.9 评测
13.9.1 评测任务
13.9.2 判断答案正确性
13.9.3 性能度量
13.10 当前和未来的挑战
13.11 总结和进一步阅读
第14章 提炼
14.1 概述
14.2 示例
14.3 相关性和冗余性
14.4 Rosetta Consortium 提炼系统
14.4.1 文档和语料库准备
14.4.2 索引
14.4.3 查询回答
14.5 其他提炼方法
14.5.1 系统架构
14.5.2 相关度
14.5.3 冗余
14.5.4 多模态提炼
14.5.5 跨语言提炼
14.6 评测和指标
14.7 总结
第15章 口语对话系统
15.1 概述
15.2 口语对话系统
15.2.1 语音识别和理解
15.2.2 语音生成
15.2.3 对话管理器
15.2.4 语音用户接口
15.3 对话形式
15.4 自然语言呼叫路由选择
15.5 三代对话应用
15.6 持续的改进循环
15.7 口语句子的转录和标注
15.8 口语对话系统的本地化
15.8.1 呼叫流程本地化
15.8.2 提示本地化
15.8.3 文法的本地化
15.8.4 源端数据
15.8.5 训练
15.8.6 测试
15.9 总结
第16章 聚合自然语言处理引擎
16.1 概述
16.2 聚合语音和NLP引擎架构的期望属性
16.2.1 灵活的分布式组件化
16.2.2 计算效率
16.2.3 数据操作功能
16.2.4 鲁棒性处理
16.3 聚合的架构
16.3.1 UIMA
16.3.2 GATE
16.3.3 InfoSphere Streams
16.4 案例研究
16.4.1 GALE 互操作性演示系统
16.4.2 跨语言自动语言开发系统
16.4.3 实时翻译服务
16.5 经验教训
16.5.1 分割涉及延迟和精度之间的权衡
16.5.2 联合优化与互操作性
16.5.3 数据模型需要使用约定
16.5.4 性能评估的挑战
16.5.5 引擎的前向波训练
16.6 总结
16.7 UIMA样本代码
索引
・ ・ ・ ・ ・ ・ (收起)★★第1篇 入门――基础知识与编程框架
-
★第1章 BERT模型很强大，你值得拥有 /2
★1.1 全球欢腾，喜迎BERT模型 /2
★1.2 为什么BERT模型这么强 /3
★1.3 怎么学习BERT模型 /4
1.3.1 BERT模型的技术体系 /4
1.3.2 学好自然语言处理的4件套――神经网络的基础知识、NLP的基础知识、编程框架的使用、BERT模型的原理及应用 /4
1.3.3 学习本书的前提条件 /5
★1.4 自然语言处理的技术趋势 /5
1.4.1 基于超大规模的高精度模型 /6
1.4.2 基于超小规模的高精度模型 /6
1.4.3 基于小样本训练的模型 /6
-
★第2章 神经网络的基础知识――可能你掌握得也没有那么牢 /7
★2.1 什么是神经网络 /7
2.1.1 神经网络能解决哪些问题 /7
2.1.2 神经网络的发展 /7
2.1.3 什么是深度学习 /8
2.1.4 什么是图神经网络 /8
2.1.5 什么是图深度学习 /9
★2.2 神经网络的工作原理 /10
2.2.1 了解单个神经元 /10
2.2.2 生物神经元与计算机神经元模型的结构相似性 /12
2.2.3 生物神经元与计算机神经元模型的工作流程相似性 /12
2.2.4 神经网络的形成 /13
★2.3 深度学习中包含了哪些神经网络 /13
2.3.1 全连接神经网络 /13
2.3.2 卷积神经网络 /17
2.3.3 循环神经网络 /23
2.3.4 带有注意力机制的神经网络 /30
2.3.5 自编码神经网络 /34
★2.4 图深度学习中包含哪些神经网络 /36
2.4.1 同构图神经网络 /37
2.4.2 异构图神经网络 /37
★2.5 激活函数――加入非线性因素，以解决线性模型的缺陷 /38
2.5.1 常用的激活函数 /38
2.5.2 更好的激活函数――Swish()与Mish() /41
2.5.3 更适合NLP任务的激活函数――GELU() /43
2.5.4 激活函数总结 /44
2.5.5 分类任务与Softmax算法 /44
★2.6 训练模型的原理 /45
2.6.1 反向传播与BP算法 /47
2.6.2 神经网络模块中的损失函数 /49
2.6.3 学习率 /50
2.6.5 优化器 /51
2.6.6 训练模型的相关算法，会用就行 /52
★2.7 【实例】用循环神经网络实现退位减法 /52
★2.8 训练模型中的常见问题及优化技巧 /56
2.8.1 过拟合与欠拟合问题 /56
2.8.2 改善模型过拟合的方法 /56
2.8.3 了解正则化技巧 /57
2.8.4 了解Dropout技巧 /57
2.8.5 Targeted Dropout与Multi-sample Dropout /58
2.8.6 批量归一化（BN）算法 /59
2.8.7 多种BN算法的介绍与选取 /64
2.8.8 全连接网络的深浅与泛化能力的联系 /64
-
★第3章 NLP的基础知识――NLP没那么“玄” /65
★3.1 NLP的本质与原理 /65
3.1.1 情感分析、相似度分析等任务的本质 /65
3.1.2 完形填空、实体词识别等任务的本质 /66
3.1.3 文章摘要任务、问答任务、翻译任务的本质 /67
★3.2 NLP的常用工具 /68
3.2.1 自然语言处理工具包――SpaCy /68
3.2.2 中文分词工具――Jieba /69
3.2.3 中文转拼音工具――Pypinyin /69
3.2.4 评估翻译质量的算法库――SacreBLEU /70
★3.3 计算机中的字符编码 /70
3.3.1 什么是ASCII编码 /71
3.3.2 为什么会出现乱码问题 /71
3.3.3 什么是Unicode /71
3.3.4 借助Unicode 处理中文字符的常用操作 /73
★3.4 计算机中的词与句 /74
3.4.1 词表与词向量 /75
3.4.2 词向量的原理及意义 /75
3.4.3 多项式分布 /76
3.4.4 什么是依存关系分析 /77
3.4.5 什么是TF /79
3.4.6 什么是IDF /79
3.4.7 什么是TF-IDF /80
3.4.8 什么是BLEU /80
★3.5 什么是语言模型 /81
3.5.1 统计语言模型 /81
3.5.2 CBOW与Skip-Gram语言模型 /81
3.5.3 自编码（Auto Encoding，AE）语言模型 /82
3.5.4 自回归（Auto Regressive，AR）语言模型 /83
★3.6 文本预处理的常用方法 /83
3.6.1 NLP数据集的获取与清洗 /83
3.6.2 基于马尔可夫链的数据增强 /84
-
★第4章 搭建编程环境――从安装开始，更适合零基础入门 /87
★4.1 编程框架介绍 /87
4.1.1 PyTorch介绍 /87
4.1.2 DGL库介绍 /88
4.1.3 支持BERT模型的常用工具库介绍 /89
★4.2 搭建Python开发环境 /89
★4.3 搭建PyTorch开发环境 /91
★4.4 搭建DGL环境 /95
★4.5 安装Transformers库 /96
-
★★第2篇 基础――神经网络与BERT模型
★第5章 PyTorch编程基础 /100
★5.1 神经网络中的基础数据类型 /100
★5.2 矩阵运算的基础 /101
5.2.1 转置矩阵 /101
5.2.2 对称矩阵及其特性 /101
5.2.3 对角矩阵与单位矩阵 /101
5.2.4 阿达玛积（Hadamard Product） /102
5.2.5 点积（Dot Product） /102
5.2.6 对角矩阵的特性与操作方法 /103
★5.3 PyTorch中的张量 /104
5.3.1 定义张量的方法 /105
5.3.2 生成随机值张量 /107
5.3.3 张量的基本操作 /108
5.3.4 在CPU和GPU控制的内存中定义张量 /112
5.3.5 张量间的数据操作 /113
★5.4 Variable类型与自动微分模块 /118
5.4.1 Variable对象与Tensor对象之间的转换 /118
5.4.2 控制梯度计算的方法 /119
5.4.3 Variable对象的属性 /121
★5.5 【实例】用PyTorch实现一个简单模型 /124
5.5.1 准备可复现的随机数据 /124
5.5.2 实现并训练模型 /125
5.5.3 可视化模型能力 /128
★5.6 定义模型结构的常用方法 /129
5.6.1 Module类的使用方法 /129
5.6.2 模型中的参数（Parameters变量） /131
5.6.3 为模型添加参数 /132
5.6.4 从模型中获取参数 /133
5.6.5 激活模型接口 /135
5.6.6 L2正则化接口 /136
5.6.7 Dropout接口 /136
5.6.8 批量归一化接口 /137
5.6.9 【实例】手动实现BN的计算方法 /139
★5.7 保存与载入模型的常用方法 /141
★5.8 训练模型的接口与使用 /143
5.8.1 选取训练模型中的损失函数 /143
5.8.2 【实例】Softmax接口的使用 /144
5.8.3 优化器的使用与优化参数的查看 /146
5.8.4 用退化学习率训练模型 /147
5.8.5 为模型添加钩子函数 /152
5.8.6 多显卡的训练方法 /153
5.8.7 梯度累加的训练方法 /153
★5.9 处理数据集的接口与使用 /154
5.9.1 用DataLoader类实现自定义数据集 /155
5.9.2 DataLoader类中的多种采样器子类 /155
5.9.3 Torchtext工具与内置数据集 /156
★5.10 【实例】训练中文词向量 /157
5.10.1 用Jieba库进行中文样本预处理 /158
5.10.2 按照Skip-Gram规则制作数据集 /159
5.10.3 搭建模型并进行训练 /161
5.10.4 夹角余弦值介绍 /164
★5.11 卷积神经网络的实现 /166
5.11.1 了解卷积接口 /166
5.11.2 卷积操作的类型 /168
5.11.3 卷积参数与卷积结果的计算规则 /169
5.11.4 【实例】卷积函数的使用 /169
5.11.5 了解池化接口 /174
5.11.6 【实例】池化函数的使用 /175
★5.12 【实例】用卷积神经网络实现文本分类任务 /177
5.12.1 了解用于文本分类的卷积网络――TextCNN模型 /177
5.12.2 编写代码实现实例 /179
5.12.3 用多GPU并行训练模型 /184
5.12.4 在多GPU的训练过程中，保存/读取模型文件的注意事项 /185
5.12.5 处理显存残留问题 /186
★5.13 RNN的实现 /187
5.13.1 LSTM与GRU接口的实现 /187
5.13.2 多项式分布采样接口 /188
★5.14 【实例】用RNN训练语言模型 /189
5.14.1 实现语言模型的思路与步骤 /189
5.14.2 准备样本与代码实现 /189
★5.15 【实例】手动实现一个带有自注意力机制的模型 /192
★5.16 【实例】利用带注意力机制的循环神经网络对文本进行分类 /194
5.16.1 制作等长数据集并实现LSTM模型 /194
5.16.2 用梯度剪辑技巧优化训练过程 /195
-
★第6章 BERT模型的原理 /197
★6.1 BERT模型的起源――Transformer模型 /197
6.1.1 Transformer模型出现之前的主流模型 /197
6.1.2 Transformer模型的原理 /199
6.1.3 Transformer模型的优缺点 /204
★6.2 【实例】用Transformer模型进行中/英文翻译 /204
★6.3 BERT模型的原理 /206
6.3.1 BERT模型的训练过程 /207
6.3.2 BERT模型的预训练方法 /207
6.3.3 BERT模型的掩码机制 /208
6.3.4 BERT模型的训练参数 /210
6.3.5 BERT模型的缺点 /210
★6.4 高精度的BERTology系列模型 /211
6.4.1 适合生成文章的模型――GPT模型 /211
6.4.2 支持人机对话的模型――DialoGPT模型 /212
6.4.3 融合了BERT模型与GPT技术的模型――MASS模型 /212
6.4.4 支持长文本输入的模型――Transformer-XL模型 /212
6.4.5 支持更长文本的模型――XLNet模型 /213
6.4.6 弥补XLNet模型不足的模型――MPNet模型 /217
6.4.7 稳健性更好的模型――RoBERTa模型 /217
6.4.8 使用了稀疏注意力的模型――Longformer、BigBird模型 /218
6.4.9 基于词掩码的模型――BERT-WWM、Wo BERT等模型 /220
6.4.10 基于小段文字掩码的模型――SpanBERT模型 /220
6.4.11 适合翻译任务的模型――T5模型 /221
6.4.12 支持多种语言的翻译模型――XLM、XLM-Roberta模型 /222
6.4.13 既能阅读又能写作的模型――UniLM 2.0模型 /223
6.4.14 适用于语法纠错任务的模型――StructBERT、Bart模型 /224
6.4.15 可以进行定向写作的模型――CTRL模型 /225
6.4.16 适合摘要生成的模型――PEGASUS模型 /226
6.4.17 支持更多语言的模型――T-ULR v2模型 /227
★6.5 小规模的BERTology系列模型 /227
6.5.1 比RoBERTa模型训练速度更快的模型――ELECTRA模型 /228
6.5.2 适用于文本分类的超小模型――PRADO、pQRNN模型 /229
6.5.3 比BERT模型更适合于部署场景的模型――DistillBERT模型 /231
6.5.4 比BERT模型更快的模型――FastBERT模型 /232
6.5.5 带有通用蒸馏方案的模型――MiniLM模型 /233
6.5.6 精简版的BERT模型――ALBERT、ALBERT_tiny、ALBERT V2模型 /234
★6.6 BERTology系列模型的预训练方法总结 /237
6.6.1 AE式训练方法的常用策略 /237
6.6.2 更多的训练经验 /237
-
★第7章 BERT模型的快速应用――BERT模型虽然强大，使用却不复杂！ /239
★7.1 了解Transformers库 /239
★7.2 Transformers库的3层应用结构 /240
★7.3 【实例】用Transformers库的管道方式完成多种NLP任务 /241
7.3.1 在管道方式中指定NLP任务 /241
7.3.2 代码实现：完成文本分类任务 /242
7.3.3 代码实现：完成特征提取任务 /243
7.3.4 代码实现：完成完形填空任务 /244
7.3.5 代码实现：完成阅读理解任务 /245
7.3.6 代码实现：完成摘要生成任务 /247
7.3.7 预训练模型文件的组成与其加载时的固定名称 /248
7.3.8 代码实现：完成实体词识别任务 /248
7.3.9 管道方式的工作原理 /249
7.3.10 在管道方式中应用指定模型 /251
★7.4 Transformers库中的自动模型（AutoModel）类 /252
7.4.1 各种AutoModel类 /252
7.4.2 AutoModel类的模型加载机制 /253
7.4.3 Transformers库中的其他语言模型 /254
★7.5 Transformers库中的BERTology系列模型 /255
7.5.1 Transformers库的文件结构 /255
7.5.2 获取和加载预训练模型文件 /257
7.5.3 查找Transformers库中可以使用的模型 /260
7.5.4 【实例】用BERT模型实现完形填空任务 /261
7.5.5 【扩展实例】用自动模型类替换BertForMaskedLM类 /264
★7.6 Transformers库中的词表工具 /264
7.6.1 PreTrainedTokenizer类中的特殊词 /265
7.6.2 PreTrainedTokenizer类中的特殊词的使用 /266
7.6.3 向PreTrainedTokenizer类中添加词 /269
7.6.4 【实例】用手动加载GPT-2模型权重的方式将句子补充完整 /270
7.6.5 子词拆分 /274
★7.7 【实例】用迁移学习训练BERT模型来对中文分类 /275
7.7.1 NLP中的迁移学习 /275
7.7.2 构建数据集 /277
7.7.3 构建并加载BERT模型的预训练模型 /279
7.7.4 Transformers库中的底层类 /280
7.7.5 用退化学习率训练模型 /281
7.7.6 用数据增强方法训练模型 /282
-
★第8章 模型的可解释性――深入模型内部，探究其工作的根源
★8.1 模型的可解释库 /283
8.1.1 了解Captum库 /283
8.1.2 Captum库的可视化工具――Captum Insights /284
★8.2 什么是梯度积分方法 /284
★8.3 【实例】对NLP模型的可解释性分析 /284
8.3.1 分析词嵌入模型 /284
8.3.2 拆解NLP模型的处理过程 /285
8.3.3 用Captum库提取NLP模型的词嵌入层 /286
8.3.4 用梯度积分的方法计算模型的可解释性 /287
8.3.5 可视化模型的可解释性 /289
★8.4 【实例】BERT模型的可解释性分析 /291
8.4.1 了解BERT模型的可解释性工具――Bertviz /291
8.4.2 用Bertviz工具可视化BERT模型的权重 /292
8.4.3 分析BERT模型的权重参数 /296
★8.5 用图神经网络解释BERT模型 /300
8.5.1 点积计算与聚合计算的关系 /300
8.5.2 从图的角度思考BERT模型 /302
-
★★第3篇 BERT模型实战★★
★第9章 图神经网络与BERT模型的结合★
★9.1 图神经网络基础 /306
9.1.1 图的相关术语和操作 /306
9.1.2 图卷积神经网络 /310
★9.2 DGL库的使用方法 /314
9.2.1 创建图结构 /314
9.2.2 DGL库与NetWorkx库的相互转换 /316
9.2.3 图的基本操作 /319
9.2.4 图的消息传播机制 /322
9.2.5 DGL库中的多图处理 /324
★9.3 【实例】用图节点的聚合方法实现BERT模型 /325
9.3.1 基于Transformers库的BERT模型修改方案 /325
9.3.2 实现图节点聚合的核心代码 /326
9.3.3 将原BERT模型的权重应用到基于图节点聚合方法实现的BERT模型上 /327
★9.4 什么是关系图卷积网络（R-GCN）模型 /329
9.4.1 R-GCN模型的原理 /329
9.4.2 基于R-GCN模型的优化 /330
9.4.3 R-GCN模型的实现 /330
★9.5 【实例】用R-GCN模型理解文本中的代词 /332
9.5.1 代词数据集（GAP）介绍 /332
9.5.2 将GAP数据集转换成“图”结构数据的思路 /333
9.5.3 用BERT模型提取代词特征 /335
9.5.4 用BERT模型提取其他词特征 /337
9.5.5 用SpaCy工具和批次图方法构建图数据集 /339
9.5.6 搭建多层R-GCN模型 /344
9.5.7 用5折交叉验证方法训练模型 /345
-
★第10章 BERT模型的行业应用 ★
★10.1 BERT模型在文本纠错领域的应用 /347
10.1.1 文本纠错中的常见任务及解决办法 /347
10.1.2 理解BERT模型的纠错能力 /348
10.1.3 改进BERT模型使其具有更强的纠错能力 /348
10.1.4 专用于文本纠错的模型――Soft-Masked BERT模型 /349
10.1.5 基于图神经网络的文本纠错模型――SpellGCN模型 /350
10.1.6 【实例】用Transformers和DGL库实现SpellGCN模型 /351
★10.2 BERT技术在聊天机器人领域的应用 /352
10.2.1 聊天机器人的种类与实现技术 /353
10.2.2 基于BERT模型完成聊天任务的思路 /354
10.2.3 【实例】用累加梯度训练支持中文的DialoGPT模型 /355
10.2.4 更强大的多轮聊天模型――Meena模型 /360
★10.3 BERT模型在服务器端部署的应用 /360
10.3.1 用transformers-cli工具快速部署BERT模型 /360
10.3.2 用torchserve库部署BERT模型 /362
・ ・ ・ ・ ・ ・ (收起)第1篇 自然语言处理基础篇
第1章 自然语言处理概述 2
1.1 什么是自然语言处理 2
1.1.1 定义 2
1.1.2 常用术语 3
1.1.3 自然语言处理的任务 3
1.1.4 自然语言处理的发展历程 4
1.2 自然语言处理中的挑战 5
1.2.1 歧义问题 5
1.2.2 语言的多样性 6
1.2.3 未登录词 6
1.2.4 数据稀疏 6
1.3 自然语言处理中的常用技术 8
1.4 机器学习中的常见问题 10
1.4.1 Batch和Epoch 10
1.4.2 Batch Size的选择 11
1.4.3 数据集不平衡问题 11
1.4.4 预训练模型与数据安全 12
1.4.5 通过开源代码学习 12
1.5 小结 13
第2章 Python自然语言处理基础 14
2.1 搭建环境 14
2.1.1 选择Python版本 14
2.1.2 安装Python 15
2.1.3 使用pip包管理工具和Python虚拟环境 17
2.1.4 使用集成开发环境 18
2.1.5 安装Python自然语言处理常用的库 21
2.2 用Python处理字符串 25
2.2.1 使用str类型 25
2.2.2 使用StringIO类 29
2.3 用Python处理语料 29
2.3.1 从文件读取语料 29
2.3.2 去重 31
2.3.3 停用词 31
2.3.4 编辑距离 31
2.3.5 文本规范化 32
2.3.6 分词 34
2.3.7 词频-逆文本频率 35
2.3.8 One-Hot 编码 35
2.4 Python的一些特性 36
2.4.1 动态的解释型语言 36
2.4.2 跨平台 37
2.4.3 性能问题 37
2.4.4 并行和并发 37
2.5 在Python中调用其他语言 38
2.5.1 通过ctypes调用C/C++代码 38
2.5.2 通过网络接口调用其他语言 40
2.6 小结 41
第2篇 PyTorch入门篇
第3章 PyTorch介绍 44
3.1 概述 44
3.2 与其他框架的比较 45
3.2.1 TensorFlow 45
3.2.2 PaddlePaddle 45
3.2.3 CNTK 46
3.3 PyTorch环境配置 46
3.3.1 通过pip安装 46
3.3.2 配置GPU环境 47
3.3.3 其他安装方法 48
3.3.4 在PyTorch中查看GPU是否可用 49
3.4 Transformers简介及安装 49
3.5 Apex简介及安装 50
3.6 小结 50
第4章 PyTorch基本使用方法 51
4.1 张量的使用 51
4.1.1 创建张量 51
4.1.2 张量的变换 53
4.1.3 张量的索引 59
4.1.4 张量的运算 59
4.2 使用torch.nn 60
4.3 激活函数 63
4.3.1 Sigmoid函数 63
4.3.2 Tanh函数 64
4.3.3 ReLU函数 64
4.3.4 Softmax函数 65
4.3.5 Softmin函数 65
4.3.6 LogSoftmax函数 66
4.4 损失函数 66
4.4.1 0-1损失函数 66
4.4.2 平方损失函数 66
4.4.3 绝对值损失函数 68
4.4.4 对数损失函数 68
4.5 优化器 69
4.5.1 SGD优化器 69
4.5.2 Adam优化器 70
4.5.3 AdamW优化器 70
4.6 数据加载 70
4.6.1 Dataset 70
4.6.2 DataLoader 71
4.7 使用PyTorch实现逻辑回归 73
4.7.1 生成随机数据 73
4.7.2 数据可视化 73
4.7.3 定义模型 74
4.7.4 训练模型 75
4.8 TorchText 76
4.8.1 安装TorchText 76
4.8.2 Data类 76
4.8.3 Datasets类 78
4.8.4 Vocab 79
4.8.5 utils 80
4.9 使用TensorBoard 81
4.9.1 安装和启动TensorBoard 81
4.9.2 在PyTorch中使用TensorBoard 81
4.10 小结 81
第5章 热身：使用字符级RNN分类帖子 82
5.1 数据与目标 82
5.1.1 数据 82
5.1.2 目标 84
5.2 输入与输出 84
5.2.1 统计数据集中出现的字符数量 85
5.2.2 使用One-Hot编码表示标题数据 85
5.2.3 使用词嵌入表示标题数据 85
5.2.4 输出 86
5.3 字符级RNN 87
5.3.1 定义模型 87
5.3.2 运行模型 87
5.4 数据预处理 89
5.4.1 合并数据并添加标签 90
5.4.2 划分训练集和数据集 90
5.5 训练与评估 90
5.5.1 训练 91
5.5.2 评估 91
5.5.3 训练模型 91
5.6 保存和加载模型 93
5.6.1 仅保存模型参数 93
5.6.2 保存模型与参数 93
5.6.3 保存词表 94
5.7 开发应用 94
5.7.1 给出任意标题的建议分类 94
5.7.2 获取用户输入并返回结果 95
5.7.3 开发Web API和Web界面 96
5.8 小结 97
第3篇 用PyTorch完成自然语言处理任务篇
第6章 分词问题 100
6.1 中文分词 100
6.1.1 中文的语言结构 100
6.1.2 未收录词 101
6.1.3 歧义 101
6.2 分词原理 101
6.2.1 基于词典匹配的分词 101
6.2.2 基于概率进行分词 102
6.2.3 基于机器学习的分词 105
6.3 使用第三方工具分词 106
6.3.1 S-MSRSeg 106
6.3.2 ICTCLAS 107
6.3.3 结巴分词 107
6.3.4 pkuseg 107
6.4 实践 109
6.4.1 对标题分词 109
6.4.2 统计词语数量与模型训练 109
6.4.3 处理用户输入 110
6.5 小结 110
第7章 RNN 111
7.1 RNN的原理 111
7.1.1 原始RNN 111
7.1.2 LSTM 113
7.1.3 GRU 114
7.2 PyTorch中的RNN 115
7.2.1 使用RNN 115
7.2.2 使用LSTM和GRU 116
7.2.3 双向RNN和多层RNN 117
7.3 RNN可以完成的任务 117
7.3.1 输入不定长，输出与输入长度相同 117
7.3.2 输入不定长，输出定长 118
7.3.3 输入定长，输出不定长 118
7.4 实践：使用PyTorch自带的RNN完成帖子分类 118
7.4.1 载入数据 118
7.4.2 定义模型 119
7.4.3 训练模型 119
7.5 小结 121
第8章 词嵌入 122
8.1 概述 122
8.1.1 词表示 122
8.1.2 PyTorch中的词嵌入 124
8.2 Word2vec 124
8.2.1 Word2vec简介 124
8.2.2 CBOW 125
8.2.3 SG 126
8.2.4 在PyTorch中使用Word2vec 126
8.3 GloVe 127
8.3.1 GloVe的原理 127
8.3.2 在PyTorch中使用GloVe预训练词向量 127
8.4 实践：使用预训练词向量完成帖子标题分类 128
8.4.1 获取预训练词向量 128
8.4.2 加载词向量 128
8.4.3 方法一：直接使用预训练词向量 129
8.4.4 方法二：在Embedding层中载入预训练词向量 130
8.5 小结 131
第9章 Seq2seq 132
9.1 概述 132
9.1.1 背景 132
9.1.2 模型结构 133
9.1.3 训练技巧 134
9.1.4 预测技巧 134
9.2 使用PyTorch实现Seq2seq 134
9.2.1 编码器 134
9.2.2 解码器 135
9.2.3 Seq2seq 136
9.2.4 Teacher Forcing 137
9.2.5 Beam Search 138
9.3 实践：使用Seq2seq完成机器翻译任务 138
9.3.1 数据集 138
9.3.2 数据预处理 139
9.3.3 构建训练集和测试集 141
9.3.4 定义模型 143
9.3.5 初始化模型 145
9.3.6 定义优化器和损失函数 146
9.3.7 训练函数和评估函数 146
9.3.8 训练模型 147
9.3.9 测试模型 148
9.4 小结 149
第10章 注意力机制 150
10.1 注意力机制的起源 150
10.1.1 在计算机视觉中的应用 150
10.1.2 在自然语言处理中的应用 151
10.2 使用注意力机制的视觉循环模型 151
10.2.1 背景 151
10.2.2 实现方法 152
10.3 Seq2seq中的注意力机制 152
10.3.1 背景 152
10.3.2 实现方法 153
10.3.3 工作原理 154
10.4 自注意力机制 155
10.4.1 背景 155
10.4.2 自注意力机制相关的工作 156
10.4.3 实现方法与应用 156
10.5 其他注意力机制 156
10.6 小结 157
第11章 Transformer 158
11.1 Transformer的背景 158
11.1.1 概述 158
11.1.2 主要技术 159
11.1.3 优势和缺点 159
11.2 基于卷积网络的Seq2seq 159
11.3 Transformer的结构 159
11.3.1 概述 160
11.3.2 Transformer中的自注意力机制 160
11.3.3 Multi-head Attention 161
11.3.4 使用Positional Encoding 162
11.4 Transformer的改进 164
11.5 小结 164
第12章 预训练语言模型 165
12.1 概述 165
12.1.1 为什么需要预训练 165
12.1.2 预训练模型的工作方式 166
12.1.3 自然语言处理预训练的发展 166
12.2 ELMo 167
12.2.1 特点 167
12.2.2 模型结构 167
12.2.3 预训练过程 168
12.3 GPT 168
12.3.1 特点 168
12.3.2 模型结构 168
12.3.3 下游任务 169
12.3.4 预训练过程 169
12.3.5 GPT-2和GPT-3 169
12.4 BERT 170
12.4.1 背景 171
12.4.2 模型结构 171
12.4.3 预训练 171
12.4.4 RoBERTa和ALBERT 171
12.5 Hugging Face Transformers 171
12.5.1 概述 172
12.5.2 使用Transformers 172
12.5.3 下载预训练模型 173
12.5.4 Tokenizer 173
12.5.5 BERT的参数 175
12.5.6 BERT的使用 176
12.5.7 GPT-2的参数 180
12.5.8 常见错误及其解决方法 181
12.6 其他开源中文预训练模型 181
12.6.1 TAL-EduBERT 181
12.6.2 Albert 182
12.7 实践：使用Hugging Face Transformers中的BERT做帖子标题分类 182
12.7.1 读取数据 182
12.7.2 导入包和设置参数 183
12.7.3 定义Dataset和DataLoader 183
12.7.4 定义评估函数 184
12.7.5 定义模型 185
12.7.6 训练模型 185
12.8 小结 186
第4篇 实战篇
第13章 项目：中文地址解析 188
13.1 数据集 188
13.1.1 实验目标与数据集介绍 188
13.1.2 载入数据集 190
13.2 词向量 195
13.2.1 查看词向量文件 195
13.2.2 载入词向量 196
13.3 BERT 196
13.3.1 导入包和配置 196
13.3.2 Dataset和DataLoader 198
13.3.3 定义模型 199
13.3.4 训练模型 200
13.3.5 获取预测结果 202
13.4 HTML5演示程序开发 203
13.4.1 项目结构 203
13.4.2 HTML5界面 204
13.4.3 创建前端事件 206
13.4.4 服务器逻辑 207
13.5 小结 211
第14章 项目：诗句补充 212
14.1 了解chinese-poetry数据集 212
14.1.1 下载chinese-poetry数据集 212
14.1.2 探索chinese-poetry数据集 213
14.2 准备训练数据 214
14.2.1 选择数据源 214
14.2.2 载入内存 214
14.2.3 切分句子 215
14.2.4 统计字频 218
14.2.5 删除低频字所在诗句 220
14.2.6 词到ID的转换 221
14.3 实现基本的LSTM 222
14.3.1 把处理好的数据和词表存入文件 222
14.3.2 切分训练集和测试集 224
14.3.3 Dataset 224
14.3.4 DataLoader 225
14.3.5 创建Dataset和DataLoader对象 226
14.3.6 定义模型 226
14.3.7 测试模型 228
14.3.8 训练模型 228
14.4 根据句子长度分组 229
14.4.1 按照句子长度分割数据集 229
14.4.2 不用考虑填充的DataLoader 230
14.4.3 创建多个DataLoader对象 230
14.4.4 处理等长句子的LSTM 231
14.4.5 评估模型效果 231
14.4.6 训练模型 232
14.5 使用预训练词向量初始化Embedding层 235
14.5.1 根据词向量调整字表 235
14.5.2 载入预训练权重 240
14.5.3 训练模型 240
14.6 使用Transformer完成诗句生成 244
14.6.1 位置编码 245
14.6.2 使用Transformer 245
14.6.3 训练和评估 246
14.7 使用GPT-2完成对诗模型 247
14.7.1 预训练模型 248
14.7.2 评估模型 249
14.7.3 Fine-tuning 252
14.8 开发HTML5演示程序 257
14.8.1 目录结构 257
14.8.2 HTML5界面 257
14.8.3 创建前端事件 259
14.8.4 服务器逻辑 260
14.8.5 检验结果 263
14.9 小结 264
参考文献 265
・ ・ ・ ・ ・ ・ (收起)第1 章 深度学习――机器大脑的结构 1
1.1 概述 3
1.1.1 可以做酸奶的面包机――通用机器的概念 3
1.1.2 连接主义 5
1.1.3 用机器设计机器 6
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 7
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 13
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 19
1.6.1 深度学习带来了什么 19
1.6.2 深度学习尚未做到什么 20
1.7 内容回顾与推荐阅读 21
1.8 参考文献 21
第2 章 知识图谱――机器大脑中的知识库 23
2.1 什么是知识图谱 25
2.2 知识图谱的构建 27
2.2.1 大规模知识库 27
2.2.2 互联网链接数据 28
2.2.3 互联网网页文本数据 29
2.2.4 多数据源的知识融合 29
2.3 知识图谱的典型应用 30
2.3.1 查询理解（Query Understanding） 30
2.3.2 自动问答（Question Answering） 32
2.3.3 文档表示（Document Representation） 33
2.4 知识图谱的主要技术 34
2.4.1 实体链指（Entity Linking） 34
2.4.2 关系抽取（Relation Extraction） 35
2.4.3 知识推理（Knowledge Reasoning） 37
2.4.4 知识表示（Knowledge Representation） 38
2.5 前景与挑战 39
2.6 内容回顾与推荐阅读 40
2.7 参考文献 41
第3 章 大数据系统――大数据背后的支撑技术 43
3.1 概述 45
3.2 高性能计算技术 46
3.2.1 超级计算机的组成 47
3.2.2 并行计算的系统支持 48
3.3 虚拟化和云计算技术 52
3.3.1 虚拟化技术 52
3.3.2 云计算服务 54
3.4 基于分布式计算的大数据系统 55
3.4.1 Hadoop 生态系统 55
3.4.2 Spark 61
3.4.3 典型的大数据基础架构 63
3.5 大规模图计算 63
3.5.1 分布式图计算框架 64
3.5.2 高效的单机图计算框架 65
3.6 NoSQL 66
3.6.1 MongoDB 简介 67
3.7 内容回顾与推荐阅读 69
3.8 参考文献 70
第4 章 智能问答――智能助手是如何炼成的 71
4.1 概述 73
4.2 问答系统的主要组成 77
4.3 文本问答系统 78
4.3.1 问题理解 78
4.3.2 知识检索 81
4.3.3 答案生成 83
4.4 社区问答系统 84
4.4.1 社区问答系统的结构 85
4.4.2 相似问题检索 86
4.4.3 答案过滤 86
4.5 多媒体问答系统 87
4.6 大型问答系统案例：IBM 沃森问答系统 89
4.6.1 沃森的总体结构 89
4.6.2 问题解析 90
4.6.3 知识储备 90
4.6.4 检索和候选答案生成 91
4.6.5 可信答案确定 92
4.7 内容回顾与推荐阅读 93
4.8 参考文献 94
第5 章 主题模型――机器的智能摘要利器 97
5.1 概述 99
5.2 主题模型出现的背景 100
5.3 第一个主题模型潜在语义分析 102
5.4 第一个正式的概率主题模型 104
5.5 第一个正式的贝叶斯主题模型 105
5.6 LDA 的概要介绍 106
5.6.1 LDA 的延伸理解――主题模型广义理解 109
5.6.2 模型求解 111
5.6.3 模型评估 112
5.6.4 模型选择：主题数目的确定 113
5.7 主题模型的变形与应用 114
5.7.1 基于LDA 的模型变种 114
5.7.2 基于LDA 的典型应用 115
5.7.3 一个基于主题模型的新浪名人话题排行榜应用 118
5.8 内容回顾与推荐阅读 122
5.9 参考文献 123
第6 章 个性化推荐系统――如何了解电脑背后的TA 129
6.1 概述 131
6.1.1 推荐系统的发展历史 132
6.1.2 推荐无处不在 133
6.1.3 从千人一面到千人千面 133
6.2 个性化推荐的基本问题 134
6.2.1 推荐系统的输入 135
6.2.2 推荐系统的输出 137
6.2.3 个性化推荐的形式化 137
6.2.4 推荐系统的三大核心问题 138
6.3 典型推荐算法浅析 139
6.3.1 推荐算法的分类 139
6.3.2 典型推荐算法介绍 140
6.3.3 基于矩阵分解的打分预测 146
6.3.4 推荐的可解释性 151
6.3.5 推荐算法的评价 153
6.3.6 我们走了多远 156
6.4 参考文献 160
第7 章 情感分析与意见挖掘――计算机如何了解人类情感 165
7.1 概述 167
7.2 情感分析的主要研究问题 172
7.3 情感分析的主要方法 175
7.3.1 构成情感和观点的基本元素 175
7.3.2 情感极性与情感词典 177
7.3.3 属性－观点对 182
7.3.4 情感分析 184
7.4 主要的情感词典资源 188
7.5 内容回顾与推荐阅读 189
7.6 参考文献 190
第8 章 面向社会媒体大数据的语言使用分析及应用 195
8.1 概述 197
8.2 面向社会媒体的自然语言使用分析 197
8.2.1 词汇的时空传播与演化 198
8.2.2 语言使用与个体差异 200
8.2.3 语言使用与社会地位 202
8.2.4 语言使用与群体分析 203
8.3 面向社会媒体的自然语言分析应用 206
8.3.1 社会预测 206
8.3.2 霸凌现象定量分析 207
8.4 未来研究的挑战与展望 208
8.5 参考文献 209
后 记 214
国际学术组织、学术会议与学术论文 214
国内学术组织、学术会议与学术论文 216
如何快速了解某个领域的研究进展 217
・ ・ ・ ・ ・ ・ (收起)1 深度计算――机器大脑的结构 1
1.1 惊人的深度学习 1
1.1.1 可以做酸奶的面包机：通用机器的概念 2
1.1.2 连接主义 4
1.1.3 用机器设计机器 5
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 6
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 14
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 20
1.6.1 深度学习带来了什么 20
1.6.2 深度学习尚未做到什么 21
1.7 内容回顾与推荐阅读 . 22
1.8 参考文献 23
2 知识图谱――机器大脑中的知识库 25
2.1 什么是知识图谱 25
2.2 知识图谱的构建 28
2.2.1 大规模知识库 28
2.2.2 互联网链接数据 29
2.2.3 互联网网页文本数据 30
2.2.4 多数据源的知识融合 31
2.3 知识图谱的典型应用 32
2.3.1 查询理解 32
2.3.2 自动问答 34
2.3.3 文档表示 35
2.4 知识图谱的主要技术 36
2.4.1 实体链指 36
2.4.2 关系抽取 37
2.4.3 知识推理 39
2.4.4 知识表示 40
2.5 前景与挑战 42
2.6 内容回顾与推荐阅读 45
2.7 参考文献 45
3 大数据系统――大数据背后的支撑技术 47
3.1 大数据有多大 47
3.2 高性能计算技术 49
3.2.1 超级计算机的组成 49
3.2.2 并行计算的系统支持 51
3.3 虚拟化和云计算技术 55
3.3.1 虚拟化技术 56
3.3.2 云计算服务 58
3.4 基于分布式计算的大数据系统 59
3.4.1 Hadoop 生态系统 60
3.4.2 Spark 67
3.4.3 典型的大数据基础架构 68
3.5 大规模图计算 69
3.5.1 分布式图计算框架 70
3.5.2 高效的单机图计算框架 71
3.6 NoSQL 72
3.6.1 NoSQL 数据库的类别 72
3.6.2 MongoDB 简介 74
3.7 内容回顾与推荐阅读 76
3.8 参考文献 77
4 主题模型――机器的智能摘要利器 78
4.1 由文档到主题 78
4.2 主题模型出现的背景 80
4.3 第一个主题模型：潜在语义分析 81
4.4 第一个正式的概率主题模型 84
4.5 第一个正式的贝叶斯主题模型 85
4.6 LDA 的概要介绍 86
4.6.1 LDA 的延伸理解：主题模型广义理解 . 90
4.6.2 模型求解 92
4.6.3 模型评估 93
4.6.4 模型选择：主题数目的确定 94
4.7 主题模型的变形与应用 95
4.7.1 基于 LDA 的变种模型 95
4.7.2 基于 LDA 的典型应用 97
4.7.3 基于主题模型的新浪名人话题排行榜应用 100
4.8 内容回顾与推荐阅读 104
4.9 参考文献 105
5 机器翻译――机器如何跨越语言障碍 110
5.1 机器翻译的意义 110
5.2 机器翻译的发展历史 111
5.2.1 基于规则的机器翻译 112
5.2.2 基于语料库的机器翻译 112
5.2.3 基于神经网络的机器翻译 114
5.3 经典的神经网络机器翻译模型 114
5.3.1 基于循环神经网络的神经网络机器翻译 114
5.3.2 从卷积序列到序列模型 117
5.3.3 基于自注意力机制的 Transformer 模型 118
5.4 机器翻译译文质量评价 120
5.5 机器翻译面临的挑战 121
5.6 参考文献 123
6 情感分析与意见挖掘――机器如何了解人类情感 125
6.1 情感可以计算吗 125
6.2 哪里需要文本情感分析 . 126
6.2.1 情感分析的宏观反映 127
6.2.2 情感分析的微观特征 128
6.3 情感分析的主要研究问题 129
6.4 情感分析的主要方法 132
6.4.1 构成情感和观点的基本元素 132
6.4.2 情感极性与情感词典 134
6.4.3 属性－观点对 141
6.4.4 情感极性分析 143
6.5 主要的情感分析资源 148
6.6 前景与挑战 149
6.7 内容回顾与推荐阅读 150
6.8 参考文献 151
7 智能问答与对话系统――智能助手是如何炼成的 154
7.1 问答：图灵测试的基本形式 154
7.2 从问答到对话 155
7.2.1 对话系统的基本过程 156
7.2.2 文本对话系统的常见场景 157
7.3 问答系统的主要组成 159
7.4 文本问答系统 161
7.4.1 问题理解 161
7.4.2 知识检索 165
7.4.3 答案生成 169
7.5 端到端的阅读理解问答技术 169
7.5.1 什么是阅读理解任务 170
7.5.2 阅读理解任务的模型 172
7.5.3 阅读理解任务的其他工程技巧 173
7.6 社区问答系统 174
7.6.1 社区问答系统的结构 174
7.6.2 相似问题检索 175
7.6.3 答案过滤 177
7.6.4 社区问答的应用 177
7.7 多媒体问答系统 179
7.8 大型问答系统案例：IBM 沃森问答系统 181
7.8.1 沃森的总体结构 182
7.8.2 问题解析 182
7.8.3 知识储备 183
7.8.4 检索和候选答案生成 184
7.8.5 可信答案确定 184
7.9 前景与挑战 186
7.10 内容回顾与推荐阅读 186
7.11 参考文献 187
8 个性化推荐系统――如何了解计算机背后的他 190
8.1 什么是推荐系统 190
8.2 推荐系统的发展历史 191
8.2.1 推荐无处不在 192
8.2.2 从千人一面到千人千面 193
8.3 个性化推荐的基本问题 194
8.3.1 推荐系统的输入 194
8.3.2 推荐系统的输出 196
8.3.3 个性化推荐的基本形式 197
8.3.4 推荐系统的三大核心问题 198
8.4 典型推荐算法浅析 199
8.4.1 推荐算法的分类 199
8.4.2 典型推荐算法介绍 200
8.4.3 基于矩阵分解的打分预测 207
8.4.4 基于神经网络的推荐算法 213
8.5 推荐的可解释性 214
8.6 推荐算法的评价 217
8.6.1 评分预测的评价 218
8.6.2 推荐列表的评价 219
8.6.3 推荐理由的评价 220
8.7 前景与挑战：我们走了多远 221
8.7.1 推荐系统面临的问题 221
8.7.2 推荐系统的新方向 223
8.8 内容回顾与推荐阅读 225
8.9 参考文献 226
9 机器写作――从分析到创造 228
9.1 什么是机器写作 228
9.2 艺术写作 229
9.2.1 机器写诗 229
9.2.2 AI 对联 233
9.3 当代写作 236
9.3.1 机器写稿 236
9.3.2 机器故事生成 239
9.4 内容回顾 241
9.5 参考文献 242
10 社交商业数据挖掘――从用户数据挖掘到商业智能应用 243
10.1 社交媒体平台中的数据宝藏 . 243
10.2 打通网络社区的束缚：用户网络社区身份的链指与融合 245
10.3 揭开社交用户的面纱：用户画像的构建 247
10.3.1 基于显式社交属性的构建方法 247
10.3.2 基于网络表示学习的构建方法 249
10.3.3 产品受众画像的构建 250
10.4 了解用户的需求：用户消费意图的识别 254
10.4.1 个体消费意图识别 254
10.4.2 群体消费意图识别 256
10.5 精准的供需匹配：面向社交平台的产品推荐算法 258
10.5.1 候选产品列表生成 258
10.5.2 基于学习排序算法的推荐框架 259
10.5.3 基于用户属性的排序特征构建 260
10.5.4 推荐系统的整体设计概览 261
10.6 前景与挑战 262
10.7 内容回顾与推荐阅读 263
10.8 参考文献 264
11 智慧医疗――信息技术在医疗领域应用的结晶 265
11.1 智慧医疗的起源 265
11.2 智慧医疗的庐山真面目 267
11.3 智慧医疗中的人工智能应用 268
11.3.1 医疗过程中的人工智能应用 268
11.3.2 医疗研究中的人工智能应用 272
11.4 前景与挑战 273
11.5 内容回顾与推荐阅读 275
11.6 参考文献 275
12 智慧司法――智能技术促进司法公正 276
12.1 智能技术与法律的碰撞 . 276
12.2 智慧司法相关研究 . 277
12.2.1 法律智能的早期研究 278
12.2.2 判决预测：虚拟法官的诞生与未来 279
12.2.3 文书生成：司法过程简化 283
12.2.4 要素提取：司法结构化 285
12.2.5 类案匹配：解决一案多判 289
12.2.6 司法问答：让机器理解法律 292
12.3 智慧司法的期望偏差与应用挑战 293
12.3.1 智慧司法的期望偏差 293
12.3.2 智慧司法的应用挑战 294
12.4 内容回顾与推荐阅读 295
12.5 参考文献 295
13 智能金融――机器金融大脑 298
13.1 智能金融正当其时 298
13.1.1 什么是智能金融 298
13.1.2 智能金融与金融科技、互联网金融的异同 298
13.1.3 智能金融适时而生 299
13.2 智能金融技术 301
13.2.1 大数据的机遇与挑战 301
13.2.2 智能金融中的自然语言处理 303
13.2.3 金融事理图谱 307
13.2.4 智能金融中的深度学习 310
13.3 智能金融应用 314
13.3.1 智能投顾 314
13.3.2 智能研报 315
13.3.3 智能客服 316
13.4 前景与挑战 317
13.5 内容回顾与推荐阅读 319
13.6 参考文献 319
14 计算社会学――透过大数据了解人类社会 320
14.1 透过数据了解人类社会 320
14.2 面向社会媒体的自然语言使用分析 321
14.2.1 词汇的时空传播与演化 322
14.2.2 语言使用与个体差异 325
14.2.3 语言使用与社会地位 326
14.2.4 语言使用与群体分析 328
14.3 面向社会媒体的自然语言分析应用 330
14.3.1 社会预测 330
14.3.2 霸凌现象定量分析 331
14.4 未来研究的挑战与展望 332
14.5 参考文献 333
后记 334
・ ・ ・ ・ ・ ・ (收起)1 深度计算――机器大脑的结构 1
1.1 惊人的深度学习 1
1.1.1 可以做酸奶的面包机：通用机器的概念 2
1.1.2 连接主义 4
1.1.3 用机器设计机器 5
1.1.4 深度网络 6
1.1.5 深度学习的用武之地 6
1.2 从人脑神经元到人工神经元 8
1.2.1 生物神经元中的计算灵感 8
1.2.2 激活函数 9
1.3 参数学习 10
1.3.1 模型的评价 11
1.3.2 有监督学习 11
1.3.3 梯度下降法 12
1.4 多层前馈网络 14
1.4.1 多层前馈网络 14
1.4.2 后向传播算法计算梯度 16
1.5 逐层预训练 17
1.6 深度学习是终极神器吗 20
1.6.1 深度学习带来了什么 20
1.6.2 深度学习尚未做到什么 21
1.7 内容回顾与推荐阅读 . 22
1.8 参考文献 23
2 知识图谱――机器大脑中的知识库 25
2.1 什么是知识图谱 25
2.2 知识图谱的构建 28
2.2.1 大规模知识库 28
2.2.2 互联网链接数据 29
2.2.3 互联网网页文本数据 30
2.2.4 多数据源的知识融合 31
2.3 知识图谱的典型应用 32
2.3.1 查询理解 32
2.3.2 自动问答 34
2.3.3 文档表示 35
2.4 知识图谱的主要技术 36
2.4.1 实体链指 36
2.4.2 关系抽取 37
2.4.3 知识推理 39
2.4.4 知识表示 40
2.5 前景与挑战 42
2.6 内容回顾与推荐阅读 45
2.7 参考文献 45
3 大数据系统――大数据背后的支撑技术 47
3.1 大数据有多大 47
3.2 高性能计算技术 49
3.2.1 超级计算机的组成 49
3.2.2 并行计算的系统支持 51
3.3 虚拟化和云计算技术 55
3.3.1 虚拟化技术 56
3.3.2 云计算服务 58
3.4 基于分布式计算的大数据系统 59
3.4.1 Hadoop 生态系统 60
3.4.2 Spark 67
3.4.3 典型的大数据基础架构 68
3.5 大规模图计算 69
3.5.1 分布式图计算框架 70
3.5.2 高效的单机图计算框架 71
3.6 NoSQL 72
3.6.1 NoSQL 数据库的类别 72
3.6.2 MongoDB 简介 74
3.7 内容回顾与推荐阅读 76
3.8 参考文献 77
4 主题模型――机器的智能摘要利器 78
4.1 由文档到主题 78
4.2 主题模型出现的背景 80
4.3 第一个主题模型：潜在语义分析 81
4.4 第一个正式的概率主题模型 84
4.5 第一个正式的贝叶斯主题模型 85
4.6 LDA 的概要介绍 86
4.6.1 LDA 的延伸理解：主题模型广义理解 . 90
4.6.2 模型求解 92
4.6.3 模型评估 93
4.6.4 模型选择：主题数目的确定 94
4.7 主题模型的变形与应用 95
4.7.1 基于 LDA 的变种模型 95
4.7.2 基于 LDA 的典型应用 97
4.7.3 基于主题模型的新浪名人话题排行榜应用 100
4.8 内容回顾与推荐阅读 104
4.9 参考文献 105
5 机器翻译――机器如何跨越语言障碍 110
5.1 机器翻译的意义 110
5.2 机器翻译的发展历史 111
5.2.1 基于规则的机器翻译 112
5.2.2 基于语料库的机器翻译 112
5.2.3 基于神经网络的机器翻译 114
5.3 经典的神经网络机器翻译模型 114
5.3.1 基于循环神经网络的神经网络机器翻译 114
5.3.2 从卷积序列到序列模型 117
5.3.3 基于自注意力机制的 Transformer 模型 118
5.4 机器翻译译文质量评价 120
5.5 机器翻译面临的挑战 121
5.6 参考文献 123
6 情感分析与意见挖掘――机器如何了解人类情感 125
6.1 情感可以计算吗 125
6.2 哪里需要文本情感分析 . 126
6.2.1 情感分析的宏观反映 127
6.2.2 情感分析的微观特征 128
6.3 情感分析的主要研究问题 129
6.4 情感分析的主要方法 132
6.4.1 构成情感和观点的基本元素 132
6.4.2 情感极性与情感词典 134
6.4.3 属性－观点对 141
6.4.4 情感极性分析 143
6.5 主要的情感分析资源 148
6.6 前景与挑战 149
6.7 内容回顾与推荐阅读 150
6.8 参考文献 151
7 智能问答与对话系统――智能助手是如何炼成的 154
7.1 问答：图灵测试的基本形式 154
7.2 从问答到对话 155
7.2.1 对话系统的基本过程 156
7.2.2 文本对话系统的常见场景 157
7.3 问答系统的主要组成 159
7.4 文本问答系统 161
7.4.1 问题理解 161
7.4.2 知识检索 165
7.4.3 答案生成 169
7.5 端到端的阅读理解问答技术 169
7.5.1 什么是阅读理解任务 170
7.5.2 阅读理解任务的模型 172
7.5.3 阅读理解任务的其他工程技巧 173
7.6 社区问答系统 174
7.6.1 社区问答系统的结构 174
7.6.2 相似问题检索 175
7.6.3 答案过滤 177
7.6.4 社区问答的应用 177
7.7 多媒体问答系统 179
7.8 大型问答系统案例：IBM 沃森问答系统 181
7.8.1 沃森的总体结构 182
7.8.2 问题解析 182
7.8.3 知识储备 183
7.8.4 检索和候选答案生成 184
7.8.5 可信答案确定 184
7.9 前景与挑战 186
7.10 内容回顾与推荐阅读 186
7.11 参考文献 187
8 个性化推荐系统――如何了解计算机背后的他 190
8.1 什么是推荐系统 190
8.2 推荐系统的发展历史 191
8.2.1 推荐无处不在 192
8.2.2 从千人一面到千人千面 193
8.3 个性化推荐的基本问题 194
8.3.1 推荐系统的输入 194
8.3.2 推荐系统的输出 196
8.3.3 个性化推荐的基本形式 197
8.3.4 推荐系统的三大核心问题 198
8.4 典型推荐算法浅析 199
8.4.1 推荐算法的分类 199
8.4.2 典型推荐算法介绍 200
8.4.3 基于矩阵分解的打分预测 207
8.4.4 基于神经网络的推荐算法 213
8.5 推荐的可解释性 214
8.6 推荐算法的评价 217
8.6.1 评分预测的评价 218
8.6.2 推荐列表的评价 219
8.6.3 推荐理由的评价 220
8.7 前景与挑战：我们走了多远 221
8.7.1 推荐系统面临的问题 221
8.7.2 推荐系统的新方向 223
8.8 内容回顾与推荐阅读 225
8.9 参考文献 226
9 机器写作――从分析到创造 228
9.1 什么是机器写作 228
9.2 艺术写作 229
9.2.1 机器写诗 229
9.2.2 AI 对联 233
9.3 当代写作 236
9.3.1 机器写稿 236
9.3.2 机器故事生成 239
9.4 内容回顾 241
9.5 参考文献 242
10 社交商业数据挖掘――从用户数据挖掘到商业智能应用 243
10.1 社交媒体平台中的数据宝藏 . 243
10.2 打通网络社区的束缚：用户网络社区身份的链指与融合 245
10.3 揭开社交用户的面纱：用户画像的构建 247
10.3.1 基于显式社交属性的构建方法 247
10.3.2 基于网络表示学习的构建方法 249
10.3.3 产品受众画像的构建 250
10.4 了解用户的需求：用户消费意图的识别 254
10.4.1 个体消费意图识别 254
10.4.2 群体消费意图识别 256
10.5 精准的供需匹配：面向社交平台的产品推荐算法 258
10.5.1 候选产品列表生成 258
10.5.2 基于学习排序算法的推荐框架 259
10.5.3 基于用户属性的排序特征构建 260
10.5.4 推荐系统的整体设计概览 261
10.6 前景与挑战 262
10.7 内容回顾与推荐阅读 263
10.8 参考文献 264
11 智慧医疗――信息技术在医疗领域应用的结晶 265
11.1 智慧医疗的起源 265
11.2 智慧医疗的庐山真面目 267
11.3 智慧医疗中的人工智能应用 268
11.3.1 医疗过程中的人工智能应用 268
11.3.2 医疗研究中的人工智能应用 272
11.4 前景与挑战 273
11.5 内容回顾与推荐阅读 275
11.6 参考文献 275
12 智慧司法――智能技术促进司法公正 276
12.1 智能技术与法律的碰撞 . 276
12.2 智慧司法相关研究 . 277
12.2.1 法律智能的早期研究 278
12.2.2 判决预测：虚拟法官的诞生与未来 279
12.2.3 文书生成：司法过程简化 283
12.2.4 要素提取：司法结构化 285
12.2.5 类案匹配：解决一案多判 289
12.2.6 司法问答：让机器理解法律 292
12.3 智慧司法的期望偏差与应用挑战 293
12.3.1 智慧司法的期望偏差 293
12.3.2 智慧司法的应用挑战 294
12.4 内容回顾与推荐阅读 295
12.5 参考文献 295
13 智能金融――机器金融大脑 298
13.1 智能金融正当其时 298
13.1.1 什么是智能金融 298
13.1.2 智能金融与金融科技、互联网金融的异同 298
13.1.3 智能金融适时而生 299
13.2 智能金融技术 301
13.2.1 大数据的机遇与挑战 301
13.2.2 智能金融中的自然语言处理 303
13.2.3 金融事理图谱 307
13.2.4 智能金融中的深度学习 310
13.3 智能金融应用 314
13.3.1 智能投顾 314
13.3.2 智能研报 315
13.3.3 智能客服 316
13.4 前景与挑战 317
13.5 内容回顾与推荐阅读 319
13.6 参考文献 319
14 计算社会学――透过大数据了解人类社会 320
14.1 透过数据了解人类社会 320
14.2 面向社会媒体的自然语言使用分析 321
14.2.1 词汇的时空传播与演化 322
14.2.2 语言使用与个体差异 325
14.2.3 语言使用与社会地位 326
14.2.4 语言使用与群体分析 328
14.3 面向社会媒体的自然语言分析应用 330
14.3.1 社会预测 330
14.3.2 霸凌现象定量分析 331
14.4 未来研究的挑战与展望 332
14.5 参考文献 333
后记 334
・ ・ ・ ・ ・ ・ (收起)目 录
第1章 数据革命 1
1.1 数据生成 1
1.2 Spark 2
1.2.1 Spark Core 3
1.2.2 Spark组件 4
1.3 设置环境 5
1.3.1 Windows 5
1.3.2 iOS 6
1.4 小结 7
第2章 机器学习简介 9
2.1 有监督机器学习 10
2.2 无监督机器学习 12
2.3 半监督机器学习 14
2.4 强化学习 14
2.5 小结 15
第3章 数据处理 17
3.1 加载和读取数据 17
3.2 添加一个新列 20
3.3 筛选数据 21
3.3.1 条件1 21
3.3.2 条件2 22
3.4 列中的非重复值 23
3.5 数据分组 23
3.6 聚合 25
3.7 用户自定义函数(UDF) 26
3.7.1 传统的Python函数 26
3.7.2 使用lambda函数 27
3.7.3 Pandas UDF(向量化的UDF) 28
3.7.4 Pandas UDF(多列) 29
3.8 去掉重复值 29
3.9 删除列 30
3.10 写入数据 30
3.10.1 csv 31
3.10.2 嵌套结构 31
3.11 小结 31
第4章 线性回归 33
4.1 变量 33
4.2 理论 34
4.3 说明 41
4.4 评估 42
4.5 代码 43
4.5.1 数据信息 43
4.5.2 步骤1：创建
SparkSession对象 44
4.5.3 步骤2：读取数据集 44
4.5.4 步骤3：探究式数据分析 44
4.5.5 步骤4：特征工程化 45
4.5.6 步骤5：划分数据集 47
4.5.7 步骤6：构建和训练线性回归模型 47
4.5.8 步骤7：在测试数据上评估线性回归模型 48
4.6 小结 48
第5章 逻辑回归 49
5.1 概率 49
5.1.1 使用线性回归 50
5.1.2 使用Logit 53
5.2 截距(回归系数) 54
5.3 虚变量 55
5.4 模型评估 56
5.4.1 正确的正面预测 56
5.4.2 正确的负面预测 57
5.4.3 错误的正面预测 57
5.4.4 错误的负面预测 57
5.4.5 准确率 57
5.4.6 召回率 57
5.4.7 精度 58
5.4.8 F1分数 58
5.4.9 截断/阈值概率 58
5.4.10 ROC曲线 58
5.5 逻辑回归代码 59
5.5.1 数据信息 59
5.5.2 步骤1：创建Spark会话对象 60
5.5.3 步骤2：读取数据集 60
5.5.4 步骤3：探究式数据分析 60
5.5.5 步骤4：特征工程 63
5.5.6 步骤5：划分数据集 68
5.5.7 步骤6：构建和训练逻辑回归模型 69
5.5.8 训练结果 69
5.5.9 步骤7：在测试数据上评估线性回归模型 70
5.5.10 混淆矩阵 71
5.6 小结 72
第6章 随机森林 73
6.1 决策树 73
6.1.1 熵 75
6.1.2 信息增益 76
6.2 随机森林 78
6.3 代码 80
6.3.1 数据信息 80
6.3.2 步骤1：创建SparkSession对象 81
6.3.3 步骤2：读取数据集 81
6.3.4 步骤3：探究式数据分析 81
6.3.5 步骤4：特征工程 85
6.3.6 步骤5：划分数据集 86
6.3.7 步骤6：构建和训练随机森林模型 87
6.3.8 步骤7：基于测试数据进行评估 87
6.3.9 准确率 89
6.3.10 精度 89
6.3.11 AUC曲线下的面积 89
6.3.12 步骤8：保存模型 90
6.4 小结 90
第7章 推荐系统 91
7.1 推荐 91
7.1.1 基于流行度的RS 92
7.1.2 基于内容的RS 93
7.1.3 基于协同过滤的RS 95
7.1.4 混合推荐系统 103
7.2 代码 104
7.2.1 数据信息 105
7.2.2 步骤1：创建SparkSession对象 105
7.2.3 步骤2：读取数据集 105
7.2.4 步骤3：探究式数据分析 105
7.2.5 步骤4：特征工程 108
7.2.6 步骤5：划分数据集 109
7.2.7 步骤6：构建和训练推荐系统模型 110
7.2.8 步骤7：基于测试数据进行预测和评估 110
7.2.9 步骤8：推荐活动用户可能会喜欢的排名靠前的电影 111
7.3 小结 114
第8章 聚类 115
8.1 初识聚类 115
8.2 用途 117
8.2.1 K-均值 117
8.2.2 层次聚类 127
8.3 代码 131
8.3.1 数据信息 131
8.3.2 步骤1：创建SparkSession对象 131
8.3.3 步骤2：读取数据集 131
8.3.4 步骤3：探究式数据分析 131
8.3.5 步骤4：特征工程 133
8.3.6 步骤5：构建K均值聚类模型 133
8.3.7 步骤6：聚类的可视化 136
8.4 小结 137
第9章 自然语言处理 139
9.1 引言 139
9.2 NLP涉及的处理步骤 139
9.3 语料 140
9.4 标记化 140
9.5 移除停用词 141
9.6 词袋 142
9.7 计数向量器 143
9.8 TF-IDF 144
9.9 使用机器学习进行文本分类 145
9.10 序列嵌入 151
9.11 嵌入 151
9.12 小结 160
・ ・ ・ ・ ・ ・ (收起)导读 1
Contributors 17
Preface 23
Part I Fundamental aspects 1
1 Ontology and the lexicon： a multidisciplinary perspective 3
1.1 Situating ontologies and lexical resources 3
1.2 The content of ontologies 10
1.3 Theoretical framework for the
ontologies／lexicons interface 14
1.4 From ontologies to the lexicon and back 21
1.5 Outline of chapters 23
2 Formal ontology as interlingua： the SUMO and
WordNet linking project and global WordNet 25
2.1 WordNet 25
2.2 Principles of construction of formal ontologies
and lexicons 29
2.3 Mappings 30
2.4 Interpreting language 32
2.5 Global WordNet 33
2.6 SUMO translation templates 35
3 Interfacing WordNet with DOLCE： towards OntoWordNet 36
3.1 Introduction 36
3.2 WordNet’s preliminary analysis 37
3.3 The DOLCE upper ontology 39
3.4 Mapping WordNet into DOLCE 48
3.5 Conclusion 52
4 Reasoning over natural language text by means of FrameNet and ontologies 53
4.1 Introduction 53
4.2 An introduction to the FrameNet lexicon 54
4.3 Linking FrameNet to ontologies for reasoning 56
4.4 Formalizing FrameNet in OWL DL 57
4.5 Reasoning over FrameNet―annotated text 62
4.6 Linking FrameNet to SUMO 66
4.7 Discussion 69
4.8 Conclusion and outlook 70
5 Synergizing ontologies and the lexicon： a roadmap 72
5.1 Formal mappings between ontologies 72
5.2 Evaluation of ontolex resources 73
5.3 Bridging different lexical models and resources 75
5.4 Technological framework 77
Part II Discovery and representation of conceptual systems 79
6 Experiments of ontology construction with Formal Concept Analysis 81
6.1 Introduction 81
6.2 Basic concepts and related work 82
6.3 Dataset selection and design of experiments 86
6.4 Evaluation and discussion 92
6.5 Conclusion and future work 96
7 Ontology， lexicon， and fact repository as leveraged to interpret events of change 98
7.1 Introduction 98
7.2 A snapshot of OntoSem 100
7.3 Motivation for pursuing deep analysis of events of change 101
7.4 Increase 102
7.5 Content divorced from its rendering 114
7.6 NLP with reasoning and for reasoning 117
7.7 Conclusion 118
8 Hantology： conceptual system discovery based on orthographic convention 122
8.1 Introduction： hanzi and conventionalized conceptualization 122
8.2 General framework 126
8.3 Conceptualization and classification of the radicals system 128
8.4 The ontology of a radical as a semantic symbol 132
8.5 The architecture of Hantology 133
8.6 OWL encoding of Hantology 137
8.7 Summary 139
8.8 Conclusion 142
9 What’s in a schema？ 144
9.1 Introduction 144
9.2 An ontology for cognitive linguistics 146
9.3 The c.DnS ontology 148
9.4 Schemata， mental spaces， and constructions 161
9.5 An embodied semiotic metamodel 166
9.6 Applying Semion to FrameNet and related resources 169
9.7 Conclusion 181
Part III Interfacing ontologies and lexical resources 183
10 Interfacing ontologies and lexical resources 185
10.1 Introduction 185
10.2 Classifying experiments in ontologies and lexical resources 185
10.3 Ontologies and their construction 188
10.4 How actual resources fit the classification 190
10.5 Two practical examples 194
10.6 Available tools for the ontology lexical resource interface 196
10.7 Conclusion 200
11 Sinica BOW （Bilingual Ontological WordNet）：integration of bilingual WordNet and SUMO 201
11.1 Background and motivation 201
11.2 Resources and structure required in the BOW approach 202
11.3 Interfacing multiple resources： a lexicon―driven approach 204
11.4 Integration of multiple knowledge sources 207
11.5 Updating and future improvements 209
11.6 Conclusion 210
12 Ontology―based semantic lexicons：mapping between terms and object descriptions 212
12.1 Introduction 212
12.2 Why we need semantic lexicons 213
12.3 More semantics than we need 215
12.4 The semantics we need is in ontologies 218
12.5 Conclusion 223
13 Merging global and specialized linguistic ontologies 224
13.1 Introduction 224
13.2 Linguistic ontologies versus formal ontologies 226
13.3 Specialized linguistic ontologies 229
13.4 The plug―in approach 230
13.5 Experiments 236
13.6 Applications and extensions 237
13.7 Conclusion 238
Part IV Learning and using ontological knowledge 239
14 The life cycle of knowledge 241
14.1 Introduction 241
14.2 Using ontolexical knowledge in NLP 242
14.3 Creating ontolexical knowledge with NLP 249
14.4 Conclusion 256
15 The Omega ontology 258
15.1 Introduction 258
15.2 Constituents of Omega 258
15.3 Structure of Omega 260
15.4 Construction of Omega via merging 263
15.5 Omega’s auxiliary knowledge sources 264
15.6 Applications 266
15.7 Omega 5 and the OntoNotes project 267
15.8 Discussion and future work 268
15.9 Conclusion 269
16 Automatic acquisition of lexico―semantic knowledge for question answering 271
16.1 Introduction 271
16.2 Lexico―semantic knowledge for QA 272
16.3 Related work 274
16.4 Extracting semantically similar words 275
16.5 Using automatically acquired role and function words 279
16.6 Using automatically acquired categorized NEs 280
16.7 Evaluation 283
16.8 Conclusion and future work 286
17 Agricultural ontology construction and maintenance in Thai 288
17.1 Introduction 288
17.2 A framework of ontology construction and maintenance 290
17.3 Ontology acquisition from texts 291
17.4 Ontology acquisitions from a dictionary and a thesaurus 301
17.5 Integration into an ontological tree 306
17.6 Conclusion 307
References 309
Index 335
・ ・ ・ ・ ・ ・ (收起)第 1 章 心爱的聊天机器人 .................................................................................................. 1
聊天机器人的受欢迎程度 .......................................................................................... 2
Python 之禅以及为什么它适用于聊天机器人 .......................................................... 3
对聊天机器人的需求 .................................................................................................. 4
商业视角 ............................................................................................................ 5
开发者视角 ........................................................................................................ 9
受聊天机器人影响的行业 ........................................................................................ 11
聊天机器人的发展历程 ............................................................................................ 12
1950 .................................................................................................................. 12
1966 .................................................................................................................. 12
1972 .................................................................................................................. 12
1981 .................................................................................................................. 12
1985 .................................................................................................................. 12
1992 .................................................................................................................. 13
1995 .................................................................................................................. 13
1996 .................................................................................................................. 13
2001 .................................................................................................................. 13
2006 .................................................................................................................. 13
2010 .................................................................................................................. 13
2012 .................................................................................................................. 14
2014 .................................................................................................................. 14
2015 .................................................................................................................. 14
2016 .................................................................................................................. 14
2017 .................................................................................................................. 14
我可以用聊天机器人解决什么样的问题 ................................................................ 15
这个问题能通过简单的问答或来回交流解决吗 ........................................... 15
这个工作是否有高度重复性，需要进行数据收集和分析 ........................... 15
你的机器人的任务可以自动化和固定化吗 ................................................... 16
一个 QnA 机器人 ...................................................................................................... 16
从聊天机器人开始 .................................................................................................... 17
聊天机器人中的决策树 ............................................................................................ 18
在聊天机器人中使用决策树 ........................................................................... 18
决策树如何起到作用 ....................................................................................... 18
最好的聊天机器人/机器人框架 ............................................................................... 21
聊天机器人组件和使用的相关术语 ........................................................................ 23
意图（Intent） ................................................................................................. 23
实体（Entities） .............................................................................................. 23
话术（Utterances） ......................................................................................... 24
训练机器人 ...................................................................................................... 24
置信度得分 ...................................................................................................... 24
第 2 章 聊天机器人中的自然语言处理 ............................................................................ 25
为什么我需要自然语言处理知识来搭建聊天机器人 ............................................ 25
spaCy 是什么 ............................................................................................................. 26
spaCy 的基准测试结果 .................................................................................... 27
spaCy 提供了什么能力 .................................................................................... 27
spaCy 的特性 ............................................................................................................. 28
安装和前置条件 .............................................................................................. 29
spaCy 模型是什么............................................................................................ 31
搭建聊天机器人所使用的自然语言处理基本方法 ................................................ 32
词性标注 .......................................................................................................... 32
词干提取和词性还原 ....................................................................................... 36
命名实体识别 .................................................................................................. 38
停用词 .............................................................................................................. 41
依存句法分析 .................................................................................................. 43
名词块 .............................................................................................................. 47
计算相似度 ...................................................................................................... 49
搭建聊天机器人时自然语言处理的一些好方法 .................................................... 51
分词 .................................................................................................................. 51
正则表达式 ...................................................................................................... 52
总结 ........................................................................................................................... 53
第 3 章 轻松搭建聊天机器人 ............................................................................................ 55
Dialogflow 简介 ........................................................................................................ 55
开始 ........................................................................................................................... 56
搭建一个点餐机器人 ....................................................................................... 57
确定范围 .......................................................................................................... 57
列举意图 .......................................................................................................... 57
列举实体 .......................................................................................................... 58
搭建点餐机器人 ........................................................................................................ 58
Dialogflow 入门 ............................................................................................... 59
创建意图的几大要点 ....................................................................................... 62
创建意图并添加自定义话术 ........................................................................... 62
为意图添加默认回复 ....................................................................................... 63
菜品描述意图及附属实体 ............................................................................... 64
理解用户需求并回复 ....................................................................................... 67
将 Dialogflow 聊天机器人发布到互联网上 ............................................................ 72
在 Facebook Messenger 上集成 Dialogflow 聊天机器人 ........................................ 75
设置 Facebook .................................................................................................. 76
创建一个 Facebook 应用程序 ......................................................................... 76
设置 Dialogflow 控制台 .................................................................................. 77
配置 Webhook .................................................................................................. 79
测试信使机器人 .............................................................................................. 80
Fulfillment .................................................................................................................. 83
启用 Webhook .................................................................................................. 85
检查响应数据 .................................................................................................. 87
总结 ........................................................................................................................... 89
第 4 章 从零开始搭建聊天机器人 .................................................................................... 91
Rasa NLU 是什么 ...................................................................................................... 92
我们为什么要使用 Rasa NLU ......................................................................... 92
深入了解 Rasa NLU ......................................................................................... 93
从零开始训练和搭建聊天机器人 ............................................................................ 94
搭建一个星座聊天机器人 ............................................................................... 94
星座机器人和用户之间的对话脚本 ............................................................... 95
为聊天机器人准备数据 ................................................................................... 96
训练聊天机器人模型 ..................................................................................... 101
从模型进行预测 ............................................................................................ 103
使用 Rasa Core 进行对话管理 ............................................................................... 105
深入了解 Rasa Core 及对话系统 .................................................................. 105
理解 Rasa 概念 ............................................................................................... 108
为聊天机器人创建域文件 ............................................................................. 111
为聊天机器人编写自定义动作 .............................................................................. 113
训练机器人的数据准备 .......................................................................................... 116
构造故事数据 ................................................................................................ 117
交互学习 ........................................................................................................ 119
将对话导出成故事......................................................................................... 132
测试机器人 .............................................................................................................. 133
测试用例一 .................................................................................................... 133
测试用例二 .................................................................................................... 134
总结 ......................................................................................................................... 135
第 5 章 部署自己的聊天机器人 ...................................................................................... 137
前提条件.................................................................................................................. 137
Rasa 的凭据管理 ..................................................................................................... 137
在 Facebook 上部署聊天机器人 ............................................................................ 139
在 Heroku 上创建一个应用 ........................................................................... 139
在本地系统中安装 Heroku ............................................................................ 140
在 Facebook 上创建和设置应用程序 ........................................................... 140
在 Heroku 上创建和部署 Rasa 动作服务器应用程序 ................................. 143
创建 Rasa 聊天机器人 API 应用程序........................................................... 144
创建一个用于 Facebook Messenger 聊天机器人的独立脚本 ..................... 144
验证对话管理应用程序在 Heroku 上的部署情况 ....................................... 147
集成 Facebook Webhook ................................................................................ 148
部署后验证：Facebook 聊天机器人 ............................................................ 149
在 Slack 上部署聊天机器人 ................................................................................... 151
为 Slack 创建独立脚本 .................................................................................. 151
编辑 Procfile ................................................................................................... 154
将 Slack 机器人最终部署到 Heroku 上 ........................................................ 154
订阅 Slack 事件 .............................................................................................. 155
订阅机器人事件 ............................................................................................ 156
部署后验证：Slack 机器人 ........................................................................... 156
独立部署聊天机器人 .............................................................................................. 157
编写脚本实现自己的聊天机器人通道 ......................................................... 158
编写 Procfile 并部署到 Web 上 ..................................................................... 159
验证你的聊天机器人 API ............................................................................. 160
绘制聊天机器人的图形界面 ......................................................................... 161
总结 ......................................................................................................................... 165
・ ・ ・ ・ ・ ・ (收起)自然语言处理Java实现 1
第1章 自然语言处理实践基础 - 1 -
1.1 准备开发环境 - 1 -
1.1.1 Windows命令行Cmd - 2 -
1.1.2 在Windows下使用Java - 3 -
1.1.3 Linux终端 - 5 -
1.1.4 在Linux下使用Java - 6 -
1.1.5 Eclipse集成开发环境 - 8 -
1.2 技术基础 - 8 -
1.2.1 机器学习 - 8 -
1.2.2 Java基础 - 9 -
1.2.3 信息采集 - 10 -
1.2.4 文本挖掘 - 11 -
1.2.5 SWIG扩展Java性能 - 11 -
1.2.6 代码移植 - 13 -
1.2.7 语义 - 15 -
1.2.8 Hadoop分布式计算框架 - 16 -
1.3 本章小结 - 19 -
1.4 专业术语 - 20 -
第2章 中文分词原理与实现 21
2.1 接口 21
2.1.1 切分方案 22
2.1.2 词典格式 23
2.2 散列表最长匹配中文分词 24
2.2.1 算法实现 24
2.2.2 使用Ant构建分词jar包 26
2.2.3 使用Maven构建分词jar包 28
2.2.4 使用Gradle构建分词jar包 30
2.2.5 生成JavaDoc 32
2.3 查找词典算法 32
2.3.1 标准Trie树 32
2.3.2 三叉Trie树 36
2.4 Trie树正向最大长度匹配法 43
2.4.1 逆向最大长度匹配法 47
2.4.2 有限状态机识别未登录串 52
2.5 概率语言模型的分词方法 59
2.5.1 一元模型 60
2.5.2 整合基于规则的方法 66
2.5.3 表示切分词图 67
2.5.4 形成切分词图 73
2.5.5 数据基础 75
2.5.6 改进一元模型 85
2.5.7 二元词典 88
2.5.8 完全二叉树组 93
2.5.9 三元词典 96
2.5.10 N元模型 97
2.5.11 N元分词 98
2.5.12 生成语言模型 105
2.5.13 评估语言模型 106
2.5.14 概率分词的流程与结构 107
2.5.15 可变长n元分词 108
2.5.16 条件随机场 108
2.6 新词发现 108
2.7 安卓中文输入法 113
2.8 词性标注 116
2.8.1 数据基础 119
2.8.2 隐马尔可夫模型 120
2.8.3 存储数据 127
2.8.4 统计数据 133
2.8.5 整合切分与词性标注 135
2.8.6 大词表 139
2.8.7 词性序列 140
2.8.8 基于转换的错误学习方法 140
2.8.9 条件随机场 142
2.9 词类模型 143
2.10 未登录词识别 144
2.10.1 未登录人名 144
2.10.2 提取候选人名 145
2.10.3 最长人名切分 151
2.10.4 一元概率人名切分 152
2.10.5 二元概率人名切分 153
2.10.6 未登录地名 156
2.10.7 未登录企业名 156
2.11 中文分词总体结构 156
2.12 平滑算法 158
2.12.1 最大熵 161
2.12.2 条件随机场 166
2.13 地名切分 167
2.13.1 识别未登录地名 167
2.13.2 整体流程 173
2.14 企业名切分 175
2.14.1 识别未登录词 175
2.14.2 整体流程 177
2.15 结果评测 177
2.16 本章小结 178
2.17 专业术语 179
第3章 语义分析 181
3.1 句法分析树 181
3.2 依存文法 186
3.2.1 中文依存文法 186
3.2.2 英文依存文法 193
3.2.3 生成依存树 201
3.2.4 机器学习的方法 205
3.3 依存语言模型 205
3.4 使用Java计算机语言的语义分析 206
3.5 小结 209
3.6 专业术语 210
第4章 文章分析与生成 211
4.1 分词 211
4.1.1 句子切分 211
4.1.2 识别未登录串 213
4.1.3 切分边界 214
4.2 词性标注 215
4.3 重点词汇 218
4.4 句子时态 219
4.5 自动写作 220
4.6 本章小结 221
第5章 文档排重 222
5.1 相似度计算 222
5.1.1 夹角余弦 222
5.1.2 最长公共子串 224
5.1.3 同义词替换 228
5.1.4 地名相似度 229
5.1.5 企业名相似度 231
5.2 文档排重 232
5.2.1 关键词排重 232
5.2.2 SimHash 235
5.2.3 分布式文档排重 246
5.2.4 使用文本排重 247
5.3 在搜索引擎中使用文本排重 247
5.4 本章小结 248
5.5 专业术语 248
第6章 信息提取 249
6.1 指代消解 249
6.2 中文关键词提取 250
6.2.1 关键词提取的基本方法 250
6.2.2 HITS算法应用于关键词提取 252
6.2.3 从网页中提取关键词 254
6.3 信息提取 254
6.3.1 提取联系方式 256
6.3.2 从互联网提取信息 256
6.3.3 提取地名 257
6.4 拼写纠错 258
6.4.1 模糊匹配问题 260
6.4.2 正确词表 269
6.4.3 英文拼写检查 271
6.4.4 中文拼写检查 272
6.5 输入提示 274
6.6 本章小结 275
6.7专业术语 275
第7章 自动摘要 276
7.1 自动摘要技术 276
7.1.1 英文文本摘要 278
7.1.2 中文文本摘要 280
7.1.3 基于篇章结构的自动摘要 284
7.1.4 句子压缩 284
7.2 指代消解 284
7.3 多文档摘要 284
7.4 分布式部署 285
7.5 本章小结 287
7.5专业术语 287
第8章 文本分类 288
8.1 地名分类 289
8.2 文本模板分类 289
8.3 特征提取 290
8.4 线性分类器 293
8.4.1 关键词加权法 294
8.4.2 朴素贝叶斯 297
8.4.3 贝叶斯文本分类 302
8.4.4 支持向量机 303
8.4.5 多级分类 311
8.4.6 使用sklearn实现文本分类 312
8.4.7 规则方法 312
8.4.8 网页分类 315
8.5 FastText文本分类 316
8.5.1 词向量 316
8.5.2 JavaCPP包装Java接口 316
8.5.3 使用JFastText 318
8.6 最大熵分类器 319
8.7 文本聚类 320
8.7.1 K均值聚类方法 320
8.7.2 K均值实现 322
8.7.3 深入理解DBScan算法 326
8.7.4 使用DBScan算法聚类实例 327
8.8 持续集成 329
8.9 本章小结 330
8.9专业术语 330
第9章 文本倾向性分析 331
9.1 确定词语的褒贬倾向 332
9.2 实现情感识别 334
9.3 本章小结 336
9.4专业术语 337
第10章 语音识别 338
10.1 总体结构 338
10.1.1 识别中文 340
10.1.2 自动问答 341
10.2 语音库 343
10.3 语音 344
10.3.1 标注语音 347
10.3.2 动态时间规整计算相似度 348
10.4 Sphinx语音识别 351
10.4.1 中文训练集 352
10.4.2 使用Sphinx4 353
10.4.3 ARPA文件格式 356
10.4.4 运行于Android的PocketSphinx 358
10.5 说话人识别 362
10.6 本章小结 363
10.7 术语表 363
第11章 问答系统 365
11.1 问答系统的结构 365
11.1.1 提取问答对 366
11.1.2 等价问题 366
11.2 问句分析 366
11.2.1 问题类型 367
11.2.2 句型 369
11.2.3 用户意图识别 370
11.2.4 业务类型 370
11.2.5 依存树 370
11.2.6 指代消解 371
11.2.7 二元关系 371
11.2.8 问句模板 373
11.2.9 结构化问句模板 375
11.2.10 检索方式 376
11.2.11 问题重写 380
11.2.12 提取事实 380
11.2.13 验证答案 382
11.3 知识库 382
11.3.1 语义库 383
11.4 AIML聊天机器人 385
11.4.1 交互式问答 387
11.4.2 垂直领域问答系统 388
11.4.3 语料库 391
11.4.4 客户端 391
11.5 自然语言生成 391
11.6 JavaFX开发界面 393
11.7 本章小结 394
11.8 术语表 394
第12章 机器翻译 395
12.1 使用机器翻译API 395
12.2 翻译日期 395
12.3 神经网络机器翻译 397
12.4 辅助机器翻译 399
12.5 机器翻译的评价 400
12.6 本章小结 400
参考资源 402
书籍 402
网址 402
后记 403
・ ・ ・ ・ ・ ・ (收起)译者序
第2版前言
第1版前言
第1版致谢
第1章　基础知识 1
1.1　概率测度 1
1.2　随机变量 2
1.2.1　连续随机变量和离散随机变量 2
1.2.2　多元随机变量的联合分布 3
1.3　条件分布 4
1.3.1　贝叶斯法则 5
1.3.2　独立随机变量与条件独立随机变量 6
1.3.3　可交换的随机变量 6
1.4　随机变量的期望 7
1.5　模型 9
1.5.1　参数模型与非参数模型 9
1.5.2　模型推断 10
1.5.3　生成模型 11
1.5.4　模型中的独立性假定 13
1.5.5　有向图模型 13
1.6　从数据场景中学习 15
1.7　贝叶斯学派和频率学派的哲学（冰山一角） 17
1.8　本章小结 17
1.9　习题 18
第2章　绪论 19
2.1　贝叶斯统计与自然语言处理的结合点概述 19
2.2　第一个例子：隐狄利克雷分配模型 22
2.2.1　狄利克雷分布 26
2.2.2　推断 28
2.2.3　总结 29
2.3　第二个例子：贝叶斯文本回归 30
2.4　本章小结 31
2.5　习题 31
第3章　先验 33
3.1　共轭先验 33
3.1.1　共轭先验和归一化常数 36
3.1.2　共轭先验在隐变量模型中的应用 37
3.1.3　混合共轭先验 38
3.1.4　重新归一化共轭分布 39
3.1.5　是否共轭的讨论 39
3.1.6　总结 40
3.2　多项式分布和类别分布的先验 40
3.2.1　再谈狄利克雷分布 41
3.2.2　Logistic正态分布 44
3.2.3　讨论 48
3.2.4　总结 49
3.3　非信息先验 49
3.3.1　均匀不正常先验 50
3.3.2　Jeffreys先验 51
3.3.3　讨论 51
3.4　共轭指数模型 52
3.5　模型中的多参数抽取 53
3.6　结构先验 54
3.7　本章小结 55
3.8　习题 56
第4章　贝叶斯估计 57
4.1　隐变量学习：两种观点 58
4.2　贝叶斯点估计 58
4.2.1　最大后验估计 59
4.2.2　基于最大后验解的后验近似 64
4.2.3　决策-理论点估计 65
4.2.4　总结 66
4.3　经验贝叶斯 66
4.4　后验的渐近行为 68
4.5　本章小结 69
4.6　习题 69
第5章　采样算法 70
5.1　MCMC算法：概述 71
5.2　MCMC推断的自然语言处理模型结构 71
5.3　吉布斯采样 73
5.3.1　坍塌吉布斯采样 76
5.3.2　运算符视图 79
5.3.3　并行化的吉布斯采样器 80
5.3.4　总结 81
5.4　Metropolis-Hastings算法 82
5.5　切片采样 84
5.5.1　辅助变量采样 85
5.5.2　切片采样和辅助变量采样在自然语言处理中的应用 85
5.6　模拟退火 86
5.7　MCMC算法的收敛性 86
5.8　马尔可夫链：基本理论 88
5.9　MCMC领域外的采样算法 89
5.10　蒙特卡罗积分 91
5.11　讨论 93
5.11.1　分布的可计算性与采样 93
5.11.2　嵌套的MCMC采样 93
5.11.3　MCMC方法的运行时间 93
5.11.4　粒子滤波 93
5.12　本章小结 95
5.13　习题 95
第6章　变分推断 97
6.1　边缘对数似然的变分界 97
6.2　平均场近似 99
6.3　平均场变分推断算法 100
6.3.1　狄利克雷-多项式变分推断 101
6.3.2　与期望最大化算法的联系 104
6.4　基于变分推断的经验贝叶斯 106
6.5　讨论 106
6.5.1　推断算法的初始化 107
6.5.2　收敛性诊断 107
6.5.3　变分推断在解码中的应用 107
6.5.4　变分推断最小化KL散度 108
6.5.5　在线的变分推断 109
6.6　本章小结 109
6.7　习题 109
第7章　非参数先验 111
7.1　狄利克雷过程：三种视角 112
7.1.1　折棍子过程 112
7.1.2　中餐馆过程 114
7.2　狄利克雷过程混合模型 115
7.2.1　基于狄利克雷过程混合模型的推断 116
7.2.2　狄利克雷过程混合是混合模型的极限 118
7.3　层次狄利克雷过程 119
7.4　Pitman?Yor过程 120
7.4.1　Pitman-Yor过程用于语言建模 121
7.4.2　Pitman-Yor过程的幂律行为 122
7.5　讨论 123
7.5.1　高斯过程 124
7.5.2　印度自助餐过程 124
7.5.3　嵌套的中餐馆过程 125
7.5.4　距离依赖的中餐馆过程 125
7.5.5　序列记忆器 126
7.6　本章小结 126
7.7　习题 127
第8章　贝叶斯语法模型 128
8.1　贝叶斯隐马尔可夫模型 129
8.2　概率上下文无关语法 131
8.2.1　作为多项式分布集的PCFG 133
8.2.2　PCFG的基本推断算法 133
8.2.3　作为隐马尔可夫模型的PCFG 136
8.3　贝叶斯概率上下文无关语法 137
8.3.1　PCFG的先验 137
8.3.2　贝叶斯PCFG的蒙特卡罗推断 138
8.3.3　贝叶斯PCFG的变分推断 139
8.4　适配器语法 140
8.4.1　Pitman-Yor适配器语法 141
8.4.2　PYAG的折棍子视角 142
8.4.3　基于PYAG的推断 143
8.5　层次狄利克雷过程PCFG 144
8.6　依存语法 147
8.7　同步语法 148
8.8　多语言学习 149
8.8.1　词性标注 149
8.8.2　语法归纳 151
8.9　延伸阅读 152
8.10　本章小结 153
8.11　习题 153
第9章　表征学习与神经网络 155
9.1　神经网络与表征学习：为什么是现在 155
9.2　词嵌入 158
9.2.1　词嵌入的skip-gram模型 158
9.2.2　贝叶斯skip-gram词嵌入 160
9.2.3　讨论 161
9.3　神经网络 162
9.3.1　频率论估计和反向传播算法 164
9.3.2　神经网络权值的先验 166
9.4　神经网络在自然语言处理中的现代应用 168
9.4.1　循环神经网络和递归神经网络 168
9.4.2　梯度消失与梯度爆炸问题 169
9.4.3　神经编码器-解码器模型 172
9.4.4　卷积神经网络 175
9.5　调整神经网络 177
9.5.1　正则化 177
9.5.2　超参数调整 178
9.6　神经网络生成建模 180
9.6.1　变分自编码器 180
9.6.2　生成对抗网络 185
9.7　本章小结 186
9.8　习题 187
结束语 189
附录A　基本概念 191
附录B　概率分布清单 197
参考文献 203
・ ・ ・ ・ ・ ・ (收起)